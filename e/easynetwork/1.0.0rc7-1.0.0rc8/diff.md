# Comparing `tmp/easynetwork-1.0.0rc7.tar.gz` & `tmp/easynetwork-1.0.0rc8.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, last modified: Sat Dec  9 18:44:15 2023, max compression
+gzip compressed data, last modified: Sat Apr  6 16:56:24 2024, max compression
```

## Comparing `easynetwork-1.0.0rc7.tar` & `easynetwork-1.0.0rc8.tar`

### file list

```diff
@@ -1,375 +1,414 @@
--rw-r--r--   0        0        0      157 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/.bandit.yml
--rw-r--r--   0        0        0      976 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/.flake8
--rw-r--r--   0        0        0     2799 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/.pre-commit-config.yaml
--rw-r--r--   0        0        0   100719 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/pdm.lock
--rw-r--r--   0        0        0     5592 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tox.ini
--rw-r--r--   0        0        0     1482 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/.vscode/settings.example.json
--rw-r--r--   0        0        0       40 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/.gitignore
--rw-r--r--   0        0        0      638 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/Makefile
--rw-r--r--   0        0        0      769 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/make.bat
--rw-r--r--   0        0        0     5126 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/conf.py
--rw-r--r--   0        0        0     4063 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/glossary.rst
--rw-r--r--   0        0        0      643 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/index.rst
--rw-r--r--   0        0        0     1059 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_extensions/sphinx_easynetwork.py
--rw-r--r--   0        0        0      300 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/link-labels.rst
--rw-r--r--   0        0        0       99 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/see-also-optional-dependencies.rst
--rw-r--r--   0        0        0      445 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/sync-async-variants.rst
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/protocols/__init__.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/protocols/basics/__init__.py
--rw-r--r--   0        0        0      331 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/protocols/basics/datagram_protocol_instance.py
--rw-r--r--   0        0        0      457 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/protocols/basics/datagram_protocol_subclass.py
--rw-r--r--   0        0        0      325 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/protocols/basics/stream_protocol_instance.py
--rw-r--r--   0        0        0      449 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/protocols/basics/stream_protocol_subclass.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/protocols/composite_converter/__init__.py
--rw-r--r--   0        0        0     1201 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/protocols/composite_converter/packet_converter_subclass.py
--rw-r--r--   0        0        0     1460 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/protocols/composite_converter/stapled_packet_converter.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/protocols/simple_converter/__init__.py
--rw-r--r--   0        0        0     1755 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/protocols/simple_converter/example1.py
--rw-r--r--   0        0        0     1517 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/protocols/simple_converter/example2.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/protocols/usage/__init__.py
--rw-r--r--   0        0        0      682 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/protocols/usage/datagram_protocol.py
--rw-r--r--   0        0        0      672 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/protocols/usage/stream_protocol.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/serializers/buffered_incremental_serializer/__init__.py
--rw-r--r--   0        0        0     3217 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/serializers/buffered_incremental_serializer/example1.py
--rw-r--r--   0        0        0     1809 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/serializers/buffered_incremental_serializer/example2.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/serializers/incremental_serializer/__init__.py
--rw-r--r--   0        0        0     1123 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/serializers/incremental_serializer/example1.py
--rw-r--r--   0        0        0     1981 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/serializers/incremental_serializer/example2.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/serializers/one_shot_serializer/__init__.py
--rw-r--r--   0        0        0      542 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/serializers/one_shot_serializer/example1.py
--rw-r--r--   0        0        0      740 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/serializers/one_shot_serializer/example2.py
--rw-r--r--   0        0        0     1060 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/serializers/one_shot_serializer/example3.py
--rw-r--r--   0        0        0      418 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/serializers/one_shot_serializer/example4_datagram.py
--rw-r--r--   0        0        0      581 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/serializers/one_shot_serializer/example4_stream.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/tcp_clients/basics/__init__.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/tcp_clients/basics/api_async/__init__.py
--rw-r--r--   0        0        0      522 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/tcp_clients/basics/api_async/connection_example1.py
--rw-r--r--   0        0        0      738 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/tcp_clients/basics/api_async/connection_example2.py
--rw-r--r--   0        0        0      689 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/tcp_clients/basics/api_async/socket_example1.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/tcp_clients/basics/api_sync/__init__.py
--rw-r--r--   0        0        0      470 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/tcp_clients/basics/api_sync/connection_example1.py
--rw-r--r--   0        0        0      620 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/tcp_clients/basics/api_sync/connection_example2.py
--rw-r--r--   0        0        0      626 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/tcp_clients/basics/api_sync/socket_example1.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/tcp_clients/concurrency/__init__.py
--rw-r--r--   0        0        0     1983 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/tcp_clients/concurrency/api_async.py
--rw-r--r--   0        0        0     2164 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/tcp_clients/concurrency/api_sync.py
--rw-r--r--   0        0        0      841 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/tcp_clients/concurrency/ssl_shared_lock.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/tcp_clients/usage/__init__.py
--rw-r--r--   0        0        0     2711 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/tcp_clients/usage/api_async.py
--rw-r--r--   0        0        0     2440 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/tcp_clients/usage/api_sync.py
--rw-r--r--   0        0        0     1231 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/tcp_servers/async_server.py
--rw-r--r--   0        0        0     7564 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/tcp_servers/request_handler_explanation.py
--rw-r--r--   0        0        0     1070 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/tcp_servers/simple_request_handler.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/udp_clients/basics/__init__.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/udp_clients/basics/api_async/__init__.py
--rw-r--r--   0        0        0      526 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/udp_clients/basics/api_async/connection_example1.py
--rw-r--r--   0        0        0      692 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/udp_clients/basics/api_async/socket_example1.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/udp_clients/basics/api_sync/__init__.py
--rw-r--r--   0        0        0      474 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/udp_clients/basics/api_sync/connection_example1.py
--rw-r--r--   0        0        0      629 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/udp_clients/basics/api_sync/socket_example1.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/udp_clients/usage/__init__.py
--rw-r--r--   0        0        0     1850 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/udp_clients/usage/api_async.py
--rw-r--r--   0        0        0     1648 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/udp_clients/usage/api_sync.py
--rw-r--r--   0        0        0     1243 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/udp_servers/async_server.py
--rw-r--r--   0        0        0     4691 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/udp_servers/request_handler_explanation.py
--rw-r--r--   0        0        0     1113 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/howto/udp_servers/simple_request_handler.py
--rw-r--r--   0        0        0      680 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/echo_client_server_tcp/async_client.py
--rw-r--r--   0        0        0      575 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/echo_client_server_tcp/async_server.py
--rw-r--r--   0        0        0      617 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/echo_client_server_tcp/client.py
--rw-r--r--   0        0        0     1295 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/echo_client_server_tcp/echo_request_handler.py
--rw-r--r--   0        0        0      518 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/echo_client_server_tcp/json_protocol.py
--rw-r--r--   0        0        0      532 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/echo_client_server_tcp/server.py
--rw-r--r--   0        0        0      696 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/echo_client_server_udp/async_client.py
--rw-r--r--   0        0        0      591 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/echo_client_server_udp/async_server.py
--rw-r--r--   0        0        0      633 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/echo_client_server_udp/client.py
--rw-r--r--   0        0        0      980 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/echo_client_server_udp/echo_request_handler.py
--rw-r--r--   0        0        0      530 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/echo_client_server_udp/json_protocol.py
--rw-r--r--   0        0        0      548 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/echo_client_server_udp/server.py
--rw-r--r--   0        0        0     1014 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/ftp_server/async_server.py
--rw-r--r--   0        0        0     1155 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/ftp_server/ftp_command.py
--rw-r--r--   0        0        0     3022 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/ftp_server/ftp_converters.py
--rw-r--r--   0        0        0      993 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/ftp_server/ftp_reply.py
--rw-r--r--   0        0        0      362 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/ftp_server/ftp_request.py
--rw-r--r--   0        0        0      823 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/ftp_server/ftp_server_protocol.py
--rw-r--r--   0        0        0     2117 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/ftp_server/ftp_server_request_handler.py
--rw-r--r--   0        0        0      958 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/ftp_server/server.py
--rw-r--r--   0        0        0       37 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/_static/css/details.css
--rw-r--r--   0        0        0       85 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/api/converter.rst
--rw-r--r--   0        0        0       86 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/api/exceptions.rst
--rw-r--r--   0        0        0      192 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/api/index.rst
--rw-r--r--   0        0        0       81 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/api/protocol.rst
--rw-r--r--   0        0        0      184 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/api/typevars.rst
--rw-r--r--   0        0        0      455 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/api/async/client.rst
--rw-r--r--   0        0        0      196 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/api/async/index.rst
--rw-r--r--   0        0        0      902 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/api/async/server.rst
--rw-r--r--   0        0        0      202 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/api/lowlevel/index.rst
--rw-r--r--   0        0        0      929 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/api/lowlevel/socket.rst
--rw-r--r--   0        0        0      137 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/api/lowlevel/typed_attr.rst
--rw-r--r--   0        0        0     3550 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/api/lowlevel/async/backend.rst
--rw-r--r--   0        0        0      322 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/api/lowlevel/async/endpoints.rst
--rw-r--r--   0        0        0      264 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/api/lowlevel/async/index.rst
--rw-r--r--   0        0        0      304 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/api/lowlevel/async/servers.rst
--rw-r--r--   0        0        0      337 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/api/lowlevel/async/transports.rst
--rw-r--r--   0        0        0      320 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/api/lowlevel/sync/endpoints.rst
--rw-r--r--   0        0        0      190 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/api/lowlevel/sync/index.rst
--rw-r--r--   0        0        0      641 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/api/lowlevel/sync/transports.rst
--rw-r--r--   0        0        0      450 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/api/serializers/abc.rst
--rw-r--r--   0        0        0      416 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/api/serializers/cbor.rst
--rw-r--r--   0        0        0      487 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/api/serializers/index.rst
--rw-r--r--   0        0        0      259 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/api/serializers/json.rst
--rw-r--r--   0        0        0      148 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/api/serializers/line.rst
--rw-r--r--   0        0        0      462 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/api/serializers/msgpack.rst
--rw-r--r--   0        0        0      470 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/api/serializers/pickle.rst
--rw-r--r--   0        0        0      438 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/api/serializers/struct.rst
--rw-r--r--   0        0        0      174 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/api/serializers/tools.rst
--rw-r--r--   0        0        0      271 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/api/serializers/wrappers/base64.rst
--rw-r--r--   0        0        0      491 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/api/serializers/wrappers/compressor.rst
--rw-r--r--   0        0        0      315 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/api/serializers/wrappers/encryptor.rst
--rw-r--r--   0        0        0      436 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/api/sync/client.rst
--rw-r--r--   0        0        0      144 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/api/sync/index.rst
--rw-r--r--   0        0        0      502 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/api/sync/server.rst
--rw-r--r--   0        0        0      158 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/howto/index.rst
--rw-r--r--   0        0        0     6149 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/howto/protocols.rst
--rw-r--r--   0        0        0    10160 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/howto/serializers.rst
--rw-r--r--   0        0        0    12677 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/howto/tcp_clients.rst
--rw-r--r--   0        0        0     6465 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/howto/tcp_servers.rst
--rw-r--r--   0        0        0     8628 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/howto/udp_clients.rst
--rw-r--r--   0        0        0     4219 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/howto/udp_servers.rst
--rw-r--r--   0        0        0     8094 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/howto/advanced/buffered_serializers.rst
--rw-r--r--   0        0        0      100 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/howto/advanced/index.rst
--rw-r--r--   0        0        0      102 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/quickstart/index.rst
--rw-r--r--   0        0        0     1040 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/quickstart/install.rst
--rw-r--r--   0        0        0      585 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/quickstart/overview.rst
--rw-r--r--   0        0        0     5098 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/tutorials/echo_client_server_tcp.rst
--rw-r--r--   0        0        0     3606 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/tutorials/echo_client_server_udp.rst
--rw-r--r--   0        0        0     9864 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/tutorials/ftp_server.rst
--rw-r--r--   0        0        0      127 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/docs/source/tutorials/index.rst
--rw-r--r--   0        0        0     1167 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/__init__.py
--rw-r--r--   0        0        0     1525 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/_typevars.py
--rw-r--r--   0        0        0     4290 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/converter.py
--rw-r--r--   0        0        0     5757 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/exceptions.py
--rw-r--r--   0        0        0    12854 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/protocol.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/py.typed
--rw-r--r--   0        0        0      926 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/version.py
--rw-r--r--   0        0        0      722 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/api_async/__init__.py
--rw-r--r--   0        0        0      853 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/api_async/client/__init__.py
--rw-r--r--   0        0        0     7562 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/api_async/client/abc.py
--rw-r--r--   0        0        0    21392 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/api_async/client/tcp.py
--rw-r--r--   0        0        0    13717 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/api_async/client/udp.py
--rw-r--r--   0        0        0     1079 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/api_async/server/__init__.py
--rw-r--r--   0        0        0     3225 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/api_async/server/abc.py
--rw-r--r--   0        0        0    10890 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/api_async/server/handler.py
--rw-r--r--   0        0        0    23808 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/api_async/server/tcp.py
--rw-r--r--   0        0        0    17706 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/api_async/server/udp.py
--rw-r--r--   0        0        0      721 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/api_sync/__init__.py
--rw-r--r--   0        0        0      825 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/api_sync/client/__init__.py
--rw-r--r--   0        0        0     6650 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/api_sync/client/abc.py
--rw-r--r--   0        0        0    18554 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/api_sync/client/tcp.py
--rw-r--r--   0        0        0    13250 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/api_sync/client/udp.py
--rw-r--r--   0        0        0      894 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/api_sync/server/__init__.py
--rw-r--r--   0        0        0     7453 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/api_sync/server/_base.py
--rw-r--r--   0        0        0     3099 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/api_sync/server/abc.py
--rw-r--r--   0        0        0     4153 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/api_sync/server/tcp.py
--rw-r--r--   0        0        0     1811 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/api_sync/server/thread.py
--rw-r--r--   0        0        0     3014 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/api_sync/server/udp.py
--rw-r--r--   0        0        0      721 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/__init__.py
--rw-r--r--   0        0        0     1829 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/_asyncgen.py
--rw-r--r--   0        0        0      644 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/_final.py
--rw-r--r--   0        0        0     1802 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/_lock.py
--rw-r--r--   0        0        0    12289 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/_stream.py
--rw-r--r--   0        0        0    10582 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/_utils.py
--rw-r--r--   0        0        0     2860 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/constants.py
--rw-r--r--   0        0        0    18261 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/socket.py
--rw-r--r--   0        0        0     3371 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/typed_attr.py
--rw-r--r--   0        0        0      720 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_async/__init__.py
--rw-r--r--   0        0        0      721 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_async/backend/__init__.py
--rw-r--r--   0        0        0     1594 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_async/backend/_sniffio_helpers.py
--rw-r--r--   0        0        0    42761 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_async/backend/abc.py
--rw-r--r--   0        0        0     4331 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_async/backend/factory.py
--rw-r--r--   0        0        0     7982 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_async/backend/futures.py
--rw-r--r--   0        0        0      726 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_async/endpoints/__init__.py
--rw-r--r--   0        0        0     5578 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_async/endpoints/datagram.py
--rw-r--r--   0        0        0     9030 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_async/endpoints/stream.py
--rw-r--r--   0        0        0      724 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_async/servers/__init__.py
--rw-r--r--   0        0        0    14574 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_async/servers/datagram.py
--rw-r--r--   0        0        0    10006 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_async/servers/stream.py
--rw-r--r--   0        0        0      727 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_async/transports/__init__.py
--rw-r--r--   0        0        0     7294 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_async/transports/abc.py
--rw-r--r--   0        0        0     1151 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_async/transports/utils.py
--rw-r--r--   0        0        0      719 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_sync/__init__.py
--rw-r--r--   0        0        0      713 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_sync/endpoints/__init__.py
--rw-r--r--   0        0        0     5961 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_sync/endpoints/datagram.py
--rw-r--r--   0        0        0    10545 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_sync/endpoints/stream.py
--rw-r--r--   0        0        0      714 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_sync/transports/__init__.py
--rw-r--r--   0        0        0     8201 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_sync/transports/abc.py
--rw-r--r--   0        0        0    11137 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_sync/transports/base_selector.py
--rw-r--r--   0        0        0    11966 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_sync/transports/socket.py
--rw-r--r--   0        0        0      781 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/std_asyncio/__init__.py
--rw-r--r--   0        0        0     9907 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/std_asyncio/_asyncio_utils.py
--rw-r--r--   0        0        0    15980 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/std_asyncio/backend.py
--rw-r--r--   0        0        0     7442 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/std_asyncio/socket.py
--rw-r--r--   0        0        0    14036 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/std_asyncio/tasks.py
--rw-r--r--   0        0        0     6735 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/std_asyncio/threads.py
--rw-r--r--   0        0        0      724 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/std_asyncio/datagram/__init__.py
--rw-r--r--   0        0        0     9642 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/std_asyncio/datagram/endpoint.py
--rw-r--r--   0        0        0     3726 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/std_asyncio/datagram/listener.py
--rw-r--r--   0        0        0     3589 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/std_asyncio/datagram/socket.py
--rw-r--r--   0        0        0      724 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/std_asyncio/stream/__init__.py
--rw-r--r--   0        0        0     7537 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/std_asyncio/stream/listener.py
--rw-r--r--   0        0        0     6071 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/lowlevel/std_asyncio/stream/socket.py
--rw-r--r--   0        0        0     1277 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/serializers/__init__.py
--rw-r--r--   0        0        0     7950 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/serializers/abc.py
--rw-r--r--   0        0        0    19179 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/serializers/base_stream.py
--rw-r--r--   0        0        0     4710 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/serializers/cbor.py
--rw-r--r--   0        0        0    16194 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/serializers/json.py
--rw-r--r--   0        0        0     5386 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/serializers/line.py
--rw-r--r--   0        0        0     6521 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/serializers/msgpack.py
--rw-r--r--   0        0        0     6362 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/serializers/pickle.py
--rw-r--r--   0        0        0    13026 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/serializers/struct.py
--rw-r--r--   0        0        0     6850 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/serializers/tools.py
--rw-r--r--   0        0        0      902 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/serializers/wrapper/__init__.py
--rw-r--r--   0        0        0     6144 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/serializers/wrapper/base64.py
--rw-r--r--   0        0        0    13563 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/serializers/wrapper/compressor.py
--rw-r--r--   0        0        0     4679 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/src/easynetwork/serializers/wrapper/encryptor.py
--rw-r--r--   0        0        0      106 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/__init__.py
--rw-r--r--   0        0        0      565 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/conftest.py
--rw-r--r--   0        0        0      896 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/import_utils.py
--rw-r--r--   0        0        0     5089 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/tools.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/__init__.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_async/__init__.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_async/test_backend/__init__.py
--rw-r--r--   0        0        0    49035 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_async/test_backend/test_asyncio_backend.py
--rw-r--r--   0        0        0     4330 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_async/test_backend/test_futures.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_communication/__init__.py
--rw-r--r--   0        0        0     5544 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_communication/conftest.py
--rw-r--r--   0        0        0     3834 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_communication/serializer.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_communication/test_async/__init__.py
--rw-r--r--   0        0        0      291 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_communication/test_async/_utils.py
--rw-r--r--   0        0        0      895 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_communication/test_async/conftest.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_communication/test_async/test_client/__init__.py
--rw-r--r--   0        0        0    23255 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_communication/test_async/test_client/test_tcp.py
--rw-r--r--   0        0        0    16079 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_communication/test_async/test_client/test_udp.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_communication/test_async/test_server/__init__.py
--rw-r--r--   0        0        0     4555 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_communication/test_async/test_server/base.py
--rw-r--r--   0        0        0    42703 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_communication/test_async/test_server/test_tcp.py
--rw-r--r--   0        0        0    23209 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_communication/test_async/test_server/test_udp.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_communication/test_sync/__init__.py
--rw-r--r--   0        0        0     1293 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_communication/test_sync/conftest.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_communication/test_sync/test_client/__init__.py
--rw-r--r--   0        0        0    15580 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_communication/test_sync/test_client/test_tcp.py
--rw-r--r--   0        0        0     6065 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_communication/test_sync/test_client/test_udp.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_communication/test_sync/test_server/__init__.py
--rw-r--r--   0        0        0     6978 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_communication/test_sync/test_server/test_standalone.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_concurrency/__init__.py
--rw-r--r--   0        0        0     2647 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_concurrency/conftest.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_concurrency/test_sync/__init__.py
--rw-r--r--   0        0        0     1117 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_concurrency/test_sync/conftest.py
--rw-r--r--   0        0        0     2182 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_concurrency/test_sync/test_threaded_client.py
--rw-r--r--   0        0        0      105 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_serializers/__init__.py
--rw-r--r--   0        0        0    14855 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_serializers/base.py
--rw-r--r--   0        0        0     7762 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_serializers/test_base64.py
--rw-r--r--   0        0        0     2220 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_serializers/test_cbor.py
--rw-r--r--   0        0        0     4331 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_serializers/test_compressors.py
--rw-r--r--   0        0        0     4563 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_serializers/test_encryptor.py
--rw-r--r--   0        0        0     4573 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_serializers/test_json.py
--rw-r--r--   0        0        0     4284 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_serializers/test_line.py
--rw-r--r--   0        0        0     1882 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_serializers/test_msgpack.py
--rw-r--r--   0        0        0     3146 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_serializers/test_namedtuple.py
--rw-r--r--   0        0        0     3287 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_serializers/test_pickle.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_serializers/samples/__init__.py
--rw-r--r--   0        0        0    10677 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_serializers/samples/json.py
--rw-r--r--   0        0        0     2349 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/functional_test/test_serializers/samples/pickle.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/other_test/__init__.py
--rw-r--r--   0        0        0     4154 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/other_test/test_import.py
--rw-r--r--   0        0        0      553 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/other_test/test_project_metadata.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/pytest_plugins/__init__.py
--rw-r--r--   0        0        0     4057 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/pytest_plugins/asyncio_event_loop.py
--rw-r--r--   0        0        0      958 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/pytest_plugins/auto_markers.py
--rw-r--r--   0        0        0      973 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/pytest_plugins/extra_features.py
--rw-r--r--   0        0        0     1078 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/pytest_plugins/ssl_module.py
--rw-r--r--   0        0        0     2569 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/scripts/async_client_test.py
--rw-r--r--   0        0        0     3168 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/scripts/async_server_test.py
--rw-r--r--   0        0        0     2636 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/scripts/threaded_client_test.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/__init__.py
--rw-r--r--   0        0        0     3845 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/_utils.py
--rw-r--r--   0        0        0     3624 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/base.py
--rw-r--r--   0        0        0     7106 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/conftest.py
--rw-r--r--   0        0        0     2788 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_converter.py
--rw-r--r--   0        0        0    29872 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_protocol.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_async/__init__.py
--rw-r--r--   0        0        0     3110 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_async/conftest.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_async/test_api/__init__.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_async/test_api/test_client/__init__.py
--rw-r--r--   0        0        0      497 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_async/test_api/test_client/base.py
--rw-r--r--   0        0        0     4922 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_async/test_api/test_client/test_abc.py
--rw-r--r--   0        0        0    55612 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_async/test_api/test_client/test_tcp.py
--rw-r--r--   0        0        0    25512 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_async/test_api/test_client/test_udp.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_async/test_api/test_server/__init__.py
--rw-r--r--   0        0        0      970 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_async/test_api/test_server/conftest.py
--rw-r--r--   0        0        0     3069 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_async/test_api/test_server/test_handler.py
--rw-r--r--   0        0        0     1123 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_async/test_api/test_server/test_tcp.py
--rw-r--r--   0        0        0     1141 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_async/test_api/test_server/test_udp.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_async/test_asyncio_backend/__init__.py
--rw-r--r--   0        0        0     1796 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_async/test_asyncio_backend/conftest.py
--rw-r--r--   0        0        0    67542 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_async/test_asyncio_backend/test_backend.py
--rw-r--r--   0        0        0    38248 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_async/test_asyncio_backend/test_datagram.py
--rw-r--r--   0        0        0    24291 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_async/test_asyncio_backend/test_socket.py
--rw-r--r--   0        0        0    39796 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_async/test_asyncio_backend/test_stream.py
--rw-r--r--   0        0        0     5894 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_async/test_asyncio_backend/test_tasks.py
--rw-r--r--   0        0        0    27652 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_async/test_asyncio_backend/test_utils.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_async/test_lowlevel_api/__init__.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_async/test_lowlevel_api/test_backend/__init__.py
--rw-r--r--   0        0        0     3197 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_async/test_lowlevel_api/test_backend/_fake_backends.py
--rw-r--r--   0        0        0    21068 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_async/test_lowlevel_api/test_backend/test_backend.py
--rw-r--r--   0        0        0     9433 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_async/test_lowlevel_api/test_backend/test_futures.py
--rw-r--r--   0        0        0     1141 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_async/test_lowlevel_api/test_backend/test_sniffio_helpers.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_async/test_lowlevel_api/test_endpoints/__init__.py
--rw-r--r--   0        0        0     9311 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_async/test_lowlevel_api/test_endpoints/test_datagram.py
--rw-r--r--   0        0        0    27270 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_async/test_lowlevel_api/test_endpoints/test_stream.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_async/test_lowlevel_api/test_servers/__init__.py
--rw-r--r--   0        0        0    11872 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_async/test_lowlevel_api/test_servers/test_datagram.py
--rw-r--r--   0        0        0     8629 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_async/test_lowlevel_api/test_servers/test_stream.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_async/test_lowlevel_api/test_transports/__init__.py
--rw-r--r--   0        0        0     1506 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_async/test_lowlevel_api/test_transports/test_abc.py
--rw-r--r--   0        0        0      105 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_serializers/__init__.py
--rw-r--r--   0        0        0      615 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_serializers/base.py
--rw-r--r--   0        0        0      211 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_serializers/conftest.py
--rw-r--r--   0        0        0    44201 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_serializers/test_abc.py
--rw-r--r--   0        0        0     6060 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_serializers/test_base64.py
--rw-r--r--   0        0        0     6053 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_serializers/test_cbor.py
--rw-r--r--   0        0        0    25214 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_serializers/test_compressor.py
--rw-r--r--   0        0        0     5059 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_serializers/test_encryptor.py
--rw-r--r--   0        0        0    24751 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_serializers/test_json.py
--rw-r--r--   0        0        0     6564 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_serializers/test_line.py
--rw-r--r--   0        0        0     7776 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_serializers/test_msgpack.py
--rw-r--r--   0        0        0    10465 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_serializers/test_pickle.py
--rw-r--r--   0        0        0    18242 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_serializers/test_struct.py
--rw-r--r--   0        0        0     8464 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_serializers/test_tools.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_sync/__init__.py
--rw-r--r--   0        0        0      427 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_sync/conftest.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_sync/test_client/__init__.py
--rw-r--r--   0        0        0     1588 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_sync/test_client/base.py
--rw-r--r--   0        0        0     1329 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_sync/test_client/conftest.py
--rw-r--r--   0        0        0     4147 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_sync/test_client/test_abc.py
--rw-r--r--   0        0        0    77483 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_sync/test_client/test_tcp.py
--rw-r--r--   0        0        0    45582 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_sync/test_client/test_udp.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_sync/test_lowlevel_api/__init__.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_sync/test_lowlevel_api/test_endpoints/__init__.py
--rw-r--r--   0        0        0    10498 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_sync/test_lowlevel_api/test_endpoints/test_datagram.py
--rw-r--r--   0        0        0    33051 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_sync/test_lowlevel_api/test_endpoints/test_stream.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_sync/test_lowlevel_api/test_transports/__init__.py
--rw-r--r--   0        0        0     4483 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_sync/test_lowlevel_api/test_transports/test_abc.py
--rw-r--r--   0        0        0    17364 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_sync/test_lowlevel_api/test_transports/test_selector.py
--rw-r--r--   0        0        0    44971 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_sync/test_lowlevel_api/test_transports/test_socket.py
--rw-r--r--   0        0        0        0 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_tools/__init__.py
--rw-r--r--   0        0        0     2965 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_tools/test_asyncgen.py
--rw-r--r--   0        0        0     2452 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_tools/test_lock.py
--rw-r--r--   0        0        0    10106 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_tools/test_socket.py
--rw-r--r--   0        0        0    45284 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_tools/test_stream.py
--rw-r--r--   0        0        0     4303 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_tools/test_typedattr.py
--rw-r--r--   0        0        0    21186 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/tests/unit_test/test_tools/test_utils.py
--rw-r--r--   0        0        0     1980 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/.gitignore
--rw-r--r--   0        0        0    11357 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/LICENSE
--rw-r--r--   0        0        0     6623 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/README.md
--rw-r--r--   0        0        0     6771 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/pyproject.toml
--rw-r--r--   0        0        0     8326 2023-12-09 18:44:15.000000 easynetwork-1.0.0rc7/PKG-INFO
+-rw-r--r--   0        0        0      316 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/.bandit.yml
+-rw-r--r--   0        0        0      555 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/.bumpversion.cfg
+-rw-r--r--   0        0        0     1161 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/.flake8
+-rw-r--r--   0        0        0     3439 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/.pre-commit-config.yaml
+-rw-r--r--   0        0        0   108378 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/pdm.lock
+-rw-r--r--   0        0        0      330 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/pytest-benchmark.ini
+-rw-r--r--   0        0        0      268 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/requirements-tox.txt
+-rw-r--r--   0        0        0     7862 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tox.ini
+-rw-r--r--   0        0        0       36 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/benchmark_server/.dockerignore
+-rw-r--r--   0        0        0        5 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/benchmark_server/.gitignore
+-rw-r--r--   0        0        0      334 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/benchmark_server/Dockerfile
+-rw-r--r--   0        0        0      285 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/benchmark_server/README.md
+-rwxr-xr-x   0        0        0     2525 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/benchmark_server/build_benchmark_image
+-rwxr-xr-x   0        0        0     3868 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/benchmark_server/datagram_echoclient.py
+-rwxr-xr-x   0        0        0    22214 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/benchmark_server/run_benchmark
+-rwxr-xr-x   0        0        0     4774 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/benchmark_server/stream_echoclient.py
+-rwxr-xr-x   0        0        0     6419 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/benchmark_server/servers/asyncio_tcp_echoserver.py
+-rwxr-xr-x   0        0        0     4476 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/benchmark_server/servers/asyncio_udp_echoserver.py
+-rwxr-xr-x   0        0        0     5617 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/benchmark_server/servers/easynetwork_tcp_echoserver.py
+-rwxr-xr-x   0        0        0     4201 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/benchmark_server/servers/easynetwork_udp_echoserver.py
+-rw-r--r--   0        0        0      125 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/benchmark_server/servers/requirements.txt
+-rw-r--r--   0        0        0      178 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/benchmark_server/servers/certs/README.md
+-rw-r--r--   0        0        0     2122 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/benchmark_server/servers/certs/ssl_cert.pem
+-rw-r--r--   0        0        0     2484 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/benchmark_server/servers/certs/ssl_key.pem
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/benchmark_server/tool/__init__.py
+-rw-r--r--   0        0        0     4974 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/benchmark_server/tool/client.py
+-rw-r--r--   0        0        0       40 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/.gitignore
+-rw-r--r--   0        0        0      638 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/Makefile
+-rw-r--r--   0        0        0      769 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/make.bat
+-rw-r--r--   0        0        0     5061 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/conf.py
+-rw-r--r--   0        0        0     4354 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/glossary.rst
+-rw-r--r--   0        0        0      643 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/index.rst
+-rw-r--r--   0        0        0     1047 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_extensions/sphinx_easynetwork.py
+-rw-r--r--   0        0        0      300 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/link-labels.rst
+-rw-r--r--   0        0        0       99 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/see-also-optional-dependencies.rst
+-rw-r--r--   0        0        0      453 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/sync-async-variants.rst
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/protocols/__init__.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/protocols/basics/__init__.py
+-rw-r--r--   0        0        0      331 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/protocols/basics/datagram_protocol_instance.py
+-rw-r--r--   0        0        0      457 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/protocols/basics/datagram_protocol_subclass.py
+-rw-r--r--   0        0        0      325 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/protocols/basics/stream_protocol_instance.py
+-rw-r--r--   0        0        0      449 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/protocols/basics/stream_protocol_subclass.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/protocols/composite_converter/__init__.py
+-rw-r--r--   0        0        0     1161 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/protocols/composite_converter/packet_converter_subclass.py
+-rw-r--r--   0        0        0     1420 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/protocols/composite_converter/stapled_packet_converter.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/protocols/simple_converter/__init__.py
+-rw-r--r--   0        0        0     1755 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/protocols/simple_converter/example1.py
+-rw-r--r--   0        0        0     1509 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/protocols/simple_converter/example2.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/protocols/usage/__init__.py
+-rw-r--r--   0        0        0      674 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/protocols/usage/datagram_protocol.py
+-rw-r--r--   0        0        0      664 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/protocols/usage/stream_protocol.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/serializers/buffered_incremental_serializer/__init__.py
+-rw-r--r--   0        0        0     3217 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/serializers/buffered_incremental_serializer/example1.py
+-rw-r--r--   0        0        0     1801 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/serializers/buffered_incremental_serializer/example2.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/serializers/incremental_serializer/__init__.py
+-rw-r--r--   0        0        0     1123 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/serializers/incremental_serializer/example1.py
+-rw-r--r--   0        0        0     1981 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/serializers/incremental_serializer/example2.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/serializers/one_shot_serializer/__init__.py
+-rw-r--r--   0        0        0      542 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/serializers/one_shot_serializer/example1.py
+-rw-r--r--   0        0        0      740 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/serializers/one_shot_serializer/example2.py
+-rw-r--r--   0        0        0     1060 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/serializers/one_shot_serializer/example3.py
+-rw-r--r--   0        0        0      410 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/serializers/one_shot_serializer/example4_datagram.py
+-rw-r--r--   0        0        0      573 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/serializers/one_shot_serializer/example4_stream.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/tcp_clients/basics/__init__.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/tcp_clients/basics/api_async/__init__.py
+-rw-r--r--   0        0        0      513 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/tcp_clients/basics/api_async/connection_example1.py
+-rw-r--r--   0        0        0      729 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/tcp_clients/basics/api_async/connection_example2.py
+-rw-r--r--   0        0        0      680 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/tcp_clients/basics/api_async/socket_example1.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/tcp_clients/basics/api_sync/__init__.py
+-rw-r--r--   0        0        0      462 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/tcp_clients/basics/api_sync/connection_example1.py
+-rw-r--r--   0        0        0      612 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/tcp_clients/basics/api_sync/connection_example2.py
+-rw-r--r--   0        0        0      618 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/tcp_clients/basics/api_sync/socket_example1.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/tcp_clients/concurrency/__init__.py
+-rw-r--r--   0        0        0      793 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/tcp_clients/concurrency/ssl_shared_lock.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/tcp_clients/usage/__init__.py
+-rw-r--r--   0        0        0     2702 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/tcp_clients/usage/api_async.py
+-rw-r--r--   0        0        0     2432 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/tcp_clients/usage/api_sync.py
+-rw-r--r--   0        0        0     1246 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/tcp_servers/async_server.py
+-rw-r--r--   0        0        0     1963 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/tcp_servers/background_server.py
+-rw-r--r--   0        0        0     2175 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/tcp_servers/background_server_ssl.py
+-rw-r--r--   0        0        0     9504 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/tcp_servers/request_handler_explanation.py
+-rw-r--r--   0        0        0     1070 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/tcp_servers/simple_request_handler.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/tcp_servers/standalone/__init__.py
+-rw-r--r--   0        0        0     2015 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/tcp_servers/standalone/background_server.py
+-rw-r--r--   0        0        0     1689 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/tcp_servers/standalone/server.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/udp_clients/basics/__init__.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/udp_clients/basics/api_async/__init__.py
+-rw-r--r--   0        0        0      517 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/udp_clients/basics/api_async/connection_example1.py
+-rw-r--r--   0        0        0      683 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/udp_clients/basics/api_async/socket_example1.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/udp_clients/basics/api_sync/__init__.py
+-rw-r--r--   0        0        0      466 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/udp_clients/basics/api_sync/connection_example1.py
+-rw-r--r--   0        0        0      621 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/udp_clients/basics/api_sync/socket_example1.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/udp_clients/usage/__init__.py
+-rw-r--r--   0        0        0     1841 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/udp_clients/usage/api_async.py
+-rw-r--r--   0        0        0     1640 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/udp_clients/usage/api_sync.py
+-rw-r--r--   0        0        0     1258 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/udp_servers/async_server.py
+-rw-r--r--   0        0        0     1975 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/udp_servers/background_server.py
+-rw-r--r--   0        0        0     6770 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/udp_servers/request_handler_explanation.py
+-rw-r--r--   0        0        0     1113 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/udp_servers/simple_request_handler.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/udp_servers/standalone/__init__.py
+-rw-r--r--   0        0        0     2027 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/udp_servers/standalone/background_server.py
+-rw-r--r--   0        0        0     1701 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/howto/udp_servers/standalone/server.py
+-rw-r--r--   0        0        0      671 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/echo_client_server_tcp/async_client.py
+-rw-r--r--   0        0        0      566 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/echo_client_server_tcp/async_server.py
+-rw-r--r--   0        0        0      609 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/echo_client_server_tcp/client.py
+-rw-r--r--   0        0        0     1295 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/echo_client_server_tcp/echo_request_handler.py
+-rw-r--r--   0        0        0      518 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/echo_client_server_tcp/json_protocol.py
+-rw-r--r--   0        0        0      524 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/echo_client_server_tcp/server.py
+-rw-r--r--   0        0        0      687 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/echo_client_server_udp/async_client.py
+-rw-r--r--   0        0        0      582 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/echo_client_server_udp/async_server.py
+-rw-r--r--   0        0        0      625 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/echo_client_server_udp/client.py
+-rw-r--r--   0        0        0      980 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/echo_client_server_udp/echo_request_handler.py
+-rw-r--r--   0        0        0      530 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/echo_client_server_udp/json_protocol.py
+-rw-r--r--   0        0        0      540 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/echo_client_server_udp/server.py
+-rw-r--r--   0        0        0     1005 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/ftp_server/async_server.py
+-rw-r--r--   0        0        0     1155 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/ftp_server/ftp_command.py
+-rw-r--r--   0        0        0     3022 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/ftp_server/ftp_converters.py
+-rw-r--r--   0        0        0      993 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/ftp_server/ftp_reply.py
+-rw-r--r--   0        0        0      362 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/ftp_server/ftp_request.py
+-rw-r--r--   0        0        0      823 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/ftp_server/ftp_server_protocol.py
+-rw-r--r--   0        0        0     2117 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/ftp_server/ftp_server_request_handler.py
+-rw-r--r--   0        0        0      950 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/ftp_server/server.py
+-rw-r--r--   0        0        0       37 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/_static/css/details.css
+-rw-r--r--   0        0        0       85 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/api/converter.rst
+-rw-r--r--   0        0        0       86 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/api/exceptions.rst
+-rw-r--r--   0        0        0      192 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/api/index.rst
+-rw-r--r--   0        0        0       81 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/api/protocol.rst
+-rw-r--r--   0        0        0       78 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/api/warnings.rst
+-rw-r--r--   0        0        0      560 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/api/async/client.rst
+-rw-r--r--   0        0        0      150 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/api/async/index.rst
+-rw-r--r--   0        0        0     1047 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/api/async/server.rst
+-rw-r--r--   0        0        0      387 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/api/lowlevel/futures.rst
+-rw-r--r--   0        0        0      213 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/api/lowlevel/index.rst
+-rw-r--r--   0        0        0      929 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/api/lowlevel/socket.rst
+-rw-r--r--   0        0        0      137 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/api/lowlevel/typed_attr.rst
+-rw-r--r--   0        0        0     2937 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/api/lowlevel/async/backend.rst
+-rw-r--r--   0        0        0      322 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/api/lowlevel/async/endpoints.rst
+-rw-r--r--   0        0        0      264 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/api/lowlevel/async/index.rst
+-rw-r--r--   0        0        0      304 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/api/lowlevel/async/servers.rst
+-rw-r--r--   0        0        0      464 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/api/lowlevel/async/transports.rst
+-rw-r--r--   0        0        0      320 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/api/lowlevel/sync/endpoints.rst
+-rw-r--r--   0        0        0      190 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/api/lowlevel/sync/index.rst
+-rw-r--r--   0        0        0      641 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/api/lowlevel/sync/transports.rst
+-rw-r--r--   0        0        0      450 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/api/serializers/abc.rst
+-rw-r--r--   0        0        0      465 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/api/serializers/cbor.rst
+-rw-r--r--   0        0        0      465 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/api/serializers/index.rst
+-rw-r--r--   0        0        0      308 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/api/serializers/json.rst
+-rw-r--r--   0        0        0      148 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/api/serializers/line.rst
+-rw-r--r--   0        0        0      514 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/api/serializers/msgpack.rst
+-rw-r--r--   0        0        0      498 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/api/serializers/pickle.rst
+-rw-r--r--   0        0        0      450 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/api/serializers/struct.rst
+-rw-r--r--   0        0        0      174 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/api/serializers/tools.rst
+-rw-r--r--   0        0        0      271 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/api/serializers/wrappers/base64.rst
+-rw-r--r--   0        0        0      491 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/api/serializers/wrappers/compressor.rst
+-rw-r--r--   0        0        0      529 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/api/sync/client.rst
+-rw-r--r--   0        0        0      117 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/api/sync/index.rst
+-rw-r--r--   0        0        0      549 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/api/sync/server.rst
+-rw-r--r--   0        0        0      366 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/api/sync/threads_helper.rst
+-rw-r--r--   0        0        0      158 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/howto/index.rst
+-rw-r--r--   0        0        0     6149 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/howto/protocols.rst
+-rw-r--r--   0        0        0    10160 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/howto/serializers.rst
+-rw-r--r--   0        0        0    12251 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/howto/tcp_clients.rst
+-rw-r--r--   0        0        0     9322 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/howto/tcp_servers.rst
+-rw-r--r--   0        0        0     8498 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/howto/udp_clients.rst
+-rw-r--r--   0        0        0     6565 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/howto/udp_servers.rst
+-rw-r--r--   0        0        0     8077 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/howto/advanced/buffered_serializers.rst
+-rw-r--r--   0        0        0      122 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/howto/advanced/index.rst
+-rw-r--r--   0        0        0     2581 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/howto/advanced/standalone_servers.rst
+-rw-r--r--   0        0        0      102 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/quickstart/index.rst
+-rw-r--r--   0        0        0      948 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/quickstart/install.rst
+-rw-r--r--   0        0        0      670 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/quickstart/overview.rst
+-rw-r--r--   0        0        0     5098 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/tutorials/echo_client_server_tcp.rst
+-rw-r--r--   0        0        0     3606 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/tutorials/echo_client_server_udp.rst
+-rw-r--r--   0        0        0     9846 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/tutorials/ftp_server.rst
+-rw-r--r--   0        0        0      127 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/docs/source/tutorials/index.rst
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/micro_benchmarks/__init__.py
+-rw-r--r--   0        0        0      225 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/micro_benchmarks/conftest.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/micro_benchmarks/serializers/__init__.py
+-rw-r--r--   0        0        0     2213 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/micro_benchmarks/serializers/bench_cbor.py
+-rw-r--r--   0        0        0     1733 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/micro_benchmarks/serializers/bench_json.py
+-rw-r--r--   0        0        0     3058 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/micro_benchmarks/serializers/bench_line.py
+-rw-r--r--   0        0        0      741 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/micro_benchmarks/serializers/bench_msgpack.py
+-rw-r--r--   0        0        0      889 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/micro_benchmarks/serializers/bench_pickle.py
+-rw-r--r--   0        0        0      974 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/micro_benchmarks/serializers/conftest.py
+-rw-r--r--   0        0        0    16091 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/micro_benchmarks/serializers/samples/data_10kb.json
+-rw-r--r--   0        0        0     1357 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/micro_benchmarks/serializers/samples/data_1kb.json
+-rw-r--r--   0        0        0     1167 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/__init__.py
+-rw-r--r--   0        0        0     1552 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/_typevars.py
+-rw-r--r--   0        0        0     4326 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/converter.py
+-rw-r--r--   0        0        0     5757 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/exceptions.py
+-rw-r--r--   0        0        0    12884 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/protocol.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/py.typed
+-rw-r--r--   0        0        0      926 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/version.py
+-rw-r--r--   0        0        0     1107 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/warnings.py
+-rw-r--r--   0        0        0      955 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/clients/__init__.py
+-rw-r--r--   0        0        0    13217 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/clients/abc.py
+-rw-r--r--   0        0        0    25593 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/clients/async_tcp.py
+-rw-r--r--   0        0        0    13682 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/clients/async_udp.py
+-rw-r--r--   0        0        0    19974 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/clients/tcp.py
+-rw-r--r--   0        0        0    13266 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/clients/udp.py
+-rw-r--r--   0        0        0      721 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/__init__.py
+-rw-r--r--   0        0        0     1934 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/_asyncgen.py
+-rw-r--r--   0        0        0      644 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/_final.py
+-rw-r--r--   0        0        0     1768 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/_lock.py
+-rw-r--r--   0        0        0    10803 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/_stream.py
+-rw-r--r--   0        0        0    12133 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/_utils.py
+-rw-r--r--   0        0        0     3015 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/constants.py
+-rw-r--r--   0        0        0    10909 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/futures.py
+-rw-r--r--   0        0        0    18369 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/socket.py
+-rw-r--r--   0        0        0     3919 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/typed_attr.py
+-rw-r--r--   0        0        0      720 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_async/__init__.py
+-rw-r--r--   0        0        0      721 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_async/backend/__init__.py
+-rw-r--r--   0        0        0     1576 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_async/backend/_sniffio_helpers.py
+-rw-r--r--   0        0        0    37319 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_async/backend/abc.py
+-rw-r--r--   0        0        0     4322 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_async/backend/factory.py
+-rw-r--r--   0        0        0      726 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_async/endpoints/__init__.py
+-rw-r--r--   0        0        0     5596 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_async/endpoints/datagram.py
+-rw-r--r--   0        0        0    11551 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_async/endpoints/stream.py
+-rw-r--r--   0        0        0      724 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_async/servers/__init__.py
+-rw-r--r--   0        0        0    13122 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_async/servers/datagram.py
+-rw-r--r--   0        0        0    12431 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_async/servers/stream.py
+-rw-r--r--   0        0        0      727 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_async/transports/__init__.py
+-rw-r--r--   0        0        0     7718 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_async/transports/abc.py
+-rw-r--r--   0        0        0    12840 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_async/transports/tls.py
+-rw-r--r--   0        0        0     1151 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_async/transports/utils.py
+-rw-r--r--   0        0        0      719 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_sync/__init__.py
+-rw-r--r--   0        0        0      713 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_sync/endpoints/__init__.py
+-rw-r--r--   0        0        0     5951 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_sync/endpoints/datagram.py
+-rw-r--r--   0        0        0    12811 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_sync/endpoints/stream.py
+-rw-r--r--   0        0        0      714 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_sync/transports/__init__.py
+-rw-r--r--   0        0        0     8201 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_sync/transports/abc.py
+-rw-r--r--   0        0        0    11203 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_sync/transports/base_selector.py
+-rw-r--r--   0        0        0    12281 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_sync/transports/socket.py
+-rw-r--r--   0        0        0      781 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/std_asyncio/__init__.py
+-rw-r--r--   0        0        0    13952 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/std_asyncio/_asyncio_utils.py
+-rw-r--r--   0        0        0     3537 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/std_asyncio/_flow_control.py
+-rw-r--r--   0        0        0     9036 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/std_asyncio/backend.py
+-rw-r--r--   0        0        0     7731 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/std_asyncio/socket.py
+-rw-r--r--   0        0        0    20254 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/std_asyncio/tasks.py
+-rw-r--r--   0        0        0     6921 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/std_asyncio/threads.py
+-rw-r--r--   0        0        0      724 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/std_asyncio/datagram/__init__.py
+-rw-r--r--   0        0        0     8244 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/std_asyncio/datagram/endpoint.py
+-rw-r--r--   0        0        0     7957 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/std_asyncio/datagram/listener.py
+-rw-r--r--   0        0        0     2458 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/std_asyncio/datagram/socket.py
+-rw-r--r--   0        0        0      724 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/std_asyncio/stream/__init__.py
+-rw-r--r--   0        0        0     6153 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/std_asyncio/stream/listener.py
+-rw-r--r--   0        0        0    13979 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/lowlevel/std_asyncio/stream/socket.py
+-rw-r--r--   0        0        0     1192 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/serializers/__init__.py
+-rw-r--r--   0        0        0     7973 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/serializers/abc.py
+-rw-r--r--   0        0        0    23327 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/serializers/base_stream.py
+-rw-r--r--   0        0        0     4655 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/serializers/cbor.py
+-rw-r--r--   0        0        0    16151 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/serializers/json.py
+-rw-r--r--   0        0        0     9985 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/serializers/line.py
+-rw-r--r--   0        0        0     6460 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/serializers/msgpack.py
+-rw-r--r--   0        0        0     6362 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/serializers/pickle.py
+-rw-r--r--   0        0        0    14772 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/serializers/struct.py
+-rw-r--r--   0        0        0     5819 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/serializers/tools.py
+-rw-r--r--   0        0        0      920 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/serializers/wrapper/__init__.py
+-rw-r--r--   0        0        0     6164 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/serializers/wrapper/base64.py
+-rw-r--r--   0        0        0    13684 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/serializers/wrapper/compressor.py
+-rw-r--r--   0        0        0     1017 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/servers/__init__.py
+-rw-r--r--   0        0        0     7738 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/servers/_base.py
+-rw-r--r--   0        0        0     5335 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/servers/abc.py
+-rw-r--r--   0        0        0    27743 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/servers/async_tcp.py
+-rw-r--r--   0        0        0    17131 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/servers/async_udp.py
+-rw-r--r--   0        0        0    11078 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/servers/handlers.py
+-rw-r--r--   0        0        0     4741 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/servers/standalone_tcp.py
+-rw-r--r--   0        0        0     3335 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/servers/standalone_udp.py
+-rw-r--r--   0        0        0     3502 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/src/easynetwork/servers/threads_helper.py
+-rw-r--r--   0        0        0      106 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/__init__.py
+-rw-r--r--   0        0        0      565 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/conftest.py
+-rw-r--r--   0        0        0      896 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/import_utils.py
+-rw-r--r--   0        0        0     5876 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/tools.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/__init__.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_async/__init__.py
+-rw-r--r--   0        0        0     8915 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_async/test_futures.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_async/test_backend/__init__.py
+-rw-r--r--   0        0        0    54723 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_async/test_backend/test_asyncio_backend.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_communication/__init__.py
+-rw-r--r--   0        0        0     5544 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_communication/conftest.py
+-rw-r--r--   0        0        0     3834 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_communication/serializer.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_communication/test_async/__init__.py
+-rw-r--r--   0        0        0      291 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_communication/test_async/_utils.py
+-rw-r--r--   0        0        0      484 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_communication/test_async/conftest.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_communication/test_async/test_client/__init__.py
+-rw-r--r--   0        0        0    20574 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_communication/test_async/test_client/test_tcp.py
+-rw-r--r--   0        0        0    15677 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_communication/test_async/test_client/test_udp.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_communication/test_async/test_server/__init__.py
+-rw-r--r--   0        0        0     6206 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_communication/test_async/test_server/base.py
+-rw-r--r--   0        0        0    49498 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_communication/test_async/test_server/test_tcp.py
+-rw-r--r--   0        0        0    26602 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_communication/test_async/test_server/test_udp.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_communication/test_sync/__init__.py
+-rw-r--r--   0        0        0     1293 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_communication/test_sync/conftest.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_communication/test_sync/test_client/__init__.py
+-rw-r--r--   0        0        0    15737 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_communication/test_sync/test_client/test_tcp.py
+-rw-r--r--   0        0        0     6096 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_communication/test_sync/test_client/test_udp.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_communication/test_sync/test_server/__init__.py
+-rw-r--r--   0        0        0     6651 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_communication/test_sync/test_server/test_standalone.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_concurrency/__init__.py
+-rw-r--r--   0        0        0     2416 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_concurrency/conftest.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_concurrency/test_sync/__init__.py
+-rw-r--r--   0        0        0     1144 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_concurrency/test_sync/conftest.py
+-rw-r--r--   0        0        0     2236 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_concurrency/test_sync/test_threaded_client.py
+-rw-r--r--   0        0        0      105 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_serializers/__init__.py
+-rw-r--r--   0        0        0    15044 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_serializers/base.py
+-rw-r--r--   0        0        0     7561 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_serializers/test_base64.py
+-rw-r--r--   0        0        0     2278 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_serializers/test_cbor.py
+-rw-r--r--   0        0        0     4330 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_serializers/test_compressors.py
+-rw-r--r--   0        0        0     4631 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_serializers/test_json.py
+-rw-r--r--   0        0        0     4101 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_serializers/test_line.py
+-rw-r--r--   0        0        0     1900 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_serializers/test_msgpack.py
+-rw-r--r--   0        0        0     3204 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_serializers/test_namedtuple.py
+-rw-r--r--   0        0        0     3305 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_serializers/test_pickle.py
+-rw-r--r--   0        0        0     2705 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_serializers/test_struct.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_serializers/samples/__init__.py
+-rw-r--r--   0        0        0    10677 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_serializers/samples/json.py
+-rw-r--r--   0        0        0     2117 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/functional_test/test_serializers/samples/pickle.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/other_test/__init__.py
+-rw-r--r--   0        0        0     4154 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/other_test/test_import.py
+-rw-r--r--   0        0        0      553 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/other_test/test_project_metadata.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/pytest_plugins/__init__.py
+-rw-r--r--   0        0        0     3471 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/pytest_plugins/asyncio_event_loop.py
+-rw-r--r--   0        0        0      958 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/pytest_plugins/auto_markers.py
+-rw-r--r--   0        0        0      973 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/pytest_plugins/extra_features.py
+-rw-r--r--   0        0        0     1080 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/pytest_plugins/ssl_module.py
+-rw-r--r--   0        0        0     2595 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/scripts/async_client_test.py
+-rw-r--r--   0        0        0     3111 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/scripts/async_server_test.py
+-rw-r--r--   0        0        0     2663 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/scripts/threaded_client_test.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/__init__.py
+-rw-r--r--   0        0        0     7566 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/_utils.py
+-rw-r--r--   0        0        0     7223 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/base.py
+-rw-r--r--   0        0        0     7614 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/conftest.py
+-rw-r--r--   0        0        0     2788 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_converter.py
+-rw-r--r--   0        0        0    29872 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_protocol.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_async/__init__.py
+-rw-r--r--   0        0        0     3110 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_async/conftest.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_async/test_api/__init__.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_async/test_api/test_client/__init__.py
+-rw-r--r--   0        0        0      497 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_async/test_api/test_client/base.py
+-rw-r--r--   0        0        0     4913 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_async/test_api/test_client/test_abc.py
+-rw-r--r--   0        0        0    58817 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_async/test_api/test_client/test_tcp.py
+-rw-r--r--   0        0        0    26649 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_async/test_api/test_client/test_udp.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_async/test_api/test_server/__init__.py
+-rw-r--r--   0        0        0      962 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_async/test_api/test_server/conftest.py
+-rw-r--r--   0        0        0     3055 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_async/test_api/test_server/test_handler.py
+-rw-r--r--   0        0        0     1883 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_async/test_api/test_server/test_tcp.py
+-rw-r--r--   0        0        0     1138 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_async/test_api/test_server/test_udp.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_async/test_asyncio_backend/__init__.py
+-rw-r--r--   0        0        0     1796 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_async/test_asyncio_backend/conftest.py
+-rw-r--r--   0        0        0    37629 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_async/test_asyncio_backend/test_backend.py
+-rw-r--r--   0        0        0    46294 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_async/test_asyncio_backend/test_datagram.py
+-rw-r--r--   0        0        0    24722 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_async/test_asyncio_backend/test_socket.py
+-rw-r--r--   0        0        0    47280 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_async/test_asyncio_backend/test_stream.py
+-rw-r--r--   0        0        0     5774 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_async/test_asyncio_backend/test_tasks.py
+-rw-r--r--   0        0        0    35516 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_async/test_asyncio_backend/test_utils.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_async/test_lowlevel_api/__init__.py
+-rw-r--r--   0        0        0    10129 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_async/test_lowlevel_api/test_futures.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_async/test_lowlevel_api/test_backend/__init__.py
+-rw-r--r--   0        0        0     2934 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_async/test_lowlevel_api/test_backend/_fake_backends.py
+-rw-r--r--   0        0        0    20221 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_async/test_lowlevel_api/test_backend/test_backend.py
+-rw-r--r--   0        0        0     1141 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_async/test_lowlevel_api/test_backend/test_sniffio_helpers.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_async/test_lowlevel_api/test_endpoints/__init__.py
+-rw-r--r--   0        0        0     8743 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_async/test_lowlevel_api/test_endpoints/test_datagram.py
+-rw-r--r--   0        0        0    28042 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_async/test_lowlevel_api/test_endpoints/test_stream.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_async/test_lowlevel_api/test_servers/__init__.py
+-rw-r--r--   0        0        0    11631 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_async/test_lowlevel_api/test_servers/test_datagram.py
+-rw-r--r--   0        0        0    23393 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_async/test_lowlevel_api/test_servers/test_stream.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_async/test_lowlevel_api/test_transports/__init__.py
+-rw-r--r--   0        0        0     1506 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_async/test_lowlevel_api/test_transports/test_abc.py
+-rw-r--r--   0        0        0    39697 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_async/test_lowlevel_api/test_transports/test_tls.py
+-rw-r--r--   0        0        0      105 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_serializers/__init__.py
+-rw-r--r--   0        0        0      615 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_serializers/base.py
+-rw-r--r--   0        0        0      211 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_serializers/conftest.py
+-rw-r--r--   0        0        0    48795 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_serializers/test_abc.py
+-rw-r--r--   0        0        0     7095 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_serializers/test_base64.py
+-rw-r--r--   0        0        0     6900 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_serializers/test_cbor.py
+-rw-r--r--   0        0        0    25214 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_serializers/test_compressor.py
+-rw-r--r--   0        0        0    21773 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_serializers/test_json.py
+-rw-r--r--   0        0        0    17878 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_serializers/test_line.py
+-rw-r--r--   0        0        0     9460 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_serializers/test_msgpack.py
+-rw-r--r--   0        0        0    10465 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_serializers/test_pickle.py
+-rw-r--r--   0        0        0    20050 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_serializers/test_struct.py
+-rw-r--r--   0        0        0     8464 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_serializers/test_tools.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_sync/__init__.py
+-rw-r--r--   0        0        0      403 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_sync/conftest.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_sync/test_client/__init__.py
+-rw-r--r--   0        0        0     1588 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_sync/test_client/base.py
+-rw-r--r--   0        0        0     1329 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_sync/test_client/conftest.py
+-rw-r--r--   0        0        0     4139 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_sync/test_client/test_abc.py
+-rw-r--r--   0        0        0    82470 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_sync/test_client/test_tcp.py
+-rw-r--r--   0        0        0    46632 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_sync/test_client/test_udp.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_sync/test_lowlevel_api/__init__.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_sync/test_lowlevel_api/test_endpoints/__init__.py
+-rw-r--r--   0        0        0     9930 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_sync/test_lowlevel_api/test_endpoints/test_datagram.py
+-rw-r--r--   0        0        0    33055 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_sync/test_lowlevel_api/test_endpoints/test_stream.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_sync/test_lowlevel_api/test_transports/__init__.py
+-rw-r--r--   0        0        0     4483 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_sync/test_lowlevel_api/test_transports/test_abc.py
+-rw-r--r--   0        0        0    17809 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_sync/test_lowlevel_api/test_transports/test_selector.py
+-rw-r--r--   0        0        0    49351 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_sync/test_lowlevel_api/test_transports/test_socket.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_sync/test_server/__init__.py
+-rw-r--r--   0        0        0      962 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_sync/test_server/conftest.py
+-rw-r--r--   0        0        0     4102 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_sync/test_server/test_standalone.py
+-rw-r--r--   0        0        0        0 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_tools/__init__.py
+-rw-r--r--   0        0        0     3470 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_tools/test_asyncgen.py
+-rw-r--r--   0        0        0     2452 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_tools/test_lock.py
+-rw-r--r--   0        0        0    10106 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_tools/test_socket.py
+-rw-r--r--   0        0        0    40704 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_tools/test_stream.py
+-rw-r--r--   0        0        0     4303 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_tools/test_typedattr.py
+-rw-r--r--   0        0        0    24980 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/tests/unit_test/test_tools/test_utils.py
+-rw-r--r--   0        0        0     2004 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/.gitignore
+-rw-r--r--   0        0        0    11357 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/LICENSE
+-rw-r--r--   0        0        0     7744 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/README.md
+-rw-r--r--   0        0        0     7330 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/pyproject.toml
+-rw-r--r--   0        0        0     9368 2024-04-06 16:56:24.000000 easynetwork-1.0.0rc8/PKG-INFO
```

### Comparing `easynetwork-1.0.0rc7/.flake8` & `easynetwork-1.0.0rc8/.flake8`

 * *Files 11% similar despite different names*

```diff
@@ -13,20 +13,25 @@
     E203
     # E266: too many leading '#' for block comment
     E266
     # E402: module level import not a top of file
     E402
     # E501: line too long (already have black for that)
     E501
+    # E701: multiple statements on one line (colon) (Thanks black)
+    E701
     # E731: do not assign a lambda expression, use a def
     E731
 per-file-ignores =
     # F401: 'module' imported but unused
     # F403: 'from module import *' used; unable to detect undefined names
     # F405: 'name' may be undefined, or defined from star imports: 'module'
     # F841: local variable 'name' is assigned to but never used
     # DALL001: There is no __all__ defined
     src/easynetwork/__init__.py:F401,F403,F405
     src/easynetwork/*/__init__.py:F401,F403,F405
     tests/*.py:DALL001
     docs/*.py:DALL001
     docs/source/_include/*.py:F841,DALL001
+    micro_benchmarks/*.py:DALL001
+    benchmark_server/*.py:DALL001
+    benchmark_server/*benchmark*:DALL001
```

### Comparing `easynetwork-1.0.0rc7/.pre-commit-config.yaml` & `easynetwork-1.0.0rc8/.pre-commit-config.yaml`

 * *Files 16% similar despite different names*

```diff
@@ -5,81 +5,97 @@
 ci:
   skip: [mypy]
   autoupdate_branch: 'pre-commit/autoupdate'
   autoupdate_schedule: quarterly
 
 repos:
   - repo: https://github.com/asottile/pyupgrade
-    rev: v3.15.0
+    rev: v3.15.2
     hooks:
       - id: pyupgrade
         args: ['--py311-plus']
   - repo: local
     hooks:
       - id: mypy
         name: mypy (project)
-        entry: tox run -q -e mypy-full
+        entry: tox run -q -r -e mypy-full
         language: system
         files: ^(src/)
         types_or: [python, pyi]
         require_serial: true
         pass_filenames: false
       - id: mypy
         name: mypy (tests)
         files: ^((src|tests)/)
-        entry: tox run -q -e mypy-test
+        entry: tox run -q -r -e mypy-test
         language: system
         types_or: [python, pyi]
         require_serial: true
         pass_filenames: false
       - id: mypy
         name: mypy (docs)
         files: ^((src|docs/source)/)
         exclude: ^(docs/source/conf.py)$
-        entry: tox run -q -e mypy-docs
+        entry: tox run -q -r -e mypy-docs
+        language: system
+        types_or: [python, pyi]
+        require_serial: true
+        pass_filenames: false
+      - id: mypy
+        name: mypy (benchmark/servers)
+        files: ^((src|benchmark_server)/)
+        entry: tox run -q -e mypy-benchmark_server
+        language: system
+        types_or: [python, pyi]
+        require_serial: true
+        pass_filenames: false
+      - id: mypy
+        name: mypy (benchmark/micro-benchs)
+        files: ^((src|micro_benchmarks)/)
+        entry: tox run -q -e mypy-micro_benchmarks
         language: system
         types_or: [python, pyi]
         require_serial: true
         pass_filenames: false
   - repo: https://github.com/PyCQA/isort
-    rev: '5.12.0'
+    rev: '5.13.2'
     hooks:
       - id: isort
         args: ['--filter-files', '--settings-file', 'pyproject.toml']
   - repo: https://github.com/psf/black
-    rev: '23.11.0'
+    rev: '24.3.0'
     hooks:
       - id: black
         args: ['--config', 'pyproject.toml']
   - repo: https://github.com/PyCQA/flake8
-    rev: '6.1.0'
+    rev: '7.0.0'
     hooks:
       - id: flake8
         args: ['--config', '.flake8']
-        types: []  # Overwrite with empty in order to fallback to types_or
-        types_or: [python, pyi]
         additional_dependencies: [
-          'flake8-pyi>=22.11.0',
           'flake8-dunder-all @ git+https://github.com/francis-clairicia/flake8-dunder-all.git@v0.1.0'
         ]
   - repo: https://github.com/PyCQA/bandit
-    rev: '1.7.5'
+    rev: '1.7.8'
     hooks:
       - id: bandit
         args: ['-c', '.bandit.yml']
   - repo: https://github.com/aio-libs/sort-all
     rev: 'v1.2.0'
     hooks:
       - id: sort-all
         types: []  # Overwrite with empty in order to fallback to types_or
         types_or: [python, pyi]
   - repo: https://github.com/pdm-project/pdm
-    rev: '2.10.4'
+    rev: '2.13.2'
     hooks:
       - id: pdm-lock-check
+      - id: pdm-export
+        name: pdm-export benchmark-servers-deps
+        args: ['--dev', '--group', 'benchmark-servers-deps', '--format', 'requirements', '--without-hashes', '--output', 'benchmark_server/servers/requirements.txt']
   - repo: https://github.com/pre-commit/pygrep-hooks
     rev: 'v1.10.0'
     hooks:
       - id: python-check-blanket-noqa
       - id: python-no-log-warn
       - id: rst-backticks
       - id: rst-directive-colons
```

### Comparing `easynetwork-1.0.0rc7/pdm.lock` & `easynetwork-1.0.0rc8/pdm.lock`

 * *Files 7% similar despite different names*

```diff
@@ -1,250 +1,278 @@
 # This file is @generated by PDM.
 # It is not intended for manual editing.
 
 [metadata]
-groups = ["default", "bandit", "build", "cbor", "coverage", "dev", "doc", "encryption", "flake8", "format", "msgpack", "mypy", "pre-commit", "sniffio", "test", "tox", "types-encryption", "types-msgpack", "uvloop"]
-strategy = ["cross_platform"]
-lock_version = "4.4"
-content_hash = "sha256:589431756c4c9ac44a636587f4153c6261ab748f9d1eb87b70bd97df5f961582"
+groups = ["default", "bandit", "benchmark-servers", "benchmark-servers-deps", "build", "cbor", "coverage", "dev", "doc", "flake8", "format", "micro-benchmark", "msgpack", "mypy", "pre-commit", "sniffio", "test", "tox", "types-msgpack", "uvloop"]
+strategy = ["cross_platform", "inherit_metadata"]
+lock_version = "4.4.1"
+content_hash = "sha256:dd6742f72e9e81f64d6cb1dc534d3ef331068d75c6f1c1282d2c0563b1dd3662"
 
 [[package]]
 name = "alabaster"
-version = "0.7.13"
-requires_python = ">=3.6"
-summary = "A configurable sidebar-enabled Sphinx theme"
+version = "0.7.16"
+requires_python = ">=3.9"
+summary = "A light, configurable Sphinx theme"
+groups = ["doc"]
 files = [
-    {file = "alabaster-0.7.13-py3-none-any.whl", hash = "sha256:1ee19aca801bbabb5ba3f5f258e4422dfa86f82f3e9cefb0859b283cdd7f62a3"},
-    {file = "alabaster-0.7.13.tar.gz", hash = "sha256:a27a4a084d5e690e16e01e03ad2b2e552c61a65469419b907243193de1a84ae2"},
+    {file = "alabaster-0.7.16-py3-none-any.whl", hash = "sha256:b46733c07dce03ae4e150330b975c75737fa60f0a7c591b6c8bf4928a28e2c92"},
+    {file = "alabaster-0.7.16.tar.gz", hash = "sha256:75a8b99c28a5dad50dd7f8ccdd447a121ddb3892da9e53d1ca5cca3106d58d65"},
 ]
 
 [[package]]
 name = "anyio"
-version = "4.1.0"
+version = "4.3.0"
 requires_python = ">=3.8"
 summary = "High level compatibility layer for multiple asynchronous event loop implementations"
+groups = ["dev"]
 dependencies = [
     "idna>=2.8",
     "sniffio>=1.1",
 ]
 files = [
-    {file = "anyio-4.1.0-py3-none-any.whl", hash = "sha256:56a415fbc462291813a94528a779597226619c8e78af7de0507333f700011e5f"},
-    {file = "anyio-4.1.0.tar.gz", hash = "sha256:5a0bec7085176715be77df87fc66d6c9d70626bd752fcc85f57cdbee5b3760da"},
+    {file = "anyio-4.3.0-py3-none-any.whl", hash = "sha256:048e05d0f6caeed70d731f3db756d35dcc1f35747c8c403364a8332c630441b8"},
+    {file = "anyio-4.3.0.tar.gz", hash = "sha256:f75253795a87df48568485fd18cdd2a3fa5c4f7c5be8e5e36637733fce06fed6"},
 ]
 
 [[package]]
 name = "apeye"
 version = "1.4.1"
 requires_python = ">=3.6.1"
 summary = "Handy tools for working with URLs and APIs."
+groups = ["doc"]
 dependencies = [
     "apeye-core>=1.0.0b2",
     "domdf-python-tools>=2.6.0",
     "platformdirs>=2.3.0",
     "requests>=2.24.0",
 ]
 files = [
     {file = "apeye-1.4.1-py3-none-any.whl", hash = "sha256:44e58a9104ec189bf42e76b3a7fe91e2b2879d96d48e9a77e5e32ff699c9204e"},
     {file = "apeye-1.4.1.tar.gz", hash = "sha256:14ea542fad689e3bfdbda2189a354a4908e90aee4bf84c15ab75d68453d76a36"},
 ]
 
 [[package]]
 name = "apeye-core"
-version = "1.1.4"
+version = "1.1.5"
 requires_python = ">=3.6.1"
 summary = "Core (offline) functionality for the apeye library."
+groups = ["doc"]
 dependencies = [
     "domdf-python-tools>=2.6.0",
     "idna>=2.5",
 ]
 files = [
-    {file = "apeye_core-1.1.4-py3-none-any.whl", hash = "sha256:084bc696448d3ac428fece41c1f2eb08fa9d9ce1d1b2f4d43187e3def4528a60"},
-    {file = "apeye_core-1.1.4.tar.gz", hash = "sha256:72bb89fed3baa647cb81aa28e1d851787edcbf9573853b5d2b5f87c02f50eaf5"},
+    {file = "apeye_core-1.1.5-py3-none-any.whl", hash = "sha256:dc27a93f8c9e246b3b238c5ea51edf6115ab2618ef029b9f2d9a190ec8228fbf"},
+    {file = "apeye_core-1.1.5.tar.gz", hash = "sha256:5de72ed3d00cc9b20fea55e54b7ab8f5ef8500eb33a5368bc162a5585e238a55"},
+]
+
+[[package]]
+name = "asyncio-dgram"
+version = "2.1.2"
+requires_python = ">=3.5"
+summary = "Higher level Datagram support for Asyncio"
+groups = ["benchmark-servers-deps"]
+files = [
+    {file = "asyncio-dgram-2.1.2.tar.gz", hash = "sha256:bc28a90bc0523009fb0da16ca983c1400ff403a315754d86e037910563697f91"},
+    {file = "asyncio_dgram-2.1.2-py3-none-any.whl", hash = "sha256:9ef55fc760f93c8212709329a1e28a1cf1c1f0fc8222f1be0227c2b7606a10a2"},
 ]
 
 [[package]]
 name = "autodocsumm"
-version = "0.2.11"
+version = "0.2.12"
 requires_python = ">=3.7"
 summary = "Extended sphinx autodoc including automatic autosummaries"
+groups = ["doc"]
 dependencies = [
     "Sphinx<8.0,>=2.2",
 ]
 files = [
-    {file = "autodocsumm-0.2.11-py3-none-any.whl", hash = "sha256:f1d0a623bf1ad64d979a9e23fd360d1fb1b8f869beaf3197f711552cddc174e2"},
-    {file = "autodocsumm-0.2.11.tar.gz", hash = "sha256:183212bd9e9f3b58a96bb21b7958ee4e06224107aa45b2fd894b61b83581b9a9"},
+    {file = "autodocsumm-0.2.12-py3-none-any.whl", hash = "sha256:b842b53c686c07a4f174721ca4e729b027367703dbf42e2508863a3c6d6c049c"},
+    {file = "autodocsumm-0.2.12.tar.gz", hash = "sha256:848fe8c38df433c6635489499b969cb47cc389ed3d7b6e75c8ccbc94d4b3bf9e"},
 ]
 
 [[package]]
 name = "babel"
-version = "2.13.1"
+version = "2.14.0"
 requires_python = ">=3.7"
 summary = "Internationalization utilities"
-dependencies = [
-    "setuptools; python_version >= \"3.12\"",
-]
+groups = ["doc"]
 files = [
-    {file = "Babel-2.13.1-py3-none-any.whl", hash = "sha256:7077a4984b02b6727ac10f1f7294484f737443d7e2e66c5e4380e41a3ae0b4ed"},
-    {file = "Babel-2.13.1.tar.gz", hash = "sha256:33e0952d7dd6374af8dbf6768cc4ddf3ccfefc244f9986d4074704f2fbd18900"},
+    {file = "Babel-2.14.0-py3-none-any.whl", hash = "sha256:efb1a25b7118e67ce3a259bed20545c29cb68be8ad2c784c83689981b7a57287"},
+    {file = "Babel-2.14.0.tar.gz", hash = "sha256:6919867db036398ba21eb5c7a0f6b28ab8cbc3ae7a73a44ebe34ae74a4e7d363"},
 ]
 
 [[package]]
 name = "bandit"
-version = "1.7.5"
-requires_python = ">=3.7"
+version = "1.7.8"
+requires_python = ">=3.8"
 summary = "Security oriented static analyser for python code."
+groups = ["bandit"]
 dependencies = [
-    "GitPython>=1.0.1",
     "PyYAML>=5.3.1",
     "colorama>=0.3.9; platform_system == \"Windows\"",
     "rich",
     "stevedore>=1.20.0",
 ]
 files = [
-    {file = "bandit-1.7.5-py3-none-any.whl", hash = "sha256:75665181dc1e0096369112541a056c59d1c5f66f9bb74a8d686c3c362b83f549"},
-    {file = "bandit-1.7.5.tar.gz", hash = "sha256:bdfc739baa03b880c2d15d0431b31c658ffc348e907fe197e54e0389dd59e11e"},
+    {file = "bandit-1.7.8-py3-none-any.whl", hash = "sha256:509f7af645bc0cd8fd4587abc1a038fc795636671ee8204d502b933aee44f381"},
+    {file = "bandit-1.7.8.tar.gz", hash = "sha256:36de50f720856ab24a24dbaa5fee2c66050ed97c1477e0a1159deab1775eab6b"},
 ]
 
 [[package]]
 name = "beautifulsoup4"
-version = "4.12.2"
+version = "4.12.3"
 requires_python = ">=3.6.0"
 summary = "Screen-scraping library"
+groups = ["doc"]
 dependencies = [
     "soupsieve>1.2",
 ]
 files = [
-    {file = "beautifulsoup4-4.12.2-py3-none-any.whl", hash = "sha256:bd2520ca0d9d7d12694a53d44ac482d181b4ec1888909b035a3dbf40d0f57d4a"},
-    {file = "beautifulsoup4-4.12.2.tar.gz", hash = "sha256:492bbc69dca35d12daac71c4db1bfff0c876c00ef4a2ffacce226d4638eb72da"},
+    {file = "beautifulsoup4-4.12.3-py3-none-any.whl", hash = "sha256:b80878c9f40111313e55da8ba20bdba06d8fa3969fc68304167741bbf9e082ed"},
+    {file = "beautifulsoup4-4.12.3.tar.gz", hash = "sha256:74e3d1928edc070d21748185c46e3fb33490f22f52a3addee9aee0f4f7781051"},
 ]
 
 [[package]]
 name = "black"
-version = "23.11.0"
+version = "24.3.0"
 requires_python = ">=3.8"
 summary = "The uncompromising code formatter."
+groups = ["format"]
 dependencies = [
     "click>=8.0.0",
     "mypy-extensions>=0.4.3",
     "packaging>=22.0",
     "pathspec>=0.9.0",
     "platformdirs>=2",
 ]
 files = [
-    {file = "black-23.11.0-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:cf57719e581cfd48c4efe28543fea3d139c6b6f1238b3f0102a9c73992cbb479"},
-    {file = "black-23.11.0-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:698c1e0d5c43354ec5d6f4d914d0d553a9ada56c85415700b81dc90125aac244"},
-    {file = "black-23.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:760415ccc20f9e8747084169110ef75d545f3b0932ee21368f63ac0fee86b221"},
-    {file = "black-23.11.0-cp311-cp311-win_amd64.whl", hash = "sha256:58e5f4d08a205b11800332920e285bd25e1a75c54953e05502052738fe16b3b5"},
-    {file = "black-23.11.0-py3-none-any.whl", hash = "sha256:54caaa703227c6e0c87b76326d0862184729a69b73d3b7305b6288e1d830067e"},
-    {file = "black-23.11.0.tar.gz", hash = "sha256:4c68855825ff432d197229846f971bc4d6666ce90492e5b02013bcaca4d9ab05"},
+    {file = "black-24.3.0-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:4f1373a7808a8f135b774039f61d59e4be7eb56b2513d3d2f02a8b9365b8a8a9"},
+    {file = "black-24.3.0-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:aadf7a02d947936ee418777e0247ea114f78aff0d0959461057cae8a04f20597"},
+    {file = "black-24.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:65c02e4ea2ae09d16314d30912a58ada9a5c4fdfedf9512d23326128ac08ac3d"},
+    {file = "black-24.3.0-cp311-cp311-win_amd64.whl", hash = "sha256:bf21b7b230718a5f08bd32d5e4f1db7fc8788345c8aea1d155fc17852b3410f5"},
+    {file = "black-24.3.0-cp312-cp312-macosx_10_9_x86_64.whl", hash = "sha256:2818cf72dfd5d289e48f37ccfa08b460bf469e67fb7c4abb07edc2e9f16fb63f"},
+    {file = "black-24.3.0-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:4acf672def7eb1725f41f38bf6bf425c8237248bb0804faa3965c036f7672d11"},
+    {file = "black-24.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:c7ed6668cbbfcd231fa0dc1b137d3e40c04c7f786e626b405c62bcd5db5857e4"},
+    {file = "black-24.3.0-cp312-cp312-win_amd64.whl", hash = "sha256:56f52cfbd3dabe2798d76dbdd299faa046a901041faf2cf33288bc4e6dae57b5"},
+    {file = "black-24.3.0-py3-none-any.whl", hash = "sha256:41622020d7120e01d377f74249e677039d20e6344ff5851de8a10f11f513bf93"},
+    {file = "black-24.3.0.tar.gz", hash = "sha256:a0c9c4a0771afc6919578cec71ce82a3e31e054904e7197deacbc9382671c41f"},
 ]
 
 [[package]]
 name = "build"
-version = "1.0.3"
-requires_python = ">= 3.7"
+version = "1.2.1"
+requires_python = ">=3.8"
 summary = "A simple, correct Python build frontend"
+groups = ["benchmark-servers", "build"]
 dependencies = [
     "colorama; os_name == \"nt\"",
-    "packaging>=19.0",
+    "packaging>=19.1",
     "pyproject-hooks",
 ]
 files = [
-    {file = "build-1.0.3-py3-none-any.whl", hash = "sha256:589bf99a67df7c9cf07ec0ac0e5e2ea5d4b37ac63301c4986d1acb126aa83f8f"},
-    {file = "build-1.0.3.tar.gz", hash = "sha256:538aab1b64f9828977f84bc63ae570b060a8ed1be419e7870b8b4fc5e6ea553b"},
+    {file = "build-1.2.1-py3-none-any.whl", hash = "sha256:75e10f767a433d9a86e50d83f418e83efc18ede923ee5ff7df93b6cb0306c5d4"},
+    {file = "build-1.2.1.tar.gz", hash = "sha256:526263f4870c26f26c433545579475377b2b7588b6f1eac76a001e873ae3e19d"},
 ]
 
 [[package]]
 name = "bump2version"
 version = "1.0.1"
 requires_python = ">=3.5"
 summary = "Version-bump your software with a single command!"
+groups = ["dev"]
 files = [
     {file = "bump2version-1.0.1-py2.py3-none-any.whl", hash = "sha256:37f927ea17cde7ae2d7baf832f8e80ce3777624554a653006c9144f8017fe410"},
     {file = "bump2version-1.0.1.tar.gz", hash = "sha256:762cb2bfad61f4ec8e2bdf452c7c267416f8c70dd9ecb1653fd0bbb01fa936e6"},
 ]
 
 [[package]]
 name = "cachecontrol"
-version = "0.13.1"
+version = "0.14.0"
 requires_python = ">=3.7"
 summary = "httplib2 caching for requests"
+groups = ["doc"]
 dependencies = [
-    "msgpack>=0.5.2",
+    "msgpack<2.0.0,>=0.5.2",
     "requests>=2.16.0",
 ]
 files = [
-    {file = "cachecontrol-0.13.1-py3-none-any.whl", hash = "sha256:95dedbec849f46dda3137866dc28b9d133fc9af55f5b805ab1291833e4457aa4"},
-    {file = "cachecontrol-0.13.1.tar.gz", hash = "sha256:f012366b79d2243a6118309ce73151bf52a38d4a5dac8ea57f09bd29087e506b"},
+    {file = "cachecontrol-0.14.0-py3-none-any.whl", hash = "sha256:f5bf3f0620c38db2e5122c0726bdebb0d16869de966ea6a2befe92470b740ea0"},
+    {file = "cachecontrol-0.14.0.tar.gz", hash = "sha256:7db1195b41c81f8274a7bbd97c956f44e8348265a1bc7641c37dfebc39f0c938"},
 ]
 
 [[package]]
 name = "cachecontrol"
-version = "0.13.1"
+version = "0.14.0"
 extras = ["filecache"]
 requires_python = ">=3.7"
 summary = "httplib2 caching for requests"
+groups = ["doc"]
 dependencies = [
-    "cachecontrol==0.13.1",
+    "cachecontrol==0.14.0",
     "filelock>=3.8.0",
 ]
 files = [
-    {file = "cachecontrol-0.13.1-py3-none-any.whl", hash = "sha256:95dedbec849f46dda3137866dc28b9d133fc9af55f5b805ab1291833e4457aa4"},
-    {file = "cachecontrol-0.13.1.tar.gz", hash = "sha256:f012366b79d2243a6118309ce73151bf52a38d4a5dac8ea57f09bd29087e506b"},
+    {file = "cachecontrol-0.14.0-py3-none-any.whl", hash = "sha256:f5bf3f0620c38db2e5122c0726bdebb0d16869de966ea6a2befe92470b740ea0"},
+    {file = "cachecontrol-0.14.0.tar.gz", hash = "sha256:7db1195b41c81f8274a7bbd97c956f44e8348265a1bc7641c37dfebc39f0c938"},
 ]
 
 [[package]]
 name = "cachetools"
-version = "5.3.2"
+version = "5.3.3"
 requires_python = ">=3.7"
 summary = "Extensible memoizing collections and decorators"
+groups = ["tox"]
 files = [
-    {file = "cachetools-5.3.2-py3-none-any.whl", hash = "sha256:861f35a13a451f94e301ce2bec7cac63e881232ccce7ed67fab9b5df4d3beaa1"},
-    {file = "cachetools-5.3.2.tar.gz", hash = "sha256:086ee420196f7b2ab9ca2db2520aca326318b68fe5ba8bc4d49cca91add450f2"},
+    {file = "cachetools-5.3.3-py3-none-any.whl", hash = "sha256:0abad1021d3f8325b2fc1d2e9c8b9c9d57b04c3932657a72465447332c24d945"},
+    {file = "cachetools-5.3.3.tar.gz", hash = "sha256:ba29e2dfa0b8b556606f097407ed1aa62080ee108ab0dc5ec9d6a723a007d105"},
 ]
 
 [[package]]
 name = "cbor2"
-version = "5.5.1"
+version = "5.6.2"
 requires_python = ">=3.8"
 summary = "CBOR (de)serializer with extensive tag support"
+groups = ["cbor"]
 files = [
-    {file = "cbor2-5.5.1-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:42155a20be46312fad2ceb85a408e2d90da059c2d36a65e0b99abca57c5357fd"},
-    {file = "cbor2-5.5.1-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:6f3827ae14c009df9b37790f1da5cd1f9d64f7ffec472a49ebf865c0af6b77e9"},
-    {file = "cbor2-5.5.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:4bfa417dbb8b4581ad3c2312469899518596551cfb0fe5bdaf8a6921cff69d7e"},
-    {file = "cbor2-5.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:e3317e7dfb4f3180be90bcd853204558d89f119b624c2168153b53dea305e79d"},
-    {file = "cbor2-5.5.1-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:1a5770bdf4340de55679efe6c38fc6d64529fda547e7a85eb0217a82717a8235"},
-    {file = "cbor2-5.5.1-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:b5d53826ad0c92fcb004b2a475896610b51e0ca010f6c37d762aae44ab0807b2"},
-    {file = "cbor2-5.5.1-cp311-cp311-win_amd64.whl", hash = "sha256:dc77cac985f7f7a20f2d8b1957d1e79393d7df823f61c7c6173d3a0011c1d770"},
-    {file = "cbor2-5.5.1-cp312-cp312-macosx_10_9_x86_64.whl", hash = "sha256:9e45d5aa8e484b4bf57240d8e7949389f1c9d4073758abb30954386321b55c9d"},
-    {file = "cbor2-5.5.1-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:93b949a66bec40dd0ca87a6d026136fea2cf1660120f921199a47ac8027af253"},
-    {file = "cbor2-5.5.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:93d601ca92d917f769370a5e6c3ead62dca6451b2b603915e4fcf300083b9fcd"},
-    {file = "cbor2-5.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:a11876abd50b9f70d114fcdbb0b5a3249ccd7d321465f0350028fd6d2317e114"},
-    {file = "cbor2-5.5.1-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:fd77c558decdba2a2a7a463e6346d53781d2163bacf205f77b999f561ba4ac73"},
-    {file = "cbor2-5.5.1-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:efb81920d80410b8e80a4a6a8b06ec9b766be0ae7f3029af8ae4b30914edcfa3"},
-    {file = "cbor2-5.5.1-cp312-cp312-win_amd64.whl", hash = "sha256:4bb35f3b1ebd4b7b37628f0cd5c839f3008dec669194a2a4a33d91bab7f8663b"},
-    {file = "cbor2-5.5.1-py3-none-any.whl", hash = "sha256:dca639c8ff81b9f0c92faf97324adfdbfb5c2a5bb97f249606c6f5b94c77cc0d"},
-    {file = "cbor2-5.5.1.tar.gz", hash = "sha256:f9e192f461a9f8f6082df28c035b006d153904213dc8640bed8a72d72bbc9475"},
+    {file = "cbor2-5.6.2-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:7ea9e150029c3976c46ee9870b6dcdb0a5baae21008fe3290564886b11aa2b64"},
+    {file = "cbor2-5.6.2-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:922e06710e5cf6f56b82b0b90d2f356aa229b99e570994534206985f675fd307"},
+    {file = "cbor2-5.6.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:b01a718e083e6de8b43296c3ccdb3aa8af6641f6bbb3ea1700427c6af73db28a"},
+    {file = "cbor2-5.6.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:ac85eb731c524d148f608b9bdb2069fa79e374a10ed5d10a2405eba9a6561e60"},
+    {file = "cbor2-5.6.2-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:03e5b68867b9d89ff2abd14ef7c6d42fbd991adc3e734a19a294935f22a4d05a"},
+    {file = "cbor2-5.6.2-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:7221b83000ee01d674572eec1d1caa366eac109d1d32c14d7af9a4aaaf496563"},
+    {file = "cbor2-5.6.2-cp311-cp311-win_amd64.whl", hash = "sha256:9aca73b63bdc6561e1a0d38618e78b9c204c942260d51e663c92c4ba6c961684"},
+    {file = "cbor2-5.6.2-cp312-cp312-macosx_10_9_x86_64.whl", hash = "sha256:377cfe9d5560c682486faef6d856226abf8b2801d95fa29d4e5d75b1615eb091"},
+    {file = "cbor2-5.6.2-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:fdc564ef2e9228bcd96ec8c6cdaa431a48ab03b3fb8326ead4b3f986330e5b9e"},
+    {file = "cbor2-5.6.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:8d1c0021d9a1f673066de7c8941f71a59abb11909cc355892dda01e79a2b3045"},
+    {file = "cbor2-5.6.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:1fde9e704e96751e0729cc58b912d0e77c34387fb6bcceea0817069e8683df45"},
+    {file = "cbor2-5.6.2-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:30e9ba8f4896726ca61869efacda50b6859aff92162ae5a0e192859664f36c81"},
+    {file = "cbor2-5.6.2-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:211a1e18e65ac71e04434ff5b58bde5c53f85b9c5bc92a3c0e2265089d3034f3"},
+    {file = "cbor2-5.6.2-cp312-cp312-win_amd64.whl", hash = "sha256:94981277b4bf448a2754c1f34a9d0055a9d1c5a8d102c933ffe95c80f1085bae"},
+    {file = "cbor2-5.6.2-py3-none-any.whl", hash = "sha256:c0b53a65673550fde483724ff683753f49462d392d45d7b6576364b39e76e54c"},
+    {file = "cbor2-5.6.2.tar.gz", hash = "sha256:b7513c2dea8868991fad7ef8899890ebcf8b199b9b4461c3c11d7ad3aef4820d"},
 ]
 
 [[package]]
 name = "certifi"
-version = "2023.11.17"
+version = "2024.2.2"
 requires_python = ">=3.6"
 summary = "Python package for providing Mozilla's CA Bundle."
+groups = ["benchmark-servers", "dev", "doc"]
 files = [
-    {file = "certifi-2023.11.17-py3-none-any.whl", hash = "sha256:e036ab49d5b79556f99cfc2d9320b34cfbe5be05c5871b51de9329f0603b0474"},
-    {file = "certifi-2023.11.17.tar.gz", hash = "sha256:9b469f3a900bf28dc19b8cfbf8019bf47f7fdd1a65a1d4ffb98fc14166beb4d1"},
+    {file = "certifi-2024.2.2-py3-none-any.whl", hash = "sha256:dc383c07b76109f368f6106eee2b593b04a011ea4d55f652c6ca24a754d1cdd1"},
+    {file = "certifi-2024.2.2.tar.gz", hash = "sha256:0569859f95fc761b18b45ef421b1290a0f65f147e92a1e5eb3e635f9a5e4e66f"},
 ]
 
 [[package]]
 name = "cffi"
 version = "1.16.0"
 requires_python = ">=3.8"
 summary = "Foreign Function Interface for Python calling C code."
+groups = ["dev", "test"]
 dependencies = [
     "pycparser",
 ]
 files = [
     {file = "cffi-1.16.0-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:b84834d0cf97e7d27dd5b7f3aca7b6e9263c56308ab9dc8aae9784abb774d404"},
     {file = "cffi-1.16.0-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:1b8ebc27c014c59692bb2664c7d13ce7a6e9a629be20e54e7271fa696ff2b417"},
     {file = "cffi-1.16.0-cp311-cp311-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:ee07e47c12890ef248766a6e55bd38ebfb2bb8edd4142d56db91b21ea68b7627"},
@@ -270,34 +298,37 @@
 ]
 
 [[package]]
 name = "cfgv"
 version = "3.4.0"
 requires_python = ">=3.8"
 summary = "Validate configuration and produce human readable error messages."
+groups = ["pre-commit"]
 files = [
     {file = "cfgv-3.4.0-py2.py3-none-any.whl", hash = "sha256:b7265b1f29fd3316bfcd2b330d63d024f2bfd8bcb8b0272f8e19a504856c48f9"},
     {file = "cfgv-3.4.0.tar.gz", hash = "sha256:e52591d4c5f5dead8e0f673fb16db7949d2cfb3f7da4582893288f0ded8fe560"},
 ]
 
 [[package]]
 name = "chardet"
 version = "5.2.0"
 requires_python = ">=3.7"
 summary = "Universal encoding detector for Python 3"
+groups = ["tox"]
 files = [
     {file = "chardet-5.2.0-py3-none-any.whl", hash = "sha256:e1cf59446890a00105fe7b7912492ea04b6e6f06d4b742b2c788469e34c82970"},
     {file = "chardet-5.2.0.tar.gz", hash = "sha256:1b3b6ff479a8c414bc3fa2c0852995695c4a026dcd6d0633b2dd092ca39c1cf7"},
 ]
 
 [[package]]
 name = "charset-normalizer"
 version = "3.3.2"
 requires_python = ">=3.7.0"
 summary = "The Real First Universal Charset Detector. Open, modern and actively maintained alternative to Chardet."
+groups = ["benchmark-servers", "doc"]
 files = [
     {file = "charset-normalizer-3.3.2.tar.gz", hash = "sha256:f30c3cb33b24454a82faecaf01b19c18562b1e89558fb6c56de4d9118a032fd5"},
     {file = "charset_normalizer-3.3.2-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:802fe99cca7457642125a8a88a084cef28ff0cf9407060f7b93dca5aa25480db"},
     {file = "charset_normalizer-3.3.2-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:573f6eac48f4769d667c4442081b1794f52919e7edada77495aaed9236d13a96"},
     {file = "charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:549a3a73da901d5bc3ce8d24e0600d1fa85524c10287f6004fbab87672bf3e1e"},
     {file = "charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:f27273b60488abe721a075bcca6d7f3964f9f6f067c8c4c605743023d7d3944f"},
     {file = "charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:1ceae2f17a9c33cb48e3263960dc5fc8005351ee19db217e9b1bb15d28c02574"},
@@ -330,1108 +361,1227 @@
 ]
 
 [[package]]
 name = "click"
 version = "8.1.7"
 requires_python = ">=3.7"
 summary = "Composable command line interface toolkit"
+groups = ["dev", "format"]
 dependencies = [
     "colorama; platform_system == \"Windows\"",
 ]
 files = [
     {file = "click-8.1.7-py3-none-any.whl", hash = "sha256:ae74fb96c20a0277a1d615f1e4d73c8414f5a98db8b799a7931d1582f3390c28"},
     {file = "click-8.1.7.tar.gz", hash = "sha256:ca9853ad459e787e2192211578cc907e7594e294c7ccc834310722b41b9ca6de"},
 ]
 
 [[package]]
 name = "colorama"
 version = "0.4.6"
 requires_python = "!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*,!=3.5.*,!=3.6.*,>=2.7"
 summary = "Cross-platform colored terminal text."
+groups = ["bandit", "benchmark-servers", "build", "dev", "doc", "format", "micro-benchmark", "test", "tox"]
 files = [
     {file = "colorama-0.4.6-py2.py3-none-any.whl", hash = "sha256:4f1d9991f5acc0ca119f9d443620b77f9d6b33703e51011c16baf57afb285fc6"},
     {file = "colorama-0.4.6.tar.gz", hash = "sha256:08695f5cb7ed6e0531a20572697297273c47b8cae5a63ffc6d6ed5c201be6e44"},
 ]
 
 [[package]]
 name = "coverage"
-version = "7.3.2"
+version = "7.4.4"
 requires_python = ">=3.8"
 summary = "Code coverage measurement for Python"
+groups = ["coverage", "test"]
 files = [
-    {file = "coverage-7.3.2-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:ac8c802fa29843a72d32ec56d0ca792ad15a302b28ca6203389afe21f8fa062c"},
-    {file = "coverage-7.3.2-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:89a937174104339e3a3ffcf9f446c00e3a806c28b1841c63edb2b369310fd074"},
-    {file = "coverage-7.3.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:e267e9e2b574a176ddb983399dec325a80dbe161f1a32715c780b5d14b5f583a"},
-    {file = "coverage-7.3.2-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:2443cbda35df0d35dcfb9bf8f3c02c57c1d6111169e3c85fc1fcc05e0c9f39a3"},
-    {file = "coverage-7.3.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:4175e10cc8dda0265653e8714b3174430b07c1dca8957f4966cbd6c2b1b8065a"},
-    {file = "coverage-7.3.2-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:0cbf38419fb1a347aaf63481c00f0bdc86889d9fbf3f25109cf96c26b403fda1"},
-    {file = "coverage-7.3.2-cp311-cp311-musllinux_1_1_i686.whl", hash = "sha256:5c913b556a116b8d5f6ef834038ba983834d887d82187c8f73dec21049abd65c"},
-    {file = "coverage-7.3.2-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:1981f785239e4e39e6444c63a98da3a1db8e971cb9ceb50a945ba6296b43f312"},
-    {file = "coverage-7.3.2-cp311-cp311-win32.whl", hash = "sha256:43668cabd5ca8258f5954f27a3aaf78757e6acf13c17604d89648ecc0cc66640"},
-    {file = "coverage-7.3.2-cp311-cp311-win_amd64.whl", hash = "sha256:e10c39c0452bf6e694511c901426d6b5ac005acc0f78ff265dbe36bf81f808a2"},
-    {file = "coverage-7.3.2-cp312-cp312-macosx_10_9_x86_64.whl", hash = "sha256:4cbae1051ab791debecc4a5dcc4a1ff45fc27b91b9aee165c8a27514dd160836"},
-    {file = "coverage-7.3.2-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:12d15ab5833a997716d76f2ac1e4b4d536814fc213c85ca72756c19e5a6b3d63"},
-    {file = "coverage-7.3.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:3c7bba973ebee5e56fe9251300c00f1579652587a9f4a5ed8404b15a0471f216"},
-    {file = "coverage-7.3.2-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:fe494faa90ce6381770746077243231e0b83ff3f17069d748f645617cefe19d4"},
-    {file = "coverage-7.3.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:f6e9589bd04d0461a417562649522575d8752904d35c12907d8c9dfeba588faf"},
-    {file = "coverage-7.3.2-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:d51ac2a26f71da1b57f2dc81d0e108b6ab177e7d30e774db90675467c847bbdf"},
-    {file = "coverage-7.3.2-cp312-cp312-musllinux_1_1_i686.whl", hash = "sha256:99b89d9f76070237975b315b3d5f4d6956ae354a4c92ac2388a5695516e47c84"},
-    {file = "coverage-7.3.2-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:fa28e909776dc69efb6ed975a63691bc8172b64ff357e663a1bb06ff3c9b589a"},
-    {file = "coverage-7.3.2-cp312-cp312-win32.whl", hash = "sha256:289fe43bf45a575e3ab10b26d7b6f2ddb9ee2dba447499f5401cfb5ecb8196bb"},
-    {file = "coverage-7.3.2-cp312-cp312-win_amd64.whl", hash = "sha256:7dbc3ed60e8659bc59b6b304b43ff9c3ed858da2839c78b804973f613d3e92ed"},
-    {file = "coverage-7.3.2-pp38.pp39.pp310-none-any.whl", hash = "sha256:ae97af89f0fbf373400970c0a21eef5aa941ffeed90aee43650b81f7d7f47637"},
-    {file = "coverage-7.3.2.tar.gz", hash = "sha256:be32ad29341b0170e795ca590e1c07e81fc061cb5b10c74ce7203491484404ef"},
+    {file = "coverage-7.4.4-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:0f9f50e7ef2a71e2fae92774c99170eb8304e3fdf9c8c3c7ae9bab3e7229c5cf"},
+    {file = "coverage-7.4.4-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:623512f8ba53c422fcfb2ce68362c97945095b864cda94a92edbaf5994201083"},
+    {file = "coverage-7.4.4-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0513b9508b93da4e1716744ef6ebc507aff016ba115ffe8ecff744d1322a7b63"},
+    {file = "coverage-7.4.4-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:40209e141059b9370a2657c9b15607815359ab3ef9918f0196b6fccce8d3230f"},
+    {file = "coverage-7.4.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:8a2b2b78c78293782fd3767d53e6474582f62443d0504b1554370bde86cc8227"},
+    {file = "coverage-7.4.4-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:73bfb9c09951125d06ee473bed216e2c3742f530fc5acc1383883125de76d9cd"},
+    {file = "coverage-7.4.4-cp311-cp311-musllinux_1_1_i686.whl", hash = "sha256:1f384c3cc76aeedce208643697fb3e8437604b512255de6d18dae3f27655a384"},
+    {file = "coverage-7.4.4-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:54eb8d1bf7cacfbf2a3186019bcf01d11c666bd495ed18717162f7eb1e9dd00b"},
+    {file = "coverage-7.4.4-cp311-cp311-win32.whl", hash = "sha256:cac99918c7bba15302a2d81f0312c08054a3359eaa1929c7e4b26ebe41e9b286"},
+    {file = "coverage-7.4.4-cp311-cp311-win_amd64.whl", hash = "sha256:b14706df8b2de49869ae03a5ccbc211f4041750cd4a66f698df89d44f4bd30ec"},
+    {file = "coverage-7.4.4-cp312-cp312-macosx_10_9_x86_64.whl", hash = "sha256:201bef2eea65e0e9c56343115ba3814e896afe6d36ffd37bab783261db430f76"},
+    {file = "coverage-7.4.4-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:41c9c5f3de16b903b610d09650e5e27adbfa7f500302718c9ffd1c12cf9d6818"},
+    {file = "coverage-7.4.4-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d898fe162d26929b5960e4e138651f7427048e72c853607f2b200909794ed978"},
+    {file = "coverage-7.4.4-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:3ea79bb50e805cd6ac058dfa3b5c8f6c040cb87fe83de10845857f5535d1db70"},
+    {file = "coverage-7.4.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:ce4b94265ca988c3f8e479e741693d143026632672e3ff924f25fab50518dd51"},
+    {file = "coverage-7.4.4-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:00838a35b882694afda09f85e469c96367daa3f3f2b097d846a7216993d37f4c"},
+    {file = "coverage-7.4.4-cp312-cp312-musllinux_1_1_i686.whl", hash = "sha256:fdfafb32984684eb03c2d83e1e51f64f0906b11e64482df3c5db936ce3839d48"},
+    {file = "coverage-7.4.4-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:69eb372f7e2ece89f14751fbcbe470295d73ed41ecd37ca36ed2eb47512a6ab9"},
+    {file = "coverage-7.4.4-cp312-cp312-win32.whl", hash = "sha256:137eb07173141545e07403cca94ab625cc1cc6bc4c1e97b6e3846270e7e1fea0"},
+    {file = "coverage-7.4.4-cp312-cp312-win_amd64.whl", hash = "sha256:d71eec7d83298f1af3326ce0ff1d0ea83c7cb98f72b577097f9083b20bdaf05e"},
+    {file = "coverage-7.4.4-pp38.pp39.pp310-none-any.whl", hash = "sha256:b2c5edc4ac10a7ef6605a966c58929ec6c1bd0917fb8c15cb3363f65aa40e677"},
+    {file = "coverage-7.4.4.tar.gz", hash = "sha256:c901df83d097649e257e803be22592aedfd5182f07b3cc87d640bbb9afd50f49"},
 ]
 
 [[package]]
 name = "coverage"
-version = "7.3.2"
+version = "7.4.4"
 extras = ["toml"]
 requires_python = ">=3.8"
 summary = "Code coverage measurement for Python"
+groups = ["test"]
 dependencies = [
-    "coverage==7.3.2",
+    "coverage==7.4.4",
 ]
 files = [
-    {file = "coverage-7.3.2-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:ac8c802fa29843a72d32ec56d0ca792ad15a302b28ca6203389afe21f8fa062c"},
-    {file = "coverage-7.3.2-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:89a937174104339e3a3ffcf9f446c00e3a806c28b1841c63edb2b369310fd074"},
-    {file = "coverage-7.3.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:e267e9e2b574a176ddb983399dec325a80dbe161f1a32715c780b5d14b5f583a"},
-    {file = "coverage-7.3.2-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:2443cbda35df0d35dcfb9bf8f3c02c57c1d6111169e3c85fc1fcc05e0c9f39a3"},
-    {file = "coverage-7.3.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:4175e10cc8dda0265653e8714b3174430b07c1dca8957f4966cbd6c2b1b8065a"},
-    {file = "coverage-7.3.2-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:0cbf38419fb1a347aaf63481c00f0bdc86889d9fbf3f25109cf96c26b403fda1"},
-    {file = "coverage-7.3.2-cp311-cp311-musllinux_1_1_i686.whl", hash = "sha256:5c913b556a116b8d5f6ef834038ba983834d887d82187c8f73dec21049abd65c"},
-    {file = "coverage-7.3.2-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:1981f785239e4e39e6444c63a98da3a1db8e971cb9ceb50a945ba6296b43f312"},
-    {file = "coverage-7.3.2-cp311-cp311-win32.whl", hash = "sha256:43668cabd5ca8258f5954f27a3aaf78757e6acf13c17604d89648ecc0cc66640"},
-    {file = "coverage-7.3.2-cp311-cp311-win_amd64.whl", hash = "sha256:e10c39c0452bf6e694511c901426d6b5ac005acc0f78ff265dbe36bf81f808a2"},
-    {file = "coverage-7.3.2-cp312-cp312-macosx_10_9_x86_64.whl", hash = "sha256:4cbae1051ab791debecc4a5dcc4a1ff45fc27b91b9aee165c8a27514dd160836"},
-    {file = "coverage-7.3.2-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:12d15ab5833a997716d76f2ac1e4b4d536814fc213c85ca72756c19e5a6b3d63"},
-    {file = "coverage-7.3.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:3c7bba973ebee5e56fe9251300c00f1579652587a9f4a5ed8404b15a0471f216"},
-    {file = "coverage-7.3.2-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:fe494faa90ce6381770746077243231e0b83ff3f17069d748f645617cefe19d4"},
-    {file = "coverage-7.3.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:f6e9589bd04d0461a417562649522575d8752904d35c12907d8c9dfeba588faf"},
-    {file = "coverage-7.3.2-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:d51ac2a26f71da1b57f2dc81d0e108b6ab177e7d30e774db90675467c847bbdf"},
-    {file = "coverage-7.3.2-cp312-cp312-musllinux_1_1_i686.whl", hash = "sha256:99b89d9f76070237975b315b3d5f4d6956ae354a4c92ac2388a5695516e47c84"},
-    {file = "coverage-7.3.2-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:fa28e909776dc69efb6ed975a63691bc8172b64ff357e663a1bb06ff3c9b589a"},
-    {file = "coverage-7.3.2-cp312-cp312-win32.whl", hash = "sha256:289fe43bf45a575e3ab10b26d7b6f2ddb9ee2dba447499f5401cfb5ecb8196bb"},
-    {file = "coverage-7.3.2-cp312-cp312-win_amd64.whl", hash = "sha256:7dbc3ed60e8659bc59b6b304b43ff9c3ed858da2839c78b804973f613d3e92ed"},
-    {file = "coverage-7.3.2-pp38.pp39.pp310-none-any.whl", hash = "sha256:ae97af89f0fbf373400970c0a21eef5aa941ffeed90aee43650b81f7d7f47637"},
-    {file = "coverage-7.3.2.tar.gz", hash = "sha256:be32ad29341b0170e795ca590e1c07e81fc061cb5b10c74ce7203491484404ef"},
+    {file = "coverage-7.4.4-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:0f9f50e7ef2a71e2fae92774c99170eb8304e3fdf9c8c3c7ae9bab3e7229c5cf"},
+    {file = "coverage-7.4.4-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:623512f8ba53c422fcfb2ce68362c97945095b864cda94a92edbaf5994201083"},
+    {file = "coverage-7.4.4-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0513b9508b93da4e1716744ef6ebc507aff016ba115ffe8ecff744d1322a7b63"},
+    {file = "coverage-7.4.4-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:40209e141059b9370a2657c9b15607815359ab3ef9918f0196b6fccce8d3230f"},
+    {file = "coverage-7.4.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:8a2b2b78c78293782fd3767d53e6474582f62443d0504b1554370bde86cc8227"},
+    {file = "coverage-7.4.4-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:73bfb9c09951125d06ee473bed216e2c3742f530fc5acc1383883125de76d9cd"},
+    {file = "coverage-7.4.4-cp311-cp311-musllinux_1_1_i686.whl", hash = "sha256:1f384c3cc76aeedce208643697fb3e8437604b512255de6d18dae3f27655a384"},
+    {file = "coverage-7.4.4-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:54eb8d1bf7cacfbf2a3186019bcf01d11c666bd495ed18717162f7eb1e9dd00b"},
+    {file = "coverage-7.4.4-cp311-cp311-win32.whl", hash = "sha256:cac99918c7bba15302a2d81f0312c08054a3359eaa1929c7e4b26ebe41e9b286"},
+    {file = "coverage-7.4.4-cp311-cp311-win_amd64.whl", hash = "sha256:b14706df8b2de49869ae03a5ccbc211f4041750cd4a66f698df89d44f4bd30ec"},
+    {file = "coverage-7.4.4-cp312-cp312-macosx_10_9_x86_64.whl", hash = "sha256:201bef2eea65e0e9c56343115ba3814e896afe6d36ffd37bab783261db430f76"},
+    {file = "coverage-7.4.4-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:41c9c5f3de16b903b610d09650e5e27adbfa7f500302718c9ffd1c12cf9d6818"},
+    {file = "coverage-7.4.4-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d898fe162d26929b5960e4e138651f7427048e72c853607f2b200909794ed978"},
+    {file = "coverage-7.4.4-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:3ea79bb50e805cd6ac058dfa3b5c8f6c040cb87fe83de10845857f5535d1db70"},
+    {file = "coverage-7.4.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:ce4b94265ca988c3f8e479e741693d143026632672e3ff924f25fab50518dd51"},
+    {file = "coverage-7.4.4-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:00838a35b882694afda09f85e469c96367daa3f3f2b097d846a7216993d37f4c"},
+    {file = "coverage-7.4.4-cp312-cp312-musllinux_1_1_i686.whl", hash = "sha256:fdfafb32984684eb03c2d83e1e51f64f0906b11e64482df3c5db936ce3839d48"},
+    {file = "coverage-7.4.4-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:69eb372f7e2ece89f14751fbcbe470295d73ed41ecd37ca36ed2eb47512a6ab9"},
+    {file = "coverage-7.4.4-cp312-cp312-win32.whl", hash = "sha256:137eb07173141545e07403cca94ab625cc1cc6bc4c1e97b6e3846270e7e1fea0"},
+    {file = "coverage-7.4.4-cp312-cp312-win_amd64.whl", hash = "sha256:d71eec7d83298f1af3326ce0ff1d0ea83c7cb98f72b577097f9083b20bdaf05e"},
+    {file = "coverage-7.4.4-pp38.pp39.pp310-none-any.whl", hash = "sha256:b2c5edc4ac10a7ef6605a966c58929ec6c1bd0917fb8c15cb3363f65aa40e677"},
+    {file = "coverage-7.4.4.tar.gz", hash = "sha256:c901df83d097649e257e803be22592aedfd5182f07b3cc87d640bbb9afd50f49"},
 ]
 
 [[package]]
 name = "cryptography"
-version = "41.0.7"
+version = "42.0.5"
 requires_python = ">=3.7"
 summary = "cryptography is a package which provides cryptographic recipes and primitives to Python developers."
+groups = ["dev", "test"]
 dependencies = [
-    "cffi>=1.12",
+    "cffi>=1.12; platform_python_implementation != \"PyPy\"",
 ]
 files = [
-    {file = "cryptography-41.0.7-cp37-abi3-macosx_10_12_universal2.whl", hash = "sha256:3c78451b78313fa81607fa1b3f1ae0a5ddd8014c38a02d9db0616133987b9cdf"},
-    {file = "cryptography-41.0.7-cp37-abi3-macosx_10_12_x86_64.whl", hash = "sha256:928258ba5d6f8ae644e764d0f996d61a8777559f72dfeb2eea7e2fe0ad6e782d"},
-    {file = "cryptography-41.0.7-cp37-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:5a1b41bc97f1ad230a41657d9155113c7521953869ae57ac39ac7f1bb471469a"},
-    {file = "cryptography-41.0.7-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:841df4caa01008bad253bce2a6f7b47f86dc9f08df4b433c404def869f590a15"},
-    {file = "cryptography-41.0.7-cp37-abi3-manylinux_2_28_aarch64.whl", hash = "sha256:5429ec739a29df2e29e15d082f1d9ad683701f0ec7709ca479b3ff2708dae65a"},
-    {file = "cryptography-41.0.7-cp37-abi3-manylinux_2_28_x86_64.whl", hash = "sha256:43f2552a2378b44869fe8827aa19e69512e3245a219104438692385b0ee119d1"},
-    {file = "cryptography-41.0.7-cp37-abi3-musllinux_1_1_aarch64.whl", hash = "sha256:af03b32695b24d85a75d40e1ba39ffe7db7ffcb099fe507b39fd41a565f1b157"},
-    {file = "cryptography-41.0.7-cp37-abi3-musllinux_1_1_x86_64.whl", hash = "sha256:49f0805fc0b2ac8d4882dd52f4a3b935b210935d500b6b805f321addc8177406"},
-    {file = "cryptography-41.0.7-cp37-abi3-win32.whl", hash = "sha256:f983596065a18a2183e7f79ab3fd4c475205b839e02cbc0efbbf9666c4b3083d"},
-    {file = "cryptography-41.0.7-cp37-abi3-win_amd64.whl", hash = "sha256:90452ba79b8788fa380dfb587cca692976ef4e757b194b093d845e8d99f612f2"},
-    {file = "cryptography-41.0.7-pp310-pypy310_pp73-macosx_10_12_x86_64.whl", hash = "sha256:079b85658ea2f59c4f43b70f8119a52414cdb7be34da5d019a77bf96d473b960"},
-    {file = "cryptography-41.0.7-pp310-pypy310_pp73-manylinux_2_28_aarch64.whl", hash = "sha256:b640981bf64a3e978a56167594a0e97db71c89a479da8e175d8bb5be5178c003"},
-    {file = "cryptography-41.0.7-pp310-pypy310_pp73-manylinux_2_28_x86_64.whl", hash = "sha256:e3114da6d7f95d2dee7d3f4eec16dacff819740bbab931aff8648cb13c5ff5e7"},
-    {file = "cryptography-41.0.7-pp310-pypy310_pp73-win_amd64.whl", hash = "sha256:d5ec85080cce7b0513cfd233914eb8b7bbd0633f1d1703aa28d1dd5a72f678ec"},
-    {file = "cryptography-41.0.7-pp38-pypy38_pp73-macosx_10_12_x86_64.whl", hash = "sha256:7a698cb1dac82c35fcf8fe3417a3aaba97de16a01ac914b89a0889d364d2f6be"},
-    {file = "cryptography-41.0.7-pp38-pypy38_pp73-manylinux_2_28_aarch64.whl", hash = "sha256:37a138589b12069efb424220bf78eac59ca68b95696fc622b6ccc1c0a197204a"},
-    {file = "cryptography-41.0.7-pp38-pypy38_pp73-manylinux_2_28_x86_64.whl", hash = "sha256:68a2dec79deebc5d26d617bfdf6e8aab065a4f34934b22d3b5010df3ba36612c"},
-    {file = "cryptography-41.0.7-pp38-pypy38_pp73-win_amd64.whl", hash = "sha256:09616eeaef406f99046553b8a40fbf8b1e70795a91885ba4c96a70793de5504a"},
-    {file = "cryptography-41.0.7-pp39-pypy39_pp73-macosx_10_12_x86_64.whl", hash = "sha256:48a0476626da912a44cc078f9893f292f0b3e4c739caf289268168d8f4702a39"},
-    {file = "cryptography-41.0.7-pp39-pypy39_pp73-manylinux_2_28_aarch64.whl", hash = "sha256:c7f3201ec47d5207841402594f1d7950879ef890c0c495052fa62f58283fde1a"},
-    {file = "cryptography-41.0.7-pp39-pypy39_pp73-manylinux_2_28_x86_64.whl", hash = "sha256:c5ca78485a255e03c32b513f8c2bc39fedb7f5c5f8535545bdc223a03b24f248"},
-    {file = "cryptography-41.0.7-pp39-pypy39_pp73-win_amd64.whl", hash = "sha256:d6c391c021ab1f7a82da5d8d0b3cee2f4b2c455ec86c8aebbc84837a631ff309"},
-    {file = "cryptography-41.0.7.tar.gz", hash = "sha256:13f93ce9bea8016c253b34afc6bd6a75993e5c40672ed5405a9c832f0d4a00bc"},
+    {file = "cryptography-42.0.5-cp37-abi3-macosx_10_12_universal2.whl", hash = "sha256:a30596bae9403a342c978fb47d9b0ee277699fa53bbafad14706af51fe543d16"},
+    {file = "cryptography-42.0.5-cp37-abi3-macosx_10_12_x86_64.whl", hash = "sha256:b7ffe927ee6531c78f81aa17e684e2ff617daeba7f189f911065b2ea2d526dec"},
+    {file = "cryptography-42.0.5-cp37-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:2424ff4c4ac7f6b8177b53c17ed5d8fa74ae5955656867f5a8affaca36a27abb"},
+    {file = "cryptography-42.0.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:329906dcc7b20ff3cad13c069a78124ed8247adcac44b10bea1130e36caae0b4"},
+    {file = "cryptography-42.0.5-cp37-abi3-manylinux_2_28_aarch64.whl", hash = "sha256:b03c2ae5d2f0fc05f9a2c0c997e1bc18c8229f392234e8a0194f202169ccd278"},
+    {file = "cryptography-42.0.5-cp37-abi3-manylinux_2_28_x86_64.whl", hash = "sha256:f8837fe1d6ac4a8052a9a8ddab256bc006242696f03368a4009be7ee3075cdb7"},
+    {file = "cryptography-42.0.5-cp37-abi3-musllinux_1_1_aarch64.whl", hash = "sha256:0270572b8bd2c833c3981724b8ee9747b3ec96f699a9665470018594301439ee"},
+    {file = "cryptography-42.0.5-cp37-abi3-musllinux_1_1_x86_64.whl", hash = "sha256:b8cac287fafc4ad485b8a9b67d0ee80c66bf3574f655d3b97ef2e1082360faf1"},
+    {file = "cryptography-42.0.5-cp37-abi3-musllinux_1_2_aarch64.whl", hash = "sha256:16a48c23a62a2f4a285699dba2e4ff2d1cff3115b9df052cdd976a18856d8e3d"},
+    {file = "cryptography-42.0.5-cp37-abi3-musllinux_1_2_x86_64.whl", hash = "sha256:2bce03af1ce5a5567ab89bd90d11e7bbdff56b8af3acbbec1faded8f44cb06da"},
+    {file = "cryptography-42.0.5-cp37-abi3-win32.whl", hash = "sha256:b6cd2203306b63e41acdf39aa93b86fb566049aeb6dc489b70e34bcd07adca74"},
+    {file = "cryptography-42.0.5-cp37-abi3-win_amd64.whl", hash = "sha256:98d8dc6d012b82287f2c3d26ce1d2dd130ec200c8679b6213b3c73c08b2b7940"},
+    {file = "cryptography-42.0.5-cp39-abi3-macosx_10_12_universal2.whl", hash = "sha256:5e6275c09d2badf57aea3afa80d975444f4be8d3bc58f7f80d2a484c6f9485c8"},
+    {file = "cryptography-42.0.5-cp39-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:e4985a790f921508f36f81831817cbc03b102d643b5fcb81cd33df3fa291a1a1"},
+    {file = "cryptography-42.0.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:7cde5f38e614f55e28d831754e8a3bacf9ace5d1566235e39d91b35502d6936e"},
+    {file = "cryptography-42.0.5-cp39-abi3-manylinux_2_28_aarch64.whl", hash = "sha256:7367d7b2eca6513681127ebad53b2582911d1736dc2ffc19f2c3ae49997496bc"},
+    {file = "cryptography-42.0.5-cp39-abi3-manylinux_2_28_x86_64.whl", hash = "sha256:cd2030f6650c089aeb304cf093f3244d34745ce0cfcc39f20c6fbfe030102e2a"},
+    {file = "cryptography-42.0.5-cp39-abi3-musllinux_1_1_aarch64.whl", hash = "sha256:a2913c5375154b6ef2e91c10b5720ea6e21007412f6437504ffea2109b5a33d7"},
+    {file = "cryptography-42.0.5-cp39-abi3-musllinux_1_1_x86_64.whl", hash = "sha256:c41fb5e6a5fe9ebcd58ca3abfeb51dffb5d83d6775405305bfa8715b76521922"},
+    {file = "cryptography-42.0.5-cp39-abi3-musllinux_1_2_aarch64.whl", hash = "sha256:3eaafe47ec0d0ffcc9349e1708be2aaea4c6dd4978d76bf6eb0cb2c13636c6fc"},
+    {file = "cryptography-42.0.5-cp39-abi3-musllinux_1_2_x86_64.whl", hash = "sha256:1b95b98b0d2af784078fa69f637135e3c317091b615cd0905f8b8a087e86fa30"},
+    {file = "cryptography-42.0.5-cp39-abi3-win32.whl", hash = "sha256:1f71c10d1e88467126f0efd484bd44bca5e14c664ec2ede64c32f20875c0d413"},
+    {file = "cryptography-42.0.5-cp39-abi3-win_amd64.whl", hash = "sha256:a011a644f6d7d03736214d38832e030d8268bcff4a41f728e6030325fea3e400"},
+    {file = "cryptography-42.0.5-pp310-pypy310_pp73-macosx_10_12_x86_64.whl", hash = "sha256:9481ffe3cf013b71b2428b905c4f7a9a4f76ec03065b05ff499bb5682a8d9ad8"},
+    {file = "cryptography-42.0.5-pp310-pypy310_pp73-manylinux_2_28_aarch64.whl", hash = "sha256:ba334e6e4b1d92442b75ddacc615c5476d4ad55cc29b15d590cc6b86efa487e2"},
+    {file = "cryptography-42.0.5-pp310-pypy310_pp73-manylinux_2_28_x86_64.whl", hash = "sha256:ba3e4a42397c25b7ff88cdec6e2a16c2be18720f317506ee25210f6d31925f9c"},
+    {file = "cryptography-42.0.5-pp310-pypy310_pp73-win_amd64.whl", hash = "sha256:111a0d8553afcf8eb02a4fea6ca4f59d48ddb34497aa8706a6cf536f1a5ec576"},
+    {file = "cryptography-42.0.5-pp39-pypy39_pp73-macosx_10_12_x86_64.whl", hash = "sha256:cd65d75953847815962c84a4654a84850b2bb4aed3f26fadcc1c13892e1e29f6"},
+    {file = "cryptography-42.0.5-pp39-pypy39_pp73-manylinux_2_28_aarch64.whl", hash = "sha256:e807b3188f9eb0eaa7bbb579b462c5ace579f1cedb28107ce8b48a9f7ad3679e"},
+    {file = "cryptography-42.0.5-pp39-pypy39_pp73-manylinux_2_28_x86_64.whl", hash = "sha256:f12764b8fffc7a123f641d7d049d382b73f96a34117e0b637b80643169cec8ac"},
+    {file = "cryptography-42.0.5-pp39-pypy39_pp73-win_amd64.whl", hash = "sha256:37dd623507659e08be98eec89323469e8c7b4c1407c85112634ae3dbdb926fdd"},
+    {file = "cryptography-42.0.5.tar.gz", hash = "sha256:6fe07eec95dfd477eb9530aef5bead34fec819b3aaf6c5bd6d20565da607bfe1"},
 ]
 
 [[package]]
 name = "cssutils"
-version = "2.9.0"
+version = "2.10.2"
 requires_python = ">=3.8"
 summary = "A CSS Cascading Style Sheets library for Python"
+groups = ["doc"]
 files = [
-    {file = "cssutils-2.9.0-py3-none-any.whl", hash = "sha256:f8b013169e281c0c6083207366c5005f5dd4549055f7aba840384fb06a78745c"},
-    {file = "cssutils-2.9.0.tar.gz", hash = "sha256:89477b3d17d790e97b9fb4def708767061055795aae6f7c82ae32e967c9be4cd"},
+    {file = "cssutils-2.10.2-py3-none-any.whl", hash = "sha256:4ad7d2f29270b22cf199f65a6b5e795f2c3130f3b9fb50c3d45e5054ef86e41a"},
+    {file = "cssutils-2.10.2.tar.gz", hash = "sha256:93cf92a350b1c123b17feff042e212f94d960975a3ed145743d84ebe8ccec7ab"},
 ]
 
 [[package]]
 name = "dict2css"
 version = "0.3.0.post1"
 requires_python = ">=3.6"
 summary = "A μ-library for constructing cascading style sheets from Python dictionaries."
+groups = ["doc"]
 dependencies = [
     "cssutils>=2.2.0",
     "domdf-python-tools>=2.2.0",
 ]
 files = [
     {file = "dict2css-0.3.0.post1-py3-none-any.whl", hash = "sha256:f006a6b774c3e31869015122ae82c491fd25e7de4a75607a62aa3e798f837e0d"},
     {file = "dict2css-0.3.0.post1.tar.gz", hash = "sha256:89c544c21c4ca7472c3fffb9d37d3d926f606329afdb751dc1de67a411b70719"},
 ]
 
 [[package]]
 name = "distlib"
-version = "0.3.7"
+version = "0.3.8"
 summary = "Distribution utilities"
+groups = ["dev", "pre-commit", "tox"]
 files = [
-    {file = "distlib-0.3.7-py2.py3-none-any.whl", hash = "sha256:2e24928bc811348f0feb63014e97aaae3037f2cf48712d51ae61df7fd6075057"},
-    {file = "distlib-0.3.7.tar.gz", hash = "sha256:9dafe54b34a028eafd95039d5e5d4851a13734540f1331060d31c9916e7147a8"},
+    {file = "distlib-0.3.8-py2.py3-none-any.whl", hash = "sha256:034db59a0b96f8ca18035f36290806a9a6e6bd9d1ff91e45a7f172eb17e51784"},
+    {file = "distlib-0.3.8.tar.gz", hash = "sha256:1530ea13e350031b6312d8580ddb6b27a104275a31106523b8f123787f494f64"},
+]
+
+[[package]]
+name = "docker"
+version = "7.0.0"
+requires_python = ">=3.8"
+summary = "A Python library for the Docker Engine API."
+groups = ["benchmark-servers"]
+dependencies = [
+    "packaging>=14.0",
+    "pywin32>=304; sys_platform == \"win32\"",
+    "requests>=2.26.0",
+    "urllib3>=1.26.0",
+]
+files = [
+    {file = "docker-7.0.0-py3-none-any.whl", hash = "sha256:12ba681f2777a0ad28ffbcc846a69c31b4dfd9752b47eb425a274ee269c5e14b"},
+    {file = "docker-7.0.0.tar.gz", hash = "sha256:323736fb92cd9418fc5e7133bc953e11a9da04f4483f828b527db553f1e7e5a3"},
 ]
 
 [[package]]
 name = "docutils"
-version = "0.18.1"
-requires_python = ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*"
+version = "0.19"
+requires_python = ">=3.7"
 summary = "Docutils -- Python Documentation Utilities"
+groups = ["doc"]
 files = [
-    {file = "docutils-0.18.1-py2.py3-none-any.whl", hash = "sha256:23010f129180089fbcd3bc08cfefccb3b890b0050e1ca00c867036e9d161b98c"},
-    {file = "docutils-0.18.1.tar.gz", hash = "sha256:679987caf361a7539d76e584cbeddc311e3aee937877c87346f31debc63e9d06"},
+    {file = "docutils-0.19-py3-none-any.whl", hash = "sha256:5e1de4d849fee02c63b040a4a3fd567f4ab104defd8a5511fbbc24a8a017efbc"},
+    {file = "docutils-0.19.tar.gz", hash = "sha256:33995a6753c30b7f577febfc2c50411fec6aac7f7ffeb7c4cfe5991072dcf9e6"},
 ]
 
 [[package]]
 name = "domdf-python-tools"
-version = "3.7.0"
+version = "3.8.0.post2"
 requires_python = ">=3.6"
 summary = "Helpful functions for Python 🐍 🛠️"
+groups = ["doc"]
 dependencies = [
     "natsort>=7.0.1",
     "typing-extensions>=3.7.4.1",
 ]
 files = [
-    {file = "domdf_python_tools-3.7.0-py3-none-any.whl", hash = "sha256:7b4d1c3bdb7402b872d43953824bf921ae2e52f893adbe5c0052a21a6efa2fe4"},
-    {file = "domdf_python_tools-3.7.0.tar.gz", hash = "sha256:df1af9a91649af0fb2a4e7b3a4b0a0936e4f78389dd7280dd6fd2f53a339ca71"},
+    {file = "domdf_python_tools-3.8.0.post2-py3-none-any.whl", hash = "sha256:ad2c763c8d00850a7fa92ad95e9891a1918281ea25322c4dbb1734fd32f905dd"},
+    {file = "domdf_python_tools-3.8.0.post2.tar.gz", hash = "sha256:a1fd255ea29f767b08de462d2da39d360262304389227d980bc307ee8aa3366a"},
 ]
 
 [[package]]
 name = "editables"
 version = "0.5"
 requires_python = ">=3.7"
 summary = "Editable installations"
+groups = ["dev"]
 files = [
     {file = "editables-0.5-py3-none-any.whl", hash = "sha256:61e5ffa82629e0d8bfe09bc44a07db3c1ab8ed1ce78a6980732870f19b5e7d4c"},
     {file = "editables-0.5.tar.gz", hash = "sha256:309627d9b5c4adc0e668d8c6fa7bac1ba7c8c5d415c2d27f60f081f8e80d1de2"},
 ]
 
 [[package]]
 name = "enum-tools"
-version = "0.11.0"
+version = "0.12.0"
 requires_python = ">=3.6"
 summary = "Tools to expand Python's enum module."
+groups = ["doc"]
 dependencies = [
     "pygments>=2.6.1",
     "typing-extensions>=3.7.4.3",
 ]
 files = [
-    {file = "enum_tools-0.11.0-py3-none-any.whl", hash = "sha256:9e76186ff4fd1798a64a855d334e245a7d2b67970c40029acccff06c58bf0535"},
-    {file = "enum_tools-0.11.0.tar.gz", hash = "sha256:ed10ae4c2109c52e6ca17505a3bdb173b2554f5f0449677621829023a9d8bd33"},
+    {file = "enum_tools-0.12.0-py3-none-any.whl", hash = "sha256:d69b019f193c7b850b17d9ce18440db7ed62381571409af80ccc08c5218b340a"},
+    {file = "enum_tools-0.12.0.tar.gz", hash = "sha256:13ceb9376a4c5f574a1e7c5f9c8eb7f3d3fbfbb361cc18a738df1a58dfefd460"},
 ]
 
 [[package]]
 name = "enum-tools"
-version = "0.11.0"
+version = "0.12.0"
 extras = ["sphinx"]
 requires_python = ">=3.6"
 summary = "Tools to expand Python's enum module."
+groups = ["doc"]
 dependencies = [
-    "enum-tools==0.11.0",
+    "enum-tools==0.12.0",
     "sphinx-jinja2-compat>=0.1.1",
     "sphinx-toolbox>=2.16.0",
     "sphinx>=3.4.0",
 ]
 files = [
-    {file = "enum_tools-0.11.0-py3-none-any.whl", hash = "sha256:9e76186ff4fd1798a64a855d334e245a7d2b67970c40029acccff06c58bf0535"},
-    {file = "enum_tools-0.11.0.tar.gz", hash = "sha256:ed10ae4c2109c52e6ca17505a3bdb173b2554f5f0449677621829023a9d8bd33"},
+    {file = "enum_tools-0.12.0-py3-none-any.whl", hash = "sha256:d69b019f193c7b850b17d9ce18440db7ed62381571409af80ccc08c5218b340a"},
+    {file = "enum_tools-0.12.0.tar.gz", hash = "sha256:13ceb9376a4c5f574a1e7c5f9c8eb7f3d3fbfbb361cc18a738df1a58dfefd460"},
 ]
 
 [[package]]
 name = "execnet"
-version = "2.0.2"
-requires_python = ">=3.7"
+version = "2.1.0"
+requires_python = ">=3.8"
 summary = "execnet: rapid multi-Python deployment"
+groups = ["test"]
 files = [
-    {file = "execnet-2.0.2-py3-none-any.whl", hash = "sha256:88256416ae766bc9e8895c76a87928c0012183da3cc4fc18016e6f050e025f41"},
-    {file = "execnet-2.0.2.tar.gz", hash = "sha256:cc59bc4423742fd71ad227122eb0dd44db51efb3dc4095b45ac9a08c770096af"},
+    {file = "execnet-2.1.0-py3-none-any.whl", hash = "sha256:ad174d7705410adc9359ba4822bad211d71cdbd59ff70304e1aa41d196b4b4d3"},
+    {file = "execnet-2.1.0.tar.gz", hash = "sha256:dc4a63f86afb40f8429f59f938d6cb97846f9e7cf7dd9eb4b8c26016965b7ac0"},
 ]
 
 [[package]]
 name = "filelock"
-version = "3.13.1"
+version = "3.13.3"
 requires_python = ">=3.8"
 summary = "A platform independent file lock."
+groups = ["dev", "doc", "pre-commit", "tox"]
 files = [
-    {file = "filelock-3.13.1-py3-none-any.whl", hash = "sha256:57dbda9b35157b05fb3e58ee91448612eb674172fab98ee235ccb0b5bee19a1c"},
-    {file = "filelock-3.13.1.tar.gz", hash = "sha256:521f5f56c50f8426f5e03ad3b281b490a87ef15bc6c526f168290f0c7148d44e"},
+    {file = "filelock-3.13.3-py3-none-any.whl", hash = "sha256:5ffa845303983e7a0b7ae17636509bc97997d58afeafa72fb141a17b152284cb"},
+    {file = "filelock-3.13.3.tar.gz", hash = "sha256:a79895a25bbefdf55d1a2a0a80968f7dbb28edcd6d4234a0afb3f37ecde4b546"},
 ]
 
 [[package]]
 name = "flake8"
-version = "6.1.0"
+version = "7.0.0"
 requires_python = ">=3.8.1"
 summary = "the modular source code checker: pep8 pyflakes and co"
+groups = ["flake8"]
 dependencies = [
     "mccabe<0.8.0,>=0.7.0",
     "pycodestyle<2.12.0,>=2.11.0",
-    "pyflakes<3.2.0,>=3.1.0",
+    "pyflakes<3.3.0,>=3.2.0",
 ]
 files = [
-    {file = "flake8-6.1.0-py2.py3-none-any.whl", hash = "sha256:ffdfce58ea94c6580c77888a86506937f9a1a227dfcd15f245d694ae20a6b6e5"},
-    {file = "flake8-6.1.0.tar.gz", hash = "sha256:d5b3857f07c030bdb5bf41c7f53799571d75c4491748a3adcd47de929e34cd23"},
+    {file = "flake8-7.0.0-py2.py3-none-any.whl", hash = "sha256:a6dfbb75e03252917f2473ea9653f7cd799c3064e54d4c8140044c5c065f53c3"},
+    {file = "flake8-7.0.0.tar.gz", hash = "sha256:33f96621059e65eec474169085dc92bf26e7b2d47366b70be2f67ab80dc25132"},
 ]
 
 [[package]]
 name = "flake8-dunder-all"
 version = "0.1.0"
 requires_python = ">=3.10"
 git = "https://github.com/francis-clairicia/flake8-dunder-all.git"
 ref = "v0.1.0"
 revision = "df321969b1a332fe4ecb8958f9994abcbb8d06bc"
 summary = "flake8 plugin dealing with modules' __all__"
+groups = ["flake8"]
 dependencies = [
     "flake8>=6.0.0",
 ]
 
 [[package]]
-name = "flake8-pyi"
-version = "23.11.0"
-requires_python = ">=3.8"
-summary = "A plugin for flake8 to enable linting .pyi stub files."
-dependencies = [
-    "flake8<7.0.0,>=6.0.0",
-    "pyflakes>=2.1.1",
-]
-files = [
-    {file = "flake8_pyi-23.11.0-py3-none-any.whl", hash = "sha256:4d36905006f65116264cf44be4ad2a75d0fd03d2d50e747af5c9f872c4d596de"},
-    {file = "flake8_pyi-23.11.0.tar.gz", hash = "sha256:93de2acdd34db508c87e7b5c5b431108e15dfd349a162ab194b9ac49f3b99103"},
-]
-
-[[package]]
-name = "gitdb"
-version = "4.0.11"
-requires_python = ">=3.7"
-summary = "Git Object Database"
-dependencies = [
-    "smmap<6,>=3.0.1",
-]
-files = [
-    {file = "gitdb-4.0.11-py3-none-any.whl", hash = "sha256:81a3407ddd2ee8df444cbacea00e2d038e40150acfa3001696fe0dcf1d3adfa4"},
-    {file = "gitdb-4.0.11.tar.gz", hash = "sha256:bf5421126136d6d0af55bc1e7c1af1c397a34f5b7bd79e776cd3e89785c2b04b"},
-]
-
-[[package]]
-name = "gitpython"
-version = "3.1.40"
-requires_python = ">=3.7"
-summary = "GitPython is a Python library used to interact with Git repositories"
-dependencies = [
-    "gitdb<5,>=4.0.1",
-]
-files = [
-    {file = "GitPython-3.1.40-py3-none-any.whl", hash = "sha256:cf14627d5a8049ffbf49915732e5eddbe8134c3bdb9d476e6182b676fc573f8a"},
-    {file = "GitPython-3.1.40.tar.gz", hash = "sha256:22b126e9ffb671fdd0c129796343a02bf67bf2994b35449ffc9321aa755e18a4"},
-]
-
-[[package]]
 name = "h11"
 version = "0.14.0"
 requires_python = ">=3.7"
 summary = "A pure-Python, bring-your-own-I/O implementation of HTTP/1.1"
+groups = ["dev"]
 files = [
     {file = "h11-0.14.0-py3-none-any.whl", hash = "sha256:e3fe4ac4b851c468cc8363d500db52c2ead036020723024a109d37346efaa761"},
     {file = "h11-0.14.0.tar.gz", hash = "sha256:8f19fbbe99e72420ff35c00b27a34cb9937e902a8b810e2c88300c6f0a3b699d"},
 ]
 
 [[package]]
 name = "hatch"
-version = "1.7.0"
-requires_python = ">=3.7"
+version = "1.9.4"
+requires_python = ">=3.8"
 summary = "Modern, extensible Python project management"
+groups = ["dev"]
 dependencies = [
-    "click>=8.0.3",
-    "hatchling>=1.14.0",
+    "click>=8.0.6",
+    "hatchling<1.22",
     "httpx>=0.22.0",
     "hyperlink>=21.0.0",
     "keyring>=23.5.0",
     "packaging>=21.3",
     "pexpect~=4.8",
     "platformdirs>=2.5.0",
-    "pyperclip>=1.8.2",
     "rich>=11.2.0",
     "shellingham>=1.4.0",
     "tomli-w>=1.0",
     "tomlkit>=0.11.1",
     "userpath~=1.7",
     "virtualenv>=20.16.2",
+    "zstandard<1",
 ]
 files = [
-    {file = "hatch-1.7.0-py3-none-any.whl", hash = "sha256:efc84112fd02ca85b7bab54f5e2ef71393a98dc849eac9aca390504031f8a1a8"},
-    {file = "hatch-1.7.0.tar.gz", hash = "sha256:7afc701fd5b33684a6650e1ecab8957e19685f824240ba7458dcacd66f90fb46"},
+    {file = "hatch-1.9.4-py3-none-any.whl", hash = "sha256:461eb86b4b46249e38a9a621c7239e61285fd8e14b5a1b5a727c394893a25300"},
+    {file = "hatch-1.9.4.tar.gz", hash = "sha256:9bb7d1c4a7a51cc1f9e16394875c940b45fa84b698f0291529316b27d74e7f32"},
 ]
 
 [[package]]
 name = "hatch-vcs"
 version = "0.4.0"
 requires_python = ">=3.8"
 summary = "Hatch plugin for versioning with your preferred VCS"
+groups = ["dev"]
 dependencies = [
     "hatchling>=1.1.0",
     "setuptools-scm>=6.4.0",
 ]
 files = [
     {file = "hatch_vcs-0.4.0-py3-none-any.whl", hash = "sha256:b8a2b6bee54cf6f9fc93762db73890017ae59c9081d1038a41f16235ceaf8b2c"},
     {file = "hatch_vcs-0.4.0.tar.gz", hash = "sha256:093810748fe01db0d451fabcf2c1ac2688caefd232d4ede967090b1c1b07d9f7"},
 ]
 
 [[package]]
 name = "hatchling"
-version = "1.18.0"
+version = "1.21.1"
 requires_python = ">=3.8"
 summary = "Modern, extensible Python build backend"
+groups = ["dev"]
 dependencies = [
     "editables>=0.3",
     "packaging>=21.3",
     "pathspec>=0.10.1",
     "pluggy>=1.0.0",
     "trove-classifiers",
 ]
 files = [
-    {file = "hatchling-1.18.0-py3-none-any.whl", hash = "sha256:b66dc254931ec42aa68b5febd1d342c58142cc5267b7ff3b12ba3fa5b4900c93"},
-    {file = "hatchling-1.18.0.tar.gz", hash = "sha256:50e99c3110ce0afc3f7bdbadff1c71c17758e476731c27607940cfa6686489ca"},
+    {file = "hatchling-1.21.1-py3-none-any.whl", hash = "sha256:21e8c13f8458b219a91cb84e5b61c15bf786695d1c4fabc29e91e78f94bfe892"},
+    {file = "hatchling-1.21.1.tar.gz", hash = "sha256:bba440453a224e7d4478457fa2e8d8c3633765bafa02975a6b53b9bf917980bc"},
 ]
 
 [[package]]
 name = "html5lib"
 version = "1.1"
 requires_python = ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*"
 summary = "HTML parser based on the WHATWG HTML specification"
+groups = ["doc"]
 dependencies = [
     "six>=1.9",
     "webencodings",
 ]
 files = [
     {file = "html5lib-1.1-py2.py3-none-any.whl", hash = "sha256:0d78f8fde1c230e99fe37986a60526d7049ed4bf8a9fadbad5f00e22e58e041d"},
     {file = "html5lib-1.1.tar.gz", hash = "sha256:b2e5b40261e20f354d198eae92afc10d750afb487ed5e50f9c4eaf07c184146f"},
 ]
 
 [[package]]
 name = "httpcore"
-version = "1.0.2"
+version = "1.0.5"
 requires_python = ">=3.8"
 summary = "A minimal low-level HTTP client."
+groups = ["dev"]
 dependencies = [
     "certifi",
     "h11<0.15,>=0.13",
 ]
 files = [
-    {file = "httpcore-1.0.2-py3-none-any.whl", hash = "sha256:096cc05bca73b8e459a1fc3dcf585148f63e534eae4339559c9b8a8d6399acc7"},
-    {file = "httpcore-1.0.2.tar.gz", hash = "sha256:9fc092e4799b26174648e54b74ed5f683132a464e95643b226e00c2ed2fa6535"},
+    {file = "httpcore-1.0.5-py3-none-any.whl", hash = "sha256:421f18bac248b25d310f3cacd198d55b8e6125c107797b609ff9b7a6ba7991b5"},
+    {file = "httpcore-1.0.5.tar.gz", hash = "sha256:34a38e2f9291467ee3b44e89dd52615370e152954ba21721378a87b2960f7a61"},
 ]
 
 [[package]]
 name = "httpx"
-version = "0.25.2"
+version = "0.27.0"
 requires_python = ">=3.8"
 summary = "The next generation HTTP client."
+groups = ["dev"]
 dependencies = [
     "anyio",
     "certifi",
     "httpcore==1.*",
     "idna",
     "sniffio",
 ]
 files = [
-    {file = "httpx-0.25.2-py3-none-any.whl", hash = "sha256:a05d3d052d9b2dfce0e3896636467f8a5342fb2b902c819428e1ac65413ca118"},
-    {file = "httpx-0.25.2.tar.gz", hash = "sha256:8b8fcaa0c8ea7b05edd69a094e63a2094c4efcb48129fb757361bc423c0ad9e8"},
+    {file = "httpx-0.27.0-py3-none-any.whl", hash = "sha256:71d5465162c13681bff01ad59b2cc68dd838ea1f10e51574bac27103f00c91a5"},
+    {file = "httpx-0.27.0.tar.gz", hash = "sha256:a0cb88a46f32dc874e04ee956e4c2764aba2aa228f650b06788ba6bda2962ab5"},
 ]
 
 [[package]]
 name = "hyperlink"
 version = "21.0.0"
 requires_python = ">=2.6, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*"
 summary = "A featureful, immutable, and correct URL for Python."
+groups = ["dev"]
 dependencies = [
     "idna>=2.5",
 ]
 files = [
     {file = "hyperlink-21.0.0-py2.py3-none-any.whl", hash = "sha256:e6b14c37ecb73e89c77d78cdb4c2cc8f3fb59a885c5b3f819ff4ed80f25af1b4"},
     {file = "hyperlink-21.0.0.tar.gz", hash = "sha256:427af957daa58bc909471c6c40f74c5450fa123dd093fc53efd2e91d2705a56b"},
 ]
 
 [[package]]
 name = "identify"
-version = "2.5.33"
+version = "2.5.35"
 requires_python = ">=3.8"
 summary = "File identification library for Python"
+groups = ["pre-commit"]
 files = [
-    {file = "identify-2.5.33-py2.py3-none-any.whl", hash = "sha256:d40ce5fcd762817627670da8a7d8d8e65f24342d14539c59488dc603bf662e34"},
-    {file = "identify-2.5.33.tar.gz", hash = "sha256:161558f9fe4559e1557e1bff323e8631f6a0e4837f7497767c1782832f16b62d"},
+    {file = "identify-2.5.35-py2.py3-none-any.whl", hash = "sha256:c4de0081837b211594f8e877a6b4fad7ca32bbfc1a9307fdd61c28bfe923f13e"},
+    {file = "identify-2.5.35.tar.gz", hash = "sha256:10a7ca245cfcd756a554a7288159f72ff105ad233c7c4b9c6f0f4d108f5f6791"},
 ]
 
 [[package]]
 name = "idna"
 version = "3.6"
 requires_python = ">=3.5"
 summary = "Internationalized Domain Names in Applications (IDNA)"
+groups = ["benchmark-servers", "dev", "doc", "test"]
 files = [
     {file = "idna-3.6-py3-none-any.whl", hash = "sha256:c05567e9c24a6b9faaa835c4821bad0590fbb9d5779e7caa6e1cc4978e7eb24f"},
     {file = "idna-3.6.tar.gz", hash = "sha256:9ecdbbd083b06798ae1e86adcbfe8ab1479cf864e4ee30fe4e46a003d12491ca"},
 ]
 
 [[package]]
 name = "imagesize"
 version = "1.4.1"
 requires_python = ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*"
 summary = "Getting image size from png/jpeg/jpeg2000/gif file"
+groups = ["doc"]
 files = [
     {file = "imagesize-1.4.1-py2.py3-none-any.whl", hash = "sha256:0d8d18d08f840c19d0ee7ca1fd82490fdc3729b7ac93f49870406ddde8ef8d8b"},
     {file = "imagesize-1.4.1.tar.gz", hash = "sha256:69150444affb9cb0d5cc5a92b3676f0b2fb7cd9ae39e947a5e11a36b4497cd4a"},
 ]
 
 [[package]]
 name = "importlib-metadata"
-version = "7.0.0"
+version = "7.1.0"
 requires_python = ">=3.8"
 summary = "Read metadata from Python packages"
+groups = ["dev", "micro-benchmark"]
 dependencies = [
     "zipp>=0.5",
 ]
 files = [
-    {file = "importlib_metadata-7.0.0-py3-none-any.whl", hash = "sha256:d97503976bb81f40a193d41ee6570868479c69d5068651eb039c40d850c59d67"},
-    {file = "importlib_metadata-7.0.0.tar.gz", hash = "sha256:7fc841f8b8332803464e5dc1c63a2e59121f46ca186c0e2e182e80bf8c1319f7"},
+    {file = "importlib_metadata-7.1.0-py3-none-any.whl", hash = "sha256:30962b96c0c223483ed6cc7280e7f0199feb01a0e40cfae4d4450fc6fab1f570"},
+    {file = "importlib_metadata-7.1.0.tar.gz", hash = "sha256:b78938b926ee8d5f020fc4772d487045805a55ddbad2ecf21c6d60938dc7fcd2"},
 ]
 
 [[package]]
 name = "iniconfig"
 version = "2.0.0"
 requires_python = ">=3.7"
 summary = "brain-dead simple config-ini parsing"
+groups = ["micro-benchmark", "test"]
 files = [
     {file = "iniconfig-2.0.0-py3-none-any.whl", hash = "sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374"},
     {file = "iniconfig-2.0.0.tar.gz", hash = "sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3"},
 ]
 
 [[package]]
 name = "isort"
-version = "5.12.0"
+version = "5.13.2"
 requires_python = ">=3.8.0"
 summary = "A Python utility / library to sort Python imports."
+groups = ["format"]
 files = [
-    {file = "isort-5.12.0-py3-none-any.whl", hash = "sha256:f84c2818376e66cf843d497486ea8fed8700b340f308f076c6fb1229dff318b6"},
-    {file = "isort-5.12.0.tar.gz", hash = "sha256:8bef7dde241278824a6d83f44a544709b065191b95b6e50894bdc722fcba0504"},
+    {file = "isort-5.13.2-py3-none-any.whl", hash = "sha256:8ca5e72a8d85860d5a3fa69b8745237f2939afe12dbf656afbcb47fe72d947a6"},
+    {file = "isort-5.13.2.tar.gz", hash = "sha256:48fdfcb9face5d58a4f6dde2e72a1fb8dcaf8ab26f95ab49fab84c2ddefb0109"},
 ]
 
 [[package]]
 name = "jaraco-classes"
-version = "3.3.0"
+version = "3.4.0"
 requires_python = ">=3.8"
 summary = "Utility functions for Python class constructs"
+groups = ["dev"]
+dependencies = [
+    "more-itertools",
+]
+files = [
+    {file = "jaraco.classes-3.4.0-py3-none-any.whl", hash = "sha256:f662826b6bed8cace05e7ff873ce0f9283b5c924470fe664fff1c2f00f581790"},
+    {file = "jaraco.classes-3.4.0.tar.gz", hash = "sha256:47a024b51d0239c0dd8c8540c6c7f484be3b8fcf0b2d85c13825780d3b3f3acd"},
+]
+
+[[package]]
+name = "jaraco-context"
+version = "5.1.0"
+requires_python = ">=3.8"
+summary = "Useful decorators and context managers"
+groups = ["dev"]
+files = [
+    {file = "jaraco.context-5.1.0-py3-none-any.whl", hash = "sha256:0e4161ebbaeead78850b4ca5465b5853217cf23ad74ec82d00ebfb69d8ea5fcb"},
+    {file = "jaraco.context-5.1.0.tar.gz", hash = "sha256:24ec1f739aec2c5766c68027ccc70d91d7b0cb931699442f5c7ed93515b955e7"},
+]
+
+[[package]]
+name = "jaraco-functools"
+version = "4.0.0"
+requires_python = ">=3.8"
+summary = "Functools like those found in stdlib"
+groups = ["dev"]
 dependencies = [
     "more-itertools",
 ]
 files = [
-    {file = "jaraco.classes-3.3.0-py3-none-any.whl", hash = "sha256:10afa92b6743f25c0cf5f37c6bb6e18e2c5bb84a16527ccfc0040ea377e7aaeb"},
-    {file = "jaraco.classes-3.3.0.tar.gz", hash = "sha256:c063dd08e89217cee02c8d5e5ec560f2c8ce6cdc2fcdc2e68f7b2e5547ed3621"},
+    {file = "jaraco.functools-4.0.0-py3-none-any.whl", hash = "sha256:daf276ddf234bea897ef14f43c4e1bf9eefeac7b7a82a4dd69228ac20acff68d"},
+    {file = "jaraco.functools-4.0.0.tar.gz", hash = "sha256:c279cb24c93d694ef7270f970d499cab4d3813f4e08273f95398651a634f0925"},
 ]
 
 [[package]]
 name = "jeepney"
 version = "0.8.0"
 requires_python = ">=3.7"
 summary = "Low-level, pure Python DBus protocol wrapper."
+groups = ["dev"]
+marker = "sys_platform == \"linux\""
 files = [
     {file = "jeepney-0.8.0-py3-none-any.whl", hash = "sha256:c0a454ad016ca575060802ee4d590dd912e35c122fa04e70306de3d076cce755"},
     {file = "jeepney-0.8.0.tar.gz", hash = "sha256:5efe48d255973902f6badc3ce55e2aa6c5c3b3bc642059ef3a91247bcfcc5806"},
 ]
 
 [[package]]
 name = "jinja2"
-version = "3.1.2"
+version = "3.1.3"
 requires_python = ">=3.7"
 summary = "A very fast and expressive template engine."
+groups = ["doc"]
 dependencies = [
     "MarkupSafe>=2.0",
 ]
 files = [
-    {file = "Jinja2-3.1.2-py3-none-any.whl", hash = "sha256:6088930bfe239f0e6710546ab9c19c9ef35e29792895fed6e6e31a023a182a61"},
-    {file = "Jinja2-3.1.2.tar.gz", hash = "sha256:31351a702a408a9e7595a8fc6150fc3f43bb6bf7e319770cbc0db9df9437e852"},
+    {file = "Jinja2-3.1.3-py3-none-any.whl", hash = "sha256:7d6d50dd97d52cbc355597bd845fabfbac3f551e1f99619e39a35ce8c370b5fa"},
+    {file = "Jinja2-3.1.3.tar.gz", hash = "sha256:ac8bd6544d4bb2c9792bf3a159e80bba8fda7f07e81bc3aed565432d5925ba90"},
 ]
 
 [[package]]
 name = "keyring"
-version = "24.3.0"
+version = "25.1.0"
 requires_python = ">=3.8"
 summary = "Store and access your passwords safely."
+groups = ["dev"]
 dependencies = [
     "SecretStorage>=3.2; sys_platform == \"linux\"",
     "importlib-metadata>=4.11.4; python_version < \"3.12\"",
     "jaraco-classes",
+    "jaraco-context",
+    "jaraco-functools",
     "jeepney>=0.4.2; sys_platform == \"linux\"",
     "pywin32-ctypes>=0.2.0; sys_platform == \"win32\"",
 ]
 files = [
-    {file = "keyring-24.3.0-py3-none-any.whl", hash = "sha256:4446d35d636e6a10b8bce7caa66913dd9eca5fd222ca03a3d42c38608ac30836"},
-    {file = "keyring-24.3.0.tar.gz", hash = "sha256:e730ecffd309658a08ee82535a3b5ec4b4c8669a9be11efb66249d8e0aeb9a25"},
+    {file = "keyring-25.1.0-py3-none-any.whl", hash = "sha256:26fc12e6a329d61d24aa47b22a7c5c3f35753df7d8f2860973cf94f4e1fb3427"},
+    {file = "keyring-25.1.0.tar.gz", hash = "sha256:7230ea690525133f6ad536a9b5def74a4bd52642abe594761028fc044d7c7893"},
 ]
 
 [[package]]
 name = "markdown-it-py"
 version = "3.0.0"
 requires_python = ">=3.8"
 summary = "Python port of markdown-it. Markdown parsing, done right!"
+groups = ["bandit", "dev"]
 dependencies = [
     "mdurl~=0.1",
 ]
 files = [
     {file = "markdown-it-py-3.0.0.tar.gz", hash = "sha256:e3f60a94fa066dc52ec76661e37c851cb232d92f9886b15cb560aaada2df8feb"},
     {file = "markdown_it_py-3.0.0-py3-none-any.whl", hash = "sha256:355216845c60bd96232cd8d8c40e8f9765cc86f46880e43a8fd22dc1a1a8cab1"},
 ]
 
 [[package]]
 name = "markupsafe"
-version = "2.1.3"
+version = "2.1.5"
 requires_python = ">=3.7"
 summary = "Safely add untrusted strings to HTML/XML markup."
+groups = ["doc"]
 files = [
-    {file = "MarkupSafe-2.1.3-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:ad9e82fb8f09ade1c3e1b996a6337afac2b8b9e365f926f5a61aacc71adc5b3c"},
-    {file = "MarkupSafe-2.1.3-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:3c0fae6c3be832a0a0473ac912810b2877c8cb9d76ca48de1ed31e1c68386575"},
-    {file = "MarkupSafe-2.1.3-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:b076b6226fb84157e3f7c971a47ff3a679d837cf338547532ab866c57930dbee"},
-    {file = "MarkupSafe-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:bfce63a9e7834b12b87c64d6b155fdd9b3b96191b6bd334bf37db7ff1fe457f2"},
-    {file = "MarkupSafe-2.1.3-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:338ae27d6b8745585f87218a3f23f1512dbf52c26c28e322dbe54bcede54ccb9"},
-    {file = "MarkupSafe-2.1.3-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:e4dd52d80b8c83fdce44e12478ad2e85c64ea965e75d66dbeafb0a3e77308fcc"},
-    {file = "MarkupSafe-2.1.3-cp311-cp311-musllinux_1_1_i686.whl", hash = "sha256:df0be2b576a7abbf737b1575f048c23fb1d769f267ec4358296f31c2479db8f9"},
-    {file = "MarkupSafe-2.1.3-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:5bbe06f8eeafd38e5d0a4894ffec89378b6c6a625ff57e3028921f8ff59318ac"},
-    {file = "MarkupSafe-2.1.3-cp311-cp311-win32.whl", hash = "sha256:dd15ff04ffd7e05ffcb7fe79f1b98041b8ea30ae9234aed2a9168b5797c3effb"},
-    {file = "MarkupSafe-2.1.3-cp311-cp311-win_amd64.whl", hash = "sha256:134da1eca9ec0ae528110ccc9e48041e0828d79f24121a1a146161103c76e686"},
-    {file = "MarkupSafe-2.1.3-cp312-cp312-macosx_10_9_universal2.whl", hash = "sha256:f698de3fd0c4e6972b92290a45bd9b1536bffe8c6759c62471efaa8acb4c37bc"},
-    {file = "MarkupSafe-2.1.3-cp312-cp312-macosx_10_9_x86_64.whl", hash = "sha256:aa57bd9cf8ae831a362185ee444e15a93ecb2e344c8e52e4d721ea3ab6ef1823"},
-    {file = "MarkupSafe-2.1.3-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:ffcc3f7c66b5f5b7931a5aa68fc9cecc51e685ef90282f4a82f0f5e9b704ad11"},
-    {file = "MarkupSafe-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:47d4f1c5f80fc62fdd7777d0d40a2e9dda0a05883ab11374334f6c4de38adffd"},
-    {file = "MarkupSafe-2.1.3-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:1f67c7038d560d92149c060157d623c542173016c4babc0c1913cca0564b9939"},
-    {file = "MarkupSafe-2.1.3-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:9aad3c1755095ce347e26488214ef77e0485a3c34a50c5a5e2471dff60b9dd9c"},
-    {file = "MarkupSafe-2.1.3-cp312-cp312-musllinux_1_1_i686.whl", hash = "sha256:14ff806850827afd6b07a5f32bd917fb7f45b046ba40c57abdb636674a8b559c"},
-    {file = "MarkupSafe-2.1.3-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:8f9293864fe09b8149f0cc42ce56e3f0e54de883a9de90cd427f191c346eb2e1"},
-    {file = "MarkupSafe-2.1.3-cp312-cp312-win32.whl", hash = "sha256:715d3562f79d540f251b99ebd6d8baa547118974341db04f5ad06d5ea3eb8007"},
-    {file = "MarkupSafe-2.1.3-cp312-cp312-win_amd64.whl", hash = "sha256:1b8dd8c3fd14349433c79fa8abeb573a55fc0fdd769133baac1f5e07abf54aeb"},
-    {file = "MarkupSafe-2.1.3.tar.gz", hash = "sha256:af598ed32d6ae86f1b747b82783958b1a4ab8f617b06fe68795c7f026abbdcad"},
+    {file = "MarkupSafe-2.1.5-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:629ddd2ca402ae6dbedfceeba9c46d5f7b2a61d9749597d4307f943ef198fc1f"},
+    {file = "MarkupSafe-2.1.5-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:5b7b716f97b52c5a14bffdf688f971b2d5ef4029127f1ad7a513973cfd818df2"},
+    {file = "MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:6ec585f69cec0aa07d945b20805be741395e28ac1627333b1c5b0105962ffced"},
+    {file = "MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:b91c037585eba9095565a3556f611e3cbfaa42ca1e865f7b8015fe5c7336d5a5"},
+    {file = "MarkupSafe-2.1.5-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:7502934a33b54030eaf1194c21c692a534196063db72176b0c4028e140f8f32c"},
+    {file = "MarkupSafe-2.1.5-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:0e397ac966fdf721b2c528cf028494e86172b4feba51d65f81ffd65c63798f3f"},
+    {file = "MarkupSafe-2.1.5-cp311-cp311-musllinux_1_1_i686.whl", hash = "sha256:c061bb86a71b42465156a3ee7bd58c8c2ceacdbeb95d05a99893e08b8467359a"},
+    {file = "MarkupSafe-2.1.5-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:3a57fdd7ce31c7ff06cdfbf31dafa96cc533c21e443d57f5b1ecc6cdc668ec7f"},
+    {file = "MarkupSafe-2.1.5-cp311-cp311-win32.whl", hash = "sha256:397081c1a0bfb5124355710fe79478cdbeb39626492b15d399526ae53422b906"},
+    {file = "MarkupSafe-2.1.5-cp311-cp311-win_amd64.whl", hash = "sha256:2b7c57a4dfc4f16f7142221afe5ba4e093e09e728ca65c51f5620c9aaeb9a617"},
+    {file = "MarkupSafe-2.1.5-cp312-cp312-macosx_10_9_universal2.whl", hash = "sha256:8dec4936e9c3100156f8a2dc89c4b88d5c435175ff03413b443469c7c8c5f4d1"},
+    {file = "MarkupSafe-2.1.5-cp312-cp312-macosx_10_9_x86_64.whl", hash = "sha256:3c6b973f22eb18a789b1460b4b91bf04ae3f0c4234a0a6aa6b0a92f6f7b951d4"},
+    {file = "MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:ac07bad82163452a6884fe8fa0963fb98c2346ba78d779ec06bd7a6262132aee"},
+    {file = "MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:f5dfb42c4604dddc8e4305050aa6deb084540643ed5804d7455b5df8fe16f5e5"},
+    {file = "MarkupSafe-2.1.5-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:ea3d8a3d18833cf4304cd2fc9cbb1efe188ca9b5efef2bdac7adc20594a0e46b"},
+    {file = "MarkupSafe-2.1.5-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:d050b3361367a06d752db6ead6e7edeb0009be66bc3bae0ee9d97fb326badc2a"},
+    {file = "MarkupSafe-2.1.5-cp312-cp312-musllinux_1_1_i686.whl", hash = "sha256:bec0a414d016ac1a18862a519e54b2fd0fc8bbfd6890376898a6c0891dd82e9f"},
+    {file = "MarkupSafe-2.1.5-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:58c98fee265677f63a4385256a6d7683ab1832f3ddd1e66fe948d5880c21a169"},
+    {file = "MarkupSafe-2.1.5-cp312-cp312-win32.whl", hash = "sha256:8590b4ae07a35970728874632fed7bd57b26b0102df2d2b233b6d9d82f6c62ad"},
+    {file = "MarkupSafe-2.1.5-cp312-cp312-win_amd64.whl", hash = "sha256:823b65d8706e32ad2df51ed89496147a42a2a6e01c13cfb6ffb8b1e92bc910bb"},
+    {file = "MarkupSafe-2.1.5.tar.gz", hash = "sha256:d283d37a890ba4c1ae73ffadf8046435c76e7bc2247bbb63c00bd1a709c6544b"},
 ]
 
 [[package]]
 name = "mccabe"
 version = "0.7.0"
 requires_python = ">=3.6"
 summary = "McCabe checker, plugin for flake8"
+groups = ["flake8"]
 files = [
     {file = "mccabe-0.7.0-py2.py3-none-any.whl", hash = "sha256:6c2d30ab6be0e4a46919781807b4f0d834ebdd6c6e3dca0bda5a15f863427b6e"},
     {file = "mccabe-0.7.0.tar.gz", hash = "sha256:348e0240c33b60bbdf4e523192ef919f28cb2c3d7d5c7794f74009290f236325"},
 ]
 
 [[package]]
 name = "mdurl"
 version = "0.1.2"
 requires_python = ">=3.7"
 summary = "Markdown URL utilities"
+groups = ["bandit", "dev"]
 files = [
     {file = "mdurl-0.1.2-py3-none-any.whl", hash = "sha256:84008a41e51615a49fc9966191ff91509e3c40b939176e643fd50a5c2196b8f8"},
     {file = "mdurl-0.1.2.tar.gz", hash = "sha256:bb413d29f5eea38f31dd4754dd7377d4465116fb207585f97bf925588687c1ba"},
 ]
 
 [[package]]
 name = "more-itertools"
-version = "10.1.0"
+version = "10.2.0"
 requires_python = ">=3.8"
 summary = "More routines for operating on iterables, beyond itertools"
+groups = ["dev"]
 files = [
-    {file = "more-itertools-10.1.0.tar.gz", hash = "sha256:626c369fa0eb37bac0291bce8259b332fd59ac792fa5497b59837309cd5b114a"},
-    {file = "more_itertools-10.1.0-py3-none-any.whl", hash = "sha256:64e0735fcfdc6f3464ea133afe8ea4483b1c5fe3a3d69852e6503b43a0b222e6"},
+    {file = "more-itertools-10.2.0.tar.gz", hash = "sha256:8fccb480c43d3e99a00087634c06dd02b0d50fbf088b380de5a41a015ec239e1"},
+    {file = "more_itertools-10.2.0-py3-none-any.whl", hash = "sha256:686b06abe565edfab151cb8fd385a05651e1fdf8f0a14191e4439283421f8684"},
 ]
 
 [[package]]
 name = "msgpack"
-version = "1.0.7"
+version = "1.0.8"
 requires_python = ">=3.8"
 summary = "MessagePack serializer"
+groups = ["doc", "msgpack"]
 files = [
-    {file = "msgpack-1.0.7-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:576eb384292b139821c41995523654ad82d1916da6a60cff129c715a6223ea84"},
-    {file = "msgpack-1.0.7-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:730076207cb816138cf1af7f7237b208340a2c5e749707457d70705715c93b93"},
-    {file = "msgpack-1.0.7-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:85765fdf4b27eb5086f05ac0491090fc76f4f2b28e09d9350c31aac25a5aaff8"},
-    {file = "msgpack-1.0.7-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:3476fae43db72bd11f29a5147ae2f3cb22e2f1a91d575ef130d2bf49afd21c46"},
-    {file = "msgpack-1.0.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6d4c80667de2e36970ebf74f42d1088cc9ee7ef5f4e8c35eee1b40eafd33ca5b"},
-    {file = "msgpack-1.0.7-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:5b0bf0effb196ed76b7ad883848143427a73c355ae8e569fa538365064188b8e"},
-    {file = "msgpack-1.0.7-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:f9a7c509542db4eceed3dcf21ee5267ab565a83555c9b88a8109dcecc4709002"},
-    {file = "msgpack-1.0.7-cp311-cp311-musllinux_1_1_i686.whl", hash = "sha256:84b0daf226913133f899ea9b30618722d45feffa67e4fe867b0b5ae83a34060c"},
-    {file = "msgpack-1.0.7-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:ec79ff6159dffcc30853b2ad612ed572af86c92b5168aa3fc01a67b0fa40665e"},
-    {file = "msgpack-1.0.7-cp311-cp311-win32.whl", hash = "sha256:3e7bf4442b310ff154b7bb9d81eb2c016b7d597e364f97d72b1acc3817a0fdc1"},
-    {file = "msgpack-1.0.7-cp311-cp311-win_amd64.whl", hash = "sha256:3f0c8c6dfa6605ab8ff0611995ee30d4f9fcff89966cf562733b4008a3d60d82"},
-    {file = "msgpack-1.0.7-cp312-cp312-macosx_10_9_universal2.whl", hash = "sha256:f0936e08e0003f66bfd97e74ee530427707297b0d0361247e9b4f59ab78ddc8b"},
-    {file = "msgpack-1.0.7-cp312-cp312-macosx_10_9_x86_64.whl", hash = "sha256:98bbd754a422a0b123c66a4c341de0474cad4a5c10c164ceed6ea090f3563db4"},
-    {file = "msgpack-1.0.7-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:b291f0ee7961a597cbbcc77709374087fa2a9afe7bdb6a40dbbd9b127e79afee"},
-    {file = "msgpack-1.0.7-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:ebbbba226f0a108a7366bf4b59bf0f30a12fd5e75100c630267d94d7f0ad20e5"},
-    {file = "msgpack-1.0.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:1e2d69948e4132813b8d1131f29f9101bc2c915f26089a6d632001a5c1349672"},
-    {file = "msgpack-1.0.7-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:bdf38ba2d393c7911ae989c3bbba510ebbcdf4ecbdbfec36272abe350c454075"},
-    {file = "msgpack-1.0.7-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:993584fc821c58d5993521bfdcd31a4adf025c7d745bbd4d12ccfecf695af5ba"},
-    {file = "msgpack-1.0.7-cp312-cp312-musllinux_1_1_i686.whl", hash = "sha256:52700dc63a4676669b341ba33520f4d6e43d3ca58d422e22ba66d1736b0a6e4c"},
-    {file = "msgpack-1.0.7-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:e45ae4927759289c30ccba8d9fdce62bb414977ba158286b5ddaf8df2cddb5c5"},
-    {file = "msgpack-1.0.7-cp312-cp312-win32.whl", hash = "sha256:27dcd6f46a21c18fa5e5deed92a43d4554e3df8d8ca5a47bf0615d6a5f39dbc9"},
-    {file = "msgpack-1.0.7-cp312-cp312-win_amd64.whl", hash = "sha256:7687e22a31e976a0e7fc99c2f4d11ca45eff652a81eb8c8085e9609298916dcf"},
-    {file = "msgpack-1.0.7.tar.gz", hash = "sha256:572efc93db7a4d27e404501975ca6d2d9775705c2d922390d878fcf768d92c87"},
+    {file = "msgpack-1.0.8-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:9517004e21664f2b5a5fd6333b0731b9cf0817403a941b393d89a2f1dc2bd836"},
+    {file = "msgpack-1.0.8-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:d16a786905034e7e34098634b184a7d81f91d4c3d246edc6bd7aefb2fd8ea6ad"},
+    {file = "msgpack-1.0.8-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:e2872993e209f7ed04d963e4b4fbae72d034844ec66bc4ca403329db2074377b"},
+    {file = "msgpack-1.0.8-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:5c330eace3dd100bdb54b5653b966de7f51c26ec4a7d4e87132d9b4f738220ba"},
+    {file = "msgpack-1.0.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:83b5c044f3eff2a6534768ccfd50425939e7a8b5cf9a7261c385de1e20dcfc85"},
+    {file = "msgpack-1.0.8-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:1876b0b653a808fcd50123b953af170c535027bf1d053b59790eebb0aeb38950"},
+    {file = "msgpack-1.0.8-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:dfe1f0f0ed5785c187144c46a292b8c34c1295c01da12e10ccddfc16def4448a"},
+    {file = "msgpack-1.0.8-cp311-cp311-musllinux_1_1_i686.whl", hash = "sha256:3528807cbbb7f315bb81959d5961855e7ba52aa60a3097151cb21956fbc7502b"},
+    {file = "msgpack-1.0.8-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:e2f879ab92ce502a1e65fce390eab619774dda6a6ff719718069ac94084098ce"},
+    {file = "msgpack-1.0.8-cp311-cp311-win32.whl", hash = "sha256:26ee97a8261e6e35885c2ecd2fd4a6d38252246f94a2aec23665a4e66d066305"},
+    {file = "msgpack-1.0.8-cp311-cp311-win_amd64.whl", hash = "sha256:eadb9f826c138e6cf3c49d6f8de88225a3c0ab181a9b4ba792e006e5292d150e"},
+    {file = "msgpack-1.0.8-cp312-cp312-macosx_10_9_universal2.whl", hash = "sha256:114be227f5213ef8b215c22dde19532f5da9652e56e8ce969bf0a26d7c419fee"},
+    {file = "msgpack-1.0.8-cp312-cp312-macosx_10_9_x86_64.whl", hash = "sha256:d661dc4785affa9d0edfdd1e59ec056a58b3dbb9f196fa43587f3ddac654ac7b"},
+    {file = "msgpack-1.0.8-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:d56fd9f1f1cdc8227d7b7918f55091349741904d9520c65f0139a9755952c9e8"},
+    {file = "msgpack-1.0.8-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0726c282d188e204281ebd8de31724b7d749adebc086873a59efb8cf7ae27df3"},
+    {file = "msgpack-1.0.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:8db8e423192303ed77cff4dce3a4b88dbfaf43979d280181558af5e2c3c71afc"},
+    {file = "msgpack-1.0.8-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:99881222f4a8c2f641f25703963a5cefb076adffd959e0558dc9f803a52d6a58"},
+    {file = "msgpack-1.0.8-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:b5505774ea2a73a86ea176e8a9a4a7c8bf5d521050f0f6f8426afe798689243f"},
+    {file = "msgpack-1.0.8-cp312-cp312-musllinux_1_1_i686.whl", hash = "sha256:ef254a06bcea461e65ff0373d8a0dd1ed3aa004af48839f002a0c994a6f72d04"},
+    {file = "msgpack-1.0.8-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:e1dd7839443592d00e96db831eddb4111a2a81a46b028f0facd60a09ebbdd543"},
+    {file = "msgpack-1.0.8-cp312-cp312-win32.whl", hash = "sha256:64d0fcd436c5683fdd7c907eeae5e2cbb5eb872fafbc03a43609d7941840995c"},
+    {file = "msgpack-1.0.8-cp312-cp312-win_amd64.whl", hash = "sha256:74398a4cf19de42e1498368c36eed45d9528f5fd0155241e82c4082b7e16cffd"},
+    {file = "msgpack-1.0.8.tar.gz", hash = "sha256:95c02b0e27e706e48d0e5426d1710ca78e0f0628d6e89d5b5a5b91a5f12274f3"},
 ]
 
 [[package]]
 name = "msgpack-types"
 version = "0.2.0"
 requires_python = ">=3.7,<4.0"
 summary = "Type stubs for msgpack"
+groups = ["types-msgpack"]
 files = [
     {file = "msgpack-types-0.2.0.tar.gz", hash = "sha256:b6b7ce9f52599f9dc3497006be8cf6bed7bd2c83fa48c4df43ac6958b97b0720"},
     {file = "msgpack_types-0.2.0-py3-none-any.whl", hash = "sha256:7e5bce9e3bba9fe08ed14005ad107aa44ea8d4b779ec28b8db880826d4c67303"},
 ]
 
 [[package]]
 name = "mypy"
-version = "1.7.1"
+version = "1.9.0"
 requires_python = ">=3.8"
 summary = "Optional static typing for Python"
+groups = ["mypy"]
 dependencies = [
     "mypy-extensions>=1.0.0",
     "typing-extensions>=4.1.0",
 ]
 files = [
-    {file = "mypy-1.7.1-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:4b901927f16224d0d143b925ce9a4e6b3a758010673eeded9b748f250cf4e8f7"},
-    {file = "mypy-1.7.1-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:2f7f6985d05a4e3ce8255396df363046c28bea790e40617654e91ed580ca7c51"},
-    {file = "mypy-1.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:944bdc21ebd620eafefc090cdf83158393ec2b1391578359776c00de00e8907a"},
-    {file = "mypy-1.7.1-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:9c7ac372232c928fff0645d85f273a726970c014749b924ce5710d7d89763a28"},
-    {file = "mypy-1.7.1-cp311-cp311-win_amd64.whl", hash = "sha256:f6efc9bd72258f89a3816e3a98c09d36f079c223aa345c659622f056b760ab42"},
-    {file = "mypy-1.7.1-cp312-cp312-macosx_10_9_x86_64.whl", hash = "sha256:6dbdec441c60699288adf051f51a5d512b0d818526d1dcfff5a41f8cd8b4aaf1"},
-    {file = "mypy-1.7.1-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:4fc3d14ee80cd22367caaaf6e014494415bf440980a3045bf5045b525680ac33"},
-    {file = "mypy-1.7.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:2c6e4464ed5f01dc44dc9821caf67b60a4e5c3b04278286a85c067010653a0eb"},
-    {file = "mypy-1.7.1-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:d9b338c19fa2412f76e17525c1b4f2c687a55b156320acb588df79f2e6fa9fea"},
-    {file = "mypy-1.7.1-cp312-cp312-win_amd64.whl", hash = "sha256:204e0d6de5fd2317394a4eff62065614c4892d5a4d1a7ee55b765d7a3d9e3f82"},
-    {file = "mypy-1.7.1-py3-none-any.whl", hash = "sha256:f7c5d642db47376a0cc130f0de6d055056e010debdaf0707cd2b0fc7e7ef30ea"},
-    {file = "mypy-1.7.1.tar.gz", hash = "sha256:fcb6d9afb1b6208b4c712af0dafdc650f518836065df0d4fb1d800f5d6773db2"},
+    {file = "mypy-1.9.0-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:653265f9a2784db65bfca694d1edd23093ce49740b2244cde583aeb134c008f3"},
+    {file = "mypy-1.9.0-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:3a3c007ff3ee90f69cf0a15cbcdf0995749569b86b6d2f327af01fd1b8aee9dc"},
+    {file = "mypy-1.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:2418488264eb41f69cc64a69a745fad4a8f86649af4b1041a4c64ee61fc61129"},
+    {file = "mypy-1.9.0-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:68edad3dc7d70f2f17ae4c6c1b9471a56138ca22722487eebacfd1eb5321d612"},
+    {file = "mypy-1.9.0-cp311-cp311-win_amd64.whl", hash = "sha256:85ca5fcc24f0b4aeedc1d02f93707bccc04733f21d41c88334c5482219b1ccb3"},
+    {file = "mypy-1.9.0-cp312-cp312-macosx_10_9_x86_64.whl", hash = "sha256:aceb1db093b04db5cd390821464504111b8ec3e351eb85afd1433490163d60cd"},
+    {file = "mypy-1.9.0-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:0235391f1c6f6ce487b23b9dbd1327b4ec33bb93934aa986efe8a9563d9349e6"},
+    {file = "mypy-1.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:d4d5ddc13421ba3e2e082a6c2d74c2ddb3979c39b582dacd53dd5d9431237185"},
+    {file = "mypy-1.9.0-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:190da1ee69b427d7efa8aa0d5e5ccd67a4fb04038c380237a0d96829cb157913"},
+    {file = "mypy-1.9.0-cp312-cp312-win_amd64.whl", hash = "sha256:fe28657de3bfec596bbeef01cb219833ad9d38dd5393fc649f4b366840baefe6"},
+    {file = "mypy-1.9.0-py3-none-any.whl", hash = "sha256:a260627a570559181a9ea5de61ac6297aa5af202f06fd7ab093ce74e7181e43e"},
+    {file = "mypy-1.9.0.tar.gz", hash = "sha256:3cc5da0127e6a478cddd906068496a97a7618a21ce9b54bde5bf7e539c7af974"},
 ]
 
 [[package]]
 name = "mypy-extensions"
 version = "1.0.0"
 requires_python = ">=3.5"
 summary = "Type system extensions for programs checked with the mypy type checker."
+groups = ["format", "mypy"]
 files = [
     {file = "mypy_extensions-1.0.0-py3-none-any.whl", hash = "sha256:4392f6c0eb8a5668a69e23d168ffa70f0be9ccfd32b5cc2d26a34ae5b844552d"},
     {file = "mypy_extensions-1.0.0.tar.gz", hash = "sha256:75dbf8955dc00442a438fc4d0666508a9a97b6bd41aa2f0ffe9d2f2725af0782"},
 ]
 
 [[package]]
 name = "natsort"
 version = "8.4.0"
 requires_python = ">=3.7"
 summary = "Simple yet flexible natural sorting in Python."
+groups = ["doc"]
 files = [
     {file = "natsort-8.4.0-py3-none-any.whl", hash = "sha256:4732914fb471f56b5cce04d7bae6f164a592c7712e1c85f9ef585e197299521c"},
     {file = "natsort-8.4.0.tar.gz", hash = "sha256:45312c4a0e5507593da193dedd04abb1469253b601ecaf63445ad80f0a1ea581"},
 ]
 
 [[package]]
-name = "nh3"
-version = "0.2.15"
-summary = "Python bindings to the ammonia HTML sanitization library."
-files = [
-    {file = "nh3-0.2.15-cp37-abi3-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl", hash = "sha256:9c0d415f6b7f2338f93035bba5c0d8c1b464e538bfbb1d598acd47d7969284f0"},
-    {file = "nh3-0.2.15-cp37-abi3-macosx_10_12_x86_64.whl", hash = "sha256:6f42f99f0cf6312e470b6c09e04da31f9abaadcd3eb591d7d1a88ea931dca7f3"},
-    {file = "nh3-0.2.15-cp37-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:ac19c0d68cd42ecd7ead91a3a032fdfff23d29302dbb1311e641a130dfefba97"},
-    {file = "nh3-0.2.15-cp37-abi3-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:5f0d77272ce6d34db6c87b4f894f037d55183d9518f948bba236fe81e2bb4e28"},
-    {file = "nh3-0.2.15-cp37-abi3-manylinux_2_17_ppc64.manylinux2014_ppc64.whl", hash = "sha256:8d595df02413aa38586c24811237e95937ef18304e108b7e92c890a06793e3bf"},
-    {file = "nh3-0.2.15-cp37-abi3-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:86e447a63ca0b16318deb62498db4f76fc60699ce0a1231262880b38b6cff911"},
-    {file = "nh3-0.2.15-cp37-abi3-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:3277481293b868b2715907310c7be0f1b9d10491d5adf9fce11756a97e97eddf"},
-    {file = "nh3-0.2.15-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:60684857cfa8fdbb74daa867e5cad3f0c9789415aba660614fe16cd66cbb9ec7"},
-    {file = "nh3-0.2.15-cp37-abi3-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:3b803a5875e7234907f7d64777dfde2b93db992376f3d6d7af7f3bc347deb305"},
-    {file = "nh3-0.2.15-cp37-abi3-musllinux_1_2_aarch64.whl", hash = "sha256:0d02d0ff79dfd8208ed25a39c12cbda092388fff7f1662466e27d97ad011b770"},
-    {file = "nh3-0.2.15-cp37-abi3-musllinux_1_2_armv7l.whl", hash = "sha256:f3b53ba93bb7725acab1e030bc2ecd012a817040fd7851b332f86e2f9bb98dc6"},
-    {file = "nh3-0.2.15-cp37-abi3-musllinux_1_2_i686.whl", hash = "sha256:b1e97221cedaf15a54f5243f2c5894bb12ca951ae4ddfd02a9d4ea9df9e1a29d"},
-    {file = "nh3-0.2.15-cp37-abi3-musllinux_1_2_x86_64.whl", hash = "sha256:a5167a6403d19c515217b6bcaaa9be420974a6ac30e0da9e84d4fc67a5d474c5"},
-    {file = "nh3-0.2.15-cp37-abi3-win32.whl", hash = "sha256:427fecbb1031db085eaac9931362adf4a796428ef0163070c484b5a768e71601"},
-    {file = "nh3-0.2.15-cp37-abi3-win_amd64.whl", hash = "sha256:bc2d086fb540d0fa52ce35afaded4ea526b8fc4d3339f783db55c95de40ef02e"},
-    {file = "nh3-0.2.15.tar.gz", hash = "sha256:d1e30ff2d8d58fb2a14961f7aac1bbb1c51f9bdd7da727be35c63826060b0bf3"},
-]
-
-[[package]]
 name = "nodeenv"
 version = "1.8.0"
 requires_python = ">=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*,!=3.5.*,!=3.6.*"
 summary = "Node.js virtual environment builder"
+groups = ["pre-commit"]
 dependencies = [
     "setuptools",
 ]
 files = [
     {file = "nodeenv-1.8.0-py2.py3-none-any.whl", hash = "sha256:df865724bb3c3adc86b3876fa209771517b0cfe596beff01a92700e0e8be4cec"},
     {file = "nodeenv-1.8.0.tar.gz", hash = "sha256:d51e0c37e64fbf47d017feac3145cdbb58836d7eee8c6f6d3b6880c5456227d2"},
 ]
 
 [[package]]
 name = "packaging"
-version = "23.2"
+version = "24.0"
 requires_python = ">=3.7"
 summary = "Core utilities for Python packages"
+groups = ["benchmark-servers", "build", "dev", "doc", "format", "micro-benchmark", "test", "tox"]
 files = [
-    {file = "packaging-23.2-py3-none-any.whl", hash = "sha256:8c491190033a9af7e1d931d0b5dacc2ef47509b34dd0de67ed209b5203fc88c7"},
-    {file = "packaging-23.2.tar.gz", hash = "sha256:048fb0e9405036518eaaf48a55953c750c11e1a1b68e0dd1a9d62ed0c092cfc5"},
+    {file = "packaging-24.0-py3-none-any.whl", hash = "sha256:2ddfb553fdf02fb784c234c7ba6ccc288296ceabec964ad2eae3777778130bc5"},
+    {file = "packaging-24.0.tar.gz", hash = "sha256:eb82c5e3e56209074766e6885bb04b8c38a0c015d0a30036ebe7ece34c9989e9"},
 ]
 
 [[package]]
 name = "pathspec"
-version = "0.11.2"
-requires_python = ">=3.7"
+version = "0.12.1"
+requires_python = ">=3.8"
 summary = "Utility library for gitignore style pattern matching of file paths."
+groups = ["dev", "format"]
 files = [
-    {file = "pathspec-0.11.2-py3-none-any.whl", hash = "sha256:1d6ed233af05e679efb96b1851550ea95bbb64b7c490b0f5aa52996c11e92a20"},
-    {file = "pathspec-0.11.2.tar.gz", hash = "sha256:e0d8d0ac2f12da61956eb2306b69f9469b42f4deb0f3cb6ed47b9cce9996ced3"},
+    {file = "pathspec-0.12.1-py3-none-any.whl", hash = "sha256:a0d503e138a4c123b27490a4f7beda6a01c6f288df0e4a8b79c7eb0dc7b4cc08"},
+    {file = "pathspec-0.12.1.tar.gz", hash = "sha256:a482d51503a1ab33b1c67a6c3813a26953dbdc71c31dacaef9a838c4e29f5712"},
 ]
 
 [[package]]
 name = "pbr"
 version = "6.0.0"
 requires_python = ">=2.6"
 summary = "Python Build Reasonableness"
+groups = ["bandit"]
 files = [
     {file = "pbr-6.0.0-py2.py3-none-any.whl", hash = "sha256:4a7317d5e3b17a3dccb6a8cfe67dab65b20551404c52c8ed41279fa4f0cb4cda"},
     {file = "pbr-6.0.0.tar.gz", hash = "sha256:d1377122a5a00e2f940ee482999518efe16d745d423a670c27773dfbc3c9a7d9"},
 ]
 
 [[package]]
 name = "pexpect"
 version = "4.9.0"
 summary = "Pexpect allows easy control of interactive console applications."
+groups = ["dev"]
 dependencies = [
     "ptyprocess>=0.5",
 ]
 files = [
     {file = "pexpect-4.9.0-py2.py3-none-any.whl", hash = "sha256:7236d1e080e4936be2dc3e326cec0af72acf9212a7e1d060210e70a47e253523"},
     {file = "pexpect-4.9.0.tar.gz", hash = "sha256:ee7d41123f3c9911050ea2c2dac107568dc43b2d3b0c7557a33212c398ead30f"},
 ]
 
 [[package]]
-name = "pkginfo"
-version = "1.9.6"
-requires_python = ">=3.6"
-summary = "Query metadata from sdists / bdists / installed packages."
+name = "platformdirs"
+version = "4.2.0"
+requires_python = ">=3.8"
+summary = "A small Python package for determining appropriate platform-specific dirs, e.g. a \"user data dir\"."
+groups = ["dev", "doc", "format", "pre-commit", "tox"]
 files = [
-    {file = "pkginfo-1.9.6-py3-none-any.whl", hash = "sha256:4b7a555a6d5a22169fcc9cf7bfd78d296b0361adad412a346c1226849af5e546"},
-    {file = "pkginfo-1.9.6.tar.gz", hash = "sha256:8fd5896e8718a4372f0ea9cc9d96f6417c9b986e23a4d116dda26b62cc29d046"},
+    {file = "platformdirs-4.2.0-py3-none-any.whl", hash = "sha256:0614df2a2f37e1a662acbd8e2b25b92ccf8632929bc6d43467e17fe89c75e068"},
+    {file = "platformdirs-4.2.0.tar.gz", hash = "sha256:ef0cc731df711022c174543cb70a9b5bd22e5a9337c8624ef2c2ceb8ddad8768"},
 ]
 
 [[package]]
-name = "platformdirs"
-version = "4.1.0"
+name = "plotly"
+version = "5.20.0"
 requires_python = ">=3.8"
-summary = "A small Python package for determining appropriate platform-specific dirs, e.g. a \"user data dir\"."
+summary = "An open-source, interactive data visualization library for Python"
+groups = ["benchmark-servers"]
+dependencies = [
+    "packaging",
+    "tenacity>=6.2.0",
+]
 files = [
-    {file = "platformdirs-4.1.0-py3-none-any.whl", hash = "sha256:11c8f37bcca40db96d8144522d925583bdb7a31f7b0e37e3ed4318400a8e2380"},
-    {file = "platformdirs-4.1.0.tar.gz", hash = "sha256:906d548203468492d432bcb294d4bc2fff751bf84971fbb2c10918cc206ee420"},
+    {file = "plotly-5.20.0-py3-none-any.whl", hash = "sha256:837a9c8aa90f2c0a2f0d747b82544d014dc2a2bdde967b5bb1da25b53932d1a9"},
+    {file = "plotly-5.20.0.tar.gz", hash = "sha256:bf901c805d22032cfa534b2ff7c5aa6b0659e037f19ec1e0cca7f585918b5c89"},
 ]
 
 [[package]]
 name = "pluggy"
-version = "1.3.0"
+version = "1.4.0"
 requires_python = ">=3.8"
 summary = "plugin and hook calling mechanisms for python"
+groups = ["dev", "micro-benchmark", "test", "tox"]
 files = [
-    {file = "pluggy-1.3.0-py3-none-any.whl", hash = "sha256:d89c696a773f8bd377d18e5ecda92b7a3793cbe66c87060a6fb58c7b6e1061f7"},
-    {file = "pluggy-1.3.0.tar.gz", hash = "sha256:cf61ae8f126ac6f7c451172cf30e3e43d3ca77615509771b3a984a0730651e12"},
+    {file = "pluggy-1.4.0-py3-none-any.whl", hash = "sha256:7db9f7b503d67d1c5b95f59773ebb58a8c1c288129a88665838012cfb07b8981"},
+    {file = "pluggy-1.4.0.tar.gz", hash = "sha256:8c85c2876142a764e5b7548e7d9a0e0ddb46f5185161049a79b7e974454223be"},
 ]
 
 [[package]]
 name = "pre-commit"
-version = "3.5.0"
-requires_python = ">=3.8"
+version = "3.7.0"
+requires_python = ">=3.9"
 summary = "A framework for managing and maintaining multi-language pre-commit hooks."
+groups = ["pre-commit"]
 dependencies = [
     "cfgv>=2.0.0",
     "identify>=1.0.0",
     "nodeenv>=0.11.1",
     "pyyaml>=5.1",
     "virtualenv>=20.10.0",
 ]
 files = [
-    {file = "pre_commit-3.5.0-py2.py3-none-any.whl", hash = "sha256:841dc9aef25daba9a0238cd27984041fa0467b4199fc4852e27950664919f660"},
-    {file = "pre_commit-3.5.0.tar.gz", hash = "sha256:5804465c675b659b0862f07907f96295d490822a450c4c40e747d0b1c6ebcb32"},
+    {file = "pre_commit-3.7.0-py2.py3-none-any.whl", hash = "sha256:5eae9e10c2b5ac51577c3452ec0a490455c45a0533f7960f993a0d01e59decab"},
+    {file = "pre_commit-3.7.0.tar.gz", hash = "sha256:e209d61b8acdcf742404408531f0c37d49d2c734fd7cff2d6076083d191cb060"},
 ]
 
 [[package]]
 name = "ptyprocess"
 version = "0.7.0"
 summary = "Run a subprocess in a pseudo terminal"
+groups = ["dev"]
 files = [
     {file = "ptyprocess-0.7.0-py2.py3-none-any.whl", hash = "sha256:4b41f3967fce3af57cc7e94b888626c18bf37a083e3651ca8feeb66d492fef35"},
     {file = "ptyprocess-0.7.0.tar.gz", hash = "sha256:5c5d0a3b48ceee0b48485e0c26037c0acd7d29765ca3fbb5cb3831d347423220"},
 ]
 
 [[package]]
+name = "py-cpuinfo"
+version = "9.0.0"
+summary = "Get CPU info with pure Python"
+groups = ["micro-benchmark"]
+files = [
+    {file = "py-cpuinfo-9.0.0.tar.gz", hash = "sha256:3cdbbf3fac90dc6f118bfd64384f309edeadd902d7c8fb17f02ffa1fc3f49690"},
+    {file = "py_cpuinfo-9.0.0-py3-none-any.whl", hash = "sha256:859625bc251f64e21f077d099d4162689c762b5d6a4c3c97553d56241c9674d5"},
+]
+
+[[package]]
 name = "pycodestyle"
 version = "2.11.1"
 requires_python = ">=3.8"
 summary = "Python style guide checker"
+groups = ["flake8"]
 files = [
     {file = "pycodestyle-2.11.1-py2.py3-none-any.whl", hash = "sha256:44fe31000b2d866f2e41841b18528a505fbd7fef9017b04eff4e2648a0fadc67"},
     {file = "pycodestyle-2.11.1.tar.gz", hash = "sha256:41ba0e7afc9752dfb53ced5489e89f8186be00e599e712660695b7a75ff2663f"},
 ]
 
 [[package]]
 name = "pycparser"
-version = "2.21"
-requires_python = ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*"
+version = "2.22"
+requires_python = ">=3.8"
 summary = "C parser in Python"
+groups = ["dev", "test"]
 files = [
-    {file = "pycparser-2.21-py2.py3-none-any.whl", hash = "sha256:8ee45429555515e1f6b185e78100aea234072576aa43ab53aefcae078162fca9"},
-    {file = "pycparser-2.21.tar.gz", hash = "sha256:e644fdec12f7872f86c58ff790da456218b10f863970249516d60a5eaca77206"},
+    {file = "pycparser-2.22-py3-none-any.whl", hash = "sha256:c3702b6d3dd8c7abc1afa565d7e63d53a1d0bd86cdc24edd75470f4de499cfcc"},
+    {file = "pycparser-2.22.tar.gz", hash = "sha256:491c8be9c040f5390f5bf44a5b07752bd07f56edf992381b05c701439eec10f6"},
 ]
 
 [[package]]
 name = "pyflakes"
-version = "3.1.0"
+version = "3.2.0"
 requires_python = ">=3.8"
 summary = "passive checker of Python programs"
+groups = ["flake8"]
 files = [
-    {file = "pyflakes-3.1.0-py2.py3-none-any.whl", hash = "sha256:4132f6d49cb4dae6819e5379898f2b8cce3c5f23994194c24b77d5da2e36f774"},
-    {file = "pyflakes-3.1.0.tar.gz", hash = "sha256:a0aae034c444db0071aa077972ba4768d40c830d9539fd45bf4cd3f8f6992efc"},
+    {file = "pyflakes-3.2.0-py2.py3-none-any.whl", hash = "sha256:84b5be138a2dfbb40689ca07e2152deb896a65c3a3e24c251c5c62489568074a"},
+    {file = "pyflakes-3.2.0.tar.gz", hash = "sha256:1c61603ff154621fb2a9172037d84dca3500def8c8b630657d1701f026f8af3f"},
+]
+
+[[package]]
+name = "pygal"
+version = "3.0.4"
+requires_python = ">=3.8"
+summary = "A Python svg graph plotting library"
+groups = ["micro-benchmark"]
+dependencies = [
+    "importlib-metadata",
+]
+files = [
+    {file = "pygal-3.0.4-py2.py3-none-any.whl", hash = "sha256:e931caf08b4be0e6ec119a4c0e20dbed2d77829c641b7dea0ed21fe6ec81f2ea"},
+    {file = "pygal-3.0.4.tar.gz", hash = "sha256:6c5da33f1041e8b30cbc980f8a34910d9edc584b833240298f6a25df65425289"},
+]
+
+[[package]]
+name = "pygaljs"
+version = "1.0.2"
+summary = "Python package providing assets from https://github.com/Kozea/pygal.js"
+groups = ["micro-benchmark"]
+files = [
+    {file = "pygaljs-1.0.2-py2.py3-none-any.whl", hash = "sha256:d75e18cb21cc2cda40c45c3ee690771e5e3d4652bf57206f20137cf475c0dbe8"},
+    {file = "pygaljs-1.0.2.tar.gz", hash = "sha256:0b71ee32495dcba5fbb4a0476ddbba07658ad65f5675e4ad409baf154dec5111"},
 ]
 
 [[package]]
 name = "pygments"
 version = "2.17.2"
 requires_python = ">=3.7"
 summary = "Pygments is a syntax highlighting package written in Python."
+groups = ["bandit", "dev", "doc"]
 files = [
     {file = "pygments-2.17.2-py3-none-any.whl", hash = "sha256:b27c2826c47d0f3219f29554824c30c5e8945175d888647acd804ddd04af846c"},
     {file = "pygments-2.17.2.tar.gz", hash = "sha256:da46cec9fd2de5be3a8a784f434e4c4ab670b4ff54d605c4c2717e9d49c4c367"},
 ]
 
 [[package]]
-name = "pyperclip"
-version = "1.8.2"
-summary = "A cross-platform clipboard module for Python. (Only handles plain text for now.)"
-files = [
-    {file = "pyperclip-1.8.2.tar.gz", hash = "sha256:105254a8b04934f0bc84e9c24eb360a591aaf6535c9def5f29d92af107a9bf57"},
-]
-
-[[package]]
 name = "pyproject-api"
 version = "1.6.1"
 requires_python = ">=3.8"
 summary = "API to interact with the python pyproject.toml based projects"
+groups = ["tox"]
 dependencies = [
     "packaging>=23.1",
 ]
 files = [
     {file = "pyproject_api-1.6.1-py3-none-any.whl", hash = "sha256:4c0116d60476b0786c88692cf4e325a9814965e2469c5998b830bba16b183675"},
     {file = "pyproject_api-1.6.1.tar.gz", hash = "sha256:1817dc018adc0d1ff9ca1ed8c60e1623d5aaca40814b953af14a9cf9a5cae538"},
 ]
 
 [[package]]
 name = "pyproject-hooks"
 version = "1.0.0"
 requires_python = ">=3.7"
 summary = "Wrappers to call pyproject.toml-based build backend hooks."
+groups = ["benchmark-servers", "build"]
 files = [
     {file = "pyproject_hooks-1.0.0-py3-none-any.whl", hash = "sha256:283c11acd6b928d2f6a7c73fa0d01cb2bdc5f07c57a2eeb6e83d5e56b97976f8"},
     {file = "pyproject_hooks-1.0.0.tar.gz", hash = "sha256:f271b298b97f5955d53fb12b72c1fb1948c22c1a6b70b315c54cedaca0264ef5"},
 ]
 
 [[package]]
 name = "pytest"
-version = "7.4.3"
+version = "7.4.4"
 requires_python = ">=3.7"
 summary = "pytest: simple powerful testing with Python"
+groups = ["micro-benchmark", "test"]
 dependencies = [
     "colorama; sys_platform == \"win32\"",
     "iniconfig",
     "packaging",
     "pluggy<2.0,>=0.12",
 ]
 files = [
-    {file = "pytest-7.4.3-py3-none-any.whl", hash = "sha256:0d009c083ea859a71b76adf7c1d502e4bc170b80a8ef002da5806527b9591fac"},
-    {file = "pytest-7.4.3.tar.gz", hash = "sha256:d989d136982de4e3b29dabcc838ad581c64e8ed52c11fbe86ddebd9da0818cd5"},
+    {file = "pytest-7.4.4-py3-none-any.whl", hash = "sha256:b090cdf5ed60bf4c45261be03239c2c1c22df034fbffe691abe93cd80cea01d8"},
+    {file = "pytest-7.4.4.tar.gz", hash = "sha256:2cf0005922c6ace4a3e2ec8b4080eb0d9753fdc93107415332f50ce9e7994280"},
 ]
 
 [[package]]
 name = "pytest-asyncio"
 version = "0.21.1"
 requires_python = ">=3.7"
 summary = "Pytest support for asyncio"
+groups = ["test"]
 dependencies = [
     "pytest>=7.0.0",
 ]
 files = [
     {file = "pytest-asyncio-0.21.1.tar.gz", hash = "sha256:40a7eae6dded22c7b604986855ea48400ab15b069ae38116e8c01238e9eeb64d"},
     {file = "pytest_asyncio-0.21.1-py3-none-any.whl", hash = "sha256:8666c1c8ac02631d7c51ba282e0c69a8a452b211ffedf2599099845da5c5c37b"},
 ]
 
 [[package]]
+name = "pytest-benchmark"
+version = "4.0.0"
+requires_python = ">=3.7"
+summary = "A ``pytest`` fixture for benchmarking code. It will group the tests into rounds that are calibrated to the chosen timer."
+groups = ["micro-benchmark"]
+dependencies = [
+    "py-cpuinfo",
+    "pytest>=3.8",
+]
+files = [
+    {file = "pytest-benchmark-4.0.0.tar.gz", hash = "sha256:fb0785b83efe599a6a956361c0691ae1dbb5318018561af10f3e915caa0048d1"},
+    {file = "pytest_benchmark-4.0.0-py3-none-any.whl", hash = "sha256:fdb7db64e31c8b277dff9850d2a2556d8b60bcb0ea6524e36e28ffd7c87f71d6"},
+]
+
+[[package]]
+name = "pytest-benchmark"
+version = "4.0.0"
+extras = ["histogram"]
+requires_python = ">=3.7"
+summary = "A ``pytest`` fixture for benchmarking code. It will group the tests into rounds that are calibrated to the chosen timer."
+groups = ["micro-benchmark"]
+dependencies = [
+    "pygal",
+    "pygaljs",
+    "pytest-benchmark==4.0.0",
+]
+files = [
+    {file = "pytest-benchmark-4.0.0.tar.gz", hash = "sha256:fb0785b83efe599a6a956361c0691ae1dbb5318018561af10f3e915caa0048d1"},
+    {file = "pytest_benchmark-4.0.0-py3-none-any.whl", hash = "sha256:fdb7db64e31c8b277dff9850d2a2556d8b60bcb0ea6524e36e28ffd7c87f71d6"},
+]
+
+[[package]]
 name = "pytest-cov"
 version = "4.1.0"
 requires_python = ">=3.7"
 summary = "Pytest plugin for measuring coverage."
+groups = ["test"]
 dependencies = [
     "coverage[toml]>=5.2.1",
     "pytest>=4.6",
 ]
 files = [
     {file = "pytest-cov-4.1.0.tar.gz", hash = "sha256:3904b13dfbfec47f003b8e77fd5b589cd11904a21ddf1ab38a64f204d6a10ef6"},
     {file = "pytest_cov-4.1.0-py3-none-any.whl", hash = "sha256:6ba70b9e97e69fcc3fb45bfeab2d0a138fb65c4d0d6a41ef33983ad114be8c3a"},
 ]
 
 [[package]]
 name = "pytest-mock"
-version = "3.12.0"
+version = "3.14.0"
 requires_python = ">=3.8"
 summary = "Thin-wrapper around the mock package for easier use with pytest"
+groups = ["test"]
 dependencies = [
-    "pytest>=5.0",
+    "pytest>=6.2.5",
 ]
 files = [
-    {file = "pytest-mock-3.12.0.tar.gz", hash = "sha256:31a40f038c22cad32287bb43932054451ff5583ff094bca6f675df2f8bc1a6e9"},
-    {file = "pytest_mock-3.12.0-py3-none-any.whl", hash = "sha256:0972719a7263072da3a21c7f4773069bcc7486027d7e8e1f81d98a47e701bc4f"},
+    {file = "pytest-mock-3.14.0.tar.gz", hash = "sha256:2719255a1efeceadbc056d6bf3df3d1c5015530fb40cf347c0f9afac88410bd0"},
+    {file = "pytest_mock-3.14.0-py3-none-any.whl", hash = "sha256:0b72c38033392a5f4621342fe11e9219ac11ec9d375f8e2a0c164539e0d70f6f"},
+]
+
+[[package]]
+name = "pytest-retry"
+version = "1.6.2"
+requires_python = ">=3.9"
+summary = "Adds the ability to retry flaky tests in CI environments"
+groups = ["test"]
+dependencies = [
+    "pytest>=7.0.0",
+]
+files = [
+    {file = "pytest-retry-1.6.2.tar.gz", hash = "sha256:8278634e21b0e8afa2b8bf85615700b9ae1a5da78463b13576b6c05aed2dbf5e"},
+    {file = "pytest_retry-1.6.2-py3-none-any.whl", hash = "sha256:85cfd067ee30735b23df37e80761ad00fb3a367941d8dbeb90977373e4fcf575"},
 ]
 
 [[package]]
 name = "pytest-xdist"
 version = "3.5.0"
 requires_python = ">=3.7"
 summary = "pytest xdist plugin for distributed testing, most importantly across multiple CPUs"
+groups = ["test"]
 dependencies = [
     "execnet>=1.1",
     "pytest>=6.2.0",
 ]
 files = [
     {file = "pytest-xdist-3.5.0.tar.gz", hash = "sha256:cbb36f3d67e0c478baa57fa4edc8843887e0f6cfc42d677530a36d7472b32d8a"},
     {file = "pytest_xdist-3.5.0-py3-none-any.whl", hash = "sha256:d075629c7e00b611df89f490a5063944bee7a4362a5ff11c7cc7824a03dfce24"},
 ]
 
 [[package]]
+name = "pywin32"
+version = "306"
+summary = "Python for Window Extensions"
+groups = ["benchmark-servers"]
+marker = "sys_platform == \"win32\""
+files = [
+    {file = "pywin32-306-cp311-cp311-win32.whl", hash = "sha256:e65028133d15b64d2ed8f06dd9fbc268352478d4f9289e69c190ecd6818b6407"},
+    {file = "pywin32-306-cp311-cp311-win_amd64.whl", hash = "sha256:a7639f51c184c0272e93f244eb24dafca9b1855707d94c192d4a0b4c01e1100e"},
+    {file = "pywin32-306-cp311-cp311-win_arm64.whl", hash = "sha256:70dba0c913d19f942a2db25217d9a1b726c278f483a919f1abfed79c9cf64d3a"},
+    {file = "pywin32-306-cp312-cp312-win32.whl", hash = "sha256:383229d515657f4e3ed1343da8be101000562bf514591ff383ae940cad65458b"},
+    {file = "pywin32-306-cp312-cp312-win_amd64.whl", hash = "sha256:37257794c1ad39ee9be652da0462dc2e394c8159dfd913a8a4e8eb6fd346da0e"},
+    {file = "pywin32-306-cp312-cp312-win_arm64.whl", hash = "sha256:5821ec52f6d321aa59e2db7e0a35b997de60c201943557d108af9d4ae1ec7040"},
+]
+
+[[package]]
 name = "pywin32-ctypes"
 version = "0.2.2"
 requires_python = ">=3.6"
 summary = "A (partial) reimplementation of pywin32 using ctypes/cffi"
+groups = ["dev"]
+marker = "sys_platform == \"win32\""
 files = [
     {file = "pywin32-ctypes-0.2.2.tar.gz", hash = "sha256:3426e063bdd5fd4df74a14fa3cf80a0b42845a87e1d1e81f6549f9daec593a60"},
     {file = "pywin32_ctypes-0.2.2-py3-none-any.whl", hash = "sha256:bf490a1a709baf35d688fe0ecf980ed4de11d2b3e37b51e5442587a75d9957e7"},
 ]
 
 [[package]]
 name = "pyyaml"
 version = "6.0.1"
 requires_python = ">=3.6"
 summary = "YAML parser and emitter for Python"
+groups = ["bandit", "pre-commit"]
 files = [
     {file = "PyYAML-6.0.1-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:6965a7bc3cf88e5a1c3bd2e0b5c22f8d677dc88a455344035f03399034eb3007"},
     {file = "PyYAML-6.0.1-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:f003ed9ad21d6a4713f0a9b5a7a0a79e08dd0f221aff4525a2be4c346ee60aab"},
     {file = "PyYAML-6.0.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:42f8152b8dbc4fe7d96729ec2b99c7097d656dc1213a3229ca5383f973a5ed6d"},
     {file = "PyYAML-6.0.1-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:062582fca9fabdd2c8b54a3ef1c978d786e0f6b3a1510e0ac93ef59e0ddae2bc"},
     {file = "PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:d2b04aac4d386b172d5b9692e2d2da8de7bfb6c387fa4f801fbf6fb2e6ba4673"},
     {file = "PyYAML-6.0.1-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:e7d73685e87afe9f3b36c799222440d6cf362062f78be1013661b00c5c6f678b"},
     {file = "PyYAML-6.0.1-cp311-cp311-win32.whl", hash = "sha256:1635fd110e8d85d55237ab316b5b011de701ea0f29d07611174a1b42f1444741"},
     {file = "PyYAML-6.0.1-cp311-cp311-win_amd64.whl", hash = "sha256:bf07ee2fef7014951eeb99f56f39c9bb4af143d8aa3c21b1677805985307da34"},
     {file = "PyYAML-6.0.1-cp312-cp312-macosx_10_9_x86_64.whl", hash = "sha256:855fb52b0dc35af121542a76b9a84f8d1cd886ea97c84703eaa6d88e37a2ad28"},
     {file = "PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:40df9b996c2b73138957fe23a16a4f0ba614f4c0efce1e9406a184b6d07fa3a9"},
+    {file = "PyYAML-6.0.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:a08c6f0fe150303c1c6b71ebcd7213c2858041a7e01975da3a99aed1e7a378ef"},
     {file = "PyYAML-6.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6c22bec3fbe2524cde73d7ada88f6566758a8f7227bfbf93a408a9d86bcc12a0"},
     {file = "PyYAML-6.0.1-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4"},
     {file = "PyYAML-6.0.1-cp312-cp312-win32.whl", hash = "sha256:d483d2cdf104e7c9fa60c544d92981f12ad66a457afae824d146093b8c294c54"},
     {file = "PyYAML-6.0.1-cp312-cp312-win_amd64.whl", hash = "sha256:0d3304d8c0adc42be59c5f8a4d9e3d7379e6955ad754aa9d6ab7a398b59dd1df"},
     {file = "PyYAML-6.0.1.tar.gz", hash = "sha256:bfdf460b1736c775f2ba9f6a92bca30bc2095067b8a9d77876d1fad6cc3b4a43"},
 ]
 
 [[package]]
-name = "readme-renderer"
-version = "42.0"
-requires_python = ">=3.8"
-summary = "readme_renderer is a library for rendering readme descriptions for Warehouse"
-dependencies = [
-    "Pygments>=2.5.1",
-    "docutils>=0.13.1",
-    "nh3>=0.2.14",
-]
-files = [
-    {file = "readme_renderer-42.0-py3-none-any.whl", hash = "sha256:13d039515c1f24de668e2c93f2e877b9dbe6c6c32328b90a40a49d8b2b85f36d"},
-    {file = "readme_renderer-42.0.tar.gz", hash = "sha256:2d55489f83be4992fe4454939d1a051c33edbab778e82761d060c9fc6b308cd1"},
-]
-
-[[package]]
 name = "requests"
 version = "2.31.0"
 requires_python = ">=3.7"
 summary = "Python HTTP for Humans."
+groups = ["benchmark-servers", "doc"]
 dependencies = [
     "certifi>=2017.4.17",
     "charset-normalizer<4,>=2",
     "idna<4,>=2.5",
     "urllib3<3,>=1.21.1",
 ]
 files = [
     {file = "requests-2.31.0-py3-none-any.whl", hash = "sha256:58cd2187c01e70e6e26505bca751777aa9f2ee0b7f4300988b709f44e013003f"},
     {file = "requests-2.31.0.tar.gz", hash = "sha256:942c5a758f98d790eaed1a29cb6eefc7ffb0d1cf7af05c3d2791656dbd6ad1e1"},
 ]
 
 [[package]]
-name = "requests-toolbelt"
-version = "1.0.0"
-requires_python = ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*"
-summary = "A utility belt for advanced users of python-requests"
-dependencies = [
-    "requests<3.0.0,>=2.0.1",
-]
-files = [
-    {file = "requests-toolbelt-1.0.0.tar.gz", hash = "sha256:7681a0a3d047012b5bdc0ee37d7f8f07ebe76ab08caeccfc3921ce23c88d5bc6"},
-    {file = "requests_toolbelt-1.0.0-py2.py3-none-any.whl", hash = "sha256:cccfdd665f0a24fcf4726e690f65639d272bb0637b9b92dfd91a5568ccf6bd06"},
-]
-
-[[package]]
-name = "rfc3986"
-version = "2.0.0"
-requires_python = ">=3.7"
-summary = "Validating URI References per RFC 3986"
-files = [
-    {file = "rfc3986-2.0.0-py2.py3-none-any.whl", hash = "sha256:50b1502b60e289cb37883f3dfd34532b8873c7de9f49bb546641ce9cbd256ebd"},
-    {file = "rfc3986-2.0.0.tar.gz", hash = "sha256:97aacf9dbd4bfd829baad6e6309fa6573aaf1be3f6fa735c8ab05e46cecb261c"},
-]
-
-[[package]]
 name = "rich"
-version = "13.7.0"
+version = "13.7.1"
 requires_python = ">=3.7.0"
 summary = "Render rich text, tables, progress bars, syntax highlighting, markdown and more to the terminal"
+groups = ["bandit", "dev"]
 dependencies = [
     "markdown-it-py>=2.2.0",
     "pygments<3.0.0,>=2.13.0",
 ]
 files = [
-    {file = "rich-13.7.0-py3-none-any.whl", hash = "sha256:6da14c108c4866ee9520bbffa71f6fe3962e193b7da68720583850cd4548e235"},
-    {file = "rich-13.7.0.tar.gz", hash = "sha256:5cb5123b5cf9ee70584244246816e9114227e0b98ad9176eede6ad54bf5403fa"},
+    {file = "rich-13.7.1-py3-none-any.whl", hash = "sha256:4edbae314f59eb482f54e9e30bf00d33350aaa94f4bfcd4e9e3110e64d0d7222"},
+    {file = "rich-13.7.1.tar.gz", hash = "sha256:9be308cb1fe2f1f57d67ce99e95af38a1e2bc71ad9813b0e247cf7ffbcc3a432"},
 ]
 
 [[package]]
 name = "ruamel-yaml"
-version = "0.18.5"
+version = "0.18.6"
 requires_python = ">=3.7"
 summary = "ruamel.yaml is a YAML parser/emitter that supports roundtrip preservation of comments, seq/map flow style, and map key order"
+groups = ["doc"]
 dependencies = [
     "ruamel-yaml-clib>=0.2.7; platform_python_implementation == \"CPython\" and python_version < \"3.13\"",
 ]
 files = [
-    {file = "ruamel.yaml-0.18.5-py3-none-any.whl", hash = "sha256:a013ac02f99a69cdd6277d9664689eb1acba07069f912823177c5eced21a6ada"},
-    {file = "ruamel.yaml-0.18.5.tar.gz", hash = "sha256:61917e3a35a569c1133a8f772e1226961bf5a1198bea7e23f06a0841dea1ab0e"},
+    {file = "ruamel.yaml-0.18.6-py3-none-any.whl", hash = "sha256:57b53ba33def16c4f3d807c0ccbc00f8a6081827e81ba2491691b76882d0c636"},
+    {file = "ruamel.yaml-0.18.6.tar.gz", hash = "sha256:8b27e6a217e786c6fbe5634d8f3f11bc63e0f80f6a5890f28863d9c45aac311b"},
 ]
 
 [[package]]
 name = "ruamel-yaml-clib"
 version = "0.2.8"
 requires_python = ">=3.6"
 summary = "C version of reader, parser and emitter for ruamel.yaml derived from libyaml"
+groups = ["doc"]
+marker = "platform_python_implementation == \"CPython\" and python_version < \"3.13\""
 files = [
     {file = "ruamel.yaml.clib-0.2.8-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:bef08cd86169d9eafb3ccb0a39edb11d8e25f3dae2b28f5c52fd997521133069"},
     {file = "ruamel.yaml.clib-0.2.8-cp311-cp311-macosx_13_0_arm64.whl", hash = "sha256:b16420e621d26fdfa949a8b4b47ade8810c56002f5389970db4ddda51dbff248"},
     {file = "ruamel.yaml.clib-0.2.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl", hash = "sha256:25c515e350e5b739842fc3228d662413ef28f295791af5e5110b543cf0b57d9b"},
     {file = "ruamel.yaml.clib-0.2.8-cp311-cp311-manylinux_2_24_aarch64.whl", hash = "sha256:1707814f0d9791df063f8c19bb51b0d1278b8e9a2353abbb676c2f685dee6afe"},
     {file = "ruamel.yaml.clib-0.2.8-cp311-cp311-musllinux_1_1_i686.whl", hash = "sha256:46d378daaac94f454b3a0e3d8d78cafd78a026b1d71443f4966c696b48a6d899"},
     {file = "ruamel.yaml.clib-0.2.8-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:09b055c05697b38ecacb7ac50bdab2240bfca1a0c4872b0fd309bb07dc9aa3a9"},
@@ -1449,38 +1599,42 @@
 ]
 
 [[package]]
 name = "secretstorage"
 version = "3.3.3"
 requires_python = ">=3.6"
 summary = "Python bindings to FreeDesktop.org Secret Service API"
+groups = ["dev"]
+marker = "sys_platform == \"linux\""
 dependencies = [
     "cryptography>=2.0",
     "jeepney>=0.6",
 ]
 files = [
     {file = "SecretStorage-3.3.3-py3-none-any.whl", hash = "sha256:f356e6628222568e3af06f2eba8df495efa13b3b63081dafd4f7d9a7b7bc9f99"},
     {file = "SecretStorage-3.3.3.tar.gz", hash = "sha256:2403533ef369eca6d2ba81718576c5e0f564d5cca1b58f73a8b23e7d4eeebd77"},
 ]
 
 [[package]]
 name = "setuptools"
-version = "69.0.2"
+version = "69.2.0"
 requires_python = ">=3.8"
 summary = "Easily download, build, install, upgrade, and uninstall Python packages"
+groups = ["dev", "pre-commit"]
 files = [
-    {file = "setuptools-69.0.2-py3-none-any.whl", hash = "sha256:1e8fdff6797d3865f37397be788a4e3cba233608e9b509382a2777d25ebde7f2"},
-    {file = "setuptools-69.0.2.tar.gz", hash = "sha256:735896e78a4742605974de002ac60562d286fa8051a7e2299445e8e8fbb01aa6"},
+    {file = "setuptools-69.2.0-py3-none-any.whl", hash = "sha256:c21c49fb1042386df081cb5d86759792ab89efca84cf114889191cd09aacc80c"},
+    {file = "setuptools-69.2.0.tar.gz", hash = "sha256:0ff4183f8f42cd8fa3acea16c45205521a4ef28f73c6391d8a25e92893134f2e"},
 ]
 
 [[package]]
 name = "setuptools-scm"
 version = "8.0.4"
 requires_python = ">=3.8"
 summary = "the blessed package to manage your versions by scm tags"
+groups = ["dev"]
 dependencies = [
     "packaging>=20",
     "setuptools",
     "typing-extensions",
 ]
 files = [
     {file = "setuptools-scm-8.0.4.tar.gz", hash = "sha256:b5f43ff6800669595193fd09891564ee9d1d7dcb196cab4b2506d53a2e1c95c7"},
@@ -1488,73 +1642,69 @@
 ]
 
 [[package]]
 name = "shellingham"
 version = "1.5.4"
 requires_python = ">=3.7"
 summary = "Tool to Detect Surrounding Shell"
+groups = ["dev"]
 files = [
     {file = "shellingham-1.5.4-py2.py3-none-any.whl", hash = "sha256:7ecfff8f2fd72616f7481040475a65b2bf8af90a56c89140852d1120324e8686"},
     {file = "shellingham-1.5.4.tar.gz", hash = "sha256:8dbca0739d487e5bd35ab3ca4b36e11c4078f3a234bfce294b0a0291363404de"},
 ]
 
 [[package]]
 name = "six"
 version = "1.16.0"
 requires_python = ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*"
 summary = "Python 2 and 3 compatibility utilities"
+groups = ["doc"]
 files = [
     {file = "six-1.16.0-py2.py3-none-any.whl", hash = "sha256:8abb2f1d86890a2dfb989f9a77cfcfd3e47c2a354b01111771326f8aa26e0254"},
     {file = "six-1.16.0.tar.gz", hash = "sha256:1e61c37477a1626458e36f7b1d82aa5c9b094fa4802892072e49de9c60c4c926"},
 ]
 
 [[package]]
-name = "smmap"
-version = "5.0.1"
-requires_python = ">=3.7"
-summary = "A pure Python implementation of a sliding window memory map manager"
-files = [
-    {file = "smmap-5.0.1-py3-none-any.whl", hash = "sha256:e6d8668fa5f93e706934a62d7b4db19c8d9eb8cf2adbb75ef1b675aa332b69da"},
-    {file = "smmap-5.0.1.tar.gz", hash = "sha256:dceeb6c0028fdb6734471eb07c0cd2aae706ccaecab45965ee83f11c8d3b1f62"},
-]
-
-[[package]]
 name = "sniffio"
-version = "1.3.0"
+version = "1.3.1"
 requires_python = ">=3.7"
 summary = "Sniff out which async library your code is running under"
+groups = ["dev", "sniffio"]
 files = [
-    {file = "sniffio-1.3.0-py3-none-any.whl", hash = "sha256:eecefdce1e5bbfb7ad2eeaabf7c1eeb404d7757c379bd1f7e5cce9d8bf425384"},
-    {file = "sniffio-1.3.0.tar.gz", hash = "sha256:e60305c5e5d314f5389259b7f22aaa33d8f7dee49763119234af3755c55b9101"},
+    {file = "sniffio-1.3.1-py3-none-any.whl", hash = "sha256:2f6da418d1f1e0fddd844478f41680e794e6051915791a034ff65e5f100525a2"},
+    {file = "sniffio-1.3.1.tar.gz", hash = "sha256:f4324edc670a0f49750a81b895f35c3adb843cca46f0530f79fc1babb23789dc"},
 ]
 
 [[package]]
 name = "snowballstemmer"
 version = "2.2.0"
 summary = "This package provides 29 stemmers for 28 languages generated from Snowball algorithms."
+groups = ["doc"]
 files = [
     {file = "snowballstemmer-2.2.0-py2.py3-none-any.whl", hash = "sha256:c8e1716e83cc398ae16824e5572ae04e0d9fc2c6b985fb0f900f5f0c96ecba1a"},
     {file = "snowballstemmer-2.2.0.tar.gz", hash = "sha256:09b16deb8547d3412ad7b590689584cd0fe25ec8db3be37788be3810cbf19cb1"},
 ]
 
 [[package]]
 name = "soupsieve"
 version = "2.5"
 requires_python = ">=3.8"
 summary = "A modern CSS selector implementation for Beautiful Soup."
+groups = ["doc"]
 files = [
     {file = "soupsieve-2.5-py3-none-any.whl", hash = "sha256:eaa337ff55a1579b6549dc679565eac1e3d000563bcb1c8ab0d0fefbc0c2cdc7"},
     {file = "soupsieve-2.5.tar.gz", hash = "sha256:5663d5a7b3bfaeee0bc4372e7fc48f9cff4940b3eec54a6451cc5299f1097690"},
 ]
 
 [[package]]
 name = "sphinx"
 version = "6.2.1"
 requires_python = ">=3.8"
 summary = "Python documentation generator"
+groups = ["doc"]
 dependencies = [
     "Jinja2>=3.0",
     "Pygments>=2.13",
     "alabaster<0.8,>=0.7",
     "babel>=2.9",
     "colorama>=0.4.5; sys_platform == \"win32\"",
     "docutils<0.20,>=0.18.1",
@@ -1575,83 +1725,89 @@
 ]
 
 [[package]]
 name = "sphinx-autodoc-typehints"
 version = "1.23.0"
 requires_python = ">=3.7"
 summary = "Type hints (PEP 484) support for the Sphinx autodoc extension"
+groups = ["doc"]
 dependencies = [
     "sphinx>=5.3",
 ]
 files = [
     {file = "sphinx_autodoc_typehints-1.23.0-py3-none-any.whl", hash = "sha256:ac099057e66b09e51b698058ba7dd76e57e1fe696cd91b54e121d3dad188f91d"},
     {file = "sphinx_autodoc_typehints-1.23.0.tar.gz", hash = "sha256:5d44e2996633cdada499b6d27a496ddf9dbc95dd1f0f09f7b37940249e61f6e9"},
 ]
 
 [[package]]
 name = "sphinx-jinja2-compat"
 version = "0.2.0.post1"
 requires_python = ">=3.6"
 summary = "Patches Jinja2 v3 to restore compatibility with earlier Sphinx versions."
+groups = ["doc"]
 dependencies = [
     "jinja2>=2.10",
     "markupsafe>=1",
 ]
 files = [
     {file = "sphinx_jinja2_compat-0.2.0.post1-py3-none-any.whl", hash = "sha256:f9d329174bdde8db19dc12c62528367196eb2f6b46c91754eca604acd0c0f6ad"},
     {file = "sphinx_jinja2_compat-0.2.0.post1.tar.gz", hash = "sha256:974289a12a9f402108dead621e9c15f7004e945d5cfcaea8d6419e94d3fa95a3"},
 ]
 
 [[package]]
 name = "sphinx-prompt"
 version = "1.5.0"
 summary = "Sphinx directive to add unselectable prompt"
+groups = ["doc"]
 dependencies = [
     "Sphinx",
     "pygments",
 ]
 files = [
     {file = "sphinx_prompt-1.5.0-py3-none-any.whl", hash = "sha256:fa4e90d8088b5a996c76087d701fc7e31175f8b9dc4aab03a507e45051067162"},
 ]
 
 [[package]]
 name = "sphinx-rtd-theme"
 version = "2.0.0"
 requires_python = ">=3.6"
 summary = "Read the Docs theme for Sphinx"
+groups = ["doc"]
 dependencies = [
     "docutils<0.21",
     "sphinx<8,>=5",
     "sphinxcontrib-jquery<5,>=4",
 ]
 files = [
     {file = "sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl", hash = "sha256:ec93d0856dc280cf3aee9a4c9807c60e027c7f7b461b77aeffed682e68f0e586"},
     {file = "sphinx_rtd_theme-2.0.0.tar.gz", hash = "sha256:bd5d7b80622406762073a04ef8fadc5f9151261563d47027de09910ce03afe6b"},
 ]
 
 [[package]]
 name = "sphinx-tabs"
-version = "3.4.4"
+version = "3.4.5"
 requires_python = "~=3.7"
 summary = "Tabbed views for Sphinx"
+groups = ["doc"]
 dependencies = [
-    "docutils~=0.18.0",
+    "docutils",
     "pygments",
     "sphinx",
 ]
 files = [
-    {file = "sphinx-tabs-3.4.4.tar.gz", hash = "sha256:f1b72c4f23d1ba9cdcaf880fd883524bc70689f561b9785719b8b3c3c5ed0aca"},
-    {file = "sphinx_tabs-3.4.4-py3-none-any.whl", hash = "sha256:85939b689a0b0a24bf0da418b9acf14b0b0fca7a7a5cd35461ee452a2d4e716b"},
+    {file = "sphinx-tabs-3.4.5.tar.gz", hash = "sha256:ba9d0c1e3e37aaadd4b5678449eb08176770e0fc227e769b6ce747df3ceea531"},
+    {file = "sphinx_tabs-3.4.5-py3-none-any.whl", hash = "sha256:92cc9473e2ecf1828ca3f6617d0efc0aa8acb06b08c56ba29d1413f2f0f6cf09"},
 ]
 
 [[package]]
 name = "sphinx-toolbox"
 version = "3.5.0"
 requires_python = ">=3.7"
 summary = "Box of handy tools for Sphinx 🧰 📔"
+groups = ["doc"]
 dependencies = [
     "apeye>=0.4.0",
     "autodocsumm>=0.2.0",
     "beautifulsoup4>=4.9.1",
     "cachecontrol[filecache]>=0.13.0",
     "dict2css>=0.2.3",
     "docutils>=0.16",
@@ -1670,268 +1826,254 @@
 files = [
     {file = "sphinx_toolbox-3.5.0-py3-none-any.whl", hash = "sha256:20dfd3566717db6f2da7a400a54dc4b946f064fb31250fa44802d54cfb9b8a03"},
     {file = "sphinx_toolbox-3.5.0.tar.gz", hash = "sha256:e5b5a7153f1997572d71a06aaf6cec225483492ec2c60097a84f15aad6df18b7"},
 ]
 
 [[package]]
 name = "sphinxcontrib-applehelp"
-version = "1.0.7"
+version = "1.0.8"
 requires_python = ">=3.9"
 summary = "sphinxcontrib-applehelp is a Sphinx extension which outputs Apple help books"
-dependencies = [
-    "Sphinx>=5",
-]
+groups = ["doc"]
 files = [
-    {file = "sphinxcontrib_applehelp-1.0.7-py3-none-any.whl", hash = "sha256:094c4d56209d1734e7d252f6e0b3ccc090bd52ee56807a5d9315b19c122ab15d"},
-    {file = "sphinxcontrib_applehelp-1.0.7.tar.gz", hash = "sha256:39fdc8d762d33b01a7d8f026a3b7d71563ea3b72787d5f00ad8465bd9d6dfbfa"},
+    {file = "sphinxcontrib_applehelp-1.0.8-py3-none-any.whl", hash = "sha256:cb61eb0ec1b61f349e5cc36b2028e9e7ca765be05e49641c97241274753067b4"},
+    {file = "sphinxcontrib_applehelp-1.0.8.tar.gz", hash = "sha256:c40a4f96f3776c4393d933412053962fac2b84f4c99a7982ba42e09576a70619"},
 ]
 
 [[package]]
 name = "sphinxcontrib-devhelp"
-version = "1.0.5"
+version = "1.0.6"
 requires_python = ">=3.9"
 summary = "sphinxcontrib-devhelp is a sphinx extension which outputs Devhelp documents"
-dependencies = [
-    "Sphinx>=5",
-]
+groups = ["doc"]
 files = [
-    {file = "sphinxcontrib_devhelp-1.0.5-py3-none-any.whl", hash = "sha256:fe8009aed765188f08fcaadbb3ea0d90ce8ae2d76710b7e29ea7d047177dae2f"},
-    {file = "sphinxcontrib_devhelp-1.0.5.tar.gz", hash = "sha256:63b41e0d38207ca40ebbeabcf4d8e51f76c03e78cd61abe118cf4435c73d4212"},
+    {file = "sphinxcontrib_devhelp-1.0.6-py3-none-any.whl", hash = "sha256:6485d09629944511c893fa11355bda18b742b83a2b181f9a009f7e500595c90f"},
+    {file = "sphinxcontrib_devhelp-1.0.6.tar.gz", hash = "sha256:9893fd3f90506bc4b97bdb977ceb8fbd823989f4316b28c3841ec128544372d3"},
 ]
 
 [[package]]
 name = "sphinxcontrib-htmlhelp"
-version = "2.0.4"
+version = "2.0.5"
 requires_python = ">=3.9"
 summary = "sphinxcontrib-htmlhelp is a sphinx extension which renders HTML help files"
-dependencies = [
-    "Sphinx>=5",
-]
+groups = ["doc"]
 files = [
-    {file = "sphinxcontrib_htmlhelp-2.0.4-py3-none-any.whl", hash = "sha256:8001661c077a73c29beaf4a79968d0726103c5605e27db92b9ebed8bab1359e9"},
-    {file = "sphinxcontrib_htmlhelp-2.0.4.tar.gz", hash = "sha256:6c26a118a05b76000738429b724a0568dbde5b72391a688577da08f11891092a"},
+    {file = "sphinxcontrib_htmlhelp-2.0.5-py3-none-any.whl", hash = "sha256:393f04f112b4d2f53d93448d4bce35842f62b307ccdc549ec1585e950bc35e04"},
+    {file = "sphinxcontrib_htmlhelp-2.0.5.tar.gz", hash = "sha256:0dc87637d5de53dd5eec3a6a01753b1ccf99494bd756aafecd74b4fa9e729015"},
 ]
 
 [[package]]
 name = "sphinxcontrib-jquery"
 version = "4.1"
 requires_python = ">=2.7"
 summary = "Extension to include jQuery on newer Sphinx releases"
+groups = ["doc"]
 dependencies = [
     "Sphinx>=1.8",
 ]
 files = [
     {file = "sphinxcontrib-jquery-4.1.tar.gz", hash = "sha256:1620739f04e36a2c779f1a131a2dfd49b2fd07351bf1968ced074365933abc7a"},
     {file = "sphinxcontrib_jquery-4.1-py2.py3-none-any.whl", hash = "sha256:f936030d7d0147dd026a4f2b5a57343d233f1fc7b363f68b3d4f1cb0993878ae"},
 ]
 
 [[package]]
 name = "sphinxcontrib-jsmath"
 version = "1.0.1"
 requires_python = ">=3.5"
 summary = "A sphinx extension which renders display math in HTML via JavaScript"
+groups = ["doc"]
 files = [
     {file = "sphinxcontrib-jsmath-1.0.1.tar.gz", hash = "sha256:a9925e4a4587247ed2191a22df5f6970656cb8ca2bd6284309578f2153e0c4b8"},
     {file = "sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl", hash = "sha256:2ec2eaebfb78f3f2078e73666b1415417a116cc848b72e5172e596c871103178"},
 ]
 
 [[package]]
 name = "sphinxcontrib-qthelp"
-version = "1.0.6"
+version = "1.0.7"
 requires_python = ">=3.9"
 summary = "sphinxcontrib-qthelp is a sphinx extension which outputs QtHelp documents"
-dependencies = [
-    "Sphinx>=5",
-]
+groups = ["doc"]
 files = [
-    {file = "sphinxcontrib_qthelp-1.0.6-py3-none-any.whl", hash = "sha256:bf76886ee7470b934e363da7a954ea2825650013d367728588732c7350f49ea4"},
-    {file = "sphinxcontrib_qthelp-1.0.6.tar.gz", hash = "sha256:62b9d1a186ab7f5ee3356d906f648cacb7a6bdb94d201ee7adf26db55092982d"},
+    {file = "sphinxcontrib_qthelp-1.0.7-py3-none-any.whl", hash = "sha256:e2ae3b5c492d58fcbd73281fbd27e34b8393ec34a073c792642cd8e529288182"},
+    {file = "sphinxcontrib_qthelp-1.0.7.tar.gz", hash = "sha256:053dedc38823a80a7209a80860b16b722e9e0209e32fea98c90e4e6624588ed6"},
 ]
 
 [[package]]
 name = "sphinxcontrib-serializinghtml"
-version = "1.1.9"
+version = "1.1.10"
 requires_python = ">=3.9"
 summary = "sphinxcontrib-serializinghtml is a sphinx extension which outputs \"serialized\" HTML files (json and pickle)"
-dependencies = [
-    "Sphinx>=5",
-]
+groups = ["doc"]
 files = [
-    {file = "sphinxcontrib_serializinghtml-1.1.9-py3-none-any.whl", hash = "sha256:9b36e503703ff04f20e9675771df105e58aa029cfcbc23b8ed716019b7416ae1"},
-    {file = "sphinxcontrib_serializinghtml-1.1.9.tar.gz", hash = "sha256:0c64ff898339e1fac29abd2bf5f11078f3ec413cfe9c046d3120d7ca65530b54"},
+    {file = "sphinxcontrib_serializinghtml-1.1.10-py3-none-any.whl", hash = "sha256:326369b8df80a7d2d8d7f99aa5ac577f51ea51556ed974e7716cfd4fca3f6cb7"},
+    {file = "sphinxcontrib_serializinghtml-1.1.10.tar.gz", hash = "sha256:93f3f5dc458b91b192fe10c397e324f262cf163d79f3282c158e8436a2c4511f"},
 ]
 
 [[package]]
 name = "stevedore"
-version = "5.1.0"
+version = "5.2.0"
 requires_python = ">=3.8"
 summary = "Manage dynamic plugins for Python applications"
+groups = ["bandit"]
 dependencies = [
     "pbr!=2.1.0,>=2.0.0",
 ]
 files = [
-    {file = "stevedore-5.1.0-py3-none-any.whl", hash = "sha256:8cc040628f3cea5d7128f2e76cf486b2251a4e543c7b938f58d9a377f6694a2d"},
-    {file = "stevedore-5.1.0.tar.gz", hash = "sha256:a54534acf9b89bc7ed264807013b505bf07f74dbe4bcfa37d32bd063870b087c"},
+    {file = "stevedore-5.2.0-py3-none-any.whl", hash = "sha256:1c15d95766ca0569cad14cb6272d4d31dae66b011a929d7c18219c176ea1b5c9"},
+    {file = "stevedore-5.2.0.tar.gz", hash = "sha256:46b93ca40e1114cea93d738a6c1e365396981bb6bb78c27045b7587c9473544d"},
 ]
 
 [[package]]
 name = "tabulate"
 version = "0.9.0"
 requires_python = ">=3.7"
 summary = "Pretty-print tabular data"
+groups = ["doc"]
 files = [
     {file = "tabulate-0.9.0-py3-none-any.whl", hash = "sha256:024ca478df22e9340661486f85298cff5f6dcdba14f3813e8830015b9ed1948f"},
     {file = "tabulate-0.9.0.tar.gz", hash = "sha256:0095b12bf5966de529c0feb1fa08671671b3368eec77d7ef7ab114be2c068b3c"},
 ]
 
 [[package]]
+name = "tenacity"
+version = "8.2.3"
+requires_python = ">=3.7"
+summary = "Retry code until it succeeds"
+groups = ["benchmark-servers"]
+files = [
+    {file = "tenacity-8.2.3-py3-none-any.whl", hash = "sha256:ce510e327a630c9e1beaf17d42e6ffacc88185044ad85cf74c0a8887c6a0f88c"},
+    {file = "tenacity-8.2.3.tar.gz", hash = "sha256:5398ef0d78e63f40007c1fb4c0bff96e1911394d2fa8d194f77619c05ff6cc8a"},
+]
+
+[[package]]
 name = "tomli-w"
 version = "1.0.0"
 requires_python = ">=3.7"
 summary = "A lil' TOML writer"
+groups = ["dev"]
 files = [
     {file = "tomli_w-1.0.0-py3-none-any.whl", hash = "sha256:9f2a07e8be30a0729e533ec968016807069991ae2fd921a78d42f429ae5f4463"},
     {file = "tomli_w-1.0.0.tar.gz", hash = "sha256:f463434305e0336248cac9c2dc8076b707d8a12d019dd349f5c1e382dd1ae1b9"},
 ]
 
 [[package]]
 name = "tomlkit"
-version = "0.12.3"
+version = "0.12.4"
 requires_python = ">=3.7"
 summary = "Style preserving TOML library"
+groups = ["dev"]
 files = [
-    {file = "tomlkit-0.12.3-py3-none-any.whl", hash = "sha256:b0a645a9156dc7cb5d3a1f0d4bab66db287fcb8e0430bdd4664a095ea16414ba"},
-    {file = "tomlkit-0.12.3.tar.gz", hash = "sha256:75baf5012d06501f07bee5bf8e801b9f343e7aac5a92581f20f80ce632e6b5a4"},
+    {file = "tomlkit-0.12.4-py3-none-any.whl", hash = "sha256:5cd82d48a3dd89dee1f9d64420aa20ae65cfbd00668d6f094d7578a78efbb77b"},
+    {file = "tomlkit-0.12.4.tar.gz", hash = "sha256:7ca1cfc12232806517a8515047ba66a19369e71edf2439d0f5824f91032b6cc3"},
 ]
 
 [[package]]
 name = "tox"
-version = "4.11.4"
+version = "4.14.2"
 requires_python = ">=3.8"
 summary = "tox is a generic virtualenv management and test command line tool"
+groups = ["tox"]
 dependencies = [
-    "cachetools>=5.3.1",
+    "cachetools>=5.3.2",
     "chardet>=5.2",
     "colorama>=0.4.6",
-    "filelock>=3.12.3",
-    "packaging>=23.1",
-    "platformdirs>=3.10",
+    "filelock>=3.13.1",
+    "packaging>=23.2",
+    "platformdirs>=4.1",
     "pluggy>=1.3",
     "pyproject-api>=1.6.1",
-    "virtualenv>=20.24.3",
+    "virtualenv>=20.25",
 ]
 files = [
-    {file = "tox-4.11.4-py3-none-any.whl", hash = "sha256:2adb83d68f27116812b69aa36676a8d6a52249cb0d173649de0e7d0c2e3e7229"},
-    {file = "tox-4.11.4.tar.gz", hash = "sha256:73a7240778fabf305aeb05ab8ea26e575e042ab5a18d71d0ed13e343a51d6ce1"},
+    {file = "tox-4.14.2-py3-none-any.whl", hash = "sha256:2900c4eb7b716af4a928a7fdc2ed248ad6575294ed7cfae2ea41203937422847"},
+    {file = "tox-4.14.2.tar.gz", hash = "sha256:0defb44f6dafd911b61788325741cc6b2e12ea71f987ac025ad4d649f1f1a104"},
 ]
 
 [[package]]
 name = "tox-pdm"
-version = "0.7.0"
+version = "0.7.2"
 requires_python = ">=3.7"
 summary = "A plugin for tox that utilizes PDM as the package manager and installer"
+groups = ["tox"]
 dependencies = [
     "tox>=4.0",
 ]
 files = [
-    {file = "tox_pdm-0.7.0-py3-none-any.whl", hash = "sha256:13b36536b29c94943ff2d2ae65eaf81ca6c7afbb5b7ba2ad7cb3738fc726d662"},
-    {file = "tox_pdm-0.7.0.tar.gz", hash = "sha256:8ecc5d04df73a329f435574222de632dc0ded36b79f27e55d6dab92e9d1741f5"},
+    {file = "tox_pdm-0.7.2-py3-none-any.whl", hash = "sha256:12f6215416b7acd00a80a9e7128f3dc3e3c89308d60707f5d0a24abdf83ac104"},
+    {file = "tox_pdm-0.7.2.tar.gz", hash = "sha256:a841a7e1e942a71805624703b9a6d286663bd6af79bba6130ba756975c315308"},
 ]
 
 [[package]]
 name = "trove-classifiers"
-version = "2023.11.29"
+version = "2024.3.25"
 summary = "Canonical source for classifiers on PyPI (pypi.org)."
+groups = ["dev", "test"]
 files = [
-    {file = "trove-classifiers-2023.11.29.tar.gz", hash = "sha256:ff8f7fd82c7932113b46e7ef6742c70091cc63640c8c65db00d91f2e940b9514"},
-    {file = "trove_classifiers-2023.11.29-py3-none-any.whl", hash = "sha256:02307750cbbac2b3d13078662f8a5bf077732bf506e9c33c97204b7f68f3699e"},
+    {file = "trove-classifiers-2024.3.25.tar.gz", hash = "sha256:6de68d06edd6fec5032162b6af22e818a4bb6f4ae2258e74699f8a41064b7cad"},
+    {file = "trove_classifiers-2024.3.25-py3-none-any.whl", hash = "sha256:c400e0bdceb018913339d53b07682d09a42aada687d070e90ee3c08477bec024"},
 ]
 
 [[package]]
 name = "trustme"
 version = "1.1.0"
 requires_python = ">=3.8"
 summary = "#1 quality TLS certs while you wait, for the discerning tester"
+groups = ["test"]
 dependencies = [
     "cryptography>=3.1",
     "idna>=2.0",
 ]
 files = [
     {file = "trustme-1.1.0-py3-none-any.whl", hash = "sha256:ce105b68fb9f6d7ac7a9ee6e95bb2347a22ce4d3be78ef9a6494d5ef890e1e16"},
     {file = "trustme-1.1.0.tar.gz", hash = "sha256:5375ad7fb427074bec956592e0d4ee2a4cf4da68934e1ba4bcf4217126bc45e6"},
 ]
 
 [[package]]
-name = "twine"
-version = "4.0.2"
-requires_python = ">=3.7"
-summary = "Collection of utilities for publishing packages on PyPI"
-dependencies = [
-    "importlib-metadata>=3.6",
-    "keyring>=15.1",
-    "pkginfo>=1.8.1",
-    "readme-renderer>=35.0",
-    "requests-toolbelt!=0.9.0,>=0.8.0",
-    "requests>=2.20",
-    "rfc3986>=1.4.0",
-    "rich>=12.0.0",
-    "urllib3>=1.26.0",
-]
-files = [
-    {file = "twine-4.0.2-py3-none-any.whl", hash = "sha256:929bc3c280033347a00f847236564d1c52a3e61b1ac2516c97c48f3ceab756d8"},
-    {file = "twine-4.0.2.tar.gz", hash = "sha256:9e102ef5fdd5a20661eb88fad46338806c3bd32cf1db729603fe3697b1bc83c8"},
-]
-
-[[package]]
-name = "types-cryptography"
-version = "3.3.23.2"
-summary = "Typing stubs for cryptography"
-files = [
-    {file = "types-cryptography-3.3.23.2.tar.gz", hash = "sha256:09cc53f273dd4d8c29fa7ad11fefd9b734126d467960162397bc5e3e604dea75"},
-    {file = "types_cryptography-3.3.23.2-py3-none-any.whl", hash = "sha256:b965d548f148f8e87f353ccf2b7bd92719fdf6c845ff7cedf2abb393a0643e4f"},
-]
-
-[[package]]
 name = "typing-extensions"
-version = "4.8.0"
+version = "4.11.0"
 requires_python = ">=3.8"
 summary = "Backported and Experimental Type Hints for Python 3.8+"
+groups = ["dev", "doc", "mypy"]
 files = [
-    {file = "typing_extensions-4.8.0-py3-none-any.whl", hash = "sha256:8f92fc8806f9a6b641eaa5318da32b44d401efaac0f6678c9bc448ba3605faa0"},
-    {file = "typing_extensions-4.8.0.tar.gz", hash = "sha256:df8e4339e9cb77357558cbdbceca33c303714cf861d1eef15e1070055ae8b7ef"},
+    {file = "typing_extensions-4.11.0-py3-none-any.whl", hash = "sha256:c1f94d72897edaf4ce775bb7558d5b79d8126906a14ea5ed1635921406c0387a"},
+    {file = "typing_extensions-4.11.0.tar.gz", hash = "sha256:83f085bd5ca59c80295fc2a82ab5dac679cbe02b9f33f7d83af68e241bea51b0"},
 ]
 
 [[package]]
 name = "urllib3"
-version = "2.1.0"
+version = "2.2.1"
 requires_python = ">=3.8"
 summary = "HTTP library with thread-safe connection pooling, file post, and more."
+groups = ["benchmark-servers", "doc"]
 files = [
-    {file = "urllib3-2.1.0-py3-none-any.whl", hash = "sha256:55901e917a5896a349ff771be919f8bd99aff50b79fe58fec595eb37bbc56bb3"},
-    {file = "urllib3-2.1.0.tar.gz", hash = "sha256:df7aa8afb0148fa78488e7899b2c59b5f4ffcfa82e6c54ccb9dd37c1d7b52d54"},
+    {file = "urllib3-2.2.1-py3-none-any.whl", hash = "sha256:450b20ec296a467077128bff42b73080516e71b56ff59a60a02bef2232c4fa9d"},
+    {file = "urllib3-2.2.1.tar.gz", hash = "sha256:d0570876c61ab9e520d776c38acbbb5b05a776d3f9ff98a5c8fd5162a444cf19"},
 ]
 
 [[package]]
 name = "userpath"
-version = "1.9.1"
+version = "1.9.2"
 requires_python = ">=3.7"
 summary = "Cross-platform tool for adding locations to the user PATH"
+groups = ["dev"]
 dependencies = [
     "click",
 ]
 files = [
-    {file = "userpath-1.9.1-py3-none-any.whl", hash = "sha256:e085053e5161f82558793c41d60375289efceb4b77d96033ea9c84fc0893f772"},
-    {file = "userpath-1.9.1.tar.gz", hash = "sha256:ce8176728d98c914b6401781bf3b23fccd968d1647539c8788c7010375e02796"},
+    {file = "userpath-1.9.2-py3-none-any.whl", hash = "sha256:2cbf01a23d655a1ff8fc166dfb78da1b641d1ceabf0fe5f970767d380b14e89d"},
+    {file = "userpath-1.9.2.tar.gz", hash = "sha256:6c52288dab069257cc831846d15d48133522455d4677ee69a9781f11dbefd815"},
 ]
 
 [[package]]
 name = "uvloop"
 version = "0.19.0"
 requires_python = ">=3.8.0"
 summary = "Fast implementation of asyncio event loop on top of libuv"
+groups = ["benchmark-servers-deps", "uvloop"]
+marker = "os_name == \"posix\""
 files = [
     {file = "uvloop-0.19.0-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:4ce6b0af8f2729a02a5d1575feacb2a94fc7b2e983868b009d51c9a9d2149bef"},
     {file = "uvloop-0.19.0-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:31e672bb38b45abc4f26e273be83b72a0d28d074d5b370fc4dcf4c4eb15417d2"},
     {file = "uvloop-0.19.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:570fc0ed613883d8d30ee40397b79207eedd2624891692471808a95069a007c1"},
     {file = "uvloop-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:5138821e40b0c3e6c9478643b4660bd44372ae1e16a322b8fc07478f92684e24"},
     {file = "uvloop-0.19.0-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:91ab01c6cd00e39cde50173ba4ec68a1e578fee9279ba64f5221810a9e786533"},
     {file = "uvloop-0.19.0-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:47bf3e9312f63684efe283f7342afb414eea4d3011542155c7e625cd799c3b12"},
@@ -1942,38 +2084,72 @@
     {file = "uvloop-0.19.0-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:5daa304d2161d2918fa9a17d5635099a2f78ae5b5960e742b2fcfbb7aefaa593"},
     {file = "uvloop-0.19.0-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:7207272c9520203fea9b93843bb775d03e1cf88a80a936ce760f60bb5add92f3"},
     {file = "uvloop-0.19.0.tar.gz", hash = "sha256:0246f4fd1bf2bf702e06b0d45ee91677ee5c31242f39aab4ea6fe0c51aedd0fd"},
 ]
 
 [[package]]
 name = "virtualenv"
-version = "20.25.0"
+version = "20.25.1"
 requires_python = ">=3.7"
 summary = "Virtual Python Environment builder"
+groups = ["dev", "pre-commit", "tox"]
 dependencies = [
     "distlib<1,>=0.3.7",
     "filelock<4,>=3.12.2",
     "platformdirs<5,>=3.9.1",
 ]
 files = [
-    {file = "virtualenv-20.25.0-py3-none-any.whl", hash = "sha256:4238949c5ffe6876362d9c0180fc6c3a824a7b12b80604eeb8085f2ed7460de3"},
-    {file = "virtualenv-20.25.0.tar.gz", hash = "sha256:bf51c0d9c7dd63ea8e44086fa1e4fb1093a31e963b86959257378aef020e1f1b"},
+    {file = "virtualenv-20.25.1-py3-none-any.whl", hash = "sha256:961c026ac520bac5f69acb8ea063e8a4f071bcc9457b9c1f28f6b085c511583a"},
+    {file = "virtualenv-20.25.1.tar.gz", hash = "sha256:e08e13ecdca7a0bd53798f356d5831434afa5b07b93f0abdf0797b7a06ffe197"},
 ]
 
 [[package]]
 name = "webencodings"
 version = "0.5.1"
 summary = "Character encoding aliases for legacy web content"
+groups = ["doc"]
 files = [
     {file = "webencodings-0.5.1-py2.py3-none-any.whl", hash = "sha256:a0af1213f3c2226497a97e2b3aa01a7e4bee4f403f95be16fc9acd2947514a78"},
     {file = "webencodings-0.5.1.tar.gz", hash = "sha256:b36a1c245f2d304965eb4e0a82848379241dc04b865afcc4aab16748587e1923"},
 ]
 
 [[package]]
 name = "zipp"
-version = "3.17.0"
+version = "3.18.1"
 requires_python = ">=3.8"
 summary = "Backport of pathlib-compatible object wrapper for zip files"
+groups = ["dev", "micro-benchmark"]
 files = [
-    {file = "zipp-3.17.0-py3-none-any.whl", hash = "sha256:0e923e726174922dce09c53c59ad483ff7bbb8e572e00c7f7c46b88556409f31"},
-    {file = "zipp-3.17.0.tar.gz", hash = "sha256:84e64a1c28cf7e91ed2078bb8cc8c259cb19b76942096c8d7b84947690cabaf0"},
+    {file = "zipp-3.18.1-py3-none-any.whl", hash = "sha256:206f5a15f2af3dbaee80769fb7dc6f249695e940acca08dfb2a4769fe61e538b"},
+    {file = "zipp-3.18.1.tar.gz", hash = "sha256:2884ed22e7d8961de1c9a05142eb69a247f120291bc0206a00a7642f09b5b715"},
+]
+
+[[package]]
+name = "zstandard"
+version = "0.22.0"
+requires_python = ">=3.8"
+summary = "Zstandard bindings for Python"
+groups = ["dev"]
+dependencies = [
+    "cffi>=1.11; platform_python_implementation == \"PyPy\"",
+]
+files = [
+    {file = "zstandard-0.22.0-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:589402548251056878d2e7c8859286eb91bd841af117dbe4ab000e6450987e08"},
+    {file = "zstandard-0.22.0-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:a97079b955b00b732c6f280d5023e0eefe359045e8b83b08cf0333af9ec78f26"},
+    {file = "zstandard-0.22.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:445b47bc32de69d990ad0f34da0e20f535914623d1e506e74d6bc5c9dc40bb09"},
+    {file = "zstandard-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:33591d59f4956c9812f8063eff2e2c0065bc02050837f152574069f5f9f17775"},
+    {file = "zstandard-0.22.0-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:888196c9c8893a1e8ff5e89b8f894e7f4f0e64a5af4d8f3c410f0319128bb2f8"},
+    {file = "zstandard-0.22.0-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:53866a9d8ab363271c9e80c7c2e9441814961d47f88c9bc3b248142c32141d94"},
+    {file = "zstandard-0.22.0-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:4ac59d5d6910b220141c1737b79d4a5aa9e57466e7469a012ed42ce2d3995e88"},
+    {file = "zstandard-0.22.0-cp311-cp311-win32.whl", hash = "sha256:2b11ea433db22e720758cba584c9d661077121fcf60ab43351950ded20283440"},
+    {file = "zstandard-0.22.0-cp311-cp311-win_amd64.whl", hash = "sha256:11f0d1aab9516a497137b41e3d3ed4bbf7b2ee2abc79e5c8b010ad286d7464bd"},
+    {file = "zstandard-0.22.0-cp312-cp312-macosx_10_9_x86_64.whl", hash = "sha256:6c25b8eb733d4e741246151d895dd0308137532737f337411160ff69ca24f93a"},
+    {file = "zstandard-0.22.0-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:f9b2cde1cd1b2a10246dbc143ba49d942d14fb3d2b4bccf4618d475c65464912"},
+    {file = "zstandard-0.22.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:a88b7df61a292603e7cd662d92565d915796b094ffb3d206579aaebac6b85d5f"},
+    {file = "zstandard-0.22.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:466e6ad8caefb589ed281c076deb6f0cd330e8bc13c5035854ffb9c2014b118c"},
+    {file = "zstandard-0.22.0-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:a1d67d0d53d2a138f9e29d8acdabe11310c185e36f0a848efa104d4e40b808e4"},
+    {file = "zstandard-0.22.0-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:39b2853efc9403927f9065cc48c9980649462acbdf81cd4f0cb773af2fd734bc"},
+    {file = "zstandard-0.22.0-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:8a1b2effa96a5f019e72874969394edd393e2fbd6414a8208fea363a22803b45"},
+    {file = "zstandard-0.22.0-cp312-cp312-win32.whl", hash = "sha256:88c5b4b47a8a138338a07fc94e2ba3b1535f69247670abfe422de4e0b344aae2"},
+    {file = "zstandard-0.22.0-cp312-cp312-win_amd64.whl", hash = "sha256:de20a212ef3d00d609d0b22eb7cc798d5a69035e81839f549b538eff4105d01c"},
+    {file = "zstandard-0.22.0.tar.gz", hash = "sha256:8226a33c542bcb54cd6bd0a366067b610b41713b64c9abec1bc4533d69f51e70"},
 ]
```

### Comparing `easynetwork-1.0.0rc7/docs/Makefile` & `easynetwork-1.0.0rc8/docs/Makefile`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/docs/make.bat` & `easynetwork-1.0.0rc8/docs/make.bat`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/docs/source/conf.py` & `easynetwork-1.0.0rc8/docs/source/conf.py`

 * *Files 1% similar despite different names*

```diff
@@ -9,15 +9,15 @@
 import os.path
 import sys
 from importlib.metadata import version as get_version
 
 # -- Project information -----------------------------------------------------
 # https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information
 project = "EasyNetwork"
-copyright = "2023, Francis Clairicia-Rose-Claire-Josephine"
+copyright = "2024, Francis Clairicia-Rose-Claire-Josephine"
 author = "FrankySnow9"
 release = get_version("easynetwork")
 version = ".".join(release.split(".")[:3])
 
 # -- General configuration ---------------------------------------------------
 # https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration
 
@@ -99,15 +99,14 @@
 
 # -- sphinx.ext.intersphinx configuration ------------------------------------
 # https://www.sphinx-doc.org/en/master/usage/extensions/intersphinx.html
 
 intersphinx_mapping = {
     "python": ("https://docs.python.org/3", None),
     "cbor2": ("https://cbor2.readthedocs.io/en/stable", None),
-    "cryptography": ("https://cryptography.io/en/stable", None),
     "msgpack": ("https://msgpack-python.readthedocs.io/en/stable", None),
 }
 
 
 # -- sphinx.ext.napoleon configuration ---------------------------------------
 # https://www.sphinx-doc.org/en/master/usage/extensions/napoleon.html
```

### Comparing `easynetwork-1.0.0rc7/docs/source/glossary.rst` & `easynetwork-1.0.0rc8/docs/source/glossary.rst`

 * *Files 2% similar despite different names*

```diff
@@ -1,14 +1,25 @@
 ********
 Glossary
 ********
 
 .. glossary::
    :sorted:
 
+   asynchronous framework
+      The library which permits you to write asynchronous tasks, such as:
+
+      * :external+python:doc:`asyncio <library/asyncio>`
+
+      * `trio <https://trio.readthedocs.io/en/stable/>`_
+
+      * `anyio <https://anyio.readthedocs.io/en/stable/>`_
+
+      * etc.
+
    buffered serializer
       See :term:`serializer`.
 
    communication protocol
       A set of formal rules describing how to transmit or exchange data, especially across a network.
 
       In EasyNetwork, it is up to the developer to define his communication protocol using a :term:`protocol object`.
```

### Comparing `easynetwork-1.0.0rc7/docs/source/index.rst` & `easynetwork-1.0.0rc8/docs/source/index.rst`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/docs/source/_extensions/sphinx_easynetwork.py` & `easynetwork-1.0.0rc8/docs/source/_extensions/sphinx_easynetwork.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,16 +1,16 @@
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, Any
 
 if TYPE_CHECKING:
     from sphinx.application import Sphinx
 
-from easynetwork.api_sync.server import AbstractNetworkServer
-from easynetwork.api_sync.server._base import BaseStandaloneNetworkServerImpl
+from easynetwork.servers._base import BaseStandaloneNetworkServerImpl
+from easynetwork.servers.abc import AbstractNetworkServer
 
 
 def _replace_base_in_place(klass: type, bases: list[type], base_to_replace: type, base_to_set_instead: type) -> None:
     if issubclass(klass, base_to_replace):
         for index, base in enumerate(bases):
             if base is base_to_replace:
                 bases[index] = base_to_set_instead
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/howto/protocols/composite_converter/packet_converter_subclass.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/howto/protocols/composite_converter/packet_converter_subclass.py`

 * *Files 10% similar despite different names*

```diff
@@ -5,36 +5,30 @@
 from typing import Any
 
 from easynetwork.converter import AbstractPacketConverterComposite
 from easynetwork.protocol import StreamProtocol
 from easynetwork.serializers import JSONSerializer
 
 
-class Request:
-    ...
+class Request: ...
 
 
-class Response:
-    ...
+class Response: ...
 
 
 class ClientConverter(AbstractPacketConverterComposite[Request, Response, dict[str, Any], dict[str, Any]]):
-    def convert_to_dto_packet(self, request: Request, /) -> dict[str, Any]:
-        ...
+    def convert_to_dto_packet(self, request: Request, /) -> dict[str, Any]: ...
 
-    def create_from_dto_packet(self, response_dict: dict[str, Any], /) -> Response:
-        ...
+    def create_from_dto_packet(self, response_dict: dict[str, Any], /) -> Response: ...
 
 
 class ServerConverter(AbstractPacketConverterComposite[Response, Request, dict[str, Any], dict[str, Any]]):
-    def convert_to_dto_packet(self, response: Response, /) -> dict[str, Any]:
-        ...
+    def convert_to_dto_packet(self, response: Response, /) -> dict[str, Any]: ...
 
-    def create_from_dto_packet(self, request_dict: dict[str, Any], /) -> Request:
-        ...
+    def create_from_dto_packet(self, request_dict: dict[str, Any], /) -> Request: ...
 
 
 serializer = JSONSerializer()
 
 client_protocol: StreamProtocol[Request, Response] = StreamProtocol(
     serializer=serializer,
     converter=ClientConverter(),
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/howto/protocols/composite_converter/stapled_packet_converter.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/howto/protocols/composite_converter/stapled_packet_converter.py`

 * *Files 7% similar despite different names*

```diff
@@ -5,36 +5,30 @@
 from typing import Any
 
 from easynetwork.converter import AbstractPacketConverter, StapledPacketConverter
 from easynetwork.protocol import StreamProtocol
 from easynetwork.serializers import JSONSerializer
 
 
-class Request:
-    ...
+class Request: ...
 
 
-class Response:
-    ...
+class Response: ...
 
 
 class RequestConverter(AbstractPacketConverter[Request, dict[str, Any]]):
-    def convert_to_dto_packet(self, request: Request, /) -> dict[str, Any]:
-        ...
+    def convert_to_dto_packet(self, request: Request, /) -> dict[str, Any]: ...
 
-    def create_from_dto_packet(self, request_dict: dict[str, Any], /) -> Request:
-        ...
+    def create_from_dto_packet(self, request_dict: dict[str, Any], /) -> Request: ...
 
 
 class ResponseConverter(AbstractPacketConverter[Response, dict[str, Any]]):
-    def convert_to_dto_packet(self, response: Response, /) -> dict[str, Any]:
-        ...
+    def convert_to_dto_packet(self, response: Response, /) -> dict[str, Any]: ...
 
-    def create_from_dto_packet(self, response_dict: dict[str, Any], /) -> Response:
-        ...
+    def create_from_dto_packet(self, response_dict: dict[str, Any], /) -> Response: ...
 
 
 serializer = JSONSerializer()
 request_converter = RequestConverter()
 response_converter = ResponseConverter()
 
 client_protocol: StreamProtocol[Request, Response] = StreamProtocol(
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/howto/protocols/simple_converter/example1.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/howto/protocols/simple_converter/example1.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/howto/protocols/simple_converter/example2.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/howto/protocols/simple_converter/example2.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 from __future__ import annotations
 
 import uuid
 
-from easynetwork.api_sync.client import TCPNetworkClient
+from easynetwork.clients import TCPNetworkClient
 from easynetwork.exceptions import DeserializeError, PacketConversionError, StreamProtocolParseError
 from easynetwork.protocol import StreamProtocol
 from easynetwork.serializers import JSONSerializer
 
 from .example1 import Person, PersonToJSONConverter
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/howto/protocols/usage/datagram_protocol.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/howto/protocols/usage/datagram_protocol.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 from __future__ import annotations
 
-from easynetwork.api_sync.client import UDPNetworkClient
+from easynetwork.clients import UDPNetworkClient
 from easynetwork.exceptions import DatagramProtocolParseError
 
 from ..basics.datagram_protocol_subclass import JSONDatagramProtocol
 
 
 def main() -> None:
     protocol = JSONDatagramProtocol()
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/howto/protocols/usage/stream_protocol.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/howto/protocols/usage/stream_protocol.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 from __future__ import annotations
 
-from easynetwork.api_sync.client import TCPNetworkClient
+from easynetwork.clients import TCPNetworkClient
 from easynetwork.exceptions import StreamProtocolParseError
 
 from ..basics.stream_protocol_subclass import JSONStreamProtocol
 
 
 def main() -> None:
     protocol = JSONStreamProtocol()
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/howto/serializers/buffered_incremental_serializer/example1.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/howto/serializers/buffered_incremental_serializer/example1.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/howto/serializers/buffered_incremental_serializer/example2.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/howto/serializers/buffered_incremental_serializer/example2.py`

 * *Files 12% similar despite different names*

```diff
@@ -26,16 +26,15 @@
                     break
                 buffer.write((yield))
                 buffer.seek(0)
 
             remainder = buffer.read()
             return packet, remainder
 
-    def _load_from_file(self, file: io.IOBase) -> ReceivedPacket:
-        ...
+    def _load_from_file(self, file: io.IOBase) -> ReceivedPacket: ...
 
     def create_deserializer_buffer(self, sizehint: int) -> memoryview:
         # Don't care about buffer size
         buffer = bytearray(sizehint)
         return memoryview(buffer)
 
     def buffered_incremental_deserialize(self, buffer: memoryview) -> Generator[None, int, tuple[ReceivedPacket, bytes]]:
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/howto/serializers/incremental_serializer/example1.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/howto/serializers/incremental_serializer/example1.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/howto/serializers/incremental_serializer/example2.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/howto/serializers/incremental_serializer/example2.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/howto/serializers/one_shot_serializer/example1.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/howto/serializers/one_shot_serializer/example1.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/howto/serializers/one_shot_serializer/example2.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/howto/serializers/one_shot_serializer/example2.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/howto/serializers/one_shot_serializer/example3.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/howto/serializers/one_shot_serializer/example3.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/howto/serializers/one_shot_serializer/example4_stream.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/howto/serializers/one_shot_serializer/example4_stream.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 from __future__ import annotations
 
-from easynetwork.api_sync.client import TCPNetworkClient
+from easynetwork.clients import TCPNetworkClient
 from easynetwork.protocol import StreamProtocol
 from easynetwork.serializers.wrapper import Base64EncoderSerializer
 
 from .example3 import MyJSONSerializer
 
 
 def main() -> None:
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/howto/tcp_clients/basics/api_async/connection_example1.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/howto/tcp_clients/basics/api_async/connection_example1.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 from __future__ import annotations
 
 import asyncio
 
-from easynetwork.api_async.client import AsyncTCPNetworkClient
+from easynetwork.clients import AsyncTCPNetworkClient
 from easynetwork.protocol import StreamProtocol
 from easynetwork.serializers import JSONSerializer
 
 
 async def main() -> None:
     protocol = StreamProtocol(JSONSerializer())
     address = ("localhost", 9000)
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/howto/tcp_clients/basics/api_async/connection_example2.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/howto/tcp_clients/basics/api_async/connection_example2.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 from __future__ import annotations
 
 import asyncio
 
-from easynetwork.api_async.client import AsyncTCPNetworkClient
+from easynetwork.clients import AsyncTCPNetworkClient
 from easynetwork.protocol import StreamProtocol
 from easynetwork.serializers import JSONSerializer
 
 
 async def main() -> None:
     protocol = StreamProtocol(JSONSerializer())
     address = ("localhost", 9000)
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/howto/tcp_clients/basics/api_async/socket_example1.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/howto/tcp_clients/basics/api_async/socket_example1.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 from __future__ import annotations
 
 import asyncio
 import socket
 
-from easynetwork.api_async.client import AsyncTCPNetworkClient
+from easynetwork.clients import AsyncTCPNetworkClient
 from easynetwork.protocol import StreamProtocol
 from easynetwork.serializers import JSONSerializer
 
 
 async def obtain_a_connected_socket() -> socket.socket:
     sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/howto/tcp_clients/basics/api_sync/connection_example2.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/howto/tcp_clients/basics/api_sync/connection_example2.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 from __future__ import annotations
 
-from easynetwork.api_sync.client import TCPNetworkClient
+from easynetwork.clients import TCPNetworkClient
 from easynetwork.protocol import StreamProtocol
 from easynetwork.serializers import JSONSerializer
 
 
 def main() -> None:
     protocol = StreamProtocol(JSONSerializer())
     address = ("localhost", 9000)
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/howto/tcp_clients/basics/api_sync/socket_example1.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/howto/tcp_clients/basics/api_sync/socket_example1.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 from __future__ import annotations
 
 import socket
 
-from easynetwork.api_sync.client import TCPNetworkClient
+from easynetwork.clients import TCPNetworkClient
 from easynetwork.protocol import StreamProtocol
 from easynetwork.serializers import JSONSerializer
 
 
 def obtain_a_connected_socket() -> socket.socket:
     sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/howto/tcp_clients/concurrency/api_async.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/howto/tcp_servers/background_server_ssl.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,69 +1,67 @@
 from __future__ import annotations
 
 import asyncio
-import contextlib
-import traceback
-from typing import Any, TypeAlias
+import ssl
+from collections.abc import AsyncGenerator
+from typing import Any
 
-from easynetwork.api_async.client import AsyncTCPNetworkClient
-from easynetwork.exceptions import StreamProtocolParseError
+from easynetwork.clients import AsyncTCPNetworkClient
 from easynetwork.protocol import StreamProtocol
 from easynetwork.serializers import JSONSerializer
+from easynetwork.servers import AsyncTCPNetworkServer
+from easynetwork.servers.handlers import AsyncStreamClient, AsyncStreamRequestHandler
 
-RequestType: TypeAlias = dict[str, Any]
-ResponseType: TypeAlias = dict[str, Any]
 
+class JSONProtocol(StreamProtocol[dict[str, Any], dict[str, Any]]):
+    def __init__(self) -> None:
+        super().__init__(JSONSerializer())
 
-async def consumer(response: ResponseType) -> None:
-    # Do some stuff...
 
-    print(response)
+class MyRequestHandler(AsyncStreamRequestHandler[dict[str, Any], dict[str, Any]]):
+    async def handle(
+        self,
+        client: AsyncStreamClient[dict[str, Any]],
+    ) -> AsyncGenerator[None, dict[str, Any]]:
+        request: dict[str, Any] = yield
 
+        current_task = asyncio.current_task()
+        assert current_task is not None
 
-async def receiver_worker(
-    client: AsyncTCPNetworkClient[RequestType, ResponseType],
-    task_group: asyncio.TaskGroup,
-) -> None:
-    while True:
-        try:
-            # Pass timeout=None to get an infinite iterator.
-            # It will be terminated when the client.close() method has been called.
-            async for response in client.iter_received_packets(timeout=None):
-                _ = task_group.create_task(consumer(response))
-        except StreamProtocolParseError:
-            print("Parsing error")
-            traceback.print_exc()
-            continue
-
-
-async def do_main_stuff(
-    client: AsyncTCPNetworkClient[RequestType, ResponseType],
-) -> None:
-    while True:
-        # Do some stuff...
-        request = {"data": 42}
+        await client.send_packet({"task": current_task.get_name(), "request": request})
 
-        await client.send_packet(request)
 
+async def client(host: str, port: int, message: str) -> None:
+    async with AsyncTCPNetworkClient((host, port), JSONProtocol(), ssl=True) as client:
+        await client.send_packet({"message": message})
+        response = await client.recv_packet()
+        print(f"From server: {response}")
 
-async def main() -> None:
-    remote_address = ("localhost", 9000)
-    protocol = StreamProtocol(JSONSerializer())
-
-    async with contextlib.AsyncExitStack() as exit_stack:
-        # task group setup
-        task_group = await exit_stack.enter_async_context(asyncio.TaskGroup())
-
-        # connect to remote
-        client = AsyncTCPNetworkClient(remote_address, protocol)
-        await exit_stack.enter_async_context(client)
 
-        # receiver_worker task setup
-        _ = task_group.create_task(receiver_worker(client, task_group))
+async def main() -> None:
+    host, port = "localhost", 9000
+    protocol = JSONProtocol()
+    handler = MyRequestHandler()
+
+    ssl_context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)
+    ssl_context.load_cert_chain(
+        "/path/to/ssl_cert.pem",
+        "/path/to/ssl_key.pem",
+    )
+    server = AsyncTCPNetworkServer(host, port, protocol, handler, ssl=ssl_context)
+
+    async with server:
+        is_up_event = asyncio.Event()
+        server_task = asyncio.create_task(server.serve_forever(is_up_event=is_up_event))
+        await is_up_event.wait()
+
+        print(f"Server loop running in task: {server_task.get_name()}")
+
+        await client(host, port, "Hello world 1")
+        await client(host, port, "Hello world 2")
+        await client(host, port, "Hello world 3")
 
-        # Setup done, let's go
-        await do_main_stuff(client)
+        await server.shutdown()
 
 
 if __name__ == "__main__":
     asyncio.run(main())
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/howto/tcp_clients/concurrency/ssl_shared_lock.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/howto/tcp_clients/concurrency/ssl_shared_lock.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,11 +1,10 @@
 from __future__ import annotations
 
-from easynetwork.api_async.client import AsyncTCPNetworkClient
-from easynetwork.api_sync.client import TCPNetworkClient
+from easynetwork.clients import AsyncTCPNetworkClient, TCPNetworkClient
 from easynetwork.protocol import StreamProtocol
 from easynetwork.serializers import JSONSerializer
 
 
 def ssl_shared_lock_for_sync_client() -> None:
     remote_address = ("remote_address", 12345)
     protocol = StreamProtocol(JSONSerializer())
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/howto/tcp_clients/usage/api_async.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/howto/tcp_clients/usage/api_async.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 from __future__ import annotations
 
 import asyncio
 import socket
 from typing import Any
 
-from easynetwork.api_async.client import AsyncTCPNetworkClient
+from easynetwork.clients import AsyncTCPNetworkClient
 from easynetwork.exceptions import StreamProtocolParseError
 from easynetwork.protocol import StreamProtocol
 from easynetwork.serializers import JSONSerializer
 
 ###############
 # Basic usage #
 ###############
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/howto/tcp_clients/usage/api_sync.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/howto/tcp_clients/usage/api_sync.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 from __future__ import annotations
 
 import socket
 from typing import Any
 
-from easynetwork.api_sync.client import TCPNetworkClient
+from easynetwork.clients import TCPNetworkClient
 from easynetwork.exceptions import StreamProtocolParseError
 from easynetwork.protocol import StreamProtocol
 from easynetwork.serializers import JSONSerializer
 
 ###############
 # Basic usage #
 ###############
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/howto/tcp_servers/async_server.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/howto/tcp_servers/async_server.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,22 +1,21 @@
 from __future__ import annotations
 
 import asyncio
 from collections.abc import AsyncGenerator
 
-from easynetwork.api_async.server import AsyncStreamClient, AsyncStreamRequestHandler, AsyncTCPNetworkServer
 from easynetwork.protocol import StreamProtocol
+from easynetwork.servers import AsyncTCPNetworkServer
+from easynetwork.servers.handlers import AsyncStreamClient, AsyncStreamRequestHandler
 
 
-class Request:
-    ...
+class Request: ...
 
 
-class Response:
-    ...
+class Response: ...
 
 
 class MyRequestHandler(AsyncStreamRequestHandler[Request, Response]):
     async def handle(
         self,
         client: AsyncStreamClient[Response],
     ) -> AsyncGenerator[None, Request]:
@@ -25,16 +24,15 @@
         ...
 
         await client.send_packet(Response())
 
 
 # NOTE: The sent packet is "Response" and the received packet is "Request"
 class ServerProtocol(StreamProtocol[Response, Request]):
-    def __init__(self) -> None:
-        ...
+    def __init__(self) -> None: ...
 
 
 async def main() -> None:
     host, port = "localhost", 9000
     protocol = ServerProtocol()
     handler = MyRequestHandler()
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/howto/tcp_servers/request_handler_explanation.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/howto/tcp_servers/request_handler_explanation.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,35 +1,36 @@
 from __future__ import annotations
 
 import asyncio
 import contextlib
+import contextvars
+import logging
+import traceback
 from collections.abc import AsyncGenerator
+from typing import ClassVar
 
-from easynetwork.api_async.server import AsyncStreamClient, AsyncStreamRequestHandler, AsyncTCPNetworkServer
 from easynetwork.exceptions import StreamProtocolParseError
+from easynetwork.lowlevel.socket import SocketAddress
+from easynetwork.servers import AsyncTCPNetworkServer
+from easynetwork.servers.handlers import AsyncStreamClient, AsyncStreamRequestHandler, INETClientAttribute
 
 
-class Request:
-    ...
+class Request: ...
 
 
-class Response:
-    ...
+class Response: ...
 
 
-class BadRequest(Response):
-    ...
+class BadRequest(Response): ...
 
 
-class InternalError(Response):
-    ...
+class InternalError(Response): ...
 
 
-class TimedOut(Response):
-    ...
+class TimedOut(Response): ...
 
 
 class MinimumRequestHandler(AsyncStreamRequestHandler[Request, Response]):
     async def handle(
         self,
         client: AsyncStreamClient[Response],
     ) -> AsyncGenerator[None, Request]:
@@ -112,14 +113,17 @@
             # You should check if the client is always usable.
             try:
                 await client.send_packet(InternalError())
             except OSError:
                 await client.aclose()
                 raise
         except Exception:
+            # Most likely a bug in EasyNetwork code. Log the error.
+            traceback.print_exc()
+
             await client.send_packet(InternalError())
         else:
             await client.send_packet(Response())
 
 
 class MultipleYieldInRequestHandler(AsyncStreamRequestHandler[Request, Response]):
     async def handle(
@@ -163,29 +167,43 @@
                 request: Request = yield
 
                 ...
 
                 await client.send_packet(Response())
 
 
-class TimeoutRequestHandler(AsyncStreamRequestHandler[Request, Response]):
+class TimeoutContextRequestHandler(AsyncStreamRequestHandler[Request, Response]):
     async def handle(
         self,
         client: AsyncStreamClient[Response],
     ) -> AsyncGenerator[None, Request]:
         try:
             async with asyncio.timeout(30):
                 # The client has 30 seconds to send the request to the server.
                 request: Request = yield
         except TimeoutError:
             await client.send_packet(TimedOut())
         else:
             await client.send_packet(Response())
 
 
+class TimeoutYieldedRequestHandler(AsyncStreamRequestHandler[Request, Response]):
+    async def handle(
+        self,
+        client: AsyncStreamClient[Response],
+    ) -> AsyncGenerator[float | None, Request]:
+        try:
+            # The client has 30 seconds to send the request to the server.
+            request: Request = yield 30.0
+        except TimeoutError:
+            await client.send_packet(TimedOut())
+        else:
+            await client.send_packet(Response())
+
+
 class ClientConnectionHooksRequestHandler(AsyncStreamRequestHandler[Request, Response]):
     async def on_connection(self, client: AsyncStreamClient[Response]) -> None:
         print(f"{client!r} is connected")
 
         # Notify the client that the service is ready.
         await client.send_packet(Response())
 
@@ -251,7 +269,43 @@
             await asyncio.sleep(1)
 
             # Do some stuff each second in background
             ...
 
     def _service_quit(self) -> None:
         print("Service stopped")
+
+
+class ClientContextRequestHandler(AsyncStreamRequestHandler[Request, Response]):
+    client_addr_var: ClassVar[contextvars.ContextVar[SocketAddress]]
+    client_addr_var = contextvars.ContextVar("client_addr")
+
+    @classmethod
+    def client_log(cls, message: str) -> None:
+        # The address of the currently handled client can be accessed
+        # without passing it explicitly to this function.
+
+        logger = logging.getLogger(cls.__name__)
+
+        client_address = cls.client_addr_var.get()
+
+        logger.info("From %s: %s", client_address, message)
+
+    async def on_connection(
+        self,
+        client: AsyncStreamClient[Response],
+    ) -> None:
+        address = client.extra(INETClientAttribute.remote_address)
+        self.client_addr_var.set(address)
+
+        # In any code that we call within "handle()" is now possible to get
+        # client's address by calling 'client_addr_var.get()'.
+
+    async def handle(
+        self,
+        client: AsyncStreamClient[Response],
+    ) -> AsyncGenerator[None, Request]:
+        request: Request = yield
+
+        self.client_log(f"Received request: {request!r}")
+
+        await client.send_packet(Response())
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/howto/tcp_servers/simple_request_handler.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/howto/tcp_servers/simple_request_handler.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 from __future__ import annotations
 
 from collections.abc import AsyncGenerator
 
-from easynetwork.api_async.server import AsyncStreamClient, AsyncStreamRequestHandler
+from easynetwork.servers.handlers import AsyncStreamClient, AsyncStreamRequestHandler
 
 
 class Request:
     """Object representing the client request."""
 
     ...
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/howto/udp_clients/basics/api_async/connection_example1.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/echo_client_server_udp/async_server.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,21 +1,25 @@
 from __future__ import annotations
 
 import asyncio
 
-from easynetwork.api_async.client import AsyncUDPNetworkClient
-from easynetwork.protocol import DatagramProtocol
-from easynetwork.serializers import JSONSerializer
+from easynetwork.servers import AsyncUDPNetworkServer
 
+from echo_request_handler import EchoRequestHandler
+from json_protocol import JSONDatagramProtocol
 
-async def main() -> None:
-    protocol = DatagramProtocol(JSONSerializer())
-    address = ("localhost", 9000)
-
-    async with AsyncUDPNetworkClient(address, protocol) as client:
-        print(f"Remote address: {client.get_remote_address()}")
 
-        ...
+async def main() -> None:
+    host = None
+    port = 9000
+    protocol = JSONDatagramProtocol()
+    handler = EchoRequestHandler()
+
+    async with AsyncUDPNetworkServer(host, port, protocol, handler) as server:
+        try:
+            await server.serve_forever()
+        except asyncio.CancelledError:
+            pass
 
 
 if __name__ == "__main__":
     asyncio.run(main())
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/howto/udp_clients/basics/api_async/socket_example1.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/howto/udp_clients/basics/api_async/socket_example1.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 from __future__ import annotations
 
 import asyncio
 import socket
 
-from easynetwork.api_async.client import AsyncUDPNetworkClient
+from easynetwork.clients import AsyncUDPNetworkClient
 from easynetwork.protocol import DatagramProtocol
 from easynetwork.serializers import JSONSerializer
 
 
 async def obtain_a_connected_socket() -> socket.socket:
     sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/howto/udp_clients/basics/api_sync/socket_example1.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/howto/udp_clients/basics/api_sync/socket_example1.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 from __future__ import annotations
 
 import socket
 
-from easynetwork.api_sync.client import UDPNetworkClient
+from easynetwork.clients import UDPNetworkClient
 from easynetwork.protocol import DatagramProtocol
 from easynetwork.serializers import JSONSerializer
 
 
 def obtain_a_connected_socket() -> socket.socket:
     sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/howto/udp_clients/usage/api_async.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/howto/udp_clients/usage/api_async.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 from __future__ import annotations
 
 import asyncio
 import socket
 from typing import Any
 
-from easynetwork.api_async.client import AsyncUDPNetworkClient
+from easynetwork.clients import AsyncUDPNetworkClient
 from easynetwork.exceptions import DatagramProtocolParseError
 
 ###############
 # Basic usage #
 ###############
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/howto/udp_clients/usage/api_sync.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/howto/udp_clients/usage/api_sync.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 from __future__ import annotations
 
 import socket
 from typing import Any
 
-from easynetwork.api_sync.client import UDPNetworkClient
+from easynetwork.clients import UDPNetworkClient
 from easynetwork.exceptions import DatagramProtocolParseError
 
 ###############
 # Basic usage #
 ###############
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/howto/udp_servers/async_server.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/howto/udp_servers/async_server.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,22 +1,21 @@
 from __future__ import annotations
 
 import asyncio
 from collections.abc import AsyncGenerator
 
-from easynetwork.api_async.server import AsyncDatagramClient, AsyncDatagramRequestHandler, AsyncUDPNetworkServer
 from easynetwork.protocol import DatagramProtocol
+from easynetwork.servers import AsyncUDPNetworkServer
+from easynetwork.servers.handlers import AsyncDatagramClient, AsyncDatagramRequestHandler
 
 
-class Request:
-    ...
+class Request: ...
 
 
-class Response:
-    ...
+class Response: ...
 
 
 class MyRequestHandler(AsyncDatagramRequestHandler[Request, Response]):
     async def handle(
         self,
         client: AsyncDatagramClient[Response],
     ) -> AsyncGenerator[None, Request]:
@@ -25,16 +24,15 @@
         ...
 
         await client.send_packet(Response())
 
 
 # NOTE: The sent packet is "Response" and the received packet is "Request"
 class ServerProtocol(DatagramProtocol[Response, Request]):
-    def __init__(self) -> None:
-        ...
+    def __init__(self) -> None: ...
 
 
 async def main() -> None:
     host, port = "localhost", 9000
     protocol = ServerProtocol()
     handler = MyRequestHandler()
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/howto/udp_servers/request_handler_explanation.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/howto/udp_servers/request_handler_explanation.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,35 +1,36 @@
 from __future__ import annotations
 
 import asyncio
 import contextlib
+import contextvars
+import logging
+import traceback
 from collections.abc import AsyncGenerator
+from typing import ClassVar
 
-from easynetwork.api_async.server import AsyncDatagramClient, AsyncDatagramRequestHandler, AsyncUDPNetworkServer
 from easynetwork.exceptions import DatagramProtocolParseError
+from easynetwork.lowlevel.socket import SocketAddress
+from easynetwork.servers import AsyncUDPNetworkServer
+from easynetwork.servers.handlers import AsyncDatagramClient, AsyncDatagramRequestHandler, INETClientAttribute
 
 
-class Request:
-    ...
+class Request: ...
 
 
-class Response:
-    ...
+class Response: ...
 
 
-class BadRequest(Response):
-    ...
+class BadRequest(Response): ...
 
 
-class InternalError(Response):
-    ...
+class InternalError(Response): ...
 
 
-class TimedOut(Response):
-    ...
+class TimedOut(Response): ...
 
 
 class MinimumRequestHandler(AsyncDatagramRequestHandler[Request, Response]):
     async def handle(
         self,
         client: AsyncDatagramClient[Response],
     ) -> AsyncGenerator[None, Request]:
@@ -80,14 +81,17 @@
         try:
             # *All* exceptions are thrown through the "yield" statement
             # (including BaseException). But you should only catch Exception subclasses.
             request: Request = yield
         except DatagramProtocolParseError:
             await client.send_packet(BadRequest())
         except Exception:
+            # Most likely a bug in EasyNetwork code. Log the error.
+            traceback.print_exc()
+
             await client.send_packet(InternalError())
         else:
             await client.send_packet(Response())
 
 
 class MultipleYieldInRequestHandler(AsyncDatagramRequestHandler[Request, Response]):
     async def handle(
@@ -107,21 +111,21 @@
 
             await client.send_packet(Response())
 
     def need_something_else(self, request: Request, client: AsyncDatagramClient[Response]) -> bool:
         return True
 
 
-class TimeoutRequestHandler(AsyncDatagramRequestHandler[Request, Response]):
+class TimeoutContextRequestHandler(AsyncDatagramRequestHandler[Request, Response]):
     async def handle(
         self,
         client: AsyncDatagramClient[Response],
     ) -> AsyncGenerator[None, Request]:
-        # It is *never* useful to have a timeout for the 1st datagram because the datagram
-        # is already in the queue.
+        # It is *never* useful to have a timeout for the 1st datagram
+        # because the datagram is already in the queue.
         request: Request = yield
 
         ...
 
         await client.send_packet(Response())
 
         try:
@@ -130,14 +134,36 @@
                 another_request: Request = yield
         except TimeoutError:
             await client.send_packet(TimedOut())
         else:
             await client.send_packet(Response())
 
 
+class TimeoutYieldedRequestHandler(AsyncDatagramRequestHandler[Request, Response]):
+    async def handle(
+        self,
+        client: AsyncDatagramClient[Response],
+    ) -> AsyncGenerator[float | None, Request]:
+        # It is *never* useful to have a timeout for the 1st datagram
+        # because the datagram is already in the queue.
+        request: Request = yield None
+
+        ...
+
+        await client.send_packet(Response())
+
+        try:
+            # The client has 30 seconds to send the 2nd request to the server.
+            another_request: Request = yield 30
+        except TimeoutError:
+            await client.send_packet(TimedOut())
+        else:
+            await client.send_packet(Response())
+
+
 class ServiceInitializationHookRequestHandler(AsyncDatagramRequestHandler[Request, Response]):
     async def service_init(
         self,
         exit_stack: contextlib.AsyncExitStack,
         server: AsyncUDPNetworkServer[Request, Response],
     ) -> None:
         exit_stack.callback(self._service_quit)
@@ -151,7 +177,39 @@
             await asyncio.sleep(1)
 
             # Do some stuff each second in background
             ...
 
     def _service_quit(self) -> None:
         print("Service stopped")
+
+
+class ClientContextRequestHandler(AsyncDatagramRequestHandler[Request, Response]):
+    client_addr_var: ClassVar[contextvars.ContextVar[SocketAddress]]
+    client_addr_var = contextvars.ContextVar("client_addr")
+
+    @classmethod
+    def client_log(cls, message: str) -> None:
+        # The address of the currently handled client can be accessed
+        # without passing it explicitly to this function.
+
+        logger = logging.getLogger(cls.__name__)
+
+        client_address = cls.client_addr_var.get()
+
+        logger.info("From %s: %s", client_address, message)
+
+    async def handle(
+        self,
+        client: AsyncDatagramClient[Response],
+    ) -> AsyncGenerator[None, Request]:
+        address = client.extra(INETClientAttribute.remote_address)
+        self.client_addr_var.set(address)
+
+        # In any code that we call within "handle()" is now possible to get
+        # client's address by calling 'client_addr_var.get()'.
+
+        request: Request = yield
+
+        self.client_log(f"Received request: {request!r}")
+
+        await client.send_packet(Response())
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/howto/udp_servers/simple_request_handler.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/howto/udp_servers/simple_request_handler.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 from __future__ import annotations
 
 from collections.abc import AsyncGenerator
 
-from easynetwork.api_async.server import AsyncDatagramClient, AsyncDatagramRequestHandler
+from easynetwork.servers.handlers import AsyncDatagramClient, AsyncDatagramRequestHandler
 
 
 class Request:
     """Object representing the client request."""
 
     ...
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/echo_client_server_tcp/async_client.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/echo_client_server_tcp/async_client.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 from __future__ import annotations
 
 import asyncio
 import sys
 
-from easynetwork.api_async.client import AsyncTCPNetworkClient
+from easynetwork.clients import AsyncTCPNetworkClient
 
 from json_protocol import JSONProtocol
 
 
 async def main() -> None:
     host = "localhost"
     port = 9000
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/echo_client_server_tcp/async_server.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/echo_client_server_tcp/async_server.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 from __future__ import annotations
 
 import asyncio
 
-from easynetwork.api_async.server import AsyncTCPNetworkServer
+from easynetwork.servers import AsyncTCPNetworkServer
 
 from echo_request_handler import EchoRequestHandler
 from json_protocol import JSONProtocol
 
 
 async def main() -> None:
     host = None
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/echo_client_server_tcp/client.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/echo_client_server_tcp/client.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 from __future__ import annotations
 
 import sys
 
-from easynetwork.api_sync.client import TCPNetworkClient
+from easynetwork.clients import TCPNetworkClient
 
 from json_protocol import JSONProtocol
 
 
 def main() -> None:
     host = "localhost"
     port = 9000
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/echo_client_server_tcp/echo_request_handler.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/echo_client_server_tcp/echo_request_handler.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 from __future__ import annotations
 
 from collections.abc import AsyncGenerator
 from typing import Any, TypeAlias
 
-from easynetwork.api_async.server import AsyncStreamClient, AsyncStreamRequestHandler, INETClientAttribute
 from easynetwork.exceptions import StreamProtocolParseError
+from easynetwork.servers.handlers import AsyncStreamClient, AsyncStreamRequestHandler, INETClientAttribute
 
 # These TypeAliases are there to help you understand
 # where requests and responses are used
 RequestType: TypeAlias = Any
 ResponseType: TypeAlias = Any
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/echo_client_server_tcp/json_protocol.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/echo_client_server_tcp/json_protocol.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/echo_client_server_tcp/server.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/echo_client_server_tcp/server.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 from __future__ import annotations
 
-from easynetwork.api_sync.server import StandaloneTCPNetworkServer
+from easynetwork.servers import StandaloneTCPNetworkServer
 
 from echo_request_handler import EchoRequestHandler
 from json_protocol import JSONProtocol
 
 
 def main() -> None:
     host = None
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/echo_client_server_udp/async_client.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/echo_client_server_udp/client.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,29 +1,28 @@
 from __future__ import annotations
 
-import asyncio
 import sys
 
-from easynetwork.api_async.client import AsyncUDPNetworkClient
+from easynetwork.clients import UDPNetworkClient
 
 from json_protocol import JSONDatagramProtocol
 
 
-async def main() -> None:
+def main() -> None:
     host = "localhost"
     port = 9000
 
     # Connect to server
-    async with AsyncUDPNetworkClient((host, port), JSONDatagramProtocol()) as client:
+    with UDPNetworkClient((host, port), JSONDatagramProtocol()) as client:
         # Send data
         request = {"command-line arguments": sys.argv[1:]}
-        await client.send_packet(request)
+        client.send_packet(request)
 
         # Receive data from the server and shut down
-        response = await client.recv_packet()
+        response = client.recv_packet()
 
     print(f"Sent:     {request}")
     print(f"Received: {response}")
 
 
 if __name__ == "__main__":
-    asyncio.run(main())
+    main()
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/echo_client_server_udp/echo_request_handler.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/echo_client_server_udp/echo_request_handler.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 from __future__ import annotations
 
 from collections.abc import AsyncGenerator
 from typing import Any, TypeAlias
 
-from easynetwork.api_async.server import AsyncDatagramClient, AsyncDatagramRequestHandler, INETClientAttribute
 from easynetwork.exceptions import DatagramProtocolParseError
+from easynetwork.servers.handlers import AsyncDatagramClient, AsyncDatagramRequestHandler, INETClientAttribute
 
 RequestType: TypeAlias = Any
 ResponseType: TypeAlias = Any
 
 
 class EchoRequestHandler(AsyncDatagramRequestHandler[RequestType, ResponseType]):
     async def handle(
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/echo_client_server_udp/json_protocol.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/echo_client_server_udp/json_protocol.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/echo_client_server_udp/server.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/echo_client_server_udp/server.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 from __future__ import annotations
 
-from easynetwork.api_sync.server import StandaloneUDPNetworkServer
+from easynetwork.servers import StandaloneUDPNetworkServer
 
 from echo_request_handler import EchoRequestHandler
 from json_protocol import JSONDatagramProtocol
 
 
 def main() -> None:
     host = None
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/ftp_server/async_server.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/ftp_server/async_server.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 from __future__ import annotations
 
 from collections.abc import Sequence
 
-from easynetwork.api_async.server import AsyncTCPNetworkServer
+from easynetwork.servers import AsyncTCPNetworkServer
 
 from ftp_reply import FTPReply
 from ftp_request import FTPRequest
 from ftp_server_protocol import FTPServerProtocol
 from ftp_server_request_handler import FTPRequestHandler
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/ftp_server/ftp_command.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/ftp_server/ftp_command.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/ftp_server/ftp_converters.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/ftp_server/ftp_converters.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/ftp_server/ftp_reply.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/ftp_server/ftp_reply.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/ftp_server/ftp_server_protocol.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/ftp_server/ftp_server_protocol.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/ftp_server/ftp_server_request_handler.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/ftp_server/ftp_server_request_handler.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,16 +1,16 @@
 from __future__ import annotations
 
 import contextlib
 import logging
 from collections.abc import AsyncGenerator
 from typing import Any
 
-from easynetwork.api_async.server import AsyncStreamClient, AsyncStreamRequestHandler, INETClientAttribute
 from easynetwork.exceptions import StreamProtocolParseError
+from easynetwork.servers.handlers import AsyncStreamClient, AsyncStreamRequestHandler, INETClientAttribute
 
 from ftp_command import FTPCommand
 from ftp_reply import FTPReply
 from ftp_request import FTPRequest
 
 
 class FTPRequestHandler(AsyncStreamRequestHandler[FTPRequest, FTPReply]):
```

### Comparing `easynetwork-1.0.0rc7/docs/source/_include/examples/tutorials/ftp_server/server.py` & `easynetwork-1.0.0rc8/docs/source/_include/examples/tutorials/ftp_server/server.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 from __future__ import annotations
 
 from collections.abc import Sequence
 
-from easynetwork.api_sync.server import StandaloneTCPNetworkServer
+from easynetwork.servers import StandaloneTCPNetworkServer
 
 from ftp_reply import FTPReply
 from ftp_request import FTPRequest
 from ftp_server_protocol import FTPServerProtocol
 from ftp_server_request_handler import FTPRequestHandler
```

### Comparing `easynetwork-1.0.0rc7/docs/source/api/async/server.rst` & `easynetwork-1.0.0rc8/docs/source/api/async/server.rst`

 * *Files 26% similar despite different names*

```diff
@@ -1,44 +1,50 @@
 ***********
 Servers API
 ***********
 
-.. automodule:: easynetwork.api_async.server
+Asynchronous network server interfaces
+
+.. currentmodule:: easynetwork.servers
 
 .. contents:: Table of Contents
    :local:
 
 ------
 
 Abstract Base Class
 ===================
 
-.. autoclass:: AbstractAsyncNetworkServer
+.. autoclass:: easynetwork.servers.abc::AbstractAsyncNetworkServer
    :members:
    :special-members: __aenter__, __aexit__
 
-.. autoprotocol:: easynetwork.api_async.server.abc::SupportsEventSet
+.. autoprotocol:: easynetwork.servers.abc::SupportsEventSet
 
 
 TCP Implementation
 ==================
 
 .. autoclass:: AsyncTCPNetworkServer
    :members:
+   :inherited-members:
 
 UDP Implementation
 ==================
 
 .. autoclass:: AsyncUDPNetworkServer
    :members:
+   :inherited-members:
 
 
 Request Handler Interface
 =========================
 
+.. currentmodule:: easynetwork.servers.handlers
+
 .. autoclass:: AsyncStreamRequestHandler
    :members:
 
 .. autoclass:: AsyncDatagramRequestHandler
    :members:
 
 Client API
```

### Comparing `easynetwork-1.0.0rc7/docs/source/api/lowlevel/socket.rst` & `easynetwork-1.0.0rc8/docs/source/api/lowlevel/socket.rst`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/docs/source/api/lowlevel/async/backend.rst` & `easynetwork-1.0.0rc8/docs/source/api/lowlevel/async/backend.rst`

 * *Files 16% similar despite different names*

```diff
@@ -68,14 +68,22 @@
 .. autoclass:: TaskGroup
    :members:
    :special-members: __aenter__, __aexit__
 
 .. autoclass:: Task
    :members:
 
+Introspection
+^^^^^^^^^^^^^
+
+.. automethod:: AsyncBackend.get_current_task
+
+.. autoclass:: TaskInfo
+   :members:
+
 Timeouts
 ^^^^^^^^
 
 .. automethod:: AsyncBackend.open_cancel_scope
 
 .. automethod:: AsyncBackend.move_on_after
 
@@ -92,31 +100,25 @@
 ----------
 
 Opening Network Connections
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
 .. automethod:: AsyncBackend.create_tcp_connection
 
-.. automethod:: AsyncBackend.create_ssl_over_tcp_connection
-
 .. automethod:: AsyncBackend.wrap_stream_socket
 
-.. automethod:: AsyncBackend.wrap_ssl_over_stream_socket_client_side
-
 .. automethod:: AsyncBackend.create_udp_endpoint
 
 .. automethod:: AsyncBackend.wrap_connected_datagram_socket
 
 Creating Network Servers
 ^^^^^^^^^^^^^^^^^^^^^^^^
 
 .. automethod:: AsyncBackend.create_tcp_listeners
 
-.. automethod:: AsyncBackend.create_ssl_over_tcp_listeners
-
 .. automethod:: AsyncBackend.create_udp_listeners
 
 
 Synchronization Primitives
 --------------------------
 
 Locks
@@ -153,41 +155,18 @@
 
 .. automethod:: AsyncBackend.create_threads_portal
 
 .. autoclass:: ThreadsPortal
    :members:
    :special-members: __aenter__, __aexit__
 
-``concurrent.futures`` Integration
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-.. automethod:: AsyncBackend.wait_future
-
-.. seealso::
-
-   :class:`.AsyncExecutor` class.
-
 
 Backend Factory
 ===============
 
-.. automodule:: easynetwork.lowlevel.api_async.backend.factory
-   :no-docstring:
-
 .. todo::
 
    Document backend factory usage.
 
-.. autoclass:: AsyncBackendFactory
-   :members:
-   :exclude-members: GROUP_NAME
-
-
-Concurrency And Multithreading (``concurrent.futures`` Integration)
-===================================================================
-
-.. automodule:: easynetwork.lowlevel.api_async.backend.futures
+.. automodule:: easynetwork.lowlevel.api_async.backend.factory
    :no-docstring:
-
-.. autoclass:: AsyncExecutor
    :members:
-   :special-members: __aenter__, __aexit__
```

### Comparing `easynetwork-1.0.0rc7/docs/source/api/lowlevel/sync/transports.rst` & `easynetwork-1.0.0rc8/docs/source/api/lowlevel/sync/transports.rst`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/docs/source/howto/protocols.rst` & `easynetwork-1.0.0rc8/docs/source/howto/protocols.rst`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/docs/source/howto/serializers.rst` & `easynetwork-1.0.0rc8/docs/source/howto/serializers.rst`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/docs/source/howto/tcp_clients.rst` & `easynetwork-1.0.0rc8/docs/source/howto/tcp_clients.rst`

 * *Files 6% similar despite different names*

```diff
@@ -5,30 +5,27 @@
 .. include:: ../_include/sync-async-variants.rst
 
 .. contents:: Table of Contents
    :local:
 
 ------
 
-The Basics
-==========
-
 The Protocol Object
--------------------
+===================
 
 The TCP clients expect a :class:`.StreamProtocol` instance to communicate with the remote endpoint.
 
 .. seealso::
 
    :doc:`protocols`
       Explains what a :class:`.StreamProtocol` is and how to use it.
 
 
 Connecting To The Remote Host
------------------------------
+=============================
 
 You need the host address (domain name or IP) and the port of connection in order to connect to the remote host:
 
 .. tabs::
 
    .. group-tab:: Synchronous
 
@@ -47,31 +44,32 @@
          The client does nothing when it enters the :keyword:`with` context. Everything is done on object creation.
 
    .. group-tab:: Asynchronous
 
       .. literalinclude:: ../_include/examples/howto/tcp_clients/basics/api_async/connection_example1.py
          :linenos:
 
-      You can control the connection timeout by adding a timeout scope using the asynchronous framework:
+      You can control the connection timeout by adding a timeout scope using the :term:`asynchronous framework`:
 
       .. literalinclude:: ../_include/examples/howto/tcp_clients/basics/api_async/connection_example2.py
          :pyobject: main
          :lineno-match:
          :emphasize-lines: 5-13
 
       .. note::
 
-         The call to ``wait_connected()`` is required to actually initialize the client, since we cannot perform asynchronous operations
-         at object creation. This is what the client does when it enters the the :keyword:`async with` context.
+         The call to :meth:`~.AsyncTCPNetworkClient.wait_connected` is required to actually initialize the client,
+         since we cannot perform asynchronous operations at object creation.
+         This is what the client does when it enters the the :keyword:`async with` context.
 
-         Once completed, ``wait_connected()`` is a no-op.
+         Once completed, :meth:`~.AsyncTCPNetworkClient.wait_connected` is a no-op.
 
 
 Using An Already Connected Socket
----------------------------------
+=================================
 
 If you have your own way to obtain a connected :class:`socket.socket` instance, you can pass it to the client.
 
 If the socket is not connected, an :exc:`OSError` is raised.
 
 .. important::
 
@@ -91,22 +89,19 @@
    .. group-tab:: Asynchronous
 
       .. literalinclude:: ../_include/examples/howto/tcp_clients/basics/api_async/socket_example1.py
          :linenos:
 
       .. note::
 
-         Even with a ready-to-use socket, the call to ``wait_connected()`` is still required.
-
+         Even with a ready-to-use socket, the call to :meth:`~.AsyncTCPNetworkClient.wait_connected` is still required.
 
-Basic Usage
-===========
 
 Sending Packets
----------------
+===============
 
 There's not much to say, except that objects passed as arguments are automatically converted to bytes to send to the remote host
 thanks to the :term:`protocol object`.
 
 .. tabs::
 
    .. group-tab:: Synchronous
@@ -123,15 +118,15 @@
          :pyobject: send_packet_example1
          :start-after: [start]
          :dedent:
          :linenos:
 
 
 Receiving Packets
------------------
+=================
 
 You get the next available packet, already parsed. Extraneous data is kept for the next call.
 
 .. tabs::
 
    .. group-tab:: Synchronous
 
@@ -188,15 +183,15 @@
             :start-after: [start]
             :dedent:
             :linenos:
             :emphasize-lines: 4-5
 
 
 Receiving Multiple Packets At Once
-----------------------------------
+==================================
 
 You can use ``iter_received_packets()`` to get all the received packets in a sequence or a set.
 
 .. tabs::
 
    .. group-tab:: Synchronous
 
@@ -224,41 +219,33 @@
          :pyobject: recv_packet_example5
          :start-after: [start]
          :dedent:
          :linenos:
 
       .. seealso::
 
-         :meth:`TCPNetworkClient.iter_received_packets() <.AbstractNetworkClient.iter_received_packets>`
+         :meth:`.TCPNetworkClient.iter_received_packets`
             The method description and usage (especially for the ``timeout`` parameter).
 
    .. group-tab:: Asynchronous
 
       .. literalinclude:: ../_include/examples/howto/tcp_clients/usage/api_async.py
          :pyobject: recv_packet_example5
          :start-after: [start]
          :dedent:
          :linenos:
 
       .. seealso::
 
-         :meth:`AsyncTCPNetworkClient.iter_received_packets() <.AbstractAsyncNetworkClient.iter_received_packets>`
+         :meth:`.AsyncTCPNetworkClient.iter_received_packets`
             The method description and usage (especially for the ``timeout`` parameter).
 
 
-Advanced Usage
-==============
-
-.. note::
-
-   This section is for people who know what they're doing and are looking for something specific.
-
-
 Close The Write-End Stream
---------------------------
+==========================
 
 If you are sure you will never reuse ``send_packet()``, you can call ``send_eof()`` to shut down the write stream.
 
 .. tabs::
 
    .. group-tab:: Synchronous
 
@@ -278,15 +265,15 @@
 
 .. note::
 
    ``send_eof()`` will block until all unsent data has been flushed before closing the stream.
 
 
 Low-Level Socket Operations
----------------------------
+===========================
 
 For low-level operations such as :meth:`~socket.socket.setsockopt`, the client object exposes the socket through a :class:`.SocketProxy`:
 
 .. tabs::
 
    .. group-tab:: Synchronous
 
@@ -302,19 +289,19 @@
          :pyobject: socket_proxy_example
          :start-after: [start]
          :dedent:
          :linenos:
 
       .. warning::
 
-         Make sure that ``wait_connected()`` has been called before.
+         Make sure that :meth:`~.AsyncTCPNetworkClient.wait_connected` has been called before.
 
 
 ``socket.recv()`` Buffer Size
------------------------------
+=============================
 
 By default, the client uses a reasonable buffer size when calling ``recv_packet()``.
 You can control this value by setting the ``max_recv_size`` parameter:
 
 .. tabs::
 
    .. group-tab:: Synchronous
@@ -337,15 +324,15 @@
 .. note::
 
    ``max_recv_size`` is also used as a size hint for :term:`buffered serializers <buffered serializer>`.
    See :ref:`this section <buffer-instantiation>` for details.
 
 
 SSL/TLS Connection
-------------------
+==================
 
 If you want to use SSL to communicate with the remote host, the easiest way is to pass ``ssl=True``:
 
 .. tabs::
 
    .. group-tab:: Synchronous
 
@@ -370,15 +357,15 @@
 
 .. danger::
 
    You can pass an :class:`~ssl.SSLContext` instead, but at this point I expect you to *really* know what you are doing.
 
 
 Concurrency And Multithreading
-------------------------------
+==============================
 
 .. tabs::
 
    .. group-tab:: Synchronous
 
       All client methods are thread-safe. Synchronization follows these rules:
 
@@ -386,35 +373,26 @@
         :class:`threading.Lock` instance.
 
       * :meth:`~.TCPNetworkClient.close` will not wait for :meth:`~.TCPNetworkClient.recv_packet`.
 
       * The :attr:`client.socket <.TCPNetworkClient.socket>` methods are also thread-safe. This means that you cannot access
         the underlying socket methods (e.g. :meth:`~socket.socket.getsockopt`) during a write operation.
 
-      This allows you to do something like this:
-
-      .. literalinclude:: ../_include/examples/howto/tcp_clients/concurrency/api_sync.py
-         :linenos:
-
    .. group-tab:: Asynchronous
 
-      All client methods do not require external task synchronization. Synchronization follows these rules:
+      All client methods do not require external task synchronization (such as :class:`asyncio.Lock`).
+      Synchronization follows these rules:
 
       * :meth:`~.AsyncTCPNetworkClient.send_packet` and :meth:`~.AsyncTCPNetworkClient.recv_packet` do not share the same lock instance.
 
-      * :meth:`~.AsyncTCPNetworkClient.close` will not wait for :meth:`~.AsyncTCPNetworkClient.recv_packet`.
+      * :meth:`~.AsyncTCPNetworkClient.aclose` will not wait for :meth:`~.AsyncTCPNetworkClient.recv_packet`.
 
-      This allows you to do something like this:
 
-      .. literalinclude:: ../_include/examples/howto/tcp_clients/concurrency/api_async.py
-         :linenos:
-
-
-SSL/TLS Considerations
-^^^^^^^^^^^^^^^^^^^^^^
+SSL/TLS Concurrency Considerations
+----------------------------------
 
 For safety, concurrent calls to ``send_packet()`` and ``recv_packet()`` are "disabled" by default when using SSL.
 In fact, they share the same synchronization lock.
 
 If you need this feature after all, you can pass ``ssl_shared_lock=False`` at object creation.
 
 .. danger::
```

### Comparing `easynetwork-1.0.0rc7/docs/source/howto/tcp_servers.rst` & `easynetwork-1.0.0rc8/docs/source/howto/udp_servers.rst`

 * *Files 15% similar despite different names*

```diff
@@ -1,213 +1,210 @@
 ********************
-How-to — TCP Servers
+How-to — UDP Servers
 ********************
 
 .. include:: ../_include/sync-async-variants.rst
 
 .. contents:: Table of Contents
    :local:
 
 ------
 
 Introduction
 ============
 
-The :mod:`easynetwork.api_async.server` module simplifies the task of writing network servers. The service creation model is inspired by
-the standard :mod:`socketserver` library, but is an enhanced version with even more abstraction.
+Creating a UDP server requires several steps:
 
-Creating a server requires several steps:
-
-#. Derive a class from :class:`.AsyncStreamRequestHandler` and redefine its :meth:`~.AsyncStreamRequestHandler.handle` method;
+#. Derive a class from :class:`.AsyncDatagramRequestHandler` and redefine its :meth:`~.AsyncDatagramRequestHandler.handle` method;
    this method will process incoming requests.
 
-#. Instantiate the :class:`.AsyncTCPNetworkServer` class passing it the server's address, the :term:`protocol object`
+#. Instantiate the :class:`.AsyncUDPNetworkServer` class passing it the server's address, the :term:`protocol object`
    and the request handler instance.
 
-#. Call :meth:`~.AsyncTCPNetworkServer.serve_forever` to process requests.
-
-Writing :term:`coroutine functions <coroutine function>` is mandatory to use this server.
-
-.. seealso::
-
-   :pep:`492` — Coroutines with async and await syntax
-      The proposal to introduce native coroutines in Python with :keyword:`async` and :keyword:`await` syntax.
-
-   :external+python:doc:`library/asyncio`
-      If you are not familiar with async/await syntax, you can use the standard library to get started with coroutines.
+#. Call :meth:`~.AsyncUDPNetworkServer.serve_forever` to process requests.
 
 
 Request Handler Objects
 =======================
 
 .. note::
 
-   Unlike :class:`socketserver.BaseRequestHandler`, there is **only one** :class:`.AsyncStreamRequestHandler` instance for the entire service.
+   Unlike :class:`socketserver.BaseRequestHandler`, there is **only one** :class:`.AsyncDatagramRequestHandler` instance for the entire service.
 
 
 Here is a simple example:
 
-.. literalinclude:: ../_include/examples/howto/tcp_servers/simple_request_handler.py
+.. literalinclude:: ../_include/examples/howto/udp_servers/simple_request_handler.py
    :linenos:
 
 
 Using ``handle()`` Generator
 ----------------------------
 
-.. seealso::
+.. important::
+   There will always be only one active generator per client.
+   All the pending datagrams received while the generator is running are queued.
 
-   :pep:`525` — Asynchronous Generators
-      The proposal that expanded on :pep:`492` by adding generator capabilities to coroutine functions.
+   This behavior is designed to act like a stream request handler.
 
 
 Minimum Requirements
 ^^^^^^^^^^^^^^^^^^^^
 
-.. literalinclude:: ../_include/examples/howto/tcp_servers/request_handler_explanation.py
+.. literalinclude:: ../_include/examples/howto/udp_servers/request_handler_explanation.py
    :pyobject: MinimumRequestHandler.handle
    :dedent:
    :linenos:
 
 
-Closing the connection
-^^^^^^^^^^^^^^^^^^^^^^
+Refuse datagrams
+^^^^^^^^^^^^^^^^
 
-.. literalinclude:: ../_include/examples/howto/tcp_servers/request_handler_explanation.py
-   :pyobject: ConnectionCloseRequestHandler.handle
+Your UDP socket can receive datagrams from anywhere. You may want to control who can send you information.
+
+.. literalinclude:: ../_include/examples/howto/udp_servers/request_handler_explanation.py
+   :pyobject: SkipDatagramRequestHandler.handle
    :dedent:
    :linenos:
-   :emphasize-lines: 11
-
-.. tip::
-
-   You can use :func:`contextlib.aclosing` to close the client at the generator exit.
-
-   .. literalinclude:: ../_include/examples/howto/tcp_servers/request_handler_explanation.py
-      :pyobject: ConnectionCloseWithContextRequestHandler.handle
-      :dedent:
-      :linenos:
-      :emphasize-lines: 5
-
-.. important::
-
-   The connection is forcibly closed under the following conditions:
-
-   * ``handle()`` raises an exception.
-
-   * ``handle()`` returns *before* the first :keyword:`yield` statement.
-
-      .. literalinclude:: ../_include/examples/howto/tcp_servers/request_handler_explanation.py
-         :pyobject: ConnectionCloseBeforeYieldRequestHandler.handle
-         :dedent:
-         :linenos:
-         :emphasize-lines: 5-6
-
+   :emphasize-lines: 5-8
 
 Error Handling
 ^^^^^^^^^^^^^^
 
-.. literalinclude:: ../_include/examples/howto/tcp_servers/request_handler_explanation.py
+.. literalinclude:: ../_include/examples/howto/udp_servers/request_handler_explanation.py
    :pyobject: ErrorHandlingInRequestHandler.handle
    :dedent:
    :linenos:
-   :emphasize-lines: 8
 
-.. note::
+.. warning::
 
-   ``handle()`` will never get a :exc:`ConnectionError` subclass. In case of an unexpected disconnect, the generator is closed,
-   so you should handle :exc:`GeneratorExit` instead.
+   You should always log or re-raise a bare :exc:`Exception` thrown in your generator.
+
+   .. literalinclude:: ../_include/examples/howto/udp_servers/request_handler_explanation.py
+      :pyobject: ErrorHandlingInRequestHandler.handle
+      :dedent:
+      :linenos:
+      :start-at: except Exception
+      :end-at: InternalError()
+      :emphasize-lines: 2-3
 
 
 Having Multiple ``yield`` Statements
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
-.. literalinclude:: ../_include/examples/howto/tcp_servers/request_handler_explanation.py
+.. literalinclude:: ../_include/examples/howto/udp_servers/request_handler_explanation.py
    :pyobject: MultipleYieldInRequestHandler.handle
    :dedent:
    :linenos:
    :emphasize-lines: 5,12
 
+.. warning::
 
-.. tip::
+   Even if this feature is supported, it is not recommended to have more than one (unless you know what you are doing) for the following reasons:
 
-   The number of :keyword:`yield` allowed is... infinite!
+   * UDP does not guarantee ordered delivery. Packets are typically "sent" in order, but they may be received out of order.
+     In large networks, it is reasonably common for some packets to arrive out of sequence (or not at all).
 
-   You can take advantage of this by having an internal main loop inside the generator:
+   * The server has no way of knowing if this client has stopped sending you requests forever.
 
-   .. literalinclude:: ../_include/examples/howto/tcp_servers/request_handler_explanation.py
-      :pyobject: ClientLoopInRequestHandler.handle
-      :dedent:
-      :linenos:
+   If you plan to use multiple yields in your request handler, you should *always* have a timeout applied. (See the section below.)
 
 
 Cancellation And Timeouts
 ^^^^^^^^^^^^^^^^^^^^^^^^^
 
-Since all :exc:`BaseException` subclasses are thrown into the generator, you can apply a timeout to the read stream
-using the asynchronous framework (the cancellation exception is retrieved in the generator):
-
-.. literalinclude:: ../_include/examples/howto/tcp_servers/request_handler_explanation.py
-   :pyobject: TimeoutRequestHandler.handle
-   :dedent:
-   :linenos:
-   :emphasize-lines: 6,9-10
+.. tabs::
 
+   .. tab:: Using ``yield`` (Recommended)
 
-Connecting/Disconnecting Hooks
-------------------------------
+      It is possible to send the timeout delay to the parent task:
 
-You can override :meth:`~.AsyncStreamRequestHandler.on_connection` and :meth:`~.AsyncStreamRequestHandler.on_disconnection` methods:
+      .. literalinclude:: ../_include/examples/howto/udp_servers/request_handler_explanation.py
+         :pyobject: TimeoutYieldedRequestHandler.handle
+         :dedent:
+         :linenos:
+         :emphasize-lines: 4,15-17
 
-* :meth:`~.AsyncStreamRequestHandler.on_connection` is called on client task startup.
+   .. tab:: Using ``with``
 
-* :meth:`~.AsyncStreamRequestHandler.on_disconnection` is called on client task teardown.
+      Since all :exc:`BaseException` subclasses are thrown into the generator, you can apply a timeout to the read stream
+      using the :term:`asynchronous framework` (the cancellation exception is retrieved in the generator):
 
-.. literalinclude:: ../_include/examples/howto/tcp_servers/request_handler_explanation.py
-   :pyobject: ClientConnectionHooksRequestHandler
-   :start-after: ClientConnectionHooksRequestHandler
-   :dedent:
-   :linenos:
-   :emphasize-lines: 1,7
+      .. literalinclude:: ../_include/examples/howto/udp_servers/request_handler_explanation.py
+         :pyobject: TimeoutContextRequestHandler.handle
+         :dedent:
+         :linenos:
+         :emphasize-lines: 14,17-18
 
+      .. warning::
 
-Wait For Client Data On Connection
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+         Note that this behavior works because the generator is always executed and closed
+         in the same asynchronous task for the current implementation.
 
-If you need to use the read stream, :meth:`~.AsyncStreamRequestHandler.on_connection` can be an asynchronous generator instead of
-a coroutine function:
-
-.. literalinclude:: ../_include/examples/howto/tcp_servers/request_handler_explanation.py
-   :pyobject: ClientConnectionAsyncGenRequestHandler
-   :start-after: ClientConnectionAsyncGenRequestHandler
-   :dedent:
-   :linenos:
-   :emphasize-lines: 6
+         This feature is available so that features like ``anyio.CancelScope`` can be used.
+         However, it may be removed in a future release.
 
 
 Service Initialization
 ----------------------
 
-The server will call :meth:`~.AsyncStreamRequestHandler.service_init` and pass it an :class:`~contextlib.AsyncExitStack`
-at the beginning of the :meth:`~.AsyncTCPNetworkServer.serve_forever` task to set up the global service.
+The server will call :meth:`~.AsyncDatagramRequestHandler.service_init` and pass it an :class:`~contextlib.AsyncExitStack`
+at the beginning of the :meth:`~.AsyncUDPNetworkServer.serve_forever` task to set up the global service.
 
 This allows you to do something like this:
 
-.. literalinclude:: ../_include/examples/howto/tcp_servers/request_handler_explanation.py
+.. literalinclude:: ../_include/examples/howto/udp_servers/request_handler_explanation.py
    :pyobject: ServiceInitializationHookRequestHandler
    :start-after: ServiceInitializationHookRequestHandler
    :dedent:
    :linenos:
    :emphasize-lines: 1
 
 
+Per-client variables (``contextvars`` integration)
+--------------------------------------------------
+
+If your :term:`asynchronous framework` supports per-task :external+python:doc:`context variables <library/contextvars>`,
+you can use this feature in your request handler:
+
+.. literalinclude:: ../_include/examples/howto/udp_servers/request_handler_explanation.py
+   :pyobject: ClientContextRequestHandler
+   :dedent:
+   :linenos:
+
+.. tip::
+
+   It is possible to initialize the context to be copied in :meth:`~.AsyncDatagramRequestHandler.service_init`.
+
+   This means that the :meth:`contextvars.ContextVar.set` calls made in ``service_init()`` will be applied
+   to subsequent client tasks.
+
+
 Server Object
 =============
 
 A basic example of how to run the server:
 
-.. literalinclude:: ../_include/examples/howto/tcp_servers/async_server.py
+.. literalinclude:: ../_include/examples/howto/udp_servers/async_server.py
    :linenos:
 
 .. seealso::
 
-   :doc:`/tutorials/echo_client_server_tcp`
+   :doc:`/tutorials/echo_client_server_udp`
       A working example of the server implementation.
+
+
+Run Server In Background
+------------------------
+
+.. literalinclude:: ../_include/examples/howto/udp_servers/background_server.py
+   :linenos:
+
+The output of the example should look something like this:
+
+.. code-block:: console
+
+   $ python background_server.py
+   Server loop running in task: Task-2
+   From server: {'task': 'Task-6', 'request': {'message': 'Hello world 1'}}
+   From server: {'task': 'Task-7', 'request': {'message': 'Hello world 2'}}
+   From server: {'task': 'Task-8', 'request': {'message': 'Hello world 3'}}
```

### Comparing `easynetwork-1.0.0rc7/docs/source/howto/udp_clients.rst` & `easynetwork-1.0.0rc8/docs/source/howto/udp_clients.rst`

 * *Files 14% similar despite different names*

```diff
@@ -5,30 +5,27 @@
 .. include:: ../_include/sync-async-variants.rst
 
 .. contents:: Table of Contents
    :local:
 
 ------
 
-The Basics
-==========
-
 The Protocol Object
--------------------
+===================
 
 The UDP clients expect a :class:`.DatagramProtocol` instance to communicate with the remote endpoint.
 
 .. seealso::
 
    :doc:`protocols`
       Explains what a :class:`.DatagramProtocol` is and how to use it.
 
 
 Connecting To The Remote Host
------------------------------
+=============================
 
 .. important::
 
    If you are familiar with UDP sockets, you know that there are no real connections (communication pipes) like there are with TCP sockets.
 
    If not, I advise you to read the Unix manual pages :manpage:`udp(7)` and :manpage:`connect(2)`.
 
@@ -48,22 +45,23 @@
    .. group-tab:: Asynchronous
 
       .. literalinclude:: ../_include/examples/howto/udp_clients/basics/api_async/connection_example1.py
          :linenos:
 
       .. note::
 
-         The call to ``wait_connected()`` is required to actually initialize the client, since we cannot perform asynchronous operations
-         at object creation. This is what the client does when it enters the the :keyword:`async with` context.
+         The call to :meth:`~.AsyncUDPNetworkClient.wait_connected` is required to actually initialize the client,
+         since we cannot perform asynchronous operations at object creation.
+         This is what the client does when it enters the the :keyword:`async with` context.
 
-         Once completed, ``wait_connected()`` is a no-op.
+         Once completed, :meth:`~.AsyncUDPNetworkClient.wait_connected` is a no-op.
 
 
 Using An Already Connected Socket
----------------------------------
+=================================
 
 If you have your own way to obtain a connected :class:`socket.socket` instance, you can pass it to the client.
 
 If the socket is not connected, an :exc:`OSError` is raised.
 
 .. important::
 
@@ -83,22 +81,19 @@
    .. group-tab:: Asynchronous
 
       .. literalinclude:: ../_include/examples/howto/udp_clients/basics/api_async/socket_example1.py
          :linenos:
 
       .. note::
 
-         Even with a ready-to-use socket, the call to ``wait_connected()`` is still required.
-
+         Even with a ready-to-use socket, the call to :meth:`~.AsyncUDPNetworkClient.wait_connected` is still required.
 
-Basic Usage
-===========
 
 Sending Packets
----------------
+===============
 
 There's not much to say, except that objects passed as arguments are automatically converted to bytes to send to the remote host
 thanks to the :term:`protocol object`.
 
 .. tabs::
 
    .. group-tab:: Synchronous
@@ -115,15 +110,15 @@
          :pyobject: send_packet_example1
          :start-after: [start]
          :dedent:
          :linenos:
 
 
 Receiving Packets
------------------
+=================
 
 You get the next available packet, already parsed.
 
 .. tabs::
 
    .. group-tab:: Synchronous
 
@@ -145,15 +140,15 @@
 
       .. literalinclude:: ../_include/examples/howto/udp_clients/usage/api_async.py
          :pyobject: recv_packet_example1
          :start-after: [start]
          :dedent:
          :linenos:
 
-      You can control the receive timeout by adding a timeout scope using the asynchronous framework:
+      You can control the receive timeout by adding a timeout scope using the :term:`asynchronous framework` :
 
       .. literalinclude:: ../_include/examples/howto/udp_clients/usage/api_async.py
          :pyobject: recv_packet_example2
          :start-after: [start]
          :dedent:
          :linenos:
 
@@ -180,15 +175,15 @@
             :start-after: [start]
             :dedent:
             :linenos:
             :emphasize-lines: 4-5
 
 
 Receiving Multiple Packets At Once
-----------------------------------
+==================================
 
 You can use ``iter_received_packets()`` to get all the received packets in a sequence or a set.
 
 .. tabs::
 
    .. group-tab:: Synchronous
 
@@ -216,41 +211,33 @@
          :pyobject: recv_packet_example5
          :start-after: [start]
          :dedent:
          :linenos:
 
       .. seealso::
 
-         :meth:`UDPNetworkClient.iter_received_packets() <.AbstractNetworkClient.iter_received_packets>`
+         :meth:`.UDPNetworkClient.iter_received_packets`
             The method description and usage (especially for the ``timeout`` parameter).
 
    .. group-tab:: Asynchronous
 
       .. literalinclude:: ../_include/examples/howto/udp_clients/usage/api_async.py
          :pyobject: recv_packet_example5
          :start-after: [start]
          :dedent:
          :linenos:
 
       .. seealso::
 
-         :meth:`AsyncUDPNetworkClient.iter_received_packets() <.AbstractAsyncNetworkClient.iter_received_packets>`
+         :meth:`.AsyncUDPNetworkClient.iter_received_packets`
             The method description and usage (especially for the ``timeout`` parameter).
 
 
-Advanced Usage
-==============
-
-.. note::
-
-   This section is for people who know what they're doing and are looking for something specific.
-
-
 Low-Level Socket Operations
----------------------------
+===========================
 
 For low-level operations such as :meth:`~socket.socket.setsockopt`, the client object exposes the socket through a :class:`.SocketProxy`:
 
 .. tabs::
 
    .. group-tab:: Synchronous
 
@@ -266,25 +253,26 @@
          :pyobject: socket_proxy_example
          :start-after: [start]
          :dedent:
          :linenos:
 
       .. warning::
 
-         Make sure that ``wait_connected()`` has been called before.
+         Make sure that :meth:`~.AsyncUDPNetworkClient.wait_connected` has been called before.
 
 
 Concurrency And Multithreading
-------------------------------
+==============================
 
 .. tabs::
 
    .. group-tab:: Synchronous
 
-      All client methods are thread-safe. Synchronization follows these rules:
+      All client methods are thread-safe (such as :class:`asyncio.Lock`).
+      Synchronization follows these rules:
 
       * :meth:`~.UDPNetworkClient.send_packet` and :meth:`~.UDPNetworkClient.recv_packet` do not share the same
         :class:`threading.Lock` instance.
 
       * :meth:`~.UDPNetworkClient.close` will not wait for :meth:`~.UDPNetworkClient.recv_packet`.
 
       * The :attr:`client.socket <.UDPNetworkClient.socket>` methods are also thread-safe. This means that you cannot access
@@ -292,8 +280,8 @@
 
    .. group-tab:: Asynchronous
 
       All client methods do not require external task synchronization. Synchronization follows these rules:
 
       * :meth:`~.AsyncUDPNetworkClient.send_packet` and :meth:`~.AsyncUDPNetworkClient.recv_packet` do not share the same lock instance.
 
-      * :meth:`~.AsyncUDPNetworkClient.close` will not wait for :meth:`~.AsyncUDPNetworkClient.recv_packet`.
+      * :meth:`~.AsyncUDPNetworkClient.aclose` will not wait for :meth:`~.AsyncUDPNetworkClient.recv_packet`.
```

### Comparing `easynetwork-1.0.0rc7/docs/source/howto/advanced/buffered_serializers.rst` & `easynetwork-1.0.0rc8/docs/source/howto/advanced/buffered_serializers.rst`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,11 @@
 *****************************
 How-to — Buffered Serializers
 *****************************
 
-.. note::
-
-   This section assumes that you have read the :doc:`../serializers` page.
-
 .. contents:: Table of Contents
    :local:
 
 ------
 
 Introduction
 ============
@@ -70,14 +66,18 @@
 It's not a quick fix that will solve application performance problems. In fact, it may not be convenient.
 
 The scenario described above is a real problem for **servers** with high workloads and/or low bandwidth
 that exchange large amounts of data with their clients. In such a situation, the slightest typo can bring
 the whole thing crashing down. Under "normal" conditions, the basic implementation is more than enough
 to keep a server running with acceptable performance.
 
+.. important::
+
+   I want to point out that it's not about speed, it's about improving global performance by minimizing memory footprint over time.
+
 ------
 
 Writing A Buffered Serializer
 =============================
 
 To write a :term:`buffered serializer`, you must create a subclass of :class:`~.BufferedIncrementalPacketSerializer` and override
 its :meth:`~.BufferedIncrementalPacketSerializer.create_deserializer_buffer` and
@@ -168,24 +168,23 @@
 .. warning::
 
    Buffer consistency
       Because of buffer reuse, it is up to you to reset the buffer state upon entry and exit of the generator.
       If you are relying only on the area that has already been written, you can skip this step (as in the example above).
 
    Growing buffers
-      It is not officially supported to increase/decrease the buffer size during deserialization. But if you want to,
-      remember to reset it to its initial size for consistency.
+      It is not officially supported to increase/decrease the buffer size during deserialization.
 
 .. tip::
 
    Buffer initialization
       :meth:`~.BufferedIncrementalPacketSerializer.buffered_incremental_deserialize` is called *before* the read.
       You can reinitialize the buffer (for example, by filling it to zero) before the first read::
 
-         def buffered_incremental_deserialize(self, buffer: bytearray) -> Generator:
+         def buffered_incremental_deserialize(self, buffer: bytearray) -> Generator[...]:
              for i in range(len(buffer)):
                  buffer[i] = 0
 
              nbytes = yield
 
              ...
```

### Comparing `easynetwork-1.0.0rc7/docs/source/quickstart/install.rst` & `easynetwork-1.0.0rc8/docs/source/quickstart/install.rst`

 * *Files 14% similar despite different names*

```diff
@@ -18,16 +18,14 @@
 
 Here is the full list:
 
 * Serializer extensions:
 
    * ``cbor``: Installs the required dependencies for :class:`.CBORSerializer`.
 
-   * ``encryption``: Installs the required dependencies for :class:`.EncryptorSerializer`.
-
    * ``msgpack``: Installs the required dependencies for :class:`.MessagePackSerializer`.
 
 * Asynchronous I/O extensions:
 
    * ``sniffio``: Installs the version supported and tested of :github:repo:`sniffio <python-trio/sniffio>`.
```

### Comparing `easynetwork-1.0.0rc7/docs/source/quickstart/overview.rst` & `easynetwork-1.0.0rc8/docs/source/quickstart/overview.rst`

 * *Files 16% similar despite different names*

```diff
@@ -3,11 +3,13 @@
 ********
 
 EasyNetwork completely encapsulates the socket handling, providing you with a higher level interface
 that allows an application/software to completely handle the logic part with Python objects,
 without worrying about how to process, send or receive data over the network.
 
 The communication protocol can be whatever you want, be it JSON, Pickle, ASCII, structure, base64 encoded,
-compressed, encrypted, or any other format that is not part of the standard library.
+compressed, or any other format that is not part of the standard library.
 You choose the data format and the library takes care of the rest.
 
+This project is especially useful for simple **message** exchange between clients and servers.
+
 Works with TCP and UDP.
```

### Comparing `easynetwork-1.0.0rc7/docs/source/tutorials/echo_client_server_tcp.rst` & `easynetwork-1.0.0rc8/docs/source/tutorials/echo_client_server_tcp.rst`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/docs/source/tutorials/echo_client_server_udp.rst` & `easynetwork-1.0.0rc8/docs/source/tutorials/echo_client_server_udp.rst`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/docs/source/tutorials/ftp_server.rst` & `easynetwork-1.0.0rc8/docs/source/tutorials/ftp_server.rst`

 * *Files 2% similar despite different names*

```diff
@@ -211,36 +211,36 @@
 .. tabs::
 
    .. group-tab:: IPv4 connection
 
       .. code-block:: console
 
          (.venv) $ python server.py
-         [ INFO ] [ easynetwork.api_async.server.tcp ] Start serving at ('::', 21000), ('0.0.0.0', 21000)
-         [ INFO ] [ easynetwork.api_async.server.tcp ] Accepted new connection (address = ('127.0.0.1', 45994))
+         [ INFO ] [ easynetwork.servers.async_tcp ] Start serving at ('::', 21000), ('0.0.0.0', 21000)
+         [ INFO ] [ easynetwork.servers.async_tcp ] Accepted new connection (address = ('127.0.0.1', 45994))
          [ INFO ] [ FTPRequestHandler ] Sent by client ('127.0.0.1', 45994): FTPRequest(command=<FTPCommand.NOOP: 'NOOP'>, args=())
          [ INFO ] [ FTPRequestHandler ] Sent by client ('127.0.0.1', 45994): FTPRequest(command=<FTPCommand.NOOP: 'NOOP'>, args=())
          [ INFO ] [ FTPRequestHandler ] Sent by client ('127.0.0.1', 45994): FTPRequest(command=<FTPCommand.STOR: 'STOR'>, args=('/path/to/file.txt',))
          [ WARNING ] [ FTPRequestHandler ] ('127.0.0.1', 45994): PacketConversionError: Command unrecognized: 'UNKNOWN'
          [ INFO ] [ FTPRequestHandler ] Sent by client ('127.0.0.1', 45994): FTPRequest(command=<FTPCommand.QUIT: 'QUIT'>, args=())
-         [ INFO ] [ easynetwork.api_async.server.tcp ] ('127.0.0.1', 45994) disconnected
+         [ INFO ] [ easynetwork.servers.async_tcp ] ('127.0.0.1', 45994) disconnected
 
    .. group-tab:: IPv6 connection
 
       .. code-block:: console
 
          (.venv) $ python server.py
-         [ INFO ] [ easynetwork.api_async.server.tcp ] Start serving at ('::', 21000), ('0.0.0.0', 21000)
-         [ INFO ] [ easynetwork.api_async.server.tcp ] Accepted new connection (address = ('::1', 45994))
+         [ INFO ] [ easynetwork.servers.async_tcp ] Start serving at ('::', 21000), ('0.0.0.0', 21000)
+         [ INFO ] [ easynetwork.servers.async_tcp ] Accepted new connection (address = ('::1', 45994))
          [ INFO ] [ FTPRequestHandler ] Sent by client ('::1', 45994): FTPRequest(command=<FTPCommand.NOOP: 'NOOP'>, args=())
          [ INFO ] [ FTPRequestHandler ] Sent by client ('::1', 45994): FTPRequest(command=<FTPCommand.NOOP: 'NOOP'>, args=())
          [ INFO ] [ FTPRequestHandler ] Sent by client ('::1', 45994): FTPRequest(command=<FTPCommand.STOR: 'STOR'>, args=('/path/to/file.txt',))
          [ WARNING ] [ FTPRequestHandler ] ('::1', 45994): PacketConversionError: Command unrecognized: 'UNKNOWN'
          [ INFO ] [ FTPRequestHandler ] Sent by client ('::1', 45994): FTPRequest(command=<FTPCommand.QUIT: 'QUIT'>, args=())
-         [ INFO ] [ easynetwork.api_async.server.tcp ] ('::1', 45994) disconnected
+         [ INFO ] [ easynetwork.servers.async_tcp ] ('::1', 45994) disconnected
 
 
 Client:
 
 .. note::
 
    The `File Transfer Protocol`_ is based on the `Telnet protocol`_.
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/__init__.py` & `easynetwork-1.0.0rc8/src/easynetwork/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -19,15 +19,15 @@
 
 from __future__ import annotations
 
 __all__ = []  # type: list[str]
 
 __author__ = "FrankySnow9"
 __contact__ = "clairicia.rcj.francis@gmail.com"
-__copyright__ = "Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine"
+__copyright__ = "Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine"
 __credits__ = ["FrankySnow9"]
 __deprecated__ = False
 __email__ = "clairicia.rcj.francis@gmail.com"
 __license__ = "Apache 2.0"
 __maintainer__ = "FrankySnow9"
 __status__ = "Development"
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/_typevars.py` & `easynetwork-1.0.0rc8/src/easynetwork/_typevars.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -13,38 +13,38 @@
 #
 #
 """Common type variables for EasyNetwork's classes"""
 
 from __future__ import annotations
 
 __all__ = [
-    "_BufferT",
-    "_DTOPacketT",
-    "_PacketT",
-    "_ReceivedDTOPacketT",
-    "_ReceivedPacketT",
-    "_RequestT",
-    "_ResponseT",
-    "_SentDTOPacketT",
-    "_SentPacketT",
+    "_T_Buffer",
+    "_T_DTOPacket",
+    "_T_Packet",
+    "_T_ReceivedDTOPacket",
+    "_T_ReceivedPacket",
+    "_T_Request",
+    "_T_Response",
+    "_T_SentDTOPacket",
+    "_T_SentPacket",
 ]
 
 import typing
 
 if typing.TYPE_CHECKING:
     from _typeshed import WriteableBuffer
 else:  # pragma: no cover
     # Needed for sphinx-autodoc
     WriteableBuffer = bytearray | memoryview
 
-_DTOPacketT = typing.TypeVar("_DTOPacketT")
-_SentDTOPacketT = typing.TypeVar("_SentDTOPacketT")
-_ReceivedDTOPacketT = typing.TypeVar("_ReceivedDTOPacketT")
-
-_SentPacketT = typing.TypeVar("_SentPacketT")
-_ReceivedPacketT = typing.TypeVar("_ReceivedPacketT")
-_PacketT = typing.TypeVar("_PacketT")
+_T_DTOPacket = typing.TypeVar("_T_DTOPacket")
+_T_SentDTOPacket = typing.TypeVar("_T_SentDTOPacket")
+_T_ReceivedDTOPacket = typing.TypeVar("_T_ReceivedDTOPacket")
+
+_T_SentPacket = typing.TypeVar("_T_SentPacket")
+_T_ReceivedPacket = typing.TypeVar("_T_ReceivedPacket")
+_T_Packet = typing.TypeVar("_T_Packet")
 
-_RequestT = typing.TypeVar("_RequestT")
-_ResponseT = typing.TypeVar("_ResponseT")
+_T_Request = typing.TypeVar("_T_Request")
+_T_Response = typing.TypeVar("_T_Response")
 
-_BufferT = typing.TypeVar("_BufferT", bound="WriteableBuffer")
+_T_Buffer = typing.TypeVar("_T_Buffer", bound="WriteableBuffer")
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/converter.py` & `easynetwork-1.0.0rc8/src/easynetwork/converter.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -22,33 +22,33 @@
     "StapledPacketConverter",
 ]
 
 from abc import ABCMeta, abstractmethod
 from dataclasses import dataclass
 from typing import Any, Generic, final
 
-from ._typevars import _DTOPacketT, _PacketT, _ReceivedDTOPacketT, _ReceivedPacketT, _SentDTOPacketT, _SentPacketT
+from ._typevars import _T_DTOPacket, _T_Packet, _T_ReceivedDTOPacket, _T_ReceivedPacket, _T_SentDTOPacket, _T_SentPacket
 from .lowlevel import _utils
 
 
 class AbstractPacketConverterComposite(
-    Generic[_SentPacketT, _ReceivedPacketT, _SentDTOPacketT, _ReceivedDTOPacketT],
+    Generic[_T_SentPacket, _T_ReceivedPacket, _T_SentDTOPacket, _T_ReceivedDTOPacket],
     metaclass=ABCMeta,
 ):
     """
     The base class for implementing a :term:`composite converter`.
 
     See Also:
         The :class:`AbstractPacketConverter` class.
     """
 
     __slots__ = ("__weakref__",)
 
     @abstractmethod
-    def create_from_dto_packet(self, packet: _ReceivedDTOPacketT, /) -> _ReceivedPacketT:
+    def create_from_dto_packet(self, packet: _T_ReceivedDTOPacket, /) -> _T_ReceivedPacket:
         """
         Constructs the business object from the :term:`DTO` `packet`.
 
         Parameters:
             packet: The :term:`data transfer object`.
 
         Raises:
@@ -56,43 +56,43 @@
 
         Returns:
             the business object.
         """
         raise NotImplementedError
 
     @abstractmethod
-    def convert_to_dto_packet(self, obj: _SentPacketT, /) -> _SentDTOPacketT:
+    def convert_to_dto_packet(self, obj: _T_SentPacket, /) -> _T_SentDTOPacket:
         """
         Creates the :term:`DTO` packet from the business object `obj`.
 
         Parameters:
             obj: The business object.
 
         Returns:
             the :term:`data transfer object`.
         """
         raise NotImplementedError
 
 
 @dataclass(frozen=True, slots=True)
 class StapledPacketConverter(
-    AbstractPacketConverterComposite[_SentPacketT, _ReceivedPacketT, _SentDTOPacketT, _ReceivedDTOPacketT]
+    AbstractPacketConverterComposite[_T_SentPacket, _T_ReceivedPacket, _T_SentDTOPacket, _T_ReceivedDTOPacket]
 ):
     """
     A :term:`composite converter` that merges two converters.
     """
 
-    sent_packet_converter: AbstractPacketConverterComposite[_SentPacketT, Any, _SentDTOPacketT, Any]
+    sent_packet_converter: AbstractPacketConverterComposite[_T_SentPacket, Any, _T_SentDTOPacket, Any]
     """Sent packet converter."""
 
-    received_packet_converter: AbstractPacketConverterComposite[Any, _ReceivedPacketT, Any, _ReceivedDTOPacketT]
+    received_packet_converter: AbstractPacketConverterComposite[Any, _T_ReceivedPacket, Any, _T_ReceivedDTOPacket]
     """Received packet converter."""
 
     @final
-    def create_from_dto_packet(self, packet: _ReceivedDTOPacketT, /) -> _ReceivedPacketT:
+    def create_from_dto_packet(self, packet: _T_ReceivedDTOPacket, /) -> _T_ReceivedPacket:
         """
         Calls ``self.received_packet_converter.create_from_dto_packet(packet)``.
 
         Parameters:
             packet: The :term:`data transfer object`.
 
         Raises:
@@ -100,42 +100,42 @@
 
         Returns:
             the business object.
         """
         return self.received_packet_converter.create_from_dto_packet(packet)
 
     @final
-    def convert_to_dto_packet(self, obj: _SentPacketT, /) -> _SentDTOPacketT:
+    def convert_to_dto_packet(self, obj: _T_SentPacket, /) -> _T_SentDTOPacket:
         """
         Calls ``self.sent_packet_converter.convert_to_dto_packet(obj)``.
 
         Parameters:
             obj: The business object.
 
         Returns:
             the :term:`data transfer object`.
         """
         return self.sent_packet_converter.convert_to_dto_packet(obj)
 
 
 class AbstractPacketConverter(
-    AbstractPacketConverterComposite[_PacketT, _PacketT, _DTOPacketT, _DTOPacketT],
-    Generic[_PacketT, _DTOPacketT],
+    AbstractPacketConverterComposite[_T_Packet, _T_Packet, _T_DTOPacket, _T_DTOPacket],
+    Generic[_T_Packet, _T_DTOPacket],
 ):
     """
     The base class for implementing a :term:`converter`.
 
     See Also:
         The :class:`AbstractPacketConverterComposite` class.
     """
 
     __slots__ = ()
 
     @abstractmethod
     @_utils.inherit_doc(AbstractPacketConverterComposite)
-    def create_from_dto_packet(self, packet: _DTOPacketT, /) -> _PacketT:
+    def create_from_dto_packet(self, packet: _T_DTOPacket, /) -> _T_Packet:
         raise NotImplementedError
 
     @abstractmethod
     @_utils.inherit_doc(AbstractPacketConverterComposite)
-    def convert_to_dto_packet(self, obj: _PacketT, /) -> _DTOPacketT:
+    def convert_to_dto_packet(self, obj: _T_Packet, /) -> _T_DTOPacket:
         raise NotImplementedError
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/exceptions.py` & `easynetwork-1.0.0rc8/src/easynetwork/exceptions.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/protocol.py` & `easynetwork-1.0.0rc8/src/easynetwork/protocol.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -21,15 +21,15 @@
     "DatagramProtocol",
     "StreamProtocol",
 ]
 
 from collections.abc import Generator
 from typing import TYPE_CHECKING, Any, Generic, Never, overload
 
-from ._typevars import _BufferT, _ReceivedDTOPacketT, _ReceivedPacketT, _SentDTOPacketT, _SentPacketT
+from ._typevars import _T_Buffer, _T_ReceivedDTOPacket, _T_ReceivedPacket, _T_SentDTOPacket, _T_SentPacket
 from .converter import AbstractPacketConverterComposite
 from .exceptions import (
     DatagramProtocolParseError,
     DeserializeError,
     IncrementalDeserializeError,
     PacketConversionError,
     StreamProtocolParseError,
@@ -37,69 +37,67 @@
 )
 from .serializers.abc import AbstractIncrementalPacketSerializer, AbstractPacketSerializer, BufferedIncrementalPacketSerializer
 
 if TYPE_CHECKING:
     from _typeshed import ReadableBuffer, WriteableBuffer
 
 
-class DatagramProtocol(Generic[_SentPacketT, _ReceivedPacketT]):
+class DatagramProtocol(Generic[_T_SentPacket, _T_ReceivedPacket]):
     """A :term:`protocol object` class for datagram communication."""
 
     __slots__ = ("__serializer", "__converter", "__weakref__")
 
     @overload
     def __init__(
         self,
-        serializer: AbstractPacketSerializer[_SentPacketT, _ReceivedPacketT],
+        serializer: AbstractPacketSerializer[_T_SentPacket, _T_ReceivedPacket],
         converter: None = ...,
-    ) -> None:
-        ...
+    ) -> None: ...
 
     @overload
     def __init__(
         self,
-        serializer: AbstractPacketSerializer[_SentDTOPacketT, _ReceivedDTOPacketT],
-        converter: AbstractPacketConverterComposite[_SentPacketT, _ReceivedPacketT, _SentDTOPacketT, _ReceivedDTOPacketT],
-    ) -> None:
-        ...
+        serializer: AbstractPacketSerializer[_T_SentDTOPacket, _T_ReceivedDTOPacket],
+        converter: AbstractPacketConverterComposite[_T_SentPacket, _T_ReceivedPacket, _T_SentDTOPacket, _T_ReceivedDTOPacket],
+    ) -> None: ...
 
     def __init__(
         self,
         serializer: AbstractPacketSerializer[Any, Any],
-        converter: AbstractPacketConverterComposite[_SentPacketT, _ReceivedPacketT, Any, Any] | None = None,
+        converter: AbstractPacketConverterComposite[_T_SentPacket, _T_ReceivedPacket, Any, Any] | None = None,
     ) -> None:
         """
         Parameters:
             serializer: The :term:`serializer` to use.
             converter: The :term:`converter` to use.
         """
 
         if not isinstance(serializer, AbstractPacketSerializer):
             raise TypeError(f"Expected a serializer instance, got {serializer!r}")
         if converter is not None and not isinstance(converter, AbstractPacketConverterComposite):
             raise TypeError(f"Expected a converter instance, got {converter!r}")
         self.__serializer: AbstractPacketSerializer[Any, Any] = serializer
-        self.__converter: AbstractPacketConverterComposite[_SentPacketT, _ReceivedPacketT, Any, Any] | None = converter
+        self.__converter: AbstractPacketConverterComposite[_T_SentPacket, _T_ReceivedPacket, Any, Any] | None = converter
 
-    def make_datagram(self, packet: _SentPacketT) -> bytes:
+    def make_datagram(self, packet: _T_SentPacket) -> bytes:
         """
         Serializes a Python object to a raw datagram :term:`packet`.
 
         Parameters:
             packet: The :term:`packet` as a Python object to serialize.
 
         Returns:
             the serialized :term:`packet`.
         """
 
         if (converter := self.__converter) is not None:
             packet = converter.convert_to_dto_packet(packet)
         return self.__serializer.serialize(packet)
 
-    def build_packet_from_datagram(self, datagram: bytes) -> _ReceivedPacketT:
+    def build_packet_from_datagram(self, datagram: bytes) -> _T_ReceivedPacket:
         """
         Creates a Python object representing the raw datagram :term:`packet`.
 
         Parameters:
             datagram: The datagram :term:`packet` to deserialize.
 
         Raises:
@@ -107,77 +105,76 @@
             DatagramProtocolParseError: in case of conversion error (if there is a :term:`converter`).
 
         Returns:
             the deserialized :term:`packet`.
         """
 
         try:
-            packet: _ReceivedPacketT = self.__serializer.deserialize(datagram)
+            packet: _T_ReceivedPacket = self.__serializer.deserialize(datagram)
         except DeserializeError as exc:
             raise DatagramProtocolParseError(exc) from exc
         if (converter := self.__converter) is not None:
             try:
                 packet = converter.create_from_dto_packet(packet)
             except PacketConversionError as exc:
                 raise DatagramProtocolParseError(exc) from exc
         return packet
 
 
-class BufferedStreamReceiver(Generic[_ReceivedPacketT, _BufferT]):
+class BufferedStreamReceiver(Generic[_T_ReceivedPacket, _T_Buffer]):
     """A specialization of :class:`StreamProtocol` in order to use a buffered :term:`incremental serializer`.
 
-    It is not recommended to instantiate `BufferedStreamReceiver` objects directly;
-    use :meth:`StreamProtocol.buffered_receiver` instead.
+    Warning:
+        It is not recommended to instantiate `BufferedStreamReceiver` objects directly;
+        use :meth:`StreamProtocol.buffered_receiver` instead.
     """
 
     __slots__ = ("__serializer", "__converter", "__weakref__")
 
     @overload
     def __init__(
         self,
-        serializer: BufferedIncrementalPacketSerializer[Any, _ReceivedPacketT, _BufferT],
+        serializer: BufferedIncrementalPacketSerializer[Any, _T_ReceivedPacket, _T_Buffer],
         converter: None = ...,
-    ) -> None:
-        ...
+    ) -> None: ...
 
     @overload
     def __init__(
         self,
-        serializer: BufferedIncrementalPacketSerializer[Any, _ReceivedDTOPacketT, _BufferT],
-        converter: AbstractPacketConverterComposite[Any, _ReceivedPacketT, Any, _ReceivedDTOPacketT],
-    ) -> None:
-        ...
+        serializer: BufferedIncrementalPacketSerializer[Any, _T_ReceivedDTOPacket, _T_Buffer],
+        converter: AbstractPacketConverterComposite[Any, _T_ReceivedPacket, Any, _T_ReceivedDTOPacket],
+    ) -> None: ...
 
     def __init__(
         self,
-        serializer: BufferedIncrementalPacketSerializer[Any, Any, _BufferT],
-        converter: AbstractPacketConverterComposite[Any, _ReceivedPacketT, Any, Any] | None = None,
+        serializer: BufferedIncrementalPacketSerializer[Any, Any, _T_Buffer],
+        converter: AbstractPacketConverterComposite[Any, _T_ReceivedPacket, Any, Any] | None = None,
     ) -> None:
         """
         Parameters:
             serializer: The buffered :term:`incremental serializer` to use.
             converter: The :term:`converter` to use.
         """
 
         if not isinstance(serializer, BufferedIncrementalPacketSerializer):
             raise TypeError(f"Expected a buffered incremental serializer instance, got {serializer!r}")
         if converter is not None and not isinstance(converter, AbstractPacketConverterComposite):
             raise TypeError(f"Expected a converter instance, got {converter!r}")
-        self.__serializer: BufferedIncrementalPacketSerializer[Any, Any, _BufferT] = serializer
-        self.__converter: AbstractPacketConverterComposite[Never, _ReceivedPacketT, Any, Any] | None = converter
+        self.__serializer: BufferedIncrementalPacketSerializer[Any, Any, _T_Buffer] = serializer
+        self.__converter: AbstractPacketConverterComposite[Never, _T_ReceivedPacket, Any, Any] | None = converter
 
-    def create_buffer(self, sizehint: int) -> _BufferT:
+    def create_buffer(self, sizehint: int) -> _T_Buffer:
         """
         Called to allocate a new receive buffer.
 
         See :meth:`.BufferedIncrementalPacketSerializer.create_deserializer_buffer` for details.
         """
         return self.__serializer.create_deserializer_buffer(sizehint)
 
-    def build_packet_from_buffer(self, buffer: _BufferT) -> Generator[int | None, int, tuple[_ReceivedPacketT, ReadableBuffer]]:
+    def build_packet_from_buffer(self, buffer: _T_Buffer) -> Generator[int | None, int, tuple[_T_ReceivedPacket, ReadableBuffer]]:
         """
         Creates a Python object representing the raw :term:`packet`.
 
         Parameters:
             buffer: The buffer allocated by :meth:`create_buffer`.
 
         Raises:
@@ -191,15 +188,15 @@
             See :meth:`.BufferedIncrementalPacketSerializer.buffered_incremental_deserialize` for details.
 
         Returns:
             a tuple with the deserialized Python object and the unused trailing data.
 
             See :meth:`.BufferedIncrementalPacketSerializer.buffered_incremental_deserialize` for details.
         """
-        packet: _ReceivedPacketT
+        packet: _T_ReceivedPacket
         try:
             packet, remaining_data = yield from self.__serializer.buffered_incremental_deserialize(buffer)
         except IncrementalDeserializeError as exc:
             raise StreamProtocolParseError(exc.remaining_data, exc) from exc
         except DeserializeError as exc:
             raise RuntimeError("DeserializeError raised instead of IncrementalDeserializeError") from exc
 
@@ -208,87 +205,85 @@
                 packet = converter.create_from_dto_packet(packet)
             except PacketConversionError as exc:
                 raise StreamProtocolParseError(remaining_data, exc) from exc
 
         return packet, remaining_data
 
 
-class StreamProtocol(Generic[_SentPacketT, _ReceivedPacketT]):
+class StreamProtocol(Generic[_T_SentPacket, _T_ReceivedPacket]):
     """A :term:`protocol object` class for connection-oriented stream communication."""
 
     __slots__ = ("__serializer", "__converter", "__buffered_receiver", "__weakref__")
 
     @overload
     def __init__(
         self,
-        serializer: AbstractIncrementalPacketSerializer[_SentPacketT, _ReceivedPacketT],
+        serializer: AbstractIncrementalPacketSerializer[_T_SentPacket, _T_ReceivedPacket],
         converter: None = ...,
-    ) -> None:
-        ...
+    ) -> None: ...
 
     @overload
     def __init__(
         self,
-        serializer: AbstractIncrementalPacketSerializer[_SentDTOPacketT, _ReceivedDTOPacketT],
-        converter: AbstractPacketConverterComposite[_SentPacketT, _ReceivedPacketT, _SentDTOPacketT, _ReceivedDTOPacketT],
-    ) -> None:
-        ...
+        serializer: AbstractIncrementalPacketSerializer[_T_SentDTOPacket, _T_ReceivedDTOPacket],
+        converter: AbstractPacketConverterComposite[_T_SentPacket, _T_ReceivedPacket, _T_SentDTOPacket, _T_ReceivedDTOPacket],
+    ) -> None: ...
 
     def __init__(
         self,
         serializer: AbstractIncrementalPacketSerializer[Any, Any],
-        converter: AbstractPacketConverterComposite[_SentPacketT, _ReceivedPacketT, Any, Any] | None = None,
+        converter: AbstractPacketConverterComposite[_T_SentPacket, _T_ReceivedPacket, Any, Any] | None = None,
     ) -> None:
         """
         Parameters:
             serializer: The :term:`incremental serializer` to use.
             converter: The :term:`converter` to use.
         """
 
         if not isinstance(serializer, AbstractIncrementalPacketSerializer):
             raise TypeError(f"Expected an incremental serializer instance, got {serializer!r}")
         if converter is not None and not isinstance(converter, AbstractPacketConverterComposite):
             raise TypeError(f"Expected a converter instance, got {converter!r}")
         self.__serializer: AbstractIncrementalPacketSerializer[Any, Any] = serializer
-        self.__converter: AbstractPacketConverterComposite[_SentPacketT, _ReceivedPacketT, Any, Any] | None = converter
+        self.__converter: AbstractPacketConverterComposite[_T_SentPacket, _T_ReceivedPacket, Any, Any] | None = converter
 
-        self.__buffered_receiver: BufferedStreamReceiver[_ReceivedPacketT, WriteableBuffer] | None
+        self.__buffered_receiver: BufferedStreamReceiver[_T_ReceivedPacket, WriteableBuffer] | None
         if isinstance(serializer, BufferedIncrementalPacketSerializer):
             self.__buffered_receiver = BufferedStreamReceiver(serializer, converter=converter)
         else:
             self.__buffered_receiver = None
 
-    def generate_chunks(self, packet: _SentPacketT) -> Generator[bytes, None, None]:
+    def generate_chunks(self, packet: _T_SentPacket) -> Generator[bytes, None, None]:
         """
         Serializes a Python object to a raw :term:`packet` part by part.
 
         Parameters:
             packet: The :term:`packet` as a Python object to serialize.
 
         Yields:
             all the parts of the :term:`packet`.
         """
 
         if (converter := self.__converter) is not None:
             packet = converter.convert_to_dto_packet(packet)
         return (yield from self.__serializer.incremental_serialize(packet))
 
-    def buffered_receiver(self) -> BufferedStreamReceiver[_ReceivedPacketT, WriteableBuffer]:
+    def buffered_receiver(self) -> BufferedStreamReceiver[_T_ReceivedPacket, WriteableBuffer]:
         """
         Get a specialization interface in order to use the buffer API.
 
         Raises:
             UnsupportedOperation: The serializer does not derive from :class:`.BufferedIncrementalPacketSerializer`.
         """
         buffered_receiver = self.__buffered_receiver
         if buffered_receiver is None:
             raise UnsupportedOperation("This protocol does not support the buffer API")
         return buffered_receiver
 
-    def build_packet_from_chunks(self) -> Generator[None, bytes, tuple[_ReceivedPacketT, bytes]]:
+    def build_packet_from_chunks(self) -> Generator[None, bytes, tuple[_T_ReceivedPacket, bytes]]:
         """
         Creates a Python object representing the raw :term:`packet`.
 
         Raises:
             StreamProtocolParseError: in case of deserialization error.
             StreamProtocolParseError: in case of conversion error (if there is a :term:`converter`).
             RuntimeError: The :term:`serializer` raised :exc:`.DeserializeError` instead of :exc:`.IncrementalDeserializeError`.
@@ -296,15 +291,15 @@
         Yields:
             :data:`None` until the whole :term:`packet` has been deserialized.
 
         Returns:
             a tuple with the deserialized Python object and the unused trailing data.
         """
 
-        packet: _ReceivedPacketT
+        packet: _T_ReceivedPacket
         try:
             packet, remaining_data = yield from self.__serializer.incremental_deserialize()
         except IncrementalDeserializeError as exc:
             raise StreamProtocolParseError(exc.remaining_data, exc) from exc
         except DeserializeError as exc:
             raise RuntimeError("DeserializeError raised instead of IncrementalDeserializeError") from exc
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/version.py` & `easynetwork-1.0.0rc8/src/easynetwork/version.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -21,9 +21,9 @@
 from __future__ import annotations
 
 __all__ = ["__version__", "__version_tuple__"]
 
 __version__: str
 __version_tuple__: tuple[str | int, ...]
 
-__version__ = '1.0.0rc7'
+__version__ = '1.0.0rc8'
 __version_tuple__ = (1, 0, 0)
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/api_async/__init__.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,21 +1,19 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 #
-"""
-Asynchronous client/server module
-"""
+"""EasyNetwork's low-level API module"""
 
 from __future__ import annotations
 
 __all__ = []  # type: list[str]
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/api_async/client/__init__.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_async/backend/__init__.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,27 +1,19 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 #
-"""Asynchronous network client module"""
+"""Asynchronous backend engine module"""
 
 from __future__ import annotations
 
-__all__ = [
-    "AbstractAsyncNetworkClient",
-    "AsyncTCPNetworkClient",
-    "AsyncUDPNetworkClient",
-]
-
-from .abc import *
-from .tcp import *
-from .udp import *
+__all__ = []  # type: list[str]
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/api_async/client/abc.py` & `easynetwork-1.0.0rc8/src/easynetwork/clients/abc.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,42 +1,205 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 #
-"""Asynchronous network client module"""
+"""Network client interfaces definition module"""
 
 from __future__ import annotations
 
-__all__ = ["AbstractAsyncNetworkClient"]
+__all__ = ["AbstractAsyncNetworkClient", "AbstractNetworkClient"]
 
-import math
 from abc import ABCMeta, abstractmethod
-from collections.abc import AsyncIterator
+from collections.abc import AsyncIterator, Iterator
 from typing import TYPE_CHECKING, Generic, Self
 
-from ..._typevars import _ReceivedPacketT, _SentPacketT
-from ...lowlevel import _utils
-from ...lowlevel.api_async.backend.factory import current_async_backend
-from ...lowlevel.socket import SocketAddress
+from .._typevars import _T_ReceivedPacket, _T_SentPacket
+from ..lowlevel import _utils
+from ..lowlevel.socket import SocketAddress
 
 if TYPE_CHECKING:
     from types import TracebackType
 
 
-class AbstractAsyncNetworkClient(Generic[_SentPacketT, _ReceivedPacketT], metaclass=ABCMeta):
+class AbstractNetworkClient(Generic[_T_SentPacket, _T_ReceivedPacket], metaclass=ABCMeta):
+    """
+    The base class for a network client interface.
+    """
+
+    __slots__ = ("__weakref__",)
+
+    def __enter__(self) -> Self:
+        return self
+
+    def __exit__(self, exc_type: type[BaseException] | None, exc_val: BaseException | None, exc_tb: TracebackType | None) -> None:
+        """
+        Calls :meth:`close`.
+        """
+        self.close()
+
+    @abstractmethod
+    def is_closed(self) -> bool:
+        """
+        Checks if the client is in a closed state.
+
+        If :data:`True`, all future operations on the client object will raise a :exc:`.ClientClosedError`.
+
+        See Also:
+            :meth:`close` method.
+
+        Returns:
+            the client state.
+        """
+        raise NotImplementedError
+
+    @abstractmethod
+    def close(self) -> None:
+        """
+        Close the client.
+
+        Once that happens, all future operations on the client object will raise a :exc:`.ClientClosedError`.
+        The remote end will receive no more data (after queued data is flushed).
+
+        Can be safely called multiple times.
+        """
+        raise NotImplementedError
+
+    @abstractmethod
+    def get_local_address(self) -> SocketAddress:
+        """
+        Returns the local socket IP address.
+
+        Raises:
+            ClientClosedError: the client object is closed.
+
+        Returns:
+            the client's local address.
+        """
+        raise NotImplementedError
+
+    @abstractmethod
+    def get_remote_address(self) -> SocketAddress:
+        """
+        Returns the remote socket IP address.
+
+        Raises:
+            ClientClosedError: the client object is closed.
+
+        Returns:
+            the client's remote address.
+        """
+        raise NotImplementedError
+
+    @abstractmethod
+    def send_packet(self, packet: _T_SentPacket, *, timeout: float | None = ...) -> None:
+        """
+        Sends `packet` to the remote endpoint.
+
+        If `timeout` is not :data:`None`, the entire send operation will take at most `timeout` seconds.
+
+        Warning:
+            A timeout on a send operation is unusual unless the implementation is using a lower-level
+            communication protocol (such as SSL/TLS).
+
+            In the case of a timeout, it is impossible to know if all the packet data has been sent.
+            This would leave the connection in an inconsistent state.
+
+        Parameters:
+            packet: the Python object to send.
+            timeout: the allowed time (in seconds) for blocking operations.
+
+        Raises:
+            ClientClosedError: the client object is closed.
+            ConnectionError: connection unexpectedly closed during operation.
+                             You should not attempt any further operation and close the client object.
+            TimeoutError: the send operation does not end up after `timeout` seconds.
+            OSError: unrelated OS error occurred. You should check :attr:`OSError.errno`.
+        """
+        raise NotImplementedError
+
+    @abstractmethod
+    def recv_packet(self, *, timeout: float | None = ...) -> _T_ReceivedPacket:
+        """
+        Waits for a new packet to arrive from the remote endpoint.
+
+        If `timeout` is not :data:`None`, the entire receive operation will take at most `timeout` seconds.
+
+        Parameters:
+            timeout: the allowed time (in seconds) for blocking operations.
+
+        Raises:
+            ClientClosedError: the client object is closed.
+            ConnectionError: connection unexpectedly closed during operation.
+                             You should not attempt any further operation and close the client object.
+            TimeoutError: the receive operation does not end up after `timeout` seconds.
+            OSError: unrelated OS error occurred. You should check :attr:`OSError.errno`.
+            BaseProtocolParseError: invalid data received.
+
+        Returns:
+            the received packet.
+        """
+        raise NotImplementedError
+
+    def iter_received_packets(self, *, timeout: float | None = 0) -> Iterator[_T_ReceivedPacket]:
+        """
+        Returns an :term:`iterator` that waits for a new packet to arrive from the remote endpoint.
+
+        If `timeout` is not :data:`None`, the entire receive operation will take at most `timeout` seconds; it defaults to zero.
+
+        Important:
+            The `timeout` is for the entire iterator::
+
+                iterator = client.iter_received_packets(timeout=10)
+
+                # Let's say that this call took 6 seconds...
+                first_packet = next(iterator)
+
+                # ...then this call has a maximum of 4 seconds, not 10.
+                second_packet = next(iterator)
+
+            The time taken outside the iterator object is not decremented to the timeout parameter.
+
+        Parameters:
+            timeout: the allowed time (in seconds) for all the receive operations.
+
+        Yields:
+            the received packet.
+        """
+        while True:
+            try:
+                with _utils.ElapsedTime() as elapsed:
+                    packet = self.recv_packet(timeout=timeout)
+            except OSError:
+                return
+            yield packet
+            if timeout is not None:
+                timeout = elapsed.recompute_timeout(timeout)
+
+    @abstractmethod
+    def fileno(self) -> int:
+        """
+        Returns the socket's file descriptor, or ``-1`` if client (or socket) is closed.
+
+        Returns:
+            the opened file descriptor.
+        """
+        raise NotImplementedError
+
+
+class AbstractAsyncNetworkClient(Generic[_T_SentPacket, _T_ReceivedPacket], metaclass=ABCMeta):
     """
     The base class for an asynchronous network client interface.
     """
 
     __slots__ = ("__weakref__",)
 
     async def __aenter__(self) -> Self:
@@ -150,15 +313,15 @@
 
         Returns:
             the client's remote address.
         """
         raise NotImplementedError
 
     @abstractmethod
-    async def send_packet(self, packet: _SentPacketT) -> None:
+    async def send_packet(self, packet: _T_SentPacket) -> None:
         """
         Sends `packet` to the remote endpoint.
 
         Warning:
             In the case of a cancellation, it is impossible to know if all the packet data has been sent.
             This would leave the connection in an inconsistent state.
 
@@ -170,15 +333,15 @@
             ConnectionError: connection unexpectedly closed during operation.
                              You should not attempt any further operation and close the client object.
             OSError: unrelated OS error occurred. You should check :attr:`OSError.errno`.
         """
         raise NotImplementedError
 
     @abstractmethod
-    async def recv_packet(self) -> _ReceivedPacketT:
+    async def recv_packet(self) -> _T_ReceivedPacket:
         """
         Waits for a new packet to arrive from the remote endpoint.
 
         Raises:
             ClientClosedError: the client object is closed.
             ConnectionError: connection unexpectedly closed during operation.
                              You should not attempt any further operation and close the client object.
@@ -186,15 +349,15 @@
             BaseProtocolParseError: invalid data received.
 
         Returns:
             the received packet.
         """
         raise NotImplementedError
 
-    async def iter_received_packets(self, *, timeout: float | None = 0) -> AsyncIterator[_ReceivedPacketT]:
+    async def iter_received_packets(self, *, timeout: float | None = 0) -> AsyncIterator[_T_ReceivedPacket]:
         """
         Returns an :term:`asynchronous iterator` that waits for a new packet to arrive from the remote endpoint.
 
         If `timeout` is not :data:`None`, the entire receive operation will take at most `timeout` seconds; it defaults to zero.
 
         Important:
             The `timeout` is for the entire iterator::
@@ -212,16 +375,18 @@
         Parameters:
             timeout: the allowed time (in seconds) for all the receive operations.
 
         Yields:
             the received packet.
         """
 
+        from ..lowlevel.api_async.backend.factory import current_async_backend
+
         if timeout is None:
-            timeout = math.inf
+            timeout = float("inf")
 
         timeout_after = current_async_backend().timeout
 
         while True:
             try:
                 with timeout_after(timeout), _utils.ElapsedTime() as elapsed:
                     packet = await self.recv_packet()
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/api_async/client/tcp.py` & `easynetwork-1.0.0rc8/src/easynetwork/clients/tcp.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,530 +1,479 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 #
-"""Asynchronous network client module"""
+"""TCP Network client implementation module"""
 
 from __future__ import annotations
 
-__all__ = ["AsyncTCPNetworkClient"]
+__all__ = ["TCPNetworkClient"]
 
 import contextlib
-import dataclasses
 import errno as _errno
 import socket as _socket
-from collections.abc import Awaitable, Callable, Iterator
-from typing import TYPE_CHECKING, Any, final, overload
+import threading
+from collections.abc import Iterator
+from typing import TYPE_CHECKING, Any, Literal, final, overload
 
 try:
-    import ssl as _ssl
+    import ssl
 except ImportError:  # pragma: no cover
     _ssl_module = None
 else:
-    _ssl_module = _ssl
-    del _ssl
+    _ssl_module = ssl
+    del ssl
 
-from ..._typevars import _ReceivedPacketT, _SentPacketT
-from ...exceptions import ClientClosedError
-from ...lowlevel import _utils, constants
-from ...lowlevel.api_async.backend.abc import CancelScope, ILock
-from ...lowlevel.api_async.backend.factory import current_async_backend
-from ...lowlevel.api_async.endpoints.stream import AsyncStreamEndpoint
-from ...lowlevel.api_async.transports.abc import AsyncStreamTransport
-from ...lowlevel.socket import (
+from .._typevars import _T_ReceivedPacket, _T_SentPacket
+from ..exceptions import ClientClosedError
+from ..lowlevel import _lock, _utils, constants
+from ..lowlevel.api_sync.endpoints.stream import StreamEndpoint
+from ..lowlevel.api_sync.transports.socket import SocketStreamTransport, SSLStreamTransport
+from ..lowlevel.socket import (
     INETSocketAttribute,
     SocketAddress,
     SocketProxy,
     new_socket_address,
     set_tcp_keepalive,
     set_tcp_nodelay,
 )
-from ...protocol import StreamProtocol
-from .abc import AbstractAsyncNetworkClient
+from ..protocol import StreamProtocol
+from .abc import AbstractNetworkClient
 
 if TYPE_CHECKING:
     import ssl as _typing_ssl
 
 
-@dataclasses.dataclass(kw_only=True, slots=True)
-class _SocketConnector:
-    lock: ILock
-    factory: Callable[[], Awaitable[tuple[AsyncStreamTransport, SocketProxy]]] | None
-    scope: CancelScope
-    _result: tuple[AsyncStreamTransport, SocketProxy] | None = dataclasses.field(init=False, default=None)
-
-    async def get(self) -> tuple[AsyncStreamTransport, SocketProxy] | None:
-        async with self.lock:
-            factory, self.factory = self.factory, None
-            if factory is not None:
-                with self.scope:
-                    self._result = await factory()
-        return self._result
-
-
-class AsyncTCPNetworkClient(AbstractAsyncNetworkClient[_SentPacketT, _ReceivedPacketT]):
+class TCPNetworkClient(AbstractNetworkClient[_T_SentPacket, _T_ReceivedPacket]):
     """
-    An asynchronous network client interface for TCP connections.
+    A network client interface for TCP connections.
     """
 
     __slots__ = (
         "__endpoint",
-        "__protocol",
-        "__socket_connector",
         "__socket_proxy",
-        "__receive_lock",
         "__send_lock",
-        "__expected_recv_size",
+        "__receive_lock",
     )
 
     @overload
     def __init__(
         self,
         address: tuple[str, int],
         /,
-        protocol: StreamProtocol[_SentPacketT, _ReceivedPacketT],
+        protocol: StreamProtocol[_T_SentPacket, _T_ReceivedPacket],
         *,
+        connect_timeout: float | None = ...,
         local_address: tuple[str, int] | None = ...,
-        happy_eyeballs_delay: float | None = ...,
         ssl: _typing_ssl.SSLContext | bool | None = ...,
         server_hostname: str | None = ...,
         ssl_handshake_timeout: float | None = ...,
         ssl_shutdown_timeout: float | None = ...,
+        ssl_standard_compatible: bool | None = ...,
         ssl_shared_lock: bool | None = ...,
         max_recv_size: int | None = ...,
-    ) -> None:
-        ...
+        retry_interval: float = ...,
+        manual_buffer_allocation: Literal["try", "no", "force"] = ...,
+    ) -> None: ...
 
     @overload
     def __init__(
         self,
         socket: _socket.socket,
         /,
-        protocol: StreamProtocol[_SentPacketT, _ReceivedPacketT],
+        protocol: StreamProtocol[_T_SentPacket, _T_ReceivedPacket],
         *,
         ssl: _typing_ssl.SSLContext | bool | None = ...,
         server_hostname: str | None = ...,
         ssl_handshake_timeout: float | None = ...,
         ssl_shutdown_timeout: float | None = ...,
+        ssl_standard_compatible: bool | None = ...,
         ssl_shared_lock: bool | None = ...,
         max_recv_size: int | None = ...,
-    ) -> None:
-        ...
+        retry_interval: float = ...,
+        manual_buffer_allocation: Literal["try", "no", "force"] = ...,
+    ) -> None: ...
 
     def __init__(
         self,
-        __arg: tuple[str, int] | _socket.socket,
+        __arg: _socket.socket | tuple[str, int],
         /,
-        protocol: StreamProtocol[_SentPacketT, _ReceivedPacketT],
+        protocol: StreamProtocol[_T_SentPacket, _T_ReceivedPacket],
         *,
         ssl: _typing_ssl.SSLContext | bool | None = None,
         server_hostname: str | None = None,
         ssl_handshake_timeout: float | None = None,
         ssl_shutdown_timeout: float | None = None,
+        ssl_standard_compatible: bool | None = None,
         ssl_shared_lock: bool | None = None,
         max_recv_size: int | None = None,
+        retry_interval: float = 1.0,
+        manual_buffer_allocation: Literal["try", "no", "force"] = "try",
         **kwargs: Any,
     ) -> None:
         """
         Common Parameters:
             protocol: The :term:`protocol object` to use.
 
         Connection Parameters:
             address: A pair of ``(host, port)`` for connection.
-            happy_eyeballs_delay: the "Connection Attempt Delay" as defined in :rfc:`8305`.
-                                  A sensible default value recommended by the RFC is 0.25 (250 milliseconds).
+            connect_timeout: The connection timeout (in seconds).
             local_address: If given, is a ``(local_host, local_port)`` tuple used to bind the socket locally.
 
         Socket Parameters:
             socket: An already connected TCP :class:`socket.socket`. If `socket` is given,
-                    none of `happy_eyeballs_delay` and `local_address` should be specified.
+                    none of `connect_timeout` and `local_address` should be specified.
 
         Keyword Arguments:
             ssl: If given and not false, a SSL/TLS transport is created (by default a plain TCP transport is created).
                  If ssl is a :class:`ssl.SSLContext` object, this context is used to create the transport;
                  if ssl is :data:`True`, a default context returned from :func:`ssl.create_default_context` is used.
             server_hostname: sets or overrides the hostname that the target server's certificate will be matched against.
                              Should only be passed if `ssl` is not :data:`None`. By default the value of the host in `address`
                              argument is used. If `socket` is provided instead, there is no default and you must pass a value
                              for `server_hostname`. If `server_hostname` is an empty string, hostname matching is disabled
                              (which is a serious security risk, allowing for potential man-in-the-middle attacks).
             ssl_handshake_timeout: (for a TLS connection) the time in seconds to wait for the TLS handshake to complete
                                    before aborting the connection. ``60.0`` seconds if :data:`None` (default).
             ssl_shutdown_timeout: the time in seconds to wait for the SSL shutdown to complete before aborting the connection.
                                   ``30.0`` seconds if :data:`None` (default).
+            ssl_standard_compatible: if :data:`False`, skip the closing handshake when closing the connection,
+                                     and don't raise an exception if the peer does the same.
             ssl_shared_lock: If :data:`True` (the default), :meth:`send_packet` and :meth:`recv_packet` uses
                              the same lock instance.
             max_recv_size: Read buffer size. If not given, a default reasonable value is used.
+            retry_interval: The maximum wait time to wait for a blocking operation before retrying.
+                            Set it to :data:`math.inf` to disable this feature.
+            manual_buffer_allocation: Select whether or not to enable the manual buffer allocation system:
+
+                                      * ``"try"``: (the default) will use the buffer API if the protocol supports it,
+                                        and fall back to the default implementation otherwise.
+
+                                      * ``"no"``: does not use the buffer API, even if the protocol object supports it.
+
+                                      * ``"force"``: requires the buffer API. Raises :exc:`.UnsupportedOperation` if it fails.
 
         See Also:
             :ref:`SSL/TLS security considerations <ssl-security>`
         """
         super().__init__()
 
-        if not isinstance(protocol, StreamProtocol):
-            raise TypeError(f"Expected a StreamProtocol object, got {protocol!r}")
-
-        backend = current_async_backend()
         if max_recv_size is None:
             max_recv_size = constants.DEFAULT_STREAM_BUFSIZE
-        if not isinstance(max_recv_size, int) or max_recv_size <= 0:
-            raise ValueError("'max_recv_size' must be a strictly positive integer")
 
-        self.__endpoint: AsyncStreamEndpoint[_SentPacketT, _ReceivedPacketT] | None = None
-        self.__socket_proxy: SocketProxy | None = None
-        self.__protocol: StreamProtocol[_SentPacketT, _ReceivedPacketT] = protocol
-
-        if ssl:
-            if _ssl_module is None:
-                raise RuntimeError("stdlib ssl module not available")
-            if isinstance(ssl, bool):
-                ssl = _ssl_module.create_default_context()
-                assert isinstance(ssl, _ssl_module.SSLContext)  # nosec assert_used
-                if server_hostname is not None and not server_hostname:
-                    ssl.check_hostname = False
-        else:
-            if server_hostname is not None:
-                raise ValueError("server_hostname is only meaningful with ssl")
+        if server_hostname is not None and not ssl:
+            raise ValueError("server_hostname is only meaningful with ssl")
+
+        if ssl_handshake_timeout is not None and not ssl:
+            raise ValueError("ssl_handshake_timeout is only meaningful with ssl")
 
-            if ssl_handshake_timeout is not None:
-                raise ValueError("ssl_handshake_timeout is only meaningful with ssl")
+        if ssl_shutdown_timeout is not None and not ssl:
+            raise ValueError("ssl_shutdown_timeout is only meaningful with ssl")
 
-            if ssl_shutdown_timeout is not None:
-                raise ValueError("ssl_shutdown_timeout is only meaningful with ssl")
+        if ssl_standard_compatible is not None and not ssl:
+            raise ValueError("ssl_standard_compatible is only meaningful with ssl")
 
-            if ssl_shared_lock is not None:
-                raise ValueError("ssl_shared_lock is only meaningful with ssl")
+        if ssl_shared_lock is not None and not ssl:
+            raise ValueError("ssl_shared_lock is only meaningful with ssl")
 
         if ssl_shared_lock is None:
             ssl_shared_lock = True
 
-        def _value_or_default(value: float | None, default: float) -> float:
-            return value if value is not None else default
+        if ssl_standard_compatible is None:
+            ssl_standard_compatible = True
 
-        socket_factory: Callable[[], Awaitable[AsyncStreamTransport]]
+        socket: _socket.socket
         match __arg:
-            case _socket.socket() as socket:
-                _utils.check_socket_no_ssl(socket)
-                _utils.check_socket_family(socket.family)
-                _utils.check_socket_is_connected(socket)
-                if ssl:
-                    if server_hostname is None:
+            case _socket.socket() if server_hostname is None and ssl:
+                raise ValueError("You must set server_hostname when using ssl without a host")
+            case _socket.socket() as socket if not kwargs:
+                pass
+            case (str(host), int(port)):
+                if server_hostname is None and ssl:
+                    # Use host as default for server_hostname.  It is an error
+                    # if host is empty or not set, e.g. when an
+                    # already-connected socket was passed or when only a port
+                    # is given.  To avoid this error, you can pass
+                    # server_hostname='' -- this will bypass the hostname
+                    # check.  (This also means that if host is a numeric
+                    # IP/IPv6 address, we will attempt to verify that exact
+                    # address; this will probably fail, but it is possible to
+                    # create a certificate for a specific IP address, so we
+                    # don't judge it here.)
+                    if not host:
                         raise ValueError("You must set server_hostname when using ssl without a host")
-                    socket_factory = _utils.make_callback(
-                        backend.wrap_ssl_over_stream_socket_client_side,
+                    server_hostname = host
+                _utils.replace_kwargs(kwargs, {"local_address": "source_address", "connect_timeout": "timeout"})
+                kwargs.setdefault("timeout", None)
+                socket = _socket.create_connection((host, port), **kwargs, all_errors=True)
+            case _:
+                raise TypeError("Invalid arguments")
+
+        try:
+            _utils.check_socket_family(socket.family)
+            _utils.check_socket_is_connected(socket)
+
+            transport: SocketStreamTransport | SSLStreamTransport
+
+            if ssl:
+                if _ssl_module is None:
+                    raise RuntimeError("stdlib ssl module not available")
+
+                if isinstance(ssl, bool):
+                    ssl = _ssl_module.create_default_context()
+                    assert isinstance(ssl, _ssl_module.SSLContext)  # nosec assert_used
+                    if not server_hostname:
+                        ssl.check_hostname = False
+                    ssl.options &= ~_ssl_module.OP_IGNORE_UNEXPECTED_EOF
+                if not server_hostname:
+                    server_hostname = None
+
+                with self.__convert_socket_error():
+                    transport = SSLStreamTransport(
                         socket,
                         ssl_context=ssl,
+                        retry_interval=retry_interval,
                         server_hostname=server_hostname,
-                        ssl_handshake_timeout=_value_or_default(ssl_handshake_timeout, constants.SSL_HANDSHAKE_TIMEOUT),
-                        ssl_shutdown_timeout=_value_or_default(ssl_shutdown_timeout, constants.SSL_SHUTDOWN_TIMEOUT),
-                        **kwargs,
-                    )
-                else:
-                    socket_factory = _utils.make_callback(backend.wrap_stream_socket, socket, **kwargs)
-            case (str(host), int(port)):
-                if ssl:
-                    socket_factory = _utils.make_callback(
-                        backend.create_ssl_over_tcp_connection,
-                        host,
-                        port,
-                        ssl_context=ssl,
-                        server_hostname=server_hostname,
-                        ssl_handshake_timeout=_value_or_default(ssl_handshake_timeout, constants.SSL_HANDSHAKE_TIMEOUT),
-                        ssl_shutdown_timeout=_value_or_default(ssl_shutdown_timeout, constants.SSL_SHUTDOWN_TIMEOUT),
-                        **kwargs,
+                        server_side=False,
+                        standard_compatible=ssl_standard_compatible,
+                        handshake_timeout=ssl_handshake_timeout,
+                        shutdown_timeout=ssl_shutdown_timeout,
                     )
-                else:
-                    socket_factory = _utils.make_callback(backend.create_tcp_connection, host, port, **kwargs)
-            case _:  # pragma: no cover
-                raise TypeError("Invalid arguments")
 
-        self.__socket_connector: _SocketConnector | None = _SocketConnector(
-            lock=backend.create_lock(),
-            factory=_utils.make_callback(self.__create_socket, socket_factory),
-            scope=backend.open_cancel_scope(),
-        )
+            else:
+                transport = SocketStreamTransport(socket, retry_interval=retry_interval)
+        except BaseException:
+            socket.close()
+            raise
+        finally:
+            del socket
 
         assert ssl_shared_lock is not None  # nosec assert_used
 
-        self.__receive_lock: ILock
-        self.__send_lock: ILock
         if ssl and ssl_shared_lock:
-            self.__send_lock = self.__receive_lock = backend.create_lock()
+            self.__send_lock = self.__receive_lock = _lock.ForkSafeLock(threading.Lock)
         else:
-            self.__receive_lock = backend.create_lock()
-            self.__send_lock = backend.create_lock()
-        self.__expected_recv_size: int = max_recv_size
+            self.__send_lock = _lock.ForkSafeLock(threading.Lock)
+            self.__receive_lock = _lock.ForkSafeLock(threading.Lock)
+
+        try:
+            self.__endpoint = StreamEndpoint(
+                transport,
+                protocol,
+                max_recv_size=max_recv_size,
+                manual_buffer_allocation=manual_buffer_allocation,
+                manual_buffer_allocation_warning_stacklevel=3,
+            )
+            self.__socket_proxy = SocketProxy(transport.extra(INETSocketAttribute.socket), lock=self.__send_lock.get)
+
+            with contextlib.suppress(OSError):
+                set_tcp_nodelay(self.socket, True)
+            with contextlib.suppress(OSError):
+                set_tcp_keepalive(self.socket, True)
+        except BaseException:
+            transport.close()
+            raise
 
     def __repr__(self) -> str:
         try:
-            endpoint = self.__endpoint
-            if endpoint is None:
-                raise AttributeError
+            return f"<{type(self).__name__} endpoint={self.__endpoint!r}>"
         except AttributeError:
             return f"<{type(self).__name__} (partially initialized)>"
-        return f"<{type(self).__name__} endpoint={endpoint!r}>"
-
-    def is_connected(self) -> bool:
-        """
-        Checks if the client initialization is finished.
 
-        See Also:
-            :meth:`wait_connected` method.
-
-        Returns:
-            the client connection state.
+    def is_closed(self) -> bool:
         """
-        return self.__endpoint is not None
-
-    async def wait_connected(self) -> None:
-        """
-        Finishes initializing the client, doing the asynchronous operations that could not be done in the constructor.
-        Does not require task synchronization.
-
-        It is not needed to call it directly if the client is used as an :term:`asynchronous context manager`::
-
-            async with client:  # wait_connected() has been called.
-                ...
-
-        Can be safely called multiple times.
-
-        Warning:
-            Due to limitations of the underlying operating system APIs,
-            it is not always possible to properly cancel a connection attempt once it has begun.
-
-            If :meth:`wait_connected` is cancelled, and is unable to abort the connection attempt, then it will forcibly
-            close the socket to prevent accidental re-use.
-
-        Raises:
-            ClientClosedError: the client object is closed.
-            ConnectionError: could not connect to remote.
-            OSError: unrelated OS error occurred. You should check :attr:`OSError.errno`.
-        """
-        await self.__ensure_connected()
-
-    @staticmethod
-    async def __create_socket(
-        socket_factory: Callable[[], Awaitable[AsyncStreamTransport]],
-    ) -> tuple[AsyncStreamTransport, SocketProxy]:
-        transport = await socket_factory()
-
-        socket_proxy = SocketProxy(transport.extra(INETSocketAttribute.socket))
-
-        _utils.check_socket_family(socket_proxy.family)
-
-        with contextlib.suppress(OSError):
-            set_tcp_nodelay(socket_proxy, True)
-        with contextlib.suppress(OSError):
-            set_tcp_keepalive(socket_proxy, True)
-        return transport, socket_proxy
-
-    def is_closing(self) -> bool:
-        """
-        Checks if the client is closed or in the process of being closed.
+        Checks if the client is in a closed state. Thread-safe.
 
         If :data:`True`, all future operations on the client object will raise a :exc:`.ClientClosedError`.
 
-        See Also:
-            :meth:`aclose` method.
-
         Returns:
             the client state.
         """
-        if self.__socket_connector is not None:
-            return False
-        endpoint = self.__endpoint
-        return endpoint is None or endpoint.is_closing()
+        with self.__send_lock.get():
+            return self.__endpoint.is_closed()
 
-    async def aclose(self) -> None:
+    def close(self) -> None:
         """
-        Close the client. Does not require task synchronization.
+        Close the client. Thread-safe.
 
         Once that happens, all future operations on the client object will raise a :exc:`.ClientClosedError`.
         The remote end will receive no more data (after queued data is flushed).
 
-        Warning:
-            :meth:`aclose` performs a graceful close, waiting for the connection to close.
-
-            If :meth:`aclose` is cancelled, the client is closed abruptly.
-
         Can be safely called multiple times.
         """
-        if self.__socket_connector is not None:
-            self.__socket_connector.scope.cancel()
-            self.__socket_connector = None
-        async with self.__send_lock:
-            if self.__endpoint is None:
-                return
-            await self.__endpoint.aclose()
+        with self.__send_lock.get():
+            self.__endpoint.close()
 
-    async def send_packet(self, packet: _SentPacketT) -> None:
+    def send_packet(self, packet: _T_SentPacket, *, timeout: float | None = None) -> None:
         """
-        Sends `packet` to the remote endpoint. Does not require task synchronization.
+        Sends `packet` to the remote endpoint. Thread-safe.
+
+        If `timeout` is not :data:`None`, the entire send operation will take at most `timeout` seconds.
 
         Warning:
-            In the case of a cancellation, it is impossible to know if all the packet data has been sent.
+            A timeout on a send operation is unusual unless you have a SSL/TLS context.
+
+            In the case of a timeout, it is impossible to know if all the packet data has been sent.
             This would leave the connection in an inconsistent state.
 
+        Important:
+            The lock acquisition time is included in the `timeout`.
+
+            This means that you may get a :exc:`TimeoutError` because it took too long to get the lock.
+
         Parameters:
             packet: the Python object to send.
+            timeout: the allowed time (in seconds) for blocking operations.
 
         Raises:
             ClientClosedError: the client object is closed.
             ConnectionError: connection unexpectedly closed during operation.
                              You should not attempt any further operation and close the client object.
+            TimeoutError: the send operation does not end up after `timeout` seconds.
             OSError: unrelated OS error occurred. You should check :attr:`OSError.errno`.
             RuntimeError: :meth:`send_eof` has been called earlier.
         """
-        async with self.__send_lock:
-            endpoint = await self.__ensure_connected()
+        with _utils.lock_with_timeout(self.__send_lock.get(), timeout) as timeout:
+            endpoint = self.__endpoint
+            if endpoint.is_closed():
+                raise ClientClosedError("Closed client")
             with self.__convert_socket_error():
-                await endpoint.send_packet(packet)
+                endpoint.send_packet(packet, timeout=timeout)
                 _utils.check_real_socket_state(endpoint.extra(INETSocketAttribute.socket))
 
-    async def send_eof(self) -> None:
+    def send_eof(self) -> None:
         """
-        Close the write end of the stream after the buffered write data is flushed. Does not require task synchronization.
+        Close the write end of the stream after the buffered write data is flushed. Thread-safe.
+
+        This method does nothing if the client is closed.
 
         Can be safely called multiple times.
 
         Raises:
             OSError: unrelated OS error occurred. You should check :attr:`OSError.errno`.
         """
-        async with self.__send_lock:
-            try:
-                endpoint = await self.__ensure_connected()
-            except ClientClosedError:
-                return
-            await endpoint.send_eof()
+        with self.__send_lock.get():
+            endpoint = self.__endpoint
+            endpoint.send_eof()
 
-    async def recv_packet(self) -> _ReceivedPacketT:
+    def recv_packet(self, *, timeout: float | None = None) -> _T_ReceivedPacket:
         """
-        Waits for a new packet to arrive from the remote endpoint. Does not require task synchronization.
+        Waits for a new packet to arrive from the remote endpoint. Thread-safe.
+
+        If `timeout` is not :data:`None`, the entire receive operation will take at most `timeout` seconds.
+
+        Important:
+            The lock acquisition time is included in the `timeout`.
+
+            This means that you may get a :exc:`TimeoutError` because it took too long to get the lock.
+
+        Parameters:
+            timeout: the allowed time (in seconds) for blocking operations.
 
         Raises:
             ClientClosedError: the client object is closed.
             ConnectionError: connection unexpectedly closed during operation.
                              You should not attempt any further operation and close the client object.
+            TimeoutError: the receive operation does not end up after `timeout` seconds.
             OSError: unrelated OS error occurred. You should check :attr:`OSError.errno`.
             StreamProtocolParseError: invalid data received.
 
         Returns:
             the received packet.
         """
-        async with self.__receive_lock:
-            endpoint = await self.__ensure_connected()
+        with _utils.lock_with_timeout(self.__receive_lock.get(), timeout) as timeout:
+            endpoint = self.__endpoint
+            if endpoint.is_closed():
+                raise ClientClosedError("Closed client")
             try:
                 with self.__convert_socket_error():
-                    return await endpoint.recv_packet()
+                    return endpoint.recv_packet(timeout=timeout)
             except EOFError:
                 raise self.__abort() from None
 
+    @contextlib.contextmanager
+    def __convert_socket_error(self) -> Iterator[None]:
+        try:
+            yield
+        except ConnectionError as exc:
+            raise self.__abort() from exc
+        except _ssl_module.SSLError if _ssl_module else () as exc:
+            if _utils.is_ssl_eof_error(exc):
+                raise self.__abort() from exc
+            raise
+        except OSError as exc:
+            if exc.errno in constants.CLOSED_SOCKET_ERRNOS:
+                raise ClientClosedError("Closed client") from exc
+            raise
+
+    @staticmethod
+    def __abort() -> OSError:
+        return _utils.error_from_errno(_errno.ECONNABORTED)
+
     def get_local_address(self) -> SocketAddress:
         """
-        Returns the local socket IP address.
-
-        If :meth:`wait_connected` was not called, an :exc:`OSError` may occurr.
+        Returns the local socket IP address. Thread-safe.
 
         Raises:
             ClientClosedError: the client object is closed.
             OSError: unrelated OS error occurred. You should check :attr:`OSError.errno`.
 
         Returns:
             the client's local address.
         """
-        endpoint = self.__get_endpoint_sync()
-        local_address = endpoint.extra(INETSocketAttribute.sockname)
-        address_family = endpoint.extra(INETSocketAttribute.family)
-        return new_socket_address(local_address, address_family)
+        with self.__send_lock.get():
+            endpoint = self.__endpoint
+            if endpoint.is_closed():
+                raise ClientClosedError("Closed client")
+            local_address = endpoint.extra(INETSocketAttribute.sockname)
+            address_family = endpoint.extra(INETSocketAttribute.family)
+            return new_socket_address(local_address, address_family)
 
     def get_remote_address(self) -> SocketAddress:
         """
-        Returns the remote socket IP address.
-
-        If :meth:`wait_connected` was not called, an :exc:`OSError` may occurr.
+        Returns the remote socket IP address. Thread-safe.
 
         Raises:
             ClientClosedError: the client object is closed.
             OSError: unrelated OS error occurred. You should check :attr:`OSError.errno`.
 
         Returns:
             the client's remote address.
         """
-        endpoint = self.__get_endpoint_sync()
-        remote_address = endpoint.extra(INETSocketAttribute.peername)
-        address_family = endpoint.extra(INETSocketAttribute.family)
-        return new_socket_address(remote_address, address_family)
-
-    async def __ensure_connected(self) -> AsyncStreamEndpoint[_SentPacketT, _ReceivedPacketT]:
-        if self.__endpoint is None:
-            endpoint_and_proxy = None
-            if (socket_connector := self.__socket_connector) is not None:
-                endpoint_and_proxy = await socket_connector.get()
-            self.__socket_connector = None
-            if endpoint_and_proxy is None:
-                raise self.__closed()
-            if self.__endpoint is None:
-                transport, self.__socket_proxy = endpoint_and_proxy
-                self.__endpoint = AsyncStreamEndpoint(transport, self.__protocol, max_recv_size=self.__expected_recv_size)
-
-        if self.__endpoint.is_closing():
-            raise self.__closed()
-        return self.__endpoint
-
-    def __get_endpoint_sync(self) -> AsyncStreamEndpoint[_SentPacketT, _ReceivedPacketT]:
-        if self.__endpoint is None:
-            if self.__socket_connector is not None:
-                raise _utils.error_from_errno(_errno.ENOTSOCK)
-            else:
-                raise self.__closed()
-        if self.__endpoint.is_closing():
-            raise self.__closed()
-        return self.__endpoint
-
-    @contextlib.contextmanager
-    def __convert_socket_error(self) -> Iterator[None]:
-        try:
-            yield
-        except ConnectionError as exc:
-            raise self.__abort() from exc
-        except OSError as exc:
-            if exc.errno in constants.CLOSED_SOCKET_ERRNOS:
-                raise self.__closed()
-            raise
+        with self.__send_lock.get():
+            endpoint = self.__endpoint
+            if endpoint.is_closed():
+                raise ClientClosedError("Closed client")
+            remote_address = endpoint.extra(INETSocketAttribute.peername)
+            address_family = endpoint.extra(INETSocketAttribute.family)
+            return new_socket_address(remote_address, address_family)
 
-    @staticmethod
-    def __abort() -> OSError:
-        return _utils.error_from_errno(_errno.ECONNABORTED)
+    def fileno(self) -> int:
+        """
+        Returns the socket's file descriptor, or ``-1`` if the client (or the socket) is closed. Thread-safe.
 
-    @staticmethod
-    def __closed() -> ClientClosedError:
-        return ClientClosedError("Client is closing, or is already closed")
+        Returns:
+            the opened file descriptor.
+        """
+        return self.socket.fileno()
 
     @property
     @final
     def socket(self) -> SocketProxy:
-        """A view to the underlying socket instance. Read-only attribute.
-
-        May raise :exc:`AttributeError` if :meth:`wait_connected` was not called.
-        """
-        if self.__socket_proxy is None:
-            raise AttributeError("Socket not connected")
+        """A view to the underlying socket instance. Read-only attribute."""
         return self.__socket_proxy
 
     @property
     @final
     def max_recv_size(self) -> int:
         """Read buffer size. Read-only attribute."""
-        endpoint = self.__endpoint
-        if endpoint is None:
-            return self.__expected_recv_size
-        return endpoint.max_recv_size
+        return self.__endpoint.max_recv_size
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/api_async/client/udp.py` & `easynetwork-1.0.0rc8/src/easynetwork/clients/async_udp.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -21,82 +21,77 @@
 import contextlib
 import dataclasses as _dataclasses
 import errno as _errno
 import socket as _socket
 from collections.abc import Awaitable, Callable, Iterator
 from typing import Any, final, overload
 
-from ..._typevars import _ReceivedPacketT, _SentPacketT
-from ...exceptions import ClientClosedError
-from ...lowlevel import _utils, constants
-from ...lowlevel.api_async.backend.abc import CancelScope, ILock
-from ...lowlevel.api_async.backend.factory import current_async_backend
-from ...lowlevel.api_async.endpoints.datagram import AsyncDatagramEndpoint
-from ...lowlevel.api_async.transports.abc import AsyncDatagramTransport
-from ...lowlevel.socket import INETSocketAttribute, SocketAddress, SocketProxy, new_socket_address
-from ...protocol import DatagramProtocol
+from .._typevars import _T_ReceivedPacket, _T_SentPacket
+from ..exceptions import ClientClosedError
+from ..lowlevel import _utils, constants
+from ..lowlevel.api_async.backend.abc import CancelScope, ILock
+from ..lowlevel.api_async.backend.factory import current_async_backend
+from ..lowlevel.api_async.endpoints.datagram import AsyncDatagramEndpoint
+from ..lowlevel.api_async.transports.abc import AsyncDatagramTransport
+from ..lowlevel.socket import INETSocketAttribute, SocketAddress, SocketProxy, new_socket_address
+from ..protocol import DatagramProtocol
 from .abc import AbstractAsyncNetworkClient
 
 
 @_dataclasses.dataclass(kw_only=True, slots=True)
 class _SocketConnector:
-    lock: ILock
-    factory: Callable[[], Awaitable[tuple[AsyncDatagramTransport, SocketProxy]]] | None
+    factory: Callable[[], Awaitable[tuple[AsyncDatagramTransport, SocketProxy]]]
     scope: CancelScope
-    _result: tuple[AsyncDatagramTransport, SocketProxy] | None = _dataclasses.field(init=False, default=None)
 
     async def get(self) -> tuple[AsyncDatagramTransport, SocketProxy] | None:
-        async with self.lock:
-            factory, self.factory = self.factory, None
-            if factory is not None:
-                with self.scope:
-                    self._result = await factory()
-        return self._result
+        result: tuple[AsyncDatagramTransport, SocketProxy] | None = None
+        with self.scope:
+            result = await self.factory()
+        return result
 
 
-class AsyncUDPNetworkClient(AbstractAsyncNetworkClient[_SentPacketT, _ReceivedPacketT]):
+class AsyncUDPNetworkClient(AbstractAsyncNetworkClient[_T_SentPacket, _T_ReceivedPacket]):
     """
     An asynchronous network client interface for UDP communication.
     """
 
     __slots__ = (
         "__endpoint",
         "__socket_proxy",
         "__socket_connector",
+        "__socket_connector_lock",
         "__receive_lock",
         "__send_lock",
         "__protocol",
     )
 
     @overload
     def __init__(
         self,
         address: tuple[str, int],
         /,
-        protocol: DatagramProtocol[_SentPacketT, _ReceivedPacketT],
+        protocol: DatagramProtocol[_T_SentPacket, _T_ReceivedPacket],
         *,
         local_address: tuple[str, int] | None = ...,
         family: int = ...,
-    ) -> None:
-        ...
+    ) -> None: ...
 
     @overload
     def __init__(
         self,
         socket: _socket.socket,
         /,
-        protocol: DatagramProtocol[_SentPacketT, _ReceivedPacketT],
-    ) -> None:
-        ...
+        protocol: DatagramProtocol[_T_SentPacket, _T_ReceivedPacket],
+    ) -> None: ...
 
     def __init__(
         self,
         __arg: tuple[str, int] | _socket.socket,
         /,
-        protocol: DatagramProtocol[_SentPacketT, _ReceivedPacketT],
+        protocol: DatagramProtocol[_T_SentPacket, _T_ReceivedPacket],
         **kwargs: Any,
     ) -> None:
         """
         Common Parameters:
             protocol: The :term:`protocol object` to use.
 
         Connection Parameters:
@@ -114,16 +109,16 @@
         super().__init__()
 
         backend = current_async_backend()
 
         if not isinstance(protocol, DatagramProtocol):
             raise TypeError(f"Expected a DatagramProtocol object, got {protocol!r}")
 
-        self.__protocol: DatagramProtocol[_SentPacketT, _ReceivedPacketT] = protocol
-        self.__endpoint: AsyncDatagramEndpoint[_SentPacketT, _ReceivedPacketT] | None = None
+        self.__protocol: DatagramProtocol[_T_SentPacket, _T_ReceivedPacket] = protocol
+        self.__endpoint: AsyncDatagramEndpoint[_T_SentPacket, _T_ReceivedPacket] | None = None
         self.__socket_proxy: SocketProxy | None = None
 
         socket_factory: Callable[[], Awaitable[AsyncDatagramTransport]]
         match __arg:
             case _socket.socket() as socket:
                 _utils.check_socket_no_ssl(socket)
                 _utils.check_socket_family(socket.family)
@@ -132,25 +127,37 @@
                 socket_factory = _utils.make_callback(backend.wrap_connected_datagram_socket, socket, **kwargs)
             case (str(host), int(port)):
                 if kwargs.get("local_address") is None:
                     kwargs["local_address"] = ("localhost", 0)
                 if (family := kwargs.get("family", _socket.AF_UNSPEC)) != _socket.AF_UNSPEC:
                     _utils.check_socket_family(family)
                 socket_factory = _utils.make_callback(backend.create_udp_endpoint, host, port, **kwargs)
-            case _:  # pragma: no cover
+            case _:
                 raise TypeError("Invalid arguments")
 
         self.__socket_connector: _SocketConnector | None = _SocketConnector(
-            lock=backend.create_lock(),
             factory=_utils.make_callback(self.__create_socket, socket_factory),
             scope=backend.open_cancel_scope(),
         )
+        self.__socket_connector_lock: ILock = backend.create_lock()
         self.__receive_lock: ILock = backend.create_lock()
         self.__send_lock: ILock = backend.create_lock()
 
+    @staticmethod
+    async def __create_socket(
+        socket_factory: Callable[[], Awaitable[AsyncDatagramTransport]],
+    ) -> tuple[AsyncDatagramTransport, SocketProxy]:
+        transport = await socket_factory()
+        socket_proxy = SocketProxy(transport.extra(INETSocketAttribute.socket))
+
+        local_address: SocketAddress = new_socket_address(transport.extra(INETSocketAttribute.sockname), socket_proxy.family)
+        if local_address.port == 0:
+            raise AssertionError(f"{transport} is not bound to a local address")
+        return transport, socket_proxy
+
     def __repr__(self) -> str:
         try:
             endpoint = self.__endpoint
             if endpoint is None:
                 raise AttributeError
         except AttributeError:
             return f"<{type(self).__name__} (partially initialized)>"
@@ -182,26 +189,14 @@
 
         Raises:
             ClientClosedError: the client object is closed.
             OSError: unrelated OS error occurred. You should check :attr:`OSError.errno`.
         """
         await self.__ensure_connected()
 
-    @staticmethod
-    async def __create_socket(
-        socket_factory: Callable[[], Awaitable[AsyncDatagramTransport]],
-    ) -> tuple[AsyncDatagramTransport, SocketProxy]:
-        transport = await socket_factory()
-        socket_proxy = SocketProxy(transport.extra(INETSocketAttribute.socket))
-
-        local_address: SocketAddress = new_socket_address(transport.extra(INETSocketAttribute.sockname), socket_proxy.family)
-        if local_address.port == 0:
-            raise AssertionError(f"{transport} is not bound to a local address")
-        return transport, socket_proxy
-
     def is_closing(self) -> bool:
         """
         Checks if the client is closed or in the process of being closed.
 
         If :data:`True`, all future operations on the client object will raise a :exc:`.ClientClosedError`.
 
         See Also:
@@ -233,15 +228,15 @@
             self.__socket_connector.scope.cancel()
             self.__socket_connector = None
         async with self.__send_lock:
             if self.__endpoint is None:
                 return
             await self.__endpoint.aclose()
 
-    async def send_packet(self, packet: _SentPacketT) -> None:
+    async def send_packet(self, packet: _T_SentPacket) -> None:
         """
         Sends `packet` to the remote endpoint. Does not require task synchronization.
 
         Warning:
             In the case of a cancellation, it is impossible to know if all the packet data has been sent.
 
         Parameters:
@@ -253,15 +248,15 @@
         """
         async with self.__send_lock:
             endpoint = await self.__ensure_connected()
             with self.__convert_socket_error():
                 await endpoint.send_packet(packet)
                 _utils.check_real_socket_state(endpoint.extra(INETSocketAttribute.socket))
 
-    async def recv_packet(self) -> _ReceivedPacketT:
+    async def recv_packet(self) -> _T_ReceivedPacket:
         """
         Waits for a new packet to arrive from the remote endpoint. Does not require task synchronization.
 
         Calls :meth:`wait_connected`.
 
         Raises:
             ClientClosedError: the client object is closed.
@@ -304,31 +299,34 @@
             the client's remote address.
         """
         endpoint = self.__get_endpoint_sync()
         remote_address = endpoint.extra(INETSocketAttribute.peername)
         address_family = endpoint.extra(INETSocketAttribute.family)
         return new_socket_address(remote_address, address_family)
 
-    async def __ensure_connected(self) -> AsyncDatagramEndpoint[_SentPacketT, _ReceivedPacketT]:
-        if self.__endpoint is None:
-            endpoint_and_proxy = None
-            if (socket_connector := self.__socket_connector) is not None:
-                endpoint_and_proxy = await socket_connector.get()
-            self.__socket_connector = None
-            if endpoint_and_proxy is None:
-                raise self.__closed()
+    async def __ensure_connected(self) -> AsyncDatagramEndpoint[_T_SentPacket, _T_ReceivedPacket]:
+        async with self.__socket_connector_lock:
             if self.__endpoint is None:
+                endpoint_and_proxy = None
+                if (socket_connector := self.__socket_connector) is not None:
+                    endpoint_and_proxy = await socket_connector.get()
+                self.__socket_connector = None
+                if endpoint_and_proxy is None:
+                    raise self.__closed()
                 transport, self.__socket_proxy = endpoint_and_proxy
                 self.__endpoint = AsyncDatagramEndpoint(transport, self.__protocol)
 
+            # If you want coverage.py to work properly, keep this "pass" :)
+            pass
+
         if self.__endpoint.is_closing():
             raise self.__closed()
         return self.__endpoint
 
-    def __get_endpoint_sync(self) -> AsyncDatagramEndpoint[_SentPacketT, _ReceivedPacketT]:
+    def __get_endpoint_sync(self) -> AsyncDatagramEndpoint[_T_SentPacket, _T_ReceivedPacket]:
         if self.__endpoint is None:
             if self.__socket_connector is not None:
                 raise _utils.error_from_errno(_errno.ENOTSOCK)
             else:
                 raise self.__closed()
         if self.__endpoint.is_closing():
             raise self.__closed()
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/api_async/server/__init__.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_sync/endpoints/__init__.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,35 +1,19 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 #
-"""Asynchronous network server module"""
+"""Low-level endpoints module"""
 
 from __future__ import annotations
 
-__all__ = [
-    "AbstractAsyncNetworkServer",
-    "AsyncBaseClientInterface",
-    "AsyncDatagramClient",
-    "AsyncDatagramRequestHandler",
-    "AsyncStreamClient",
-    "AsyncStreamRequestHandler",
-    "AsyncTCPNetworkServer",
-    "AsyncUDPNetworkServer",
-    "INETClientAttribute",
-    "SupportsEventSet",
-]
-
-from .abc import *
-from .handler import *
-from .tcp import *
-from .udp import *
+__all__ = []  # type: list[str]
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/api_async/server/handler.py` & `easynetwork-1.0.0rc8/src/easynetwork/servers/handlers.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -24,44 +24,40 @@
     "AsyncStreamRequestHandler",
     "INETClientAttribute",
 ]
 
 import contextlib
 from abc import ABCMeta, abstractmethod
 from collections.abc import AsyncGenerator, Coroutine
-from typing import TYPE_CHECKING, Any, Generic
+from typing import Any, Generic
 
-from ..._typevars import _RequestT, _ResponseT
-from ...lowlevel import socket as socket_tools, typed_attr
-
-if TYPE_CHECKING:
-    from ..server.tcp import AsyncTCPNetworkServer
-    from ..server.udp import AsyncUDPNetworkServer
+from .._typevars import _T_Request, _T_Response
+from ..lowlevel import socket as socket_tools, typed_attr
 
 
 class INETClientAttribute(typed_attr.TypedAttributeSet):
     socket: socket_tools.ISocket = socket_tools.SocketAttribute.socket
     """:class:`socket.socket` instance."""
 
     local_address: socket_tools.SocketAddress = socket_tools.SocketAttribute.sockname
     """the socket's own address, result of :meth:`socket.socket.getsockname`."""
 
     remote_address: socket_tools.SocketAddress = socket_tools.SocketAttribute.peername
     """the remote address to which the socket is connected, result of :meth:`socket.socket.getpeername`."""
 
 
-class AsyncBaseClientInterface(typed_attr.TypedAttributeProvider, Generic[_ResponseT], metaclass=ABCMeta):
+class AsyncBaseClientInterface(typed_attr.TypedAttributeProvider, Generic[_T_Response], metaclass=ABCMeta):
     """
     The base class for a client interface, used by request handlers.
     """
 
     __slots__ = ("__weakref__",)
 
     @abstractmethod
-    async def send_packet(self, packet: _ResponseT, /) -> None:
+    async def send_packet(self, packet: _T_Response, /) -> None:
         """
         Sends `packet` to the remote endpoint. Does not require task synchronization.
 
         Warning:
             In the case of a cancellation, it is impossible to know if all the packet data has been sent.
             This would leave the connection in an inconsistent state.
 
@@ -85,15 +81,15 @@
 
         Returns:
             the client state.
         """
         raise NotImplementedError
 
 
-class AsyncStreamClient(AsyncBaseClientInterface[_ResponseT]):
+class AsyncStreamClient(AsyncBaseClientInterface[_T_Response]):
     """
     A client interface for stream oriented connection, used by stream request handlers.
     """
 
     __slots__ = ()
 
     @abstractmethod
@@ -110,54 +106,52 @@
             :meth:`aclose` performs a graceful close, waiting for the connection to close.
 
             If :meth:`aclose` is cancelled, the client is closed abruptly.
         """
         raise NotImplementedError
 
 
-class AsyncDatagramClient(AsyncBaseClientInterface[_ResponseT]):
+class AsyncDatagramClient(AsyncBaseClientInterface[_T_Response]):
     """
     A client interface for datagram oriented connection, used by datagram request handlers.
+
+    Unlike :class:`AsyncStreamClient`, the client object can be recreated on each ``handle()`` call,
+    but implements ``__hash__()`` and ``__eq__()`` for uniqueness checking, so it can be used in a :class:`set` for example.
     """
 
     __slots__ = ()
 
     @abstractmethod
     def __hash__(self) -> int:
         raise NotImplementedError
 
     @abstractmethod
     def __eq__(self, other: object, /) -> bool:
         raise NotImplementedError
 
 
-class AsyncStreamRequestHandler(Generic[_RequestT, _ResponseT], metaclass=ABCMeta):
+class AsyncStreamRequestHandler(Generic[_T_Request, _T_Response], metaclass=ABCMeta):
     """
     The base class for a stream request handler, used by TCP network servers.
     """
 
     __slots__ = ("__weakref__",)
 
-    async def service_init(
-        self,
-        exit_stack: contextlib.AsyncExitStack,
-        server: AsyncTCPNetworkServer[_RequestT, _ResponseT],
-        /,
-    ) -> None:
+    async def service_init(self, exit_stack: contextlib.AsyncExitStack, server: Any, /) -> None:
         """
-        Called at the server startup. The default implementation does nothing.
+        Called at server startup. The default implementation does nothing.
 
         Parameters:
             exit_stack: An :class:`~contextlib.AsyncExitStack` that can be used to add actions on server's tear down.
             server: A :func:`weakref.proxy` to the server instance.
         """
         pass
 
     @abstractmethod
-    def handle(self, client: AsyncStreamClient[_ResponseT], /) -> AsyncGenerator[None, _RequestT]:
+    def handle(self, client: AsyncStreamClient[_T_Response], /) -> AsyncGenerator[float | None, _T_Request]:
         """
         This function must do all the work required to service a request.
 
         It is an :term:`asynchronous generator` function::
 
             async def handle(self, client):
                 request = yield
@@ -176,22 +170,25 @@
 
         Note:
             There is one exception: if the generator returns before the first :keyword:`yield` statement,
             the connection is forcibly closed.
 
         Parameters:
             client: An interface to communicate with the remote endpoint.
+
+        Yields:
+            :data:`None` or a number interpreted as the timeout delay.
         """
         raise NotImplementedError
 
     def on_connection(
         self,
-        client: AsyncStreamClient[_ResponseT],
+        client: AsyncStreamClient[_T_Response],
         /,
-    ) -> Coroutine[Any, Any, None] | AsyncGenerator[None, _RequestT]:
+    ) -> Coroutine[Any, Any, None] | AsyncGenerator[float | None, _T_Request]:
         """
         Called once the client is connected to perform any initialization actions required.
         The default implementation does nothing.
 
         It can be either a :term:`coroutine function`::
 
             async def on_connection(self, client):
@@ -210,69 +207,66 @@
                 ...
 
         In the latter case, as for :meth:`handle`, :meth:`on_connection` can :keyword:`yield` whenever a request from
         the `client` is needed.
 
         Parameters:
             client: An interface to communicate with the remote endpoint.
+
+        Yields:
+            If it is an :term:`asynchronous generator`, :data:`None` or a number interpreted as the timeout delay.
         """
 
         async def _pass() -> None:
             pass
 
         return _pass()
 
-    async def on_disconnection(self, client: AsyncStreamClient[_ResponseT], /) -> None:
+    async def on_disconnection(self, client: AsyncStreamClient[_T_Response], /) -> None:
         """
         Called once the client is disconnected to perform any clean-up actions required. The default implementation does nothing.
 
         This function will not be called in the following conditions:
 
-        * If :meth:`on_connection` is a simple :term:`coroutine function` and raises an exception.
+        * If :meth:`on_connection` raises an exception.
 
-        * If :meth:`on_connection` is an :term:`asynchronous generator` function and raises an exception
-          before the first :keyword:`yield`.
+        * If :meth:`on_connection` is an :term:`asynchronous generator` function and the connection is closed.
 
         Important:
             :meth:`AsyncStreamClient.is_closing` should return :data:`True` when this function is called.
             However, if :meth:`handle` raises an exception, the client task is shut down and the connection is forcibly closed
             *after* :meth:`on_disconnection` is called.
 
             This behavior allows you to notify the client that something unusual has occurred.
 
         Parameters:
             client: An interface to communicate with the remote endpoint.
         """
         pass
 
 
-class AsyncDatagramRequestHandler(Generic[_RequestT, _ResponseT], metaclass=ABCMeta):
+class AsyncDatagramRequestHandler(Generic[_T_Request, _T_Response], metaclass=ABCMeta):
     """
     The base class for a datagram request handler, used by UDP network servers.
     """
 
     __slots__ = ("__weakref__",)
 
-    async def service_init(
-        self,
-        exit_stack: contextlib.AsyncExitStack,
-        server: AsyncUDPNetworkServer[_RequestT, _ResponseT],
-        /,
-    ) -> None:
+    async def service_init(self, exit_stack: contextlib.AsyncExitStack, server: Any, /) -> None:
         """
-        Called at the server startup. The default implementation does nothing.
+        Called at server startup. The default implementation does nothing.
 
         Parameters:
             exit_stack: An :class:`~contextlib.AsyncExitStack` that can be used to add actions on server's tear down.
             server: A :func:`weakref.proxy` to the server instance.
         """
         pass
 
     @abstractmethod
-    def handle(self, client: AsyncDatagramClient[_ResponseT], /) -> AsyncGenerator[None, _RequestT]:
+    def handle(self, client: AsyncDatagramClient[_T_Response], /) -> AsyncGenerator[float | None, _T_Request]:
         """
         This function must do all the work required to service a request.
 
         It is an :term:`asynchronous generator` function::
 
             async def handle(self, client):
                 request = yield
@@ -301,9 +295,12 @@
             If the generator returns before the first :keyword:`yield` statement, the received datagram is discarded.
 
             This is useful when a client that you do not expect to see sends something; the datagrams are parsed only when
             the generator hits a :keyword:`yield` statement.
 
         Parameters:
             client: An interface to communicate with the remote endpoint.
+
+        Yields:
+            :data:`None` or a number interpreted as the timeout delay.
         """
         raise NotImplementedError
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/api_async/server/tcp.py` & `easynetwork-1.0.0rc8/src/easynetwork/servers/async_udp.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -12,539 +12,404 @@
 # limitations under the License.
 #
 #
 """Asynchronous network server module"""
 
 from __future__ import annotations
 
-__all__ = ["AsyncTCPNetworkServer"]
+__all__ = ["AsyncUDPNetworkServer"]
 
 import contextlib
-import inspect
 import logging
+import types
 import weakref
 from collections import deque
-from collections.abc import AsyncGenerator, Callable, Coroutine, Iterator, Mapping, Sequence
+from collections.abc import AsyncGenerator, Callable, Coroutine, Mapping, Sequence
 from typing import TYPE_CHECKING, Any, Generic, NoReturn, final
 
-from ..._typevars import _RequestT, _ResponseT
-from ...exceptions import ClientClosedError, ServerAlreadyRunning, ServerClosedError
-from ...lowlevel import _asyncgen, _utils, constants
-from ...lowlevel._final import runtime_final_class
-from ...lowlevel.api_async.backend.factory import current_async_backend
-from ...lowlevel.api_async.servers import stream as _stream_server
-from ...lowlevel.socket import (
-    INETSocketAttribute,
-    ISocket,
-    SocketAddress,
-    SocketProxy,
-    enable_socket_linger,
-    new_socket_address,
-    set_tcp_keepalive,
-    set_tcp_nodelay,
-)
-from ...protocol import StreamProtocol
+from .._typevars import _T_Request, _T_Response
+from ..exceptions import ClientClosedError, ServerAlreadyRunning, ServerClosedError
+from ..lowlevel import _utils
+from ..lowlevel._asyncgen import AsyncGenAction, SendAction, ThrowAction
+from ..lowlevel._final import runtime_final_class
+from ..lowlevel.api_async.backend.factory import current_async_backend
+from ..lowlevel.api_async.servers import datagram as _datagram_server
+from ..lowlevel.api_async.transports.abc import AsyncDatagramListener
+from ..lowlevel.socket import INETSocketAttribute, SocketAddress, SocketProxy, new_socket_address
+from ..protocol import DatagramProtocol
 from .abc import AbstractAsyncNetworkServer, SupportsEventSet
-from .handler import AsyncStreamClient, AsyncStreamRequestHandler, INETClientAttribute
+from .handlers import AsyncDatagramClient, AsyncDatagramRequestHandler, INETClientAttribute
 
 if TYPE_CHECKING:
-    import ssl as _typing_ssl
+    from ..lowlevel.api_async.backend.abc import CancelScope, IEvent, Task, TaskGroup
 
-    from ...lowlevel.api_async.backend.abc import CancelScope, IEvent, Task, TaskGroup
-    from ...lowlevel.api_async.transports.abc import AsyncListener, AsyncStreamTransport
 
-
-class AsyncTCPNetworkServer(AbstractAsyncNetworkServer, Generic[_RequestT, _ResponseT]):
+class AsyncUDPNetworkServer(AbstractAsyncNetworkServer, Generic[_T_Request, _T_Response]):
     """
-    An asynchronous network server for TCP connections.
+    An asynchronous network server for UDP communication.
     """
 
     __slots__ = (
         "__servers",
         "__listeners_factory",
         "__listeners_factory_scope",
         "__protocol",
         "__request_handler",
         "__is_shutdown",
-        "__shutdown_asked",
-        "__max_recv_size",
         "__servers_tasks",
-        "__mainloop_task",
-        "__active_tasks",
-        "__client_connection_log_level",
+        "__server_run_scope",
         "__logger",
     )
 
     def __init__(
         self,
         host: str | None | Sequence[str],
         port: int,
-        protocol: StreamProtocol[_ResponseT, _RequestT],
-        request_handler: AsyncStreamRequestHandler[_RequestT, _ResponseT],
+        protocol: DatagramProtocol[_T_Response, _T_Request],
+        request_handler: AsyncDatagramRequestHandler[_T_Request, _T_Response],
         *,
-        ssl: _typing_ssl.SSLContext | None = None,
-        ssl_handshake_timeout: float | None = None,
-        ssl_shutdown_timeout: float | None = None,
-        backlog: int | None = None,
         reuse_port: bool = False,
-        max_recv_size: int | None = None,
-        log_client_connection: bool | None = None,
         logger: logging.Logger | None = None,
     ) -> None:
         """
         Parameters:
-            host: Can be set to several types which determine where the server would be listening:
-
-                  * If `host` is a string, the TCP server is bound to a single network interface specified by `host`.
-
-                  * If `host` is a sequence of strings, the TCP server is bound to all network interfaces specified by the sequence.
-
-                  * If `host` is :data:`None`, all interfaces are assumed and a list of multiple sockets will be returned
-                    (most likely one for IPv4 and another one for IPv6).
+            host: specify which network interface to which the server should bind.
             port: specify which port the server should listen on. If the value is ``0``, a random unused port will be selected
                   (note that if `host` resolves to multiple network interfaces, a different random port will be selected
                   for each interface).
             protocol: The :term:`protocol object` to use.
             request_handler: The request handler to use.
 
         Keyword Arguments:
-            ssl: can be set to an :class:`ssl.SSLContext` instance to enable TLS over the accepted connections.
-            ssl_handshake_timeout: (for a TLS connection) the time in seconds to wait for the TLS handshake to complete
-                                   before aborting the connection. ``60.0`` seconds if :data:`None` (default).
-            ssl_shutdown_timeout: the time in seconds to wait for the SSL shutdown to complete before aborting the connection.
-                                  ``30.0`` seconds if :data:`None` (default).
-            backlog: is the maximum number of queued connections passed to :class:`~socket.socket.listen` (defaults to ``100``).
             reuse_port: tells the kernel to allow this endpoint to be bound to the same port as other existing endpoints
                         are bound to, so long as they all set this flag when being created.
                         This option is not supported on Windows.
-            max_recv_size: Read buffer size. If not given, a default reasonable value is used.
-            log_client_connection: If :data:`True`, log clients connection/disconnection in :data:`logging.INFO` level.
-                                   (This log will always be available in :data:`logging.DEBUG` level.)
             logger: If given, the logger instance to use.
-
-        See Also:
-            :ref:`SSL/TLS security considerations <ssl-security>`
         """
         super().__init__()
 
-        if not isinstance(protocol, StreamProtocol):
-            raise TypeError(f"Expected a StreamProtocol object, got {protocol!r}")
-        if not isinstance(request_handler, AsyncStreamRequestHandler):
-            raise TypeError(f"Expected an AsyncStreamRequestHandler object, got {request_handler!r}")
+        if not isinstance(protocol, DatagramProtocol):
+            raise TypeError(f"Expected a DatagramProtocol object, got {protocol!r}")
+        if not isinstance(request_handler, AsyncDatagramRequestHandler):
+            raise TypeError(f"Expected an AsyncDatagramRequestHandler object, got {request_handler!r}")
 
         backend = current_async_backend()
 
-        if backlog is None:
-            backlog = 100
-
-        if log_client_connection is None:
-            log_client_connection = True
-
-        if max_recv_size is None:
-            max_recv_size = constants.DEFAULT_STREAM_BUFSIZE
-        if not isinstance(max_recv_size, int) or max_recv_size <= 0:
-            raise ValueError("'max_recv_size' must be a strictly positive integer")
-
-        if ssl_handshake_timeout is not None and not ssl:
-            raise ValueError("ssl_handshake_timeout is only meaningful with ssl")
-
-        if ssl_shutdown_timeout is not None and not ssl:
-            raise ValueError("ssl_shutdown_timeout is only meaningful with ssl")
-
-        def _value_or_default(value: float | None, default: float) -> float:
-            return value if value is not None else default
-
-        self.__listeners_factory: Callable[[], Coroutine[Any, Any, Sequence[AsyncListener[AsyncStreamTransport]]]] | None
-        if ssl:
-            self.__listeners_factory = _utils.make_callback(
-                backend.create_ssl_over_tcp_listeners,
-                host,
-                port,
-                backlog=backlog,
-                ssl_context=ssl,
-                ssl_handshake_timeout=_value_or_default(ssl_handshake_timeout, constants.SSL_HANDSHAKE_TIMEOUT),
-                ssl_shutdown_timeout=_value_or_default(ssl_shutdown_timeout, constants.SSL_SHUTDOWN_TIMEOUT),
-                reuse_port=reuse_port,
-            )
-        else:
-            self.__listeners_factory = _utils.make_callback(
-                backend.create_tcp_listeners,
-                host,
-                port,
-                backlog=backlog,
-                reuse_port=reuse_port,
-            )
+        self.__listeners_factory: Callable[[], Coroutine[Any, Any, Sequence[AsyncDatagramListener[tuple[Any, ...]]]]] | None
+        self.__listeners_factory = _utils.make_callback(
+            backend.create_udp_listeners,
+            host,
+            port,
+            reuse_port=reuse_port,
+        )
         self.__listeners_factory_scope: CancelScope | None = None
+        self.__server_run_scope: CancelScope | None = None
 
-        self.__servers: tuple[_stream_server.AsyncStreamServer[_RequestT, _ResponseT], ...] | None = None
-        self.__protocol: StreamProtocol[_ResponseT, _RequestT] = protocol
-        self.__request_handler: AsyncStreamRequestHandler[_RequestT, _ResponseT] = request_handler
+        self.__servers: tuple[_datagram_server.AsyncDatagramServer[_T_Request, _T_Response, tuple[Any, ...]], ...] | None
+        self.__servers = None
+        self.__protocol: DatagramProtocol[_T_Response, _T_Request] = protocol
+        self.__request_handler: AsyncDatagramRequestHandler[_T_Request, _T_Response] = request_handler
         self.__is_shutdown: IEvent = backend.create_event()
         self.__is_shutdown.set()
-        self.__shutdown_asked: bool = False
-        self.__max_recv_size: int = max_recv_size
         self.__servers_tasks: deque[Task[NoReturn]] = deque()
-        self.__mainloop_task: Task[NoReturn] | None = None
         self.__logger: logging.Logger = logger or logging.getLogger(__name__)
-        self.__client_connection_log_level: int
-        if log_client_connection:
-            self.__client_connection_log_level = logging.INFO
-        else:
-            self.__client_connection_log_level = logging.DEBUG
-        self.__active_tasks: int = 0
 
     @_utils.inherit_doc(AbstractAsyncNetworkServer)
     def is_serving(self) -> bool:
         return self.__servers is not None and all(not server.is_closing() for server in self.__servers)
 
-    def stop_listening(self) -> None:
-        """
-        Schedules the shutdown of all listener sockets.
-
-        After that, all new connections will be refused, but the server will continue to run and handle
-        previously accepted connections.
-
-        Further calls to :meth:`is_serving` will return :data:`False`.
-        """
-        with contextlib.ExitStack() as exit_stack:
-            for listener_task in self.__servers_tasks:
-                exit_stack.callback(listener_task.cancel)
-                del listener_task
-
     @_utils.inherit_doc(AbstractAsyncNetworkServer)
     async def server_close(self) -> None:
         if self.__listeners_factory_scope is not None:
             self.__listeners_factory_scope.cancel()
         self.__listeners_factory = None
         await self.__close_servers()
 
     async def __close_servers(self) -> None:
         async with contextlib.AsyncExitStack() as exit_stack:
+            server_close_group = await exit_stack.enter_async_context(current_async_backend().create_task_group())
+
             servers, self.__servers = self.__servers, None
             if servers is not None:
+                exit_stack.push_async_callback(current_async_backend().cancel_shielded_coro_yield)
                 for server in servers:
-                    exit_stack.push_async_callback(server.aclose)
+                    exit_stack.callback(server_close_group.start_soon, server.aclose)
                     del server
 
             for server_task in self.__servers_tasks:
                 server_task.cancel()
                 exit_stack.push_async_callback(server_task.wait)
                 del server_task
 
+            if self.__server_run_scope is not None:
+                self.__server_run_scope.cancel()
+
             await current_async_backend().cancel_shielded_coro_yield()
 
     @_utils.inherit_doc(AbstractAsyncNetworkServer)
     async def shutdown(self) -> None:
-        if self.__mainloop_task is not None:
-            self.__mainloop_task.cancel()
-        if self.__shutdown_asked:
-            await self.__is_shutdown.wait()
-            return
-        self.__shutdown_asked = True
-        try:
-            await self.__is_shutdown.wait()
-        finally:
-            self.__shutdown_asked = False
+        if self.__server_run_scope is not None:
+            self.__server_run_scope.cancel()
+        await self.__is_shutdown.wait()
 
     @_utils.inherit_doc(AbstractAsyncNetworkServer)
-    async def serve_forever(self, *, is_up_event: SupportsEventSet | None = None) -> NoReturn:
+    async def serve_forever(self, *, is_up_event: SupportsEventSet | None = None) -> None:
         async with contextlib.AsyncExitStack() as server_exit_stack:
             # Wake up server
             if not self.__is_shutdown.is_set():
                 raise ServerAlreadyRunning("Server is already running")
             self.__is_shutdown = is_shutdown = current_async_backend().create_event()
             server_exit_stack.callback(is_shutdown.set)
+            self.__server_run_scope = server_exit_stack.enter_context(current_async_backend().open_cancel_scope())
+
+            def reset_scope() -> None:
+                self.__server_run_scope = None
+
+            server_exit_stack.callback(reset_scope)
             ################
 
             # Bind and activate
             assert self.__servers is None  # nosec assert_used
             assert self.__listeners_factory_scope is None  # nosec assert_used
             if self.__listeners_factory is None:
                 raise ServerClosedError("Closed server")
-            listeners: list[AsyncListener[AsyncStreamTransport]] = []
+            listeners: list[AsyncDatagramListener[tuple[Any, ...]]] = []
             try:
                 with current_async_backend().open_cancel_scope() as self.__listeners_factory_scope:
                     await current_async_backend().coro_yield()
                     listeners.extend(await self.__listeners_factory())
                 if self.__listeners_factory_scope.cancelled_caught():
-                    raise ServerClosedError("Closed server")
+                    raise ServerClosedError("Server has been closed during task setup")
             finally:
                 self.__listeners_factory_scope = None
             if not listeners:
                 raise OSError("empty listeners list")
-            self.__servers = tuple(
-                _stream_server.AsyncStreamServer(listener, self.__protocol, max_recv_size=self.__max_recv_size)
-                for listener in listeners
-            )
+            self.__servers = tuple(_datagram_server.AsyncDatagramServer(listener, self.__protocol) for listener in listeners)
             del listeners
             ###################
 
             # Final teardown
             server_exit_stack.callback(self.__logger.info, "Server stopped")
-            server_exit_stack.push_async_callback(self.__close_servers)
             ################
 
             # Initialize request handler
             await self.__request_handler.service_init(
                 await server_exit_stack.enter_async_context(contextlib.AsyncExitStack()),
                 weakref.proxy(self),
             )
+            server_exit_stack.push_async_callback(self.__close_servers)
             ############################
 
             # Setup task group
-            self.__active_tasks = 0
             server_exit_stack.callback(self.__servers_tasks.clear)
-            task_group = await server_exit_stack.enter_async_context(current_async_backend().create_task_group())
+            task_group: TaskGroup = await server_exit_stack.enter_async_context(current_async_backend().create_task_group())
             server_exit_stack.callback(self.__logger.info, "Server loop break, waiting for remaining tasks...")
             ##################
 
             # Enable listener
-            self.__servers_tasks.extend(task_group.start_soon(self.__serve, server, task_group) for server in self.__servers)
+            self.__servers_tasks.extend(
+                [
+                    await task_group.start(server.serve, self.__datagram_received_coroutine, task_group)
+                    for server in self.__servers
+                ]
+            )
             self.__logger.info("Start serving at %s", ", ".join(map(str, self.get_addresses())))
             #################
 
             # Server is up
-            if is_up_event is not None and not self.__shutdown_asked:
+            if is_up_event is not None:
                 is_up_event.set()
             ##############
 
             # Main loop
-            self.__mainloop_task = task_group.start_soon(current_async_backend().sleep_forever)
-            if self.__shutdown_asked:
-                self.__mainloop_task.cancel()
             try:
-                await self.__mainloop_task.join()
+                await current_async_backend().sleep_forever()
             finally:
-                self.__mainloop_task = None
-
-        raise AssertionError("sleep_forever() does not return")
+                reset_scope()
 
-    async def __serve(
+    async def __datagram_received_coroutine(
         self,
-        server: _stream_server.AsyncStreamServer[_RequestT, _ResponseT],
-        task_group: TaskGroup,
-    ) -> NoReturn:
-        self.__attach_server()
-        try:
-            async with contextlib.aclosing(server):
-                await server.serve(self.__client_coroutine, task_group)
-        finally:
-            self.__detach_server()
-
-    async def __client_coroutine(
-        self,
-        lowlevel_client: _stream_server.AsyncStreamClient[_ResponseT],
-    ) -> AsyncGenerator[None, _RequestT]:
-        async with contextlib.AsyncExitStack() as client_exit_stack:
-            self.__attach_server()
-            client_exit_stack.callback(self.__detach_server)
-
-            client_address = lowlevel_client.extra(INETSocketAttribute.peername, None)
-            if client_address is None:
-                # The remote host closed the connection before starting the task.
-                # See this test for details:
-                # test____serve_forever____accept_client____client_sent_RST_packet_right_after_accept
-                self.__logger.warning("A client connection was interrupted just after listener.accept()")
+        lowlevel_client: _datagram_server.DatagramClientContext[_T_Response, tuple[Any, ...]],
+    ) -> AsyncGenerator[float | None, _T_Request]:
+        with _ClientErrorHandler(lowlevel_client, self.__logger):
+            request_handler_generator = self.__request_handler.handle(_ClientAPI(lowlevel_client))
+            timeout: float | None
+            try:
+                timeout = await anext(request_handler_generator)
+            except StopAsyncIteration:
                 return
-
-            client_address = new_socket_address(client_address, lowlevel_client.extra(INETSocketAttribute.family))
-
-            client_exit_stack.enter_context(self.__suppress_and_log_remaining_exception(client_address=client_address))
-            # If the socket was not closed gracefully, (i.e. client.aclose() failed )
-            # tell the OS to immediately abort the connection when calling socket.socket.close()
-            client_exit_stack.callback(self.__set_socket_linger_if_not_closed, lowlevel_client.extra(INETSocketAttribute.socket))
-
-            logger: logging.Logger = self.__logger
-            client = _ConnectedClientAPI(client_address, lowlevel_client, logger)
-
-            del lowlevel_client
-
-            logger.log(self.__client_connection_log_level, "Accepted new connection (address = %s)", client_address)
-            client_exit_stack.callback(self.__logger.log, self.__client_connection_log_level, "%s disconnected", client_address)
-            client_exit_stack.push_async_callback(client._force_close)
-
-            request_handler_generator: AsyncGenerator[None, _RequestT] | None = None
-            _on_connection_hook = self.__request_handler.on_connection(client)
-            if isinstance(_on_connection_hook, AsyncGenerator):
-                try:
-                    await anext(_on_connection_hook)
-                except StopAsyncIteration:
-                    pass
-                else:
-                    request_handler_generator = _on_connection_hook
             else:
-                assert inspect.isawaitable(_on_connection_hook)  # nosec assert_used
-                await _on_connection_hook
-            del _on_connection_hook
-
-            async def disconnect_client() -> None:
-                try:
-                    await self.__request_handler.on_disconnection(client)
-                except* ConnectionError:
-                    self.__logger.warning("ConnectionError raised in request_handler.on_disconnection()")
-
-            client_exit_stack.push_async_callback(disconnect_client)
-
-            del client_exit_stack
-
-            backend = current_async_backend()
-            try:
-                action: _asyncgen.AsyncGenAction[None, _RequestT]
-                while not client.is_closing():
-                    if request_handler_generator is None:
-                        request_handler_generator = self.__request_handler.handle(client)
-                        try:
-                            await anext(request_handler_generator)
-                        except StopAsyncIteration:
-                            request_handler_generator = None
-                            break
+                action: AsyncGenAction[_T_Request]
+                while True:
                     try:
-                        action = _asyncgen.SendAction((yield))
-                    except ConnectionError:
-                        break
+                        action = SendAction((yield timeout))
                     except BaseException as exc:
-                        action = _asyncgen.ThrowAction(exc)
+                        action = ThrowAction(_utils.remove_traceback_frames_in_place(exc, 1))
                     try:
-                        await action.asend(request_handler_generator)
+                        timeout = await action.asend(request_handler_generator)
                     except StopAsyncIteration:
-                        request_handler_generator = None
+                        return
+                    except BaseException as exc:
+                        # Remove action.asend() frames
+                        _utils.remove_traceback_frames_in_place(exc, 2)
+                        raise
                     finally:
                         del action
-                    await backend.cancel_shielded_coro_yield()
             finally:
-                if request_handler_generator is not None:
-                    await request_handler_generator.aclose()
-
-    def __attach_server(self) -> None:
-        self.__active_tasks += 1
-
-    def __detach_server(self) -> None:
-        self.__active_tasks -= 1
-        if self.__active_tasks < 0:
-            raise AssertionError("self.__active_tasks < 0")
-        if not self.__active_tasks:
-            if self.__mainloop_task is not None:
-                self.__mainloop_task.cancel()
-
-    @staticmethod
-    def __set_socket_linger_if_not_closed(socket: ISocket) -> None:
-        with contextlib.suppress(OSError):
-            if socket.fileno() > -1:
-                enable_socket_linger(socket, timeout=0)
-
-    @contextlib.contextmanager
-    def __suppress_and_log_remaining_exception(self, client_address: SocketAddress) -> Iterator[None]:
-        try:
-            try:
-                yield
-            except* ClientClosedError as excgrp:
-                _utils.remove_traceback_frames_in_place(excgrp, 1)  # Removes the 'yield' frame just above
-                self.__logger.warning(
-                    "There have been attempts to do operation on closed client %s",
-                    client_address,
-                    exc_info=excgrp,
-                )
-            except* ConnectionError:
-                # This exception come from the request handler ( most likely due to client.send_packet() )
-                # It is up to the user to log the ConnectionError stack trace
-                # There is already a "disconnected" info log
-                pass
-        except Exception as exc:
-            _utils.remove_traceback_frames_in_place(exc, 1)  # Removes the 'yield' frame just above
-            self.__logger.error("-" * 40)
-            self.__logger.error("Exception occurred during processing of request from %s", client_address, exc_info=exc)
-            self.__logger.error("-" * 40)
+                await request_handler_generator.aclose()
 
     @_utils.inherit_doc(AbstractAsyncNetworkServer)
     def get_addresses(self) -> Sequence[SocketAddress]:
         if (servers := self.__servers) is None:
             return ()
         return tuple(
             new_socket_address(server.extra(INETSocketAttribute.sockname), server.extra(INETSocketAttribute.family))
             for server in servers
             if not server.is_closing()
         )
 
-    @property
-    def sockets(self) -> Sequence[SocketProxy]:
-        """The listeners sockets. Read-only attribute."""
+    def get_sockets(self) -> Sequence[SocketProxy]:
+        """Gets the listeners sockets.
+
+        Returns:
+            a read-only sequence of :class:`.SocketProxy` objects.
+
+            If the server is not running, an empty sequence is returned.
+        """
         if (servers := self.__servers) is None:
             return ()
         return tuple(SocketProxy(server.extra(INETSocketAttribute.socket)) for server in servers)
 
+
+@final
+@runtime_final_class
+class _ClientAPI(AsyncDatagramClient[_T_Response]):
+    __slots__ = (
+        "__context",
+        "__h",
+        "__extra_attributes_cache",
+    )
+
+    def __init__(self, context: _datagram_server.DatagramClientContext[_T_Response, tuple[Any, ...]]) -> None:
+        super().__init__()
+        self.__context: _datagram_server.DatagramClientContext[_T_Response, tuple[Any, ...]] = context
+        self.__h: int | None = None
+        self.__extra_attributes_cache: Mapping[Any, Callable[[], Any]] | None = None
+
+    def __repr__(self) -> str:
+        return f"<client with address {self.__context.address} at {id(self):#x}>"
+
+    def __hash__(self) -> int:
+        if (h := self.__h) is None:
+            self.__h = h = hash(self.__context)
+        return h
+
+    def __eq__(self, other: object) -> bool:
+        if not isinstance(other, _ClientAPI):
+            return NotImplemented
+        return self.__context == other.__context
+
+    def is_closing(self) -> bool:
+        return self.__context.server.is_closing()
+
+    async def send_packet(self, packet: _T_Response, /) -> None:
+        server = self.__context.server
+        address = self.__context.address
+        if server.is_closing():
+            raise ClientClosedError("Closed client")
+        await server.send_packet_to(packet, address)
+
     @property
-    def logger(self) -> logging.Logger:
-        """The server's logger."""
-        return self.__logger
+    def extra_attributes(self) -> Mapping[Any, Callable[[], Any]]:
+        if (extra_attributes_cache := self.__extra_attributes_cache) is not None:
+            return extra_attributes_cache
+        server = self.__context.server
+        self.__extra_attributes_cache = extra_attributes_cache = {
+            **server.extra_attributes,
+            INETClientAttribute.socket: lambda: SocketProxy(server.extra(INETSocketAttribute.socket)),
+            INETClientAttribute.local_address: lambda: new_socket_address(
+                server.extra(INETSocketAttribute.sockname),
+                server.extra(INETSocketAttribute.family),
+            ),
+            INETClientAttribute.remote_address: lambda: new_socket_address(
+                self.__context.address,
+                server.extra(INETSocketAttribute.family),
+            ),
+        }
+        return extra_attributes_cache
 
 
 @final
 @runtime_final_class
-class _ConnectedClientAPI(AsyncStreamClient[_ResponseT]):
+class _ClientErrorHandler:
     __slots__ = (
-        "__client",
-        "__closed",
-        "__send_lock",
-        "__address",
-        "__proxy",
+        "__lowlevel_client",
         "__logger",
     )
 
     def __init__(
         self,
-        address: SocketAddress,
-        client: _stream_server.AsyncStreamClient[_ResponseT],
+        lowlevel_client: _datagram_server.DatagramClientContext[Any, tuple[Any, ...]],
         logger: logging.Logger,
     ) -> None:
-        self.__client: _stream_server.AsyncStreamClient[_ResponseT] = client
-        self.__closed: bool = False
-        self.__send_lock = current_async_backend().create_lock()
+        self.__lowlevel_client: _datagram_server.DatagramClientContext[Any, tuple[Any, ...]] = lowlevel_client
         self.__logger: logging.Logger = logger
-        self.__proxy: SocketProxy = SocketProxy(client.extra(INETSocketAttribute.socket))
-        self.__address: SocketAddress = address
 
-        with contextlib.suppress(OSError):
-            set_tcp_nodelay(self.__proxy, True)
-        with contextlib.suppress(OSError):
-            set_tcp_keepalive(self.__proxy, True)
+    def __enter__(self) -> None:
+        return
 
-    def __repr__(self) -> str:
-        return f"<client with address {self.__address} at {id(self):#x}>"
+    def __exit__(
+        self,
+        exc_type: type[BaseException] | None,
+        exc_val: BaseException | None,
+        exc_tb: types.TracebackType | None,
+        /,
+    ) -> bool:
+        # Fast path.
+        if exc_val is None:
+            return False
 
-    def is_closing(self) -> bool:
-        return self.__closed or self.__client.is_closing()
+        del exc_type, exc_tb
 
-    async def _force_close(self) -> None:
-        self.__closed = True
-        async with self.__send_lock:  # If self.aclose() took the lock, wait for it to finish
-            pass
-
-    async def aclose(self) -> None:
-        async with self.__send_lock:
-            self.__closed = True
-            await self.__client.aclose()
-
-    async def send_packet(self, packet: _ResponseT, /) -> None:
-        self.__check_closed()
-        self.__logger.debug("A response will be sent to %s", self.__address)
-        async with self.__send_lock:
-            self.__check_closed()
-            await self.__client.send_packet(packet)
-            _utils.check_real_socket_state(self.__proxy)
-            self.__logger.debug("Data sent to %s", self.__address)
+        try:
+            match exc_val:
+                case BaseExceptionGroup():
+                    connection_errors, exc_val = exc_val.split(ClientClosedError)
+                    if connection_errors is not None:
+                        self.__log_closed_client_errors(connection_errors)
+                    match exc_val:
+                        case None:
+                            return True
+                        case Exception():
+                            self.__log_exception(exc_val)
+                            return True
+                        case _:  # pragma: no cover
+                            del connection_errors
+                            raise exc_val
+                case ClientClosedError():
+                    self.__log_closed_client_errors(ExceptionGroup("", [exc_val]))
+                    return True
+                case Exception():
+                    self.__log_exception(exc_val)
+                    return True
+                case _:
+                    return False
+        finally:
+            del exc_val
 
-    def __check_closed(self) -> None:
-        if self.__closed:
-            raise ClientClosedError("Closed client")
+    def __log_closed_client_errors(self, exc: ExceptionGroup[ClientClosedError]) -> None:
+        lowlevel_client = self.__lowlevel_client
+        self.__logger.warning(
+            "There have been attempts to do operation on closed client %s",
+            new_socket_address(lowlevel_client.address, lowlevel_client.server.extra(INETSocketAttribute.family)),
+            exc_info=exc,
+        )
 
-    @property
-    def extra_attributes(self) -> Mapping[Any, Callable[[], Any]]:
-        client = self.__client
-        return {
-            **client.extra_attributes,
-            INETClientAttribute.socket: lambda: self.__proxy,
-            INETClientAttribute.local_address: lambda: new_socket_address(
-                client.extra(INETSocketAttribute.sockname),
-                client.extra(INETSocketAttribute.family),
-            ),
-            INETClientAttribute.remote_address: lambda: self.__address,
-        }
+    def __log_exception(self, exc: Exception) -> None:
+        lowlevel_client = self.__lowlevel_client
+        self.__logger.error("-" * 40)
+        self.__logger.error(
+            "Exception occurred during processing of request from %s",
+            new_socket_address(lowlevel_client.address, lowlevel_client.server.extra(INETSocketAttribute.family)),
+            exc_info=exc,
+        )
+        self.__logger.error("-" * 40)
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/api_sync/__init__.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/std_asyncio/datagram/__init__.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,21 +1,20 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 #
-"""
-Synchronous client/server module
+"""asyncio engine for easynetwork.async
 """
 
 from __future__ import annotations
 
 __all__ = []  # type: list[str]
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/api_sync/client/__init__.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/std_asyncio/stream/__init__.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,27 +1,20 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 #
-"""Network client module"""
+"""asyncio engine for easynetwork.async
+"""
 
 from __future__ import annotations
 
-__all__ = [
-    "AbstractNetworkClient",
-    "TCPNetworkClient",
-    "UDPNetworkClient",
-]
-
-from .abc import *
-from .tcp import *
-from .udp import *
+__all__ = []  # type: list[str]
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/api_sync/client/tcp.py` & `easynetwork-1.0.0rc8/src/easynetwork/clients/udp.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,262 +1,138 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 #
-"""TCP Network client implementation module"""
+"""UDP Network client implementation module"""
 
 from __future__ import annotations
 
-__all__ = ["TCPNetworkClient"]
+__all__ = ["UDPNetworkClient"]
 
 import contextlib
-import errno as _errno
 import socket as _socket
+import threading
 from collections.abc import Iterator
-from typing import TYPE_CHECKING, Any, final, overload
+from typing import Any, final, overload
 
-try:
-    import ssl
-except ImportError:  # pragma: no cover
-    _ssl_module = None
-else:
-    _ssl_module = ssl
-    del ssl
-
-from ..._typevars import _ReceivedPacketT, _SentPacketT
-from ...exceptions import ClientClosedError
-from ...lowlevel import _lock, _utils, constants
-from ...lowlevel.api_sync.endpoints.stream import StreamEndpoint
-from ...lowlevel.api_sync.transports.socket import SocketStreamTransport, SSLStreamTransport
-from ...lowlevel.socket import (
-    INETSocketAttribute,
-    SocketAddress,
-    SocketProxy,
-    new_socket_address,
-    set_tcp_keepalive,
-    set_tcp_nodelay,
-)
-from ...protocol import StreamProtocol
+from .._typevars import _T_ReceivedPacket, _T_SentPacket
+from ..exceptions import ClientClosedError
+from ..lowlevel import _lock, _utils, constants
+from ..lowlevel.api_sync.endpoints.datagram import DatagramEndpoint
+from ..lowlevel.api_sync.transports.socket import SocketDatagramTransport
+from ..lowlevel.socket import INETSocketAttribute, SocketAddress, SocketProxy, new_socket_address
+from ..protocol import DatagramProtocol
 from .abc import AbstractNetworkClient
 
-if TYPE_CHECKING:
-    import ssl as _typing_ssl
 
-
-class TCPNetworkClient(AbstractNetworkClient[_SentPacketT, _ReceivedPacketT]):
+class UDPNetworkClient(AbstractNetworkClient[_T_SentPacket, _T_ReceivedPacket]):
     """
-    A network client interface for TCP connections.
+    A network client interface for UDP communication.
     """
 
     __slots__ = (
         "__endpoint",
         "__socket_proxy",
         "__send_lock",
         "__receive_lock",
     )
 
     @overload
     def __init__(
         self,
         address: tuple[str, int],
         /,
-        protocol: StreamProtocol[_SentPacketT, _ReceivedPacketT],
+        protocol: DatagramProtocol[_T_SentPacket, _T_ReceivedPacket],
         *,
-        connect_timeout: float | None = ...,
         local_address: tuple[str, int] | None = ...,
-        ssl: _typing_ssl.SSLContext | bool | None = ...,
-        server_hostname: str | None = ...,
-        ssl_handshake_timeout: float | None = ...,
-        ssl_shutdown_timeout: float | None = ...,
-        ssl_shared_lock: bool | None = ...,
-        max_recv_size: int | None = ...,
+        family: int = ...,
+        reuse_port: bool = ...,
         retry_interval: float = ...,
-    ) -> None:
-        ...
+    ) -> None: ...
 
     @overload
     def __init__(
         self,
         socket: _socket.socket,
         /,
-        protocol: StreamProtocol[_SentPacketT, _ReceivedPacketT],
+        protocol: DatagramProtocol[_T_SentPacket, _T_ReceivedPacket],
         *,
-        ssl: _typing_ssl.SSLContext | bool | None = ...,
-        server_hostname: str | None = ...,
-        ssl_handshake_timeout: float | None = ...,
-        ssl_shutdown_timeout: float | None = ...,
-        ssl_shared_lock: bool | None = ...,
-        max_recv_size: int | None = ...,
         retry_interval: float = ...,
-    ) -> None:
-        ...
+    ) -> None: ...
 
     def __init__(
         self,
         __arg: _socket.socket | tuple[str, int],
         /,
-        protocol: StreamProtocol[_SentPacketT, _ReceivedPacketT],
+        protocol: DatagramProtocol[_T_SentPacket, _T_ReceivedPacket],
         *,
-        ssl: _typing_ssl.SSLContext | bool | None = None,
-        server_hostname: str | None = None,
-        ssl_handshake_timeout: float | None = None,
-        ssl_shutdown_timeout: float | None = None,
-        ssl_shared_lock: bool | None = None,
-        max_recv_size: int | None = None,
         retry_interval: float = 1.0,
         **kwargs: Any,
     ) -> None:
         """
         Common Parameters:
             protocol: The :term:`protocol object` to use.
 
         Connection Parameters:
             address: A pair of ``(host, port)`` for connection.
-            connect_timeout: The connection timeout (in seconds).
             local_address: If given, is a ``(local_host, local_port)`` tuple used to bind the socket locally.
+            reuse_port: Tells the kernel to allow this endpoint to be bound to the same port as other existing
+                        endpoints are bound to, so long as they all set this flag when being created.
+                        This option is not supported on Windows and some Unixes.
+                        If the SO_REUSEPORT constant is not defined then this capability is unsupported.
 
         Socket Parameters:
-            socket: An already connected TCP :class:`socket.socket`. If `socket` is given,
-                    none of `connect_timeout` and `local_address` should be specified.
+            socket: An already connected UDP :class:`socket.socket`. If `socket` is given,
+                    none of and `local_address` and `reuse_port` should be specified.
 
         Keyword Arguments:
-            ssl: If given and not false, a SSL/TLS transport is created (by default a plain TCP transport is created).
-                 If ssl is a :class:`ssl.SSLContext` object, this context is used to create the transport;
-                 if ssl is :data:`True`, a default context returned from :func:`ssl.create_default_context` is used.
-            server_hostname: sets or overrides the hostname that the target server's certificate will be matched against.
-                             Should only be passed if `ssl` is not :data:`None`. By default the value of the host in `address`
-                             argument is used. If `socket` is provided instead, there is no default and you must pass a value
-                             for `server_hostname`. If `server_hostname` is an empty string, hostname matching is disabled
-                             (which is a serious security risk, allowing for potential man-in-the-middle attacks).
-            ssl_handshake_timeout: (for a TLS connection) the time in seconds to wait for the TLS handshake to complete
-                                   before aborting the connection. ``60.0`` seconds if :data:`None` (default).
-            ssl_shutdown_timeout: the time in seconds to wait for the SSL shutdown to complete before aborting the connection.
-                                  ``30.0`` seconds if :data:`None` (default).
-            ssl_shared_lock: If :data:`True` (the default), :meth:`send_packet` and :meth:`recv_packet` uses
-                             the same lock instance.
-            max_recv_size: Read buffer size. If not given, a default reasonable value is used.
             retry_interval: The maximum wait time to wait for a blocking operation before retrying.
                             Set it to :data:`math.inf` to disable this feature.
-
-        See Also:
-            :ref:`SSL/TLS security considerations <ssl-security>`
         """
         super().__init__()
 
-        if max_recv_size is None:
-            max_recv_size = constants.DEFAULT_STREAM_BUFSIZE
-
-        if server_hostname is not None and not ssl:
-            raise ValueError("server_hostname is only meaningful with ssl")
-
-        if ssl_handshake_timeout is not None and not ssl:
-            raise ValueError("ssl_handshake_timeout is only meaningful with ssl")
-
-        if ssl_shutdown_timeout is not None and not ssl:
-            raise ValueError("ssl_shutdown_timeout is only meaningful with ssl")
-
-        if ssl_shared_lock is not None and not ssl:
-            raise ValueError("ssl_shared_lock is only meaningful with ssl")
-
-        if ssl_shared_lock is None:
-            ssl_shared_lock = True
-
         socket: _socket.socket
         match __arg:
-            case _socket.socket() if server_hostname is None and ssl:
-                raise ValueError("You must set server_hostname when using ssl without a host")
             case _socket.socket() as socket if not kwargs:
-                pass
+                _utils.ensure_datagram_socket_bound(socket)
             case (str(host), int(port)):
-                if server_hostname is None and ssl:
-                    # Use host as default for server_hostname.  It is an error
-                    # if host is empty or not set, e.g. when an
-                    # already-connected socket was passed or when only a port
-                    # is given.  To avoid this error, you can pass
-                    # server_hostname='' -- this will bypass the hostname
-                    # check.  (This also means that if host is a numeric
-                    # IP/IPv6 address, we will attempt to verify that exact
-                    # address; this will probably fail, but it is possible to
-                    # create a certificate for a specific IP address, so we
-                    # don't judge it here.)
-                    if not host:
-                        raise ValueError("You must set server_hostname when using ssl without a host")
-                    server_hostname = host
-                _utils.replace_kwargs(kwargs, {"local_address": "source_address", "connect_timeout": "timeout"})
-                kwargs.setdefault("timeout", None)
-                socket = _socket.create_connection((host, port), **kwargs, all_errors=True)
-            case _:  # pragma: no cover
+                if (family := kwargs.get("family", _socket.AF_UNSPEC)) != _socket.AF_UNSPEC:
+                    _utils.check_socket_family(family)
+                socket = _create_udp_socket(remote_address=(host, port), **kwargs)
+            case _:
                 raise TypeError("Invalid arguments")
 
         try:
             _utils.check_socket_family(socket.family)
             _utils.check_socket_is_connected(socket)
 
-            transport: SocketStreamTransport | SSLStreamTransport
-
-            if ssl:
-                if _ssl_module is None:
-                    raise RuntimeError("stdlib ssl module not available")
-
-                if isinstance(ssl, bool):
-                    ssl = _ssl_module.create_default_context()
-                    assert isinstance(ssl, _ssl_module.SSLContext)  # nosec assert_used
-                    if not server_hostname:
-                        ssl.check_hostname = False
-                    if hasattr(_ssl_module, "OP_IGNORE_UNEXPECTED_EOF"):
-                        ssl.options &= ~getattr(_ssl_module, "OP_IGNORE_UNEXPECTED_EOF")
-                if not server_hostname:
-                    server_hostname = None
-
-                with self.__convert_socket_error():
-                    transport = SSLStreamTransport(
-                        socket,
-                        ssl_context=ssl,
-                        retry_interval=retry_interval,
-                        server_hostname=server_hostname,
-                        server_side=False,
-                        standard_compatible=True,
-                        handshake_timeout=ssl_handshake_timeout,
-                        shutdown_timeout=ssl_shutdown_timeout,
-                    )
-
-            else:
-                transport = SocketStreamTransport(socket, retry_interval=retry_interval)
+            transport = SocketDatagramTransport(
+                socket,
+                retry_interval=retry_interval,
+                max_datagram_size=constants.MAX_DATAGRAM_BUFSIZE,
+            )
         except BaseException:
             socket.close()
             raise
-        finally:
-            del socket
-
-        assert ssl_shared_lock is not None  # nosec assert_used
-
-        if ssl and ssl_shared_lock:
-            self.__send_lock = self.__receive_lock = _lock.ForkSafeLock()
-        else:
-            self.__send_lock = _lock.ForkSafeLock()
-            self.__receive_lock = _lock.ForkSafeLock()
 
+        self.__send_lock = _lock.ForkSafeLock(threading.Lock)
+        self.__receive_lock = _lock.ForkSafeLock(threading.Lock)
         try:
-            self.__endpoint = StreamEndpoint(transport, protocol, max_recv_size=max_recv_size)
+            self.__endpoint: DatagramEndpoint[_T_SentPacket, _T_ReceivedPacket] = DatagramEndpoint(transport, protocol)
             self.__socket_proxy = SocketProxy(transport.extra(INETSocketAttribute.socket), lock=self.__send_lock.get)
-
-            with contextlib.suppress(OSError):
-                set_tcp_nodelay(self.socket, True)
-            with contextlib.suppress(OSError):
-                set_tcp_keepalive(self.socket, True)
         except BaseException:
             transport.close()
             raise
 
     def __repr__(self) -> str:
         try:
             return f"<{type(self).__name__} endpoint={self.__endpoint!r}>"
@@ -283,160 +159,124 @@
         The remote end will receive no more data (after queued data is flushed).
 
         Can be safely called multiple times.
         """
         with self.__send_lock.get():
             self.__endpoint.close()
 
-    def send_packet(self, packet: _SentPacketT, *, timeout: float | None = None) -> None:
+    def get_local_address(self) -> SocketAddress:
+        """
+        Returns the local socket IP address. Thread-safe.
+
+        Raises:
+            ClientClosedError: the client object is closed.
+            OSError: unrelated OS error occurred. You should check :attr:`OSError.errno`.
+
+        Returns:
+            the client's local address.
+        """
+        with self.__send_lock.get():
+            endpoint = self.__endpoint
+            if endpoint.is_closed():
+                raise ClientClosedError("Closed client")
+            local_address = endpoint.extra(INETSocketAttribute.sockname)
+            address_family = endpoint.extra(INETSocketAttribute.family)
+            return new_socket_address(local_address, address_family)
+
+    def get_remote_address(self) -> SocketAddress:
+        """
+        Returns the remote socket IP address. Thread-safe.
+
+        Raises:
+            ClientClosedError: the client object is closed.
+            OSError: unrelated OS error occurred. You should check :attr:`OSError.errno`.
+
+        Returns:
+            the client's remote address.
+        """
+        with self.__send_lock.get():
+            endpoint = self.__endpoint
+            if endpoint.is_closed():
+                raise ClientClosedError("Closed client")
+            remote_address = endpoint.extra(INETSocketAttribute.peername)
+            address_family = endpoint.extra(INETSocketAttribute.family)
+            return new_socket_address(remote_address, address_family)
+
+    def send_packet(self, packet: _T_SentPacket, *, timeout: float | None = None) -> None:
         """
         Sends `packet` to the remote endpoint. Thread-safe.
 
         If `timeout` is not :data:`None`, the entire send operation will take at most `timeout` seconds.
 
         Warning:
-            A timeout on a send operation is unusual unless you have a SSL/TLS context.
+            A timeout on a send operation is unusual.
 
             In the case of a timeout, it is impossible to know if all the packet data has been sent.
-            This would leave the connection in an inconsistent state.
 
         Important:
             The lock acquisition time is included in the `timeout`.
 
             This means that you may get a :exc:`TimeoutError` because it took too long to get the lock.
 
         Parameters:
             packet: the Python object to send.
             timeout: the allowed time (in seconds) for blocking operations.
 
         Raises:
             ClientClosedError: the client object is closed.
-            ConnectionError: connection unexpectedly closed during operation.
-                             You should not attempt any further operation and close the client object.
             TimeoutError: the send operation does not end up after `timeout` seconds.
             OSError: unrelated OS error occurred. You should check :attr:`OSError.errno`.
-            RuntimeError: :meth:`send_eof` has been called earlier.
         """
         with _utils.lock_with_timeout(self.__send_lock.get(), timeout) as timeout:
             endpoint = self.__endpoint
             if endpoint.is_closed():
                 raise ClientClosedError("Closed client")
             with self.__convert_socket_error():
                 endpoint.send_packet(packet, timeout=timeout)
                 _utils.check_real_socket_state(endpoint.extra(INETSocketAttribute.socket))
 
-    def send_eof(self) -> None:
-        """
-        Close the write end of the stream after the buffered write data is flushed. Thread-safe.
-
-        This method does nothing if the client is closed.
-
-        Can be safely called multiple times.
-
-        Raises:
-            OSError: unrelated OS error occurred. You should check :attr:`OSError.errno`.
-        """
-        with self.__send_lock.get():
-            endpoint = self.__endpoint
-            endpoint.send_eof()
-
-    def recv_packet(self, *, timeout: float | None = None) -> _ReceivedPacketT:
+    def recv_packet(self, *, timeout: float | None = None) -> _T_ReceivedPacket:
         """
-        Waits for a new packet to arrive from the remote endpoint. Thread-safe.
+        Waits for a new packet from the remote endpoint. Thread-safe.
 
         If `timeout` is not :data:`None`, the entire receive operation will take at most `timeout` seconds.
 
         Important:
             The lock acquisition time is included in the `timeout`.
 
             This means that you may get a :exc:`TimeoutError` because it took too long to get the lock.
 
         Parameters:
             timeout: the allowed time (in seconds) for blocking operations.
 
         Raises:
             ClientClosedError: the client object is closed.
-            ConnectionError: connection unexpectedly closed during operation.
-                             You should not attempt any further operation and close the client object.
             TimeoutError: the receive operation does not end up after `timeout` seconds.
             OSError: unrelated OS error occurred. You should check :attr:`OSError.errno`.
-            StreamProtocolParseError: invalid data received.
+            DatagramProtocolParseError: invalid data received.
 
         Returns:
             the received packet.
         """
         with _utils.lock_with_timeout(self.__receive_lock.get(), timeout) as timeout:
             endpoint = self.__endpoint
             if endpoint.is_closed():
                 raise ClientClosedError("Closed client")
-            try:
-                with self.__convert_socket_error():
-                    return endpoint.recv_packet(timeout=timeout)
-            except EOFError:
-                raise self.__abort() from None
+            with self.__convert_socket_error():
+                return endpoint.recv_packet(timeout=timeout)
 
     @contextlib.contextmanager
     def __convert_socket_error(self) -> Iterator[None]:
         try:
             yield
-        except ConnectionError as exc:
-            raise self.__abort() from exc
-        except _ssl_module.SSLZeroReturnError if _ssl_module else () as exc:
-            raise self.__abort() from exc
-        except _ssl_module.SSLError if _ssl_module else () as exc:
-            if _utils.is_ssl_eof_error(exc):
-                raise self.__abort() from exc
-            raise
         except OSError as exc:
             if exc.errno in constants.CLOSED_SOCKET_ERRNOS:
                 raise ClientClosedError("Closed client") from exc
             raise
 
-    @staticmethod
-    def __abort() -> OSError:
-        return _utils.error_from_errno(_errno.ECONNABORTED)
-
-    def get_local_address(self) -> SocketAddress:
-        """
-        Returns the local socket IP address. Thread-safe.
-
-        Raises:
-            ClientClosedError: the client object is closed.
-            OSError: unrelated OS error occurred. You should check :attr:`OSError.errno`.
-
-        Returns:
-            the client's local address.
-        """
-        with self.__send_lock.get():
-            endpoint = self.__endpoint
-            if endpoint.is_closed():
-                raise ClientClosedError("Closed client")
-            local_address = endpoint.extra(INETSocketAttribute.sockname)
-            address_family = endpoint.extra(INETSocketAttribute.family)
-            return new_socket_address(local_address, address_family)
-
-    def get_remote_address(self) -> SocketAddress:
-        """
-        Returns the remote socket IP address. Thread-safe.
-
-        Raises:
-            ClientClosedError: the client object is closed.
-            OSError: unrelated OS error occurred. You should check :attr:`OSError.errno`.
-
-        Returns:
-            the client's remote address.
-        """
-        with self.__send_lock.get():
-            endpoint = self.__endpoint
-            if endpoint.is_closed():
-                raise ClientClosedError("Closed client")
-            remote_address = endpoint.extra(INETSocketAttribute.peername)
-            address_family = endpoint.extra(INETSocketAttribute.family)
-            return new_socket_address(remote_address, address_family)
-
     def fileno(self) -> int:
         """
         Returns the socket's file descriptor, or ``-1`` if the client (or the socket) is closed. Thread-safe.
 
         Returns:
             the opened file descriptor.
         """
@@ -444,12 +284,86 @@
 
     @property
     @final
     def socket(self) -> SocketProxy:
         """A view to the underlying socket instance. Read-only attribute."""
         return self.__socket_proxy
 
-    @property
-    @final
-    def max_recv_size(self) -> int:
-        """Read buffer size. Read-only attribute."""
-        return self.__endpoint.max_recv_size
+
+def _create_udp_socket(
+    *,
+    local_address: tuple[str, int] | None = None,
+    remote_address: tuple[str, int] | None = None,
+    family: int = _socket.AF_UNSPEC,
+    reuse_port: bool = False,
+) -> _socket.socket:
+    local_host: str | None
+    local_port: int
+    if local_address is None:
+        local_host = "localhost"
+        local_port = 0
+        local_address = (local_host, local_port)
+    else:
+        local_host, local_port = local_address
+
+    flags: int = 0
+    if not remote_address:
+        flags |= _socket.AI_PASSIVE
+
+    errors: list[OSError] = []
+
+    for family, _, proto, _, sockaddr in _socket.getaddrinfo(
+        *(remote_address or local_address),
+        family=family,
+        type=_socket.SOCK_DGRAM,
+        flags=flags,
+    ):
+        try:
+            socket = _socket.socket(family, _socket.SOCK_DGRAM, proto)
+        except OSError as exc:
+            errors.append(exc)
+            continue
+        except BaseException:
+            errors.clear()
+            raise
+        try:
+            if reuse_port:
+                _utils.set_reuseport(socket)
+
+            if remote_address is None:
+                local_sockaddr = sockaddr
+                remote_sockaddr = None
+            else:
+                local_sockaddr = local_address
+                remote_sockaddr = sockaddr
+
+            del sockaddr
+
+            try:
+                socket.bind(local_sockaddr)
+            except OSError as exc:
+                msg = f"error while attempting to bind to address {local_sockaddr!r}: {exc.strerror.lower()}"
+                raise OSError(exc.errno, msg).with_traceback(exc.__traceback__) from None
+            if remote_sockaddr:
+                socket.connect(remote_sockaddr)
+
+            errors.clear()
+            return socket
+        except OSError as exc:
+            socket.close()
+            errors.append(exc)
+            continue
+        except BaseException:
+            errors.clear()
+            socket.close()
+            raise
+
+    if errors:
+        try:
+            raise ExceptionGroup("Could not create the UDP socket", errors)
+        finally:
+            errors.clear()
+    elif remote_address is not None:
+        remote_host = remote_address[0]
+        raise OSError(f"getaddrinfo({remote_host!r}) returned empty list")
+    else:
+        raise OSError(f"getaddrinfo({local_host!r}) returned empty list")
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/api_sync/client/udp.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/_utils.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,370 +1,391 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 #
-"""UDP Network client implementation module"""
-
 from __future__ import annotations
 
-__all__ = ["UDPNetworkClient"]
+__all__ = [
+    "ElapsedTime",
+    "adjust_leftover_buffer",
+    "check_real_socket_state",
+    "check_socket_family",
+    "check_socket_no_ssl",
+    "ensure_datagram_socket_bound",
+    "error_from_errno",
+    "exception_with_notes",
+    "get_callable_name",
+    "is_ssl_eof_error",
+    "is_ssl_socket",
+    "iterate_exceptions",
+    "lock_with_timeout",
+    "make_callback",
+    "missing_extra_deps",
+    "prepend_argument",
+    "remove_traceback_frames_in_place",
+    "replace_kwargs",
+    "set_reuseport",
+    "supports_socket_sendmsg",
+    "validate_timeout_delay",
+]
 
 import contextlib
+import errno as _errno
+import functools
+import math
+import os
 import socket as _socket
-from collections.abc import Iterator
-from typing import Any, final, overload
+import threading
+import time
+from abc import abstractmethod
+from collections import deque
+from collections.abc import Callable, Iterable, Iterator
+from typing import TYPE_CHECKING, Any, Concatenate, Final, ParamSpec, Protocol, Self, TypeGuard, TypeVar, overload
+
+try:
+    import ssl as _ssl
+except ImportError:  # pragma: no cover
+    ssl = None
+else:
+    ssl = _ssl
+    del _ssl
+
+from . import constants
+
+if TYPE_CHECKING:
+    from ssl import SSLError as _SSLError, SSLSocket as _SSLSocket
+
+    from _typeshed import ReadableBuffer
+
+    from .socket import ISocket, SupportsSocketOptions
+
+_P = ParamSpec("_P")
+_T_Return = TypeVar("_T_Return")
+_T_Arg = TypeVar("_T_Arg")
+
+_T_Exception = TypeVar("_T_Exception", bound=BaseException)
+_T_Func = TypeVar("_T_Func", bound=Callable[..., Any])
+
+
+def replace_kwargs(kwargs: dict[str, Any], keys: dict[str, str]) -> None:
+    if not keys:
+        raise ValueError("Empty key dict")
+    for old_key, new_key in keys.items():
+        if new_key in kwargs:
+            raise TypeError(f"Cannot set {old_key!r} to {new_key!r}: {new_key!r} in dictionary")
+        try:
+            kwargs[new_key] = kwargs.pop(old_key)
+        except KeyError:
+            pass
 
-from ..._typevars import _ReceivedPacketT, _SentPacketT
-from ...exceptions import ClientClosedError
-from ...lowlevel import _lock, _utils, constants
-from ...lowlevel.api_sync.endpoints.datagram import DatagramEndpoint
-from ...lowlevel.api_sync.transports.socket import SocketDatagramTransport
-from ...lowlevel.socket import INETSocketAttribute, SocketAddress, SocketProxy, new_socket_address
-from ...protocol import DatagramProtocol
-from .abc import AbstractNetworkClient
 
+def make_callback(func: Callable[_P, _T_Return], /, *args: _P.args, **kwargs: _P.kwargs) -> Callable[[], _T_Return]:
+    return functools.partial(func, *args, **kwargs)
 
-class UDPNetworkClient(AbstractNetworkClient[_SentPacketT, _ReceivedPacketT]):
-    """
-    A network client interface for UDP communication.
-    """
 
-    __slots__ = (
-        "__endpoint",
-        "__socket_proxy",
-        "__send_lock",
-        "__receive_lock",
+@overload
+def prepend_argument(
+    arg: _T_Arg,
+    func: None = ...,
+) -> Callable[[Callable[Concatenate[_T_Arg, _P], _T_Return]], Callable[_P, _T_Return]]: ...
+
+
+@overload
+def prepend_argument(
+    arg: _T_Arg,
+    func: Callable[Concatenate[_T_Arg, _P], _T_Return],
+) -> Callable[_P, _T_Return]: ...
+
+
+def prepend_argument(
+    arg: _T_Arg,
+    func: Callable[Concatenate[_T_Arg, _P], _T_Return] | None = None,
+) -> Callable[[Callable[Concatenate[_T_Arg, _P], _T_Return]], Callable[_P, _T_Return]] | Callable[_P, _T_Return]:
+    def decorator(func: Callable[Concatenate[_T_Arg, _P], _T_Return], /) -> Callable[_P, _T_Return]:
+        return functools.partial(func, arg)
+
+    if func is not None:
+        return decorator(func)
+    return decorator
+
+
+def get_callable_name(func: Callable[..., Any]) -> str:
+    qualname: str | None = getattr(func, "__qualname__", None)
+    if not qualname:
+        qualname = getattr(func, "__name__", None)
+        if not qualname:
+            return ""
+    module: str | None = getattr(func, "__module__", None)
+    if not module:
+        return qualname
+    return f"{module}.{qualname}"
+
+
+def inherit_doc(base_cls: type[Any]) -> Callable[[_T_Func], _T_Func]:
+    assert isinstance(base_cls, type)  # nosec assert_used
+
+    def decorator(dest_func: _T_Func) -> _T_Func:
+        ref_func: Any = getattr(base_cls, dest_func.__name__)
+        dest_func.__doc__ = ref_func.__doc__
+        return dest_func
+
+    return decorator
+
+
+def error_from_errno(errno: int) -> OSError:
+    return OSError(errno, os.strerror(errno))
+
+
+def missing_extra_deps(extra_name: str, *, feature_name: str = "") -> ModuleNotFoundError:
+    if not feature_name:
+        feature_name = extra_name
+    return exception_with_notes(
+        ModuleNotFoundError(f"{feature_name} dependencies are missing. Consider adding {extra_name!r} extra"),
+        [f'example: pip install "easynetwork[{extra_name}]"'],
     )
 
-    @overload
-    def __init__(
-        self,
-        address: tuple[str, int],
-        /,
-        protocol: DatagramProtocol[_SentPacketT, _ReceivedPacketT],
-        *,
-        local_address: tuple[str, int] | None = ...,
-        family: int = ...,
-        reuse_port: bool = ...,
-        retry_interval: float = ...,
-    ) -> None:
-        ...
-
-    @overload
-    def __init__(
-        self,
-        socket: _socket.socket,
-        /,
-        protocol: DatagramProtocol[_SentPacketT, _ReceivedPacketT],
-        *,
-        retry_interval: float = ...,
-    ) -> None:
-        ...
-
-    def __init__(
-        self,
-        __arg: _socket.socket | tuple[str, int],
-        /,
-        protocol: DatagramProtocol[_SentPacketT, _ReceivedPacketT],
-        *,
-        retry_interval: float = 1.0,
-        **kwargs: Any,
-    ) -> None:
-        """
-        Common Parameters:
-            protocol: The :term:`protocol object` to use.
-
-        Connection Parameters:
-            address: A pair of ``(host, port)`` for connection.
-            local_address: If given, is a ``(local_host, local_port)`` tuple used to bind the socket locally.
-            reuse_port: Tells the kernel to allow this endpoint to be bound to the same port as other existing
-                        endpoints are bound to, so long as they all set this flag when being created.
-                        This option is not supported on Windows and some Unixes.
-                        If the SO_REUSEPORT constant is not defined then this capability is unsupported.
-
-        Socket Parameters:
-            socket: An already connected UDP :class:`socket.socket`. If `socket` is given,
-                    none of and `local_address` and `reuse_port` should be specified.
-
-        Keyword Arguments:
-            retry_interval: The maximum wait time to wait for a blocking operation before retrying.
-                            Set it to :data:`math.inf` to disable this feature.
-        """
-        super().__init__()
-
-        socket: _socket.socket
-        match __arg:
-            case _socket.socket() as socket if not kwargs:
-                _utils.ensure_datagram_socket_bound(socket)
-            case (str(host), int(port)):
-                if (family := kwargs.get("family", _socket.AF_UNSPEC)) != _socket.AF_UNSPEC:
-                    _utils.check_socket_family(family)
-                socket = _create_udp_socket(remote_address=(host, port), **kwargs)
-            case _:  # pragma: no cover
-                raise TypeError("Invalid arguments")
 
-        try:
-            _utils.check_socket_family(socket.family)
-            _utils.check_socket_is_connected(socket)
+def check_socket_family(family: int) -> None:
+    if family not in {_socket.AF_INET, _socket.AF_INET6}:
+        raise ValueError("Only these families are supported: AF_INET, AF_INET6")
 
-            transport = SocketDatagramTransport(
-                socket,
-                retry_interval=retry_interval,
-                max_datagram_size=constants.MAX_DATAGRAM_BUFSIZE,
-            )
-        except BaseException:
-            socket.close()
-            raise
 
-        self.__send_lock = _lock.ForkSafeLock()
-        self.__receive_lock = _lock.ForkSafeLock()
-        try:
-            self.__endpoint: DatagramEndpoint[_SentPacketT, _ReceivedPacketT] = DatagramEndpoint(transport, protocol)
-            self.__socket_proxy = SocketProxy(transport.extra(INETSocketAttribute.socket), lock=self.__send_lock.get)
-        except BaseException:
-            transport.close()
-            raise
+def check_real_socket_state(socket: ISocket) -> None:
+    """Verify socket saved error and raise OSError if there is one
 
-    def __repr__(self) -> str:
-        try:
-            return f"<{type(self).__name__} endpoint={self.__endpoint!r}>"
-        except AttributeError:
-            return f"<{type(self).__name__} (partially initialized)>"
-
-    def is_closed(self) -> bool:
-        """
-        Checks if the client is in a closed state. Thread-safe.
-
-        If :data:`True`, all future operations on the client object will raise a :exc:`.ClientClosedError`.
-
-        Returns:
-            the client state.
-        """
-        with self.__send_lock.get():
-            return self.__endpoint.is_closed()
-
-    def close(self) -> None:
-        """
-        Close the client. Thread-safe.
-
-        Once that happens, all future operations on the client object will raise a :exc:`.ClientClosedError`.
-        The remote end will receive no more data (after queued data is flushed).
-
-        Can be safely called multiple times.
-        """
-        with self.__send_lock.get():
-            self.__endpoint.close()
-
-    def get_local_address(self) -> SocketAddress:
-        """
-        Returns the local socket IP address. Thread-safe.
-
-        Raises:
-            ClientClosedError: the client object is closed.
-            OSError: unrelated OS error occurred. You should check :attr:`OSError.errno`.
-
-        Returns:
-            the client's local address.
-        """
-        with self.__send_lock.get():
-            endpoint = self.__endpoint
-            if endpoint.is_closed():
-                raise ClientClosedError("Closed client")
-            local_address = endpoint.extra(INETSocketAttribute.sockname)
-            address_family = endpoint.extra(INETSocketAttribute.family)
-            return new_socket_address(local_address, address_family)
-
-    def get_remote_address(self) -> SocketAddress:
-        """
-        Returns the remote socket IP address. Thread-safe.
-
-        Raises:
-            ClientClosedError: the client object is closed.
-            OSError: unrelated OS error occurred. You should check :attr:`OSError.errno`.
-
-        Returns:
-            the client's remote address.
-        """
-        with self.__send_lock.get():
-            endpoint = self.__endpoint
-            if endpoint.is_closed():
-                raise ClientClosedError("Closed client")
-            remote_address = endpoint.extra(INETSocketAttribute.peername)
-            address_family = endpoint.extra(INETSocketAttribute.family)
-            return new_socket_address(remote_address, address_family)
-
-    def send_packet(self, packet: _SentPacketT, *, timeout: float | None = None) -> None:
-        """
-        Sends `packet` to the remote endpoint. Thread-safe.
-
-        If `timeout` is not :data:`None`, the entire send operation will take at most `timeout` seconds.
-
-        Warning:
-            A timeout on a send operation is unusual.
-
-            In the case of a timeout, it is impossible to know if all the packet data has been sent.
-
-        Important:
-            The lock acquisition time is included in the `timeout`.
-
-            This means that you may get a :exc:`TimeoutError` because it took too long to get the lock.
-
-        Parameters:
-            packet: the Python object to send.
-            timeout: the allowed time (in seconds) for blocking operations.
-
-        Raises:
-            ClientClosedError: the client object is closed.
-            TimeoutError: the send operation does not end up after `timeout` seconds.
-            OSError: unrelated OS error occurred. You should check :attr:`OSError.errno`.
-        """
-        with _utils.lock_with_timeout(self.__send_lock.get(), timeout) as timeout:
-            endpoint = self.__endpoint
-            if endpoint.is_closed():
-                raise ClientClosedError("Closed client")
-            with self.__convert_socket_error():
-                endpoint.send_packet(packet, timeout=timeout)
-                _utils.check_real_socket_state(endpoint.extra(INETSocketAttribute.socket))
-
-    def recv_packet(self, *, timeout: float | None = None) -> _ReceivedPacketT:
-        """
-        Waits for a new packet from the remote endpoint. Thread-safe.
-
-        If `timeout` is not :data:`None`, the entire receive operation will take at most `timeout` seconds.
-
-        Important:
-            The lock acquisition time is included in the `timeout`.
-
-            This means that you may get a :exc:`TimeoutError` because it took too long to get the lock.
-
-        Parameters:
-            timeout: the allowed time (in seconds) for blocking operations.
-
-        Raises:
-            ClientClosedError: the client object is closed.
-            TimeoutError: the receive operation does not end up after `timeout` seconds.
-            OSError: unrelated OS error occurred. You should check :attr:`OSError.errno`.
-            DatagramProtocolParseError: invalid data received.
-
-        Returns:
-            the received packet.
-        """
-        with _utils.lock_with_timeout(self.__receive_lock.get(), timeout) as timeout:
-            endpoint = self.__endpoint
-            if endpoint.is_closed():
-                raise ClientClosedError("Closed client")
-            with self.__convert_socket_error():
-                return endpoint.recv_packet(timeout=timeout)
+    There are some functions such as socket.send() which do not immediately fail and save the errno
+    in SO_ERROR socket option because the error spawns after the action was sent to the kernel (Something weird)
 
-    @contextlib.contextmanager
-    def __convert_socket_error(self) -> Iterator[None]:
-        try:
-            yield
-        except OSError as exc:
-            if exc.errno in constants.CLOSED_SOCKET_ERRNOS:
-                raise ClientClosedError("Closed client") from exc
-            raise
+    On Windows: The returned value should be the error returned by WSAGetLastError(), but the socket methods always call
+    this function to raise an error, so getsockopt(SO_ERROR) will most likely always return zero :)
+    """
+    if socket.fileno() < 0:
+        return
+    errno = socket.getsockopt(_socket.SOL_SOCKET, _socket.SO_ERROR)
+    if errno != 0:
+        # The SO_ERROR is automatically reset to zero after getting the value
+        raise error_from_errno(errno)
 
-    def fileno(self) -> int:
-        """
-        Returns the socket's file descriptor, or ``-1`` if the client (or the socket) is closed. Thread-safe.
-
-        Returns:
-            the opened file descriptor.
-        """
-        return self.socket.fileno()
-
-    @property
-    @final
-    def socket(self) -> SocketProxy:
-        """A view to the underlying socket instance. Read-only attribute."""
-        return self.__socket_proxy
-
-
-def _create_udp_socket(
-    *,
-    local_address: tuple[str, int] | None = None,
-    remote_address: tuple[str, int] | None = None,
-    family: int = _socket.AF_UNSPEC,
-    reuse_port: bool = False,
-) -> _socket.socket:
-    local_host: str | None
-    local_port: int
-    if local_address is None:
-        local_host = "localhost"
-        local_port = 0
-        local_address = (local_host, local_port)
-    else:
-        local_host, local_port = local_address
 
-    flags: int = 0
-    if not remote_address:
-        flags |= _socket.AI_PASSIVE
-
-    errors: list[OSError] = []
-
-    for family, _, proto, _, sockaddr in _socket.getaddrinfo(
-        *(remote_address or local_address),
-        family=family,
-        type=_socket.SOCK_DGRAM,
-        flags=flags,
-    ):
-        try:
-            socket = _socket.socket(family, _socket.SOCK_DGRAM, proto)
-        except OSError as exc:
-            errors.append(exc)
-            continue
-        except BaseException:
-            errors.clear()
+_HAS_SENDMSG: Final[bool] = hasattr(_socket.socket, "sendmsg")
+
+
+class _SupportsSocketSendMSG(Protocol):
+    @abstractmethod
+    def sendmsg(self, buffers: Iterable[ReadableBuffer], /) -> int: ...
+
+
+def supports_socket_sendmsg(sock: _socket.socket) -> TypeGuard[_SupportsSocketSendMSG]:
+    assert isinstance(sock, _socket.SocketType)  # nosec assert_used
+    return _HAS_SENDMSG
+
+
+def is_ssl_socket(socket: _socket.socket) -> TypeGuard[_SSLSocket]:
+    if ssl is None:
+        return False
+    return isinstance(socket, ssl.SSLSocket)
+
+
+def check_socket_no_ssl(socket: _socket.socket) -> None:
+    if is_ssl_socket(socket):
+        raise TypeError("ssl.SSLSocket instances are forbidden")
+
+
+def validate_timeout_delay(delay: float, *, positive_check: bool) -> float:
+    if math.isnan(delay):
+        raise ValueError("Invalid delay: NaN (not a number)")
+    if positive_check and delay < 0:
+        raise ValueError("Invalid delay: negative value")
+    return delay
+
+
+def is_ssl_eof_error(exc: BaseException) -> TypeGuard[_SSLError]:
+    if ssl is None:
+        return False
+
+    match exc:
+        case ssl.SSLEOFError():
+            return True
+        case ssl.SSLError() if hasattr(exc, "strerror") and "UNEXPECTED_EOF_WHILE_READING" in exc.strerror:
+            # From Trio project:
+            # There appears to be a bug on Python 3.10, where SSLErrors
+            # aren't properly translated into SSLEOFErrors.
+            # This stringly-typed error check is borrowed from the AnyIO
+            # project.
+            return True
+    return False
+
+
+def iter_bytes(b: bytes | bytearray | memoryview) -> Iterator[bytes]:
+    return map(int.to_bytes, b)
+
+
+def adjust_leftover_buffer(buffers: deque[memoryview], nbytes: int) -> None:
+    while nbytes > 0:
+        b = buffers.popleft()
+        b_len = len(b)
+        if b_len <= nbytes:
+            nbytes -= b_len
+        else:
+            buffers.appendleft(b[nbytes:])
+            break
+
+
+def is_socket_connected(sock: ISocket) -> bool:
+    try:
+        sock.getpeername()
+    except OSError as exc:
+        if exc.errno not in constants.NOT_CONNECTED_SOCKET_ERRNOS:
             raise
-        try:
-            if reuse_port:
-                _utils.set_reuseport(socket)
+        connected = False
+    else:
+        connected = True
+    return connected
+
 
-            if remote_address is None:
-                local_sockaddr = sockaddr
-                remote_sockaddr = None
-            else:
-                local_sockaddr = local_address
-                remote_sockaddr = sockaddr
-
-            del sockaddr
-
-            try:
-                socket.bind(local_sockaddr)
-            except OSError as exc:
-                msg = f"error while attempting to bind to address {local_sockaddr!r}: {exc.strerror.lower()}"
-                raise OSError(exc.errno, msg).with_traceback(exc.__traceback__) from None
-            if remote_sockaddr:
-                socket.connect(remote_sockaddr)
-
-            errors.clear()
-            return socket
-        except OSError as exc:
-            socket.close()
-            errors.append(exc)
-            continue
-        except BaseException:
-            errors.clear()
-            socket.close()
+def check_socket_is_connected(sock: ISocket) -> None:
+    if not is_socket_connected(sock):
+        raise error_from_errno(_errno.ENOTCONN)
+
+
+def ensure_datagram_socket_bound(sock: _socket.socket) -> None:
+    check_socket_family(sock.family)
+    if sock.type != _socket.SOCK_DGRAM:
+        raise ValueError("A 'SOCK_DGRAM' socket is expected")
+    try:
+        is_bound: bool = sock.getsockname()[1] > 0
+    except OSError as exc:
+        if exc.errno != _errno.EINVAL:
             raise
+        is_bound = False
 
-    if errors:
+    if not is_bound:
+        sock.bind(("localhost", 0))
+
+
+def set_reuseport(sock: SupportsSocketOptions) -> None:
+    if not hasattr(_socket, "SO_REUSEPORT"):
+        raise ValueError("reuse_port not supported by socket module")
+    else:
         try:
-            raise ExceptionGroup("Could not create the UDP socket", errors)
-        finally:
-            errors.clear()
-    elif remote_address is not None:
-        remote_host = remote_address[0]
-        raise OSError(f"getaddrinfo({remote_host!r}) returned empty list")
+            sock.setsockopt(_socket.SOL_SOCKET, _socket.SO_REUSEPORT, True)
+        except OSError:
+            raise ValueError("reuse_port not supported by socket module, SO_REUSEPORT defined but not implemented.") from None
+
+
+def exception_with_notes(exc: _T_Exception, notes: str | Iterable[str]) -> _T_Exception:
+    if isinstance(notes, str):
+        notes = (notes,)
+    for note in notes:
+        exc.add_note(note)
+    return exc
+
+
+def remove_traceback_frames_in_place(exc: _T_Exception, n: int) -> _T_Exception:
+    tb = exc.__traceback__
+    for _ in range(n):
+        if tb is None:
+            break
+        tb = tb.tb_next
+    return exc.with_traceback(tb)
+
+
+def iterate_exceptions(exception: BaseException) -> Iterator[BaseException]:
+    if isinstance(exception, BaseExceptionGroup):
+        for exc in exception.exceptions:
+            yield from iterate_exceptions(exc)
     else:
-        raise OSError(f"getaddrinfo({local_host!r}) returned empty list")
+        yield exception
+
+
+class ElapsedTime:
+    __slots__ = ("_current_time_func", "_start_time", "_end_time")
+
+    def __init__(self) -> None:
+        self._current_time_func: Callable[[], float] = time.perf_counter
+        self._start_time: float | None = None
+        self._end_time: float | None = None
+
+    def __enter__(self) -> Self:
+        if self._start_time is not None:
+            raise RuntimeError("Already entered")
+        self._start_time = self._current_time_func()
+        return self
+
+    def __exit__(self, *args: Any) -> None:
+        end_time = self._current_time_func()
+        if self._end_time is not None:
+            raise RuntimeError("Already exited")
+        self._end_time = end_time
+
+    def get_elapsed(self) -> float:
+        start_time = self._start_time
+        if start_time is None:
+            raise RuntimeError("Not entered")
+        end_time = self._end_time
+        if end_time is None:
+            raise RuntimeError("Within context")
+        return end_time - start_time
+
+    def recompute_timeout(self, old_timeout: float) -> float:
+        elapsed_time = self.get_elapsed()
+        new_timeout = old_timeout - elapsed_time
+        if new_timeout < 0.0:
+            new_timeout = 0.0
+        return new_timeout
+
+
+@contextlib.contextmanager
+def lock_with_timeout(
+    lock: threading.RLock | threading.Lock,
+    timeout: float | None,
+) -> Iterator[float | None]:
+    if timeout is None or timeout == math.inf:
+        with lock:
+            yield timeout
+        return
+    timeout = validate_timeout_delay(timeout, positive_check=True)
+    with contextlib.ExitStack() as stack:
+        # Try to acquire without blocking first
+        if lock.acquire(blocking=False):
+            stack.push(lock)
+        else:
+            with ElapsedTime() as elapsed:
+                if timeout == 0 or not lock.acquire(True, timeout):
+                    raise error_from_errno(_errno.ETIMEDOUT)
+            stack.push(lock)
+            timeout = elapsed.recompute_timeout(timeout)
+        yield timeout
+
+
+class ResourceGuard:
+    __slots__ = (
+        "__held",
+        "__msg",
+    )
+
+    def __init__(self, message: str) -> None:
+        self.__held: bool = False
+        self.__msg = message
+
+    def __enter__(self) -> None:
+        if self.__held:
+            from ..exceptions import BusyResourceError
+
+            msg = self.__msg
+            raise BusyResourceError(msg)
+        self.__held = True
+
+    def __exit__(self, *args: Any) -> None:
+        if not self.__held:
+            raise AssertionError("ResourceGuard released too many times")
+        self.__held = False
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/api_sync/server/__init__.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_sync/transports/__init__.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,29 +1,19 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 #
-"""Network server module"""
+"""Low-level transports module"""
 
 from __future__ import annotations
 
-__all__ = [
-    "AbstractNetworkServer",
-    "NetworkServerThread",
-    "StandaloneTCPNetworkServer",
-    "StandaloneUDPNetworkServer",
-]
-
-from .abc import *
-from .tcp import *
-from .thread import *
-from .udp import *
+__all__ = []  # type: list[str]
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/api_sync/server/_base.py` & `easynetwork-1.0.0rc8/src/easynetwork/servers/_base.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -18,112 +18,119 @@
 
 __all__ = ["BaseStandaloneNetworkServerImpl"]
 
 import concurrent.futures
 import contextlib
 import threading as _threading
 from collections.abc import Callable, Mapping, Sequence
-from typing import TYPE_CHECKING, Any, NoReturn
+from typing import Any
 
-from ...api_async.server.abc import SupportsEventSet
-from ...exceptions import ServerAlreadyRunning, ServerClosedError
-from ...lowlevel import _utils
-from ...lowlevel._lock import ForkSafeLock
-from ...lowlevel.api_async.backend.abc import AsyncBackend, ThreadsPortal
-from ...lowlevel.api_async.backend.factory import AsyncBackendFactory as _Factory, current_async_backend as _current_backend
-from ...lowlevel.socket import SocketAddress
-from .abc import AbstractNetworkServer
-
-if TYPE_CHECKING:
-    from ...api_async.server.abc import AbstractAsyncNetworkServer
+from ..exceptions import ServerAlreadyRunning, ServerClosedError
+from ..lowlevel import _utils
+from ..lowlevel._lock import ForkSafeLock
+from ..lowlevel.api_async.backend.abc import ThreadsPortal
+from ..lowlevel.api_async.backend.factory import AsyncBackendFactory as _Factory
+from ..lowlevel.socket import SocketAddress
+from .abc import AbstractAsyncNetworkServer, AbstractNetworkServer, SupportsEventSet
 
 
 class BaseStandaloneNetworkServerImpl(AbstractNetworkServer):
     __slots__ = (
         "__server_factory",
+        "__default_runner_options",
         "__private_server",
-        "__backend",
+        "__backend_name",
         "__close_lock",
         "__bootstrap_lock",
         "__private_threads_portal",
         "__is_shutdown",
         "__is_closed",
     )
 
-    def __init__(self, backend: str, server_factory: Callable[[], AbstractAsyncNetworkServer]) -> None:
+    def __init__(
+        self,
+        backend: str,
+        server_factory: Callable[[], AbstractAsyncNetworkServer],
+        *,
+        runner_options: Mapping[str, Any] | None = None,
+    ) -> None:
         super().__init__()
-        self.__backend: AsyncBackend = _Factory.get_backend(backend)
+        self.__backend_name: str = backend
         self.__server_factory: Callable[[], AbstractAsyncNetworkServer] = server_factory
         self.__private_server: AbstractAsyncNetworkServer | None = None
         self.__private_threads_portal: ThreadsPortal | None = None
         self.__is_shutdown = _threading.Event()
         self.__is_shutdown.set()
         self.__is_closed = _threading.Event()
         self.__close_lock = ForkSafeLock()
         self.__bootstrap_lock = ForkSafeLock()
+        self.__default_runner_options: dict[str, Any] = dict(runner_options) if runner_options else {}
 
     @_utils.inherit_doc(AbstractNetworkServer)
     def is_serving(self) -> bool:
         if (portal := self._portal) is not None and (server := self._server) is not None:
             with contextlib.suppress(RuntimeError):
                 return portal.run_sync(server.is_serving)
         return False
 
     @_utils.inherit_doc(AbstractNetworkServer)
     def server_close(self) -> None:
-        with self.__close_lock.get(), contextlib.ExitStack() as stack, contextlib.suppress(RuntimeError):
+        with self.__close_lock.get(), contextlib.ExitStack() as stack:
             stack.callback(self.__is_closed.set)
 
             # Ensure we are not in the interval between the server shutdown and the scheduler shutdown
             stack.callback(self.__is_shutdown.wait)
 
             if (server := self._server) is not None and (portal := self._portal) is not None:
-                with contextlib.suppress(concurrent.futures.CancelledError):
+                with contextlib.suppress(RuntimeError, concurrent.futures.CancelledError):
                     portal.run_coroutine(server.server_close)
 
     @_utils.inherit_doc(AbstractNetworkServer)
     def shutdown(self, timeout: float | None = None) -> None:
         if (portal := self._portal) is not None and (server := self._server) is not None:
-            with contextlib.suppress(RuntimeError, concurrent.futures.CancelledError):
+            with contextlib.suppress(RuntimeError, concurrent.futures.CancelledError), _utils.ElapsedTime() as elapsed:
                 # If shutdown() have been cancelled, that means the scheduler itself is shutting down, and this is what we want
                 if timeout is None:
                     portal.run_coroutine(server.shutdown)
                 else:
-                    elapsed = _utils.ElapsedTime()
-                    try:
-                        with elapsed:
-                            portal.run_coroutine(self.__do_shutdown_with_timeout, server, timeout)
-                    finally:
-                        timeout = elapsed.recompute_timeout(timeout)
+                    portal.run_coroutine(self.__do_shutdown_with_timeout, server, timeout)
+            if timeout is not None:
+                timeout = elapsed.recompute_timeout(timeout)
         self.__is_shutdown.wait(timeout)
 
     @staticmethod
     async def __do_shutdown_with_timeout(server: AbstractAsyncNetworkServer, timeout_delay: float) -> None:
-        with _current_backend().move_on_after(timeout_delay):
+        with _Factory.current().move_on_after(timeout_delay):
             await server.shutdown()
 
     def serve_forever(
         self,
         *,
         is_up_event: SupportsEventSet | None = None,
         runner_options: Mapping[str, Any] | None = None,
     ) -> None:
         """
         Starts the server's main loop.
 
         Parameters:
             is_up_event: If given, will be triggered when the server is ready to accept new clients.
-            runner_options: Options to pass to the :meth:`~AsyncBackend.bootstrap` method.
+            runner_options: Options to pass to the :meth:`.AsyncBackend.bootstrap` method.
+                            The specified keys override the keys passed at initialization.
 
         Raises:
             ServerClosedError: The server is closed.
             ServerAlreadyRunning: Another task already called :meth:`serve_forever`.
         """
+        if self.__default_runner_options:
+            if runner_options:
+                runner_options = {**self.__default_runner_options, **runner_options}
+            else:
+                runner_options = self.__default_runner_options.copy()
 
-        backend = self.__backend
+        backend = _Factory.get_backend(self.__backend_name)
         with contextlib.ExitStack() as server_exit_stack, contextlib.suppress(backend.get_cancelled_exc_class()):
             # locks_stack is used to acquire locks until
             # serve_forever() coroutine creates the thread portal
             locks_stack = server_exit_stack.enter_context(contextlib.ExitStack())
             locks_stack.enter_context(self.__close_lock.get())
             locks_stack.enter_context(self.__bootstrap_lock.get())
 
@@ -142,15 +149,15 @@
 
             def acquire_bootstrap_lock() -> None:
                 locks_stack.enter_context(self.__bootstrap_lock.get())
 
             server_exit_stack.callback(reset_values)
             server_exit_stack.callback(acquire_bootstrap_lock)
 
-            async def serve_forever() -> NoReturn:
+            async def serve_forever() -> None:
                 async with (
                     self.__server_factory() as self.__private_server,
                     backend.create_threads_portal() as self.__private_threads_portal,
                 ):
                     server = self.__private_server
                     # Initialization finished; release the locks
                     locks_stack.close()
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/api_sync/server/tcp.py` & `easynetwork-1.0.0rc8/src/easynetwork/servers/standalone_tcp.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -17,82 +17,89 @@
 from __future__ import annotations
 
 __all__ = [
     "StandaloneTCPNetworkServer",
 ]
 
 import contextlib
-from collections.abc import Sequence
-from typing import TYPE_CHECKING, Any, Generic
+from collections.abc import Mapping, Sequence
+from typing import TYPE_CHECKING, Any, Generic, Literal
 
-from ..._typevars import _RequestT, _ResponseT
-from ...api_async.server.tcp import AsyncTCPNetworkServer
-from ...lowlevel import _utils
-from ...lowlevel.socket import SocketProxy
+from .._typevars import _T_Request, _T_Response
+from ..lowlevel import _utils
+from ..lowlevel.socket import SocketProxy
 from . import _base
+from .async_tcp import AsyncTCPNetworkServer
 
 if TYPE_CHECKING:
     import logging
     from ssl import SSLContext as _SSLContext
 
-    from ...api_async.server.handler import AsyncStreamRequestHandler
-    from ...protocol import StreamProtocol
+    from ..protocol import StreamProtocol
+    from .handlers import AsyncStreamRequestHandler
 
 
-class StandaloneTCPNetworkServer(_base.BaseStandaloneNetworkServerImpl, Generic[_RequestT, _ResponseT]):
+class StandaloneTCPNetworkServer(_base.BaseStandaloneNetworkServerImpl, Generic[_T_Request, _T_Response]):
     """
     A network server for TCP connections.
 
     It embeds an :class:`.AsyncTCPNetworkServer` instance.
     """
 
     __slots__ = ()
 
     def __init__(
         self,
         host: str | None | Sequence[str],
         port: int,
-        protocol: StreamProtocol[_ResponseT, _RequestT],
-        request_handler: AsyncStreamRequestHandler[_RequestT, _ResponseT],
+        protocol: StreamProtocol[_T_Response, _T_Request],
+        request_handler: AsyncStreamRequestHandler[_T_Request, _T_Response],
         backend: str = "asyncio",
         *,
+        runner_options: Mapping[str, Any] | None = None,
         ssl: _SSLContext | None = None,
         ssl_handshake_timeout: float | None = None,
         ssl_shutdown_timeout: float | None = None,
+        ssl_standard_compatible: bool | None = None,
         backlog: int | None = None,
         reuse_port: bool = False,
         max_recv_size: int | None = None,
+        manual_buffer_allocation: Literal["try", "no", "force"] = "try",
         log_client_connection: bool | None = None,
         logger: logging.Logger | None = None,
         **kwargs: Any,
     ) -> None:
         """
-        For the arguments, see :class:`.AsyncTCPNetworkServer` documentation.
+        For the other arguments, see :class:`.AsyncTCPNetworkServer` documentation.
 
-        Note:
-            The backend interface must be explicitly given. It defaults to ``asyncio``.
+        Parameters:
+            backend: The event loop to use. It defaults to ``asyncio``.
+            runner_options: Options to pass to the :meth:`.AsyncBackend.bootstrap` method.
         """
         super().__init__(
             backend,
             _utils.make_callback(
                 AsyncTCPNetworkServer,  # type: ignore[arg-type]
                 host=host,
                 port=port,
                 protocol=protocol,
                 request_handler=request_handler,
                 ssl=ssl,
                 ssl_handshake_timeout=ssl_handshake_timeout,
                 ssl_shutdown_timeout=ssl_shutdown_timeout,
+                ssl_standard_compatible=ssl_standard_compatible,
                 backlog=backlog,
                 reuse_port=reuse_port,
                 max_recv_size=max_recv_size,
+                manual_buffer_allocation=manual_buffer_allocation,
                 log_client_connection=log_client_connection,
                 logger=logger,
                 **kwargs,
             ),
+            runner_options=runner_options,
         )
 
     def stop_listening(self) -> None:
         """
         Schedules the shutdown of all listener sockets. Thread-safe.
 
         After that, all new connections will be refused, but the server will continue to run and handle
@@ -100,21 +107,25 @@
 
         Further calls to :meth:`is_serving` will return :data:`False`.
         """
         if (portal := self._portal) is not None and (server := self._server) is not None:
             with contextlib.suppress(RuntimeError):
                 portal.run_sync(server.stop_listening)
 
-    @property
-    @_utils.inherit_doc(AsyncTCPNetworkServer)
-    def sockets(self) -> Sequence[SocketProxy]:
+    def get_sockets(self) -> Sequence[SocketProxy]:
+        """Gets the listeners sockets. Thread-safe.
+
+        Returns:
+            a read-only sequence of :class:`.SocketProxy` objects.
+
+            If the server is not running, an empty sequence is returned.
+        """
         if (portal := self._portal) is not None and (server := self._server) is not None:
             with contextlib.suppress(RuntimeError):
-                sockets = portal.run_sync(lambda: server.sockets)
+                sockets = portal.run_sync(server.get_sockets)
                 return tuple(SocketProxy(sock, runner=portal.run_sync) for sock in sockets)
         return ()
 
     if TYPE_CHECKING:
 
         @property
-        def _server(self) -> AsyncTCPNetworkServer[_RequestT, _ResponseT] | None:
-            ...
+        def _server(self) -> AsyncTCPNetworkServer[_T_Request, _T_Response] | None: ...
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/lowlevel/__init__.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_async/transports/__init__.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,19 +1,19 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 #
-"""EasyNetwork's low-level API module"""
+"""Low-level asynchronous transports module"""
 
 from __future__ import annotations
 
 __all__ = []  # type: list[str]
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/lowlevel/_asyncgen.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/_asyncgen.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -23,35 +23,37 @@
 from collections.abc import AsyncGenerator
 from typing import Any, Generic, TypeVar
 
 _T_Send = TypeVar("_T_Send")
 _T_Yield = TypeVar("_T_Yield")
 
 
-class AsyncGenAction(Generic[_T_Yield, _T_Send], metaclass=ABCMeta):
+class AsyncGenAction(Generic[_T_Send], metaclass=ABCMeta):
     __slots__ = ()
 
     @abstractmethod
     async def asend(self, generator: AsyncGenerator[_T_Yield, _T_Send]) -> _T_Yield:
         raise NotImplementedError
 
 
 @dataclasses.dataclass(slots=True)
-class SendAction(AsyncGenAction[_T_Yield, _T_Send]):
+class SendAction(AsyncGenAction[_T_Send]):
     value: _T_Send
 
     async def asend(self, generator: AsyncGenerator[_T_Yield, _T_Send]) -> _T_Yield:
-        try:
-            return await generator.asend(self.value)
-        finally:
-            del self
+        return await generator.asend(self.value)
 
 
 @dataclasses.dataclass(slots=True)
-class ThrowAction(AsyncGenAction[_T_Yield, Any]):
+class ThrowAction(AsyncGenAction[Any]):
     exception: BaseException
 
     async def asend(self, generator: AsyncGenerator[_T_Yield, Any]) -> _T_Yield:
         try:
-            return await generator.athrow(self.exception)
+            match self.exception:
+                case GeneratorExit():
+                    await generator.aclose()
+                    raise self.exception
+                case _:
+                    return await generator.athrow(self.exception)
         finally:
             del generator, self  # Needed to avoid circular reference with raised exception
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/lowlevel/_final.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/_final.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/lowlevel/_lock.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/_lock.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -21,33 +21,31 @@
 __all__ = ["ForkSafeLock"]
 
 import os
 import threading
 from collections.abc import Callable
 from typing import Generic, TypeVar, cast, overload
 
-_LockType = TypeVar("_LockType", bound="threading.RLock | threading.Lock")
+_T_Lock = TypeVar("_T_Lock", bound="threading.RLock | threading.Lock")
 
 
-class ForkSafeLock(Generic[_LockType]):
+class ForkSafeLock(Generic[_T_Lock]):
     __slots__ = ("__pid", "__unsafe_lock", "__lock_factory", "__weakref__")
 
     @overload
-    def __init__(self: ForkSafeLock[threading.RLock], lock_factory: None = ...) -> None:
-        ...
+    def __init__(self: ForkSafeLock[threading.RLock], lock_factory: None = ...) -> None: ...
 
     @overload
-    def __init__(self, lock_factory: Callable[[], _LockType]) -> None:
-        ...
+    def __init__(self, lock_factory: Callable[[], _T_Lock]) -> None: ...
 
-    def __init__(self, lock_factory: Callable[[], _LockType] | None = None) -> None:
+    def __init__(self, lock_factory: Callable[[], _T_Lock] | None = None) -> None:
         if lock_factory is None:
-            lock_factory = cast(Callable[[], _LockType], threading.RLock)
-        self.__unsafe_lock: _LockType = lock_factory()
+            lock_factory = cast(Callable[[], _T_Lock], threading.RLock)
+        self.__unsafe_lock: _T_Lock = lock_factory()
         self.__pid: int = os.getpid()
-        self.__lock_factory: Callable[[], _LockType] = lock_factory
+        self.__lock_factory: Callable[[], _T_Lock] = lock_factory
 
-    def get(self) -> _LockType:
+    def get(self) -> _T_Lock:
         if self.__pid != os.getpid():
             self.__unsafe_lock = self.__lock_factory()
             self.__pid = os.getpid()
         return self.__unsafe_lock
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/lowlevel/_stream.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/_stream.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -17,227 +17,165 @@
 from __future__ import annotations
 
 __all__ = [
     "StreamDataConsumer",
     "StreamDataProducer",
 ]
 
-from collections import deque
-from collections.abc import Generator, Iterator
+from collections.abc import Generator
 from typing import TYPE_CHECKING, Any, Generic, final
 
-from .._typevars import _ReceivedPacketT, _SentPacketT
+from .._typevars import _T_ReceivedPacket, _T_SentPacket
 from ..exceptions import StreamProtocolParseError
 from ..protocol import BufferedStreamReceiver, StreamProtocol
 from ._final import runtime_final_class
 
 if TYPE_CHECKING:
     from _typeshed import ReadableBuffer, WriteableBuffer
 
 
 @final
 @runtime_final_class
-@Iterator.register
-class StreamDataProducer(Generic[_SentPacketT]):
-    __slots__ = ("__p", "__g", "__q")
+class StreamDataProducer(Generic[_T_SentPacket]):
+    __slots__ = ("__protocol",)
 
-    def __init__(self, protocol: StreamProtocol[_SentPacketT, Any]) -> None:
+    def __init__(self, protocol: StreamProtocol[_T_SentPacket, Any]) -> None:
         super().__init__()
         _check_protocol(protocol)
-        self.__p: StreamProtocol[_SentPacketT, Any] = protocol
-        self.__g: Generator[bytes, None, None] | None = None
-        self.__q: deque[_SentPacketT] = deque()
+        self.__protocol: StreamProtocol[_T_SentPacket, Any] = protocol
 
-    def __del__(self) -> None:  # pragma: no cover
+    def generate(self, packet: _T_SentPacket) -> Generator[bytes, None, None]:
         try:
-            generator, self.__g = self.__g, None
-        except AttributeError:
-            return
-        try:
-            if generator is not None:
-                generator.close()
-        finally:
-            del generator
-
-    def __iter__(self) -> Iterator[bytes]:
-        return self
-
-    def __next__(self) -> bytes:
-        protocol = self.__p
-        queue: deque[_SentPacketT] = self.__q
-        generator: Generator[bytes, None, None] | None
-        while (generator := self.__g) is not None or queue:
-            if generator is None:
-                generator = protocol.generate_chunks(queue.popleft())
-            else:
-                self.__g = None
-            try:
-                chunk = next(filter(None, map(bytes, generator)))
-            except StopIteration:
-                pass
-            except Exception as exc:
-                raise RuntimeError("protocol.generate_chunks() crashed") from exc
-            else:
-                self.__g = generator
-                return chunk
-            finally:
-                del generator
-        raise StopIteration
-
-    def pending_packets(self) -> bool:
-        return self.__g is not None or bool(self.__q)
-
-    def enqueue(self, *packets: _SentPacketT) -> None:
-        self.__q.extend(packets)
-
-    def clear(self) -> None:
-        self.__q.clear()
-        generator, self.__g = self.__g, None
-        if generator is not None:
-            generator.close()
+            yield from self.__protocol.generate_chunks(packet)
+        except Exception as exc:
+            raise RuntimeError("protocol.generate_chunks() crashed") from exc
 
 
 @final
 @runtime_final_class
-@Iterator.register
-class StreamDataConsumer(Generic[_ReceivedPacketT]):
-    __slots__ = ("__p", "__b", "__c")
+class StreamDataConsumer(Generic[_T_ReceivedPacket]):
+    __slots__ = ("__protocol", "__buffer", "__consumer")
 
-    def __init__(self, protocol: StreamProtocol[Any, _ReceivedPacketT]) -> None:
+    def __init__(self, protocol: StreamProtocol[Any, _T_ReceivedPacket]) -> None:
         super().__init__()
         _check_protocol(protocol)
-        self.__p: StreamProtocol[Any, _ReceivedPacketT] = protocol
-        self.__c: Generator[None, bytes, tuple[_ReceivedPacketT, bytes]] | None = None
-        self.__b: bytes = b""
+        self.__protocol: StreamProtocol[Any, _T_ReceivedPacket] = protocol
+        self.__consumer: Generator[None, bytes, tuple[_T_ReceivedPacket, bytes]] | None = None
+        self.__buffer: bytes = b""
 
-    def __del__(self) -> None:  # pragma: no cover
+    def __del__(self) -> None:
         try:
-            consumer, self.__c = self.__c, None
+            self.clear()
         except AttributeError:
             return
-        try:
-            if consumer is not None:
-                consumer.close()
-        finally:
-            del consumer
-
-    def __iter__(self) -> Iterator[_ReceivedPacketT]:
-        return self
-
-    def __next__(self) -> _ReceivedPacketT:
-        chunk: bytes = self.__b
-        if not chunk:
-            raise StopIteration
-        consumer, self.__c = self.__c, None
-        if consumer is None:
-            consumer = self.__p.build_packet_from_chunks()
+
+    def next(self, received_chunk: bytes | None) -> _T_ReceivedPacket:
+        if not received_chunk:
+            if not (received_chunk := self.__buffer):
+                raise StopIteration
+        elif self.__buffer:
+            received_chunk = self.__buffer + received_chunk
+        if (consumer := self.__consumer) is None:
+            consumer = self.__protocol.build_packet_from_chunks()
             try:
                 next(consumer)
             except StopIteration:
+                self.__buffer = bytes(received_chunk)
                 raise RuntimeError("protocol.build_packet_from_chunks() did not yield") from None
             except Exception as exc:
                 raise RuntimeError("protocol.build_packet_from_chunks() crashed") from exc
-        self.__b = b""
-        packet: _ReceivedPacketT
+        else:
+            # Reset consumer
+            # Will be re-assigned if needed
+            self.__consumer = None
+        self.__buffer = b""
+        packet: _T_ReceivedPacket
         remaining: bytes
         try:
-            consumer.send(chunk)
+            consumer.send(received_chunk)
         except StopIteration as exc:
             packet, remaining = exc.value
-            self.__b = bytes(remaining)
+            self.__buffer = bytes(remaining)
             return packet
         except StreamProtocolParseError as exc:
-            self.__b = bytes(exc.remaining_data)
+            self.__buffer = bytes(exc.remaining_data)
             raise
         except Exception as exc:
             raise RuntimeError("protocol.build_packet_from_chunks() crashed") from exc
         else:
-            self.__c = consumer
+            self.__consumer = consumer
             raise StopIteration
-        finally:
-            del consumer, chunk
-
-    def feed(self, chunk: ReadableBuffer) -> None:
-        chunk = bytes(chunk)
-        if not chunk:
-            return
-        if self.__b:
-            self.__b += chunk
-        else:
-            self.__b = chunk
 
     def get_buffer(self) -> memoryview:
-        return memoryview(self.__b)
+        return memoryview(self.__buffer)
 
     def clear(self) -> None:
-        self.__b = b""
-        consumer, self.__c = self.__c, None
+        self.__buffer = b""
+        consumer, self.__consumer = self.__consumer, None
         if consumer is not None:
             consumer.close()
 
 
 @final
 @runtime_final_class
-@Iterator.register
-class BufferedStreamDataConsumer(Generic[_ReceivedPacketT]):
+class BufferedStreamDataConsumer(Generic[_T_ReceivedPacket]):
     __slots__ = (
         "__buffered_receiver",
         "__buffer",
-        "__buffer_view",
+        "__exported_write_buffer_view",
         "__buffer_start",
         "__already_written",
         "__sizehint",
         "__consumer",
     )
 
-    def __init__(self, protocol: StreamProtocol[Any, _ReceivedPacketT], buffer_size_hint: int) -> None:
+    def __init__(self, protocol: StreamProtocol[Any, _T_ReceivedPacket], buffer_size_hint: int) -> None:
         super().__init__()
         _check_protocol(protocol)
         if not isinstance(buffer_size_hint, int) or buffer_size_hint <= 0:
             raise ValueError(f"{buffer_size_hint=!r}")
-        self.__buffered_receiver: BufferedStreamReceiver[_ReceivedPacketT, WriteableBuffer] = protocol.buffered_receiver()
-        self.__consumer: Generator[int | None, int, tuple[_ReceivedPacketT, ReadableBuffer]] | None = None
+        self.__buffered_receiver: BufferedStreamReceiver[_T_ReceivedPacket, WriteableBuffer] = protocol.buffered_receiver()
+        self.__consumer: Generator[int | None, int, tuple[_T_ReceivedPacket, ReadableBuffer]] | None = None
         self.__buffer: WriteableBuffer | None = None
-        self.__buffer_view: memoryview | None = None
+        self.__exported_write_buffer_view: memoryview | None = None
         self.__buffer_start: int | None = None
         self.__already_written: int = 0
         self.__sizehint: int = buffer_size_hint
 
-    def __del__(self) -> None:  # pragma: no cover
-        self.__buffer = None
+    def __del__(self) -> None:
         try:
-            consumer, self.__consumer = self.__consumer, None
+            self.clear()
         except AttributeError:
             return
-        try:
-            if consumer is not None:
-                consumer.close()
-        finally:
-            del consumer
-
-    def __iter__(self) -> Iterator[_ReceivedPacketT]:
-        return self
-
-    def __next__(self) -> _ReceivedPacketT:
-        consumer = self.__consumer
-        if consumer is None:
-            raise StopIteration
 
-        nb_updated_bytes, self.__already_written = self.__already_written, 0
-        if nb_updated_bytes == 0:
+    def next(self, nb_updated_bytes: int | None) -> _T_ReceivedPacket:
+        if nb_updated_bytes is None:
+            nb_updated_bytes = 0
+        else:
+            if (buffer_view := self.__exported_write_buffer_view) is None:
+                raise RuntimeError("next() has been called whilst get_write_buffer() was never called")
+            if not (0 <= nb_updated_bytes <= buffer_view.nbytes):
+                raise RuntimeError("Invalid value given")
+            del buffer_view
+
+        if (consumer := self.__consumer) is None:
             raise StopIteration
 
-        # Forcibly reset buffer view
-        self.__buffer_view = None
+        nb_updated_bytes += self.__already_written
+        self.__already_written = 0
+        self.__release_write_buffer_view()
+
+        if not nb_updated_bytes:
+            raise StopIteration
 
         # Reset consumer
         # Will be re-assigned if needed
         self.__consumer = None
 
-        packet: _ReceivedPacketT
+        packet: _T_ReceivedPacket
         remaining: ReadableBuffer
         try:
             self.__buffer_start = consumer.send(nb_updated_bytes)
         except StopIteration as exc:
             packet, remaining = exc.value
             self.__save_remainder_in_buffer(remaining)
             return packet
@@ -247,20 +185,18 @@
         except Exception as exc:
             # Reset buffer, since we do not know if the buffer state is still valid
             self.__buffer = None
             raise RuntimeError("protocol.build_packet_from_buffer() crashed") from exc
         else:
             self.__consumer = consumer
             raise StopIteration
-        finally:
-            del consumer
 
     def get_write_buffer(self) -> WriteableBuffer:
-        if self.__buffer_view is not None:
-            return self.__buffer_view
+        if self.__exported_write_buffer_view is not None:
+            return self.__exported_write_buffer_view
 
         if self.__buffer is None:
             whole_buffer = self.__buffered_receiver.create_buffer(self.__sizehint)
             self.__validate_created_buffer(whole_buffer)
             self.__buffer = whole_buffer
 
         if self.__consumer is None:
@@ -285,26 +221,17 @@
 
         if self.__already_written:
             buffer = buffer[self.__already_written :]
 
         if not buffer:
             raise RuntimeError("The start position is set to the end of the buffer")
 
-        self.__buffer_view = buffer
+        self.__exported_write_buffer_view = buffer
         return buffer
 
-    def buffer_updated(self, nbytes: int) -> None:
-        if nbytes < 0:
-            raise ValueError("Negative value given")
-        if self.__buffer_view is None:
-            raise RuntimeError("buffer_updated() has been called whilst get_buffer() was never called")
-        if nbytes > self.__buffer_view.nbytes:
-            raise RuntimeError("nbytes > buffer_view.nbytes")
-        self.__update_write_count(nbytes)
-
     def get_value(self, *, full: bool = False) -> bytes | None:
         if self.__buffer is None:
             return None
         if full:
             return bytes(self.__buffer)
         buffer = memoryview(self.__buffer).cast("B")
         if self.__buffer_start is None:
@@ -312,40 +239,45 @@
         elif self.__buffer_start < 0:
             nbytes = self.__buffer_start + len(buffer) + self.__already_written
         else:
             nbytes = self.__buffer_start + self.__already_written
         return buffer[:nbytes].tobytes()
 
     def clear(self) -> None:
-        self.__buffer = self.__buffer_view = self.__buffer_start = None
+        self.__release_write_buffer_view()
+        self.__buffer = self.__buffer_start = None
         self.__already_written = 0
         consumer, self.__consumer = self.__consumer, None
         if consumer is not None:
             consumer.close()
 
     def __save_remainder_in_buffer(self, remaining_data: ReadableBuffer) -> None:
+        # Copy remaining_data because it can be a view to the wrapped buffer
+        # NOTE: remaining_data is not copied if it is already a "bytes" object
         remaining_data = bytes(remaining_data)
-        nbytes = len(remaining_data)
-        if nbytes == 0:
+        if not remaining_data:
             # Nothing to save.
             return
+        nbytes = len(remaining_data)
         with memoryview(self.get_write_buffer()) as buffer:
             buffer[:nbytes] = remaining_data
-        self.__update_write_count(nbytes)
-
-    def __update_write_count(self, nbytes: int) -> None:
         self.__already_written += nbytes
-        self.__buffer_view = None
+        self.__release_write_buffer_view()
+
+    def __release_write_buffer_view(self) -> None:
+        buffer_view, self.__exported_write_buffer_view = self.__exported_write_buffer_view, None
+        if buffer_view is not None:
+            buffer_view.release()
 
     @staticmethod
     def __validate_created_buffer(buffer: WriteableBuffer) -> None:
         with memoryview(buffer) as buffer:
             if buffer.readonly:
                 raise ValueError("protocol.create_buffer() returned a read-only buffer")
-            if not len(buffer):
+            if not buffer:
                 raise ValueError("protocol.create_buffer() returned a null buffer")
 
     @property
     def buffer_size(self) -> int:
         if self.__buffer is None:
             return 0
         with memoryview(self.__buffer) as buffer:
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/lowlevel/constants.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/constants.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -15,21 +15,21 @@
 """EasyNetwork's constants module"""
 
 from __future__ import annotations
 
 __all__ = [
     "ACCEPT_CAPACITY_ERRNOS",
     "ACCEPT_CAPACITY_ERROR_SLEEP_TIME",
+    "DEFAULT_SERIALIZER_LIMIT",
     "DEFAULT_STREAM_BUFSIZE",
     "MAX_DATAGRAM_BUFSIZE",
     "NOT_CONNECTED_SOCKET_ERRNOS",
     "SC_IOV_MAX",
     "SSL_HANDSHAKE_TIMEOUT",
     "SSL_SHUTDOWN_TIMEOUT",
-    "_DEFAULT_LIMIT",
 ]
 
 import errno as _errno
 from typing import Final
 
 # Buffer size for a recv(2) operation
 DEFAULT_STREAM_BUFSIZE: Final[int] = 16 * 1024  # 16KiB
@@ -75,16 +75,20 @@
 # The default timeout matches that of Nginx.
 SSL_HANDSHAKE_TIMEOUT: Final[float] = 60.0
 
 # Number of seconds to wait for SSL shutdown to complete
 # The default timeout mimics lingering_time
 SSL_SHUTDOWN_TIMEOUT: Final[float] = 30.0
 
+# "Connection Attempt Delay" for concurrent connections
+# Recommended value by the RFC 6555
+HAPPY_EYEBALLS_DELAY: Final[float] = 0.25
+
 # Buffer size limit when waiting for a byte sequence
-_DEFAULT_LIMIT: Final[int] = 64 * 1024  # 64 KiB
+DEFAULT_SERIALIZER_LIMIT: Final[int] = 64 * 1024  # 64 KiB
 
 
 def __get_sysconf(name: str, /) -> int:
     import os
 
     try:
         # os.sysconf() can return a negative value if 'name' is not defined
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/lowlevel/socket.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/socket.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -60,15 +60,15 @@
 from . import typed_attr
 from ._final import runtime_final_class
 
 if TYPE_CHECKING:
     import ssl as _typing_ssl
 
 _P = ParamSpec("_P")
-_R = TypeVar("_R")
+_T_Return = TypeVar("_T_Return")
 
 
 class SocketAttribute(typed_attr.TypedAttributeSet):
     __slots__ = ()
 
     socket: ISocket = typed_attr.typed_attribute()
     """:class:`socket.socket` instance."""
@@ -120,15 +120,15 @@
     """the TLS protocol version (e.g. TLSv1.2)"""
 
 
 class IPv4SocketAddress(NamedTuple):
     host: str
     port: int
 
-    def __str__(self) -> str:  # pragma: no cover
+    def __str__(self) -> str:
         return f"({self.host!r}, {self.port:d})"
 
     def for_connection(self) -> tuple[str, int]:
         """
         Returns:
             A pair of (host, port)
         """
@@ -137,15 +137,15 @@
 
 class IPv6SocketAddress(NamedTuple):
     host: str
     port: int
     flowinfo: int = 0
     scope_id: int = 0
 
-    def __str__(self) -> str:  # pragma: no cover
+    def __str__(self) -> str:
         return f"({self.host!r}, {self.port:d})"
 
     def for_connection(self) -> tuple[str, int]:
         """
         Returns:
             A pair of (host, port)
         """
@@ -153,28 +153,25 @@
 
 
 SocketAddress: TypeAlias = IPv4SocketAddress | IPv6SocketAddress
 """Alias for :class:`IPv4SocketAddress` | :class:`IPv6SocketAddress`"""
 
 
 @overload
-def new_socket_address(addr: tuple[str, int], family: Literal[_socket.AddressFamily.AF_INET]) -> IPv4SocketAddress:
-    ...
+def new_socket_address(addr: tuple[str, int], family: Literal[_socket.AddressFamily.AF_INET]) -> IPv4SocketAddress: ...
 
 
 @overload
 def new_socket_address(
     addr: tuple[str, int] | tuple[str, int, int, int], family: Literal[_socket.AddressFamily.AF_INET6]
-) -> IPv6SocketAddress:
-    ...
+) -> IPv6SocketAddress: ...
 
 
 @overload
-def new_socket_address(addr: tuple[Any, ...], family: int) -> SocketAddress:
-    ...
+def new_socket_address(addr: tuple[Any, ...], family: int) -> SocketAddress: ...
 
 
 def new_socket_address(addr: tuple[Any, ...], family: int) -> SocketAddress:
     """
     Factory to create a :data:`SocketAddress` from `addr`.
 
     Example:
@@ -210,38 +207,34 @@
             raise ValueError(f"Unsupported address family {family!r}")
 
 
 @runtime_checkable
 class SupportsSocketOptions(Protocol):
     @overload
     @abstractmethod
-    def getsockopt(self, level: int, optname: int, /) -> int:
-        ...
+    def getsockopt(self, level: int, optname: int, /) -> int: ...
 
     @overload
     @abstractmethod
-    def getsockopt(self, level: int, optname: int, buflen: int, /) -> bytes:
-        ...
+    def getsockopt(self, level: int, optname: int, buflen: int, /) -> bytes: ...
 
     @abstractmethod
     def getsockopt(self, *args: Any) -> int | bytes:
         """
         Similar to :meth:`socket.socket.getsockopt`.
         """
         ...
 
     @overload
     @abstractmethod
-    def setsockopt(self, level: int, optname: int, value: int | bytes, /) -> None:
-        ...
+    def setsockopt(self, level: int, optname: int, value: int | bytes, /) -> None: ...
 
     @overload
     @abstractmethod
-    def setsockopt(self, level: int, optname: int, value: None, optlen: int, /) -> None:
-        ...
+    def setsockopt(self, level: int, optname: int, value: None, optlen: int, /) -> None: ...
 
     @abstractmethod
     def setsockopt(self, *args: Any) -> None:
         """
         Similar to :meth:`socket.socket.setsockopt`.
         """
         ...
@@ -374,15 +367,15 @@
                 if raddr:
                     s = f"{s}, raddr={raddr}"
             except _socket.error:
                 pass
 
         return f"{s}>"
 
-    def __execute(self, func: Callable[_P, _R], /, *args: _P.args, **kwargs: _P.kwargs) -> _R:
+    def __execute(self, func: Callable[_P, _T_Return], /, *args: _P.args, **kwargs: _P.kwargs) -> _T_Return:
         with lock_ctx() if (lock_ctx := self.__lock_ctx) is not None else contextlib.nullcontext():
             if (run := self.__runner) is not None:
                 if args or kwargs:
                     func = functools.partial(func, *args, **kwargs)
                 return run(func)
             return func(*args, **kwargs)
 
@@ -395,34 +388,30 @@
     def get_inheritable(self) -> bool:
         """
         Calls :meth:`ISocket.get_inheritable`.
         """
         return self.__execute(self.__socket.get_inheritable)
 
     @overload
-    def getsockopt(self, level: int, optname: int, /) -> int:
-        ...
+    def getsockopt(self, level: int, optname: int, /) -> int: ...
 
     @overload
-    def getsockopt(self, level: int, optname: int, buflen: int, /) -> bytes:
-        ...
+    def getsockopt(self, level: int, optname: int, buflen: int, /) -> bytes: ...
 
     def getsockopt(self, *args: Any) -> int | bytes:
         """
         Calls :meth:`ISocket.getsockopt <SupportsSocketOptions.getsockopt>`.
         """
         return self.__execute(self.__socket.getsockopt, *args)  # type: ignore[arg-type]
 
     @overload
-    def setsockopt(self, level: int, optname: int, value: int | bytes, /) -> None:
-        ...
+    def setsockopt(self, level: int, optname: int, value: int | bytes, /) -> None: ...
 
     @overload
-    def setsockopt(self, level: int, optname: int, value: None, optlen: int, /) -> None:
-        ...
+    def setsockopt(self, level: int, optname: int, value: None, optlen: int, /) -> None: ...
 
     def setsockopt(self, *args: Any) -> None:
         """
         Calls :meth:`ISocket.setsockopt <SupportsSocketOptions.setsockopt>`.
         """
         return self.__execute(self.__socket.setsockopt, *args)  # type: ignore[arg-type]
 
@@ -566,16 +555,16 @@
     sock.setsockopt(_socket.SOL_SOCKET, _socket.SO_LINGER, _linger_struct.pack(False, 0))
 
 
 def _get_socket_extra(sock: ISocket, *, wrap_in_proxy: bool = True) -> dict[Any, Callable[[], Any]]:
     return {
         SocketAttribute.socket: (lambda: sock) if not wrap_in_proxy else (lambda: SocketProxy(sock)),
         SocketAttribute.family: lambda: _cast_socket_family(sock.family),
-        SocketAttribute.sockname: lambda: _address_or_lookup_error(sock.getsockname),
-        SocketAttribute.peername: lambda: _address_or_lookup_error(sock.getpeername),
+        SocketAttribute.sockname: lambda: _address_or_lookup_error(sock.fileno, sock.getsockname),
+        SocketAttribute.peername: lambda: _address_or_lookup_error(sock.fileno, sock.getpeername),
     }
 
 
 def _get_tls_extra(ssl_object: _typing_ssl.SSLObject | _typing_ssl.SSLSocket) -> dict[Any, Callable[[], Any]]:
     return {
         TLSAttribute.sslcontext: lambda: ssl_object.context,
         TLSAttribute.peercert: lambda: _value_or_lookup_error(ssl_object.getpeercert()),
@@ -595,22 +584,26 @@
 def _cast_socket_kind(kind: int) -> int:
     try:
         return _socket.SocketKind(kind)
     except ValueError:
         return kind
 
 
-def _address_or_lookup_error(getsockaddr: Callable[[], _R]) -> _R:
+def _address_or_lookup_error(fileno: Callable[[], int], getsockaddr: Callable[[], _T_Return]) -> _T_Return:
     try:
+        if fileno() < 0:
+            from errno import EBADF
+
+            raise OSError(EBADF, os.strerror(EBADF))
         return getsockaddr()
     except OSError as exc:
         from ..exceptions import TypedAttributeLookupError
 
         raise TypedAttributeLookupError("address not available") from exc
 
 
-def _value_or_lookup_error(value: _R | None) -> _R:
+def _value_or_lookup_error(value: _T_Return | None) -> _T_Return:
     if value is None:
         from ..exceptions import TypedAttributeLookupError
 
         raise TypedAttributeLookupError("value not available")
     return value
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/lowlevel/typed_attr.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/typed_attr.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,34 +1,40 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
+# Code taken from anyio (https://github.com/agronholm/anyio/tree/4.2.0)
 #
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+# Copyright (c) 2018 Alex Grönholm
 #
+# Permission is hereby granted, free of charge, to any person obtaining a copy of
+# this software and associated documentation files (the "Software"), to deal in
+# the Software without restriction, including without limitation the rights to
+# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of
+# the Software, and to permit persons to whom the Software is furnished to do so,
+# subject to the following conditions:
+# The above copyright notice and this permission notice shall be included in all
+# copies or substantial portions of the Software.
 #
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS
+# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR
+# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER
+# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+
 """Backport of AnyIO's typed attributes."""
 
 from __future__ import annotations
 
 __all__ = ["TypedAttributeProvider", "TypedAttributeSet"]
 
 from collections.abc import Callable, Mapping
 from typing import Any, TypeVar, final, overload
 
 from ..exceptions import TypedAttributeLookupError
 
-T_Attr = TypeVar("T_Attr")
-T_Default = TypeVar("T_Default")
+_T_Attr = TypeVar("_T_Attr")
+_T_Default = TypeVar("_T_Default")
 undefined = object()
 
 
 def typed_attribute() -> Any:
     """Return a unique object, used to mark typed attributes."""
     return object()
 
@@ -62,25 +68,23 @@
         A mapping of the extra attributes to callables that return the corresponding
         values.
 
         If the provider wraps another provider, the attributes from that wrapper should
         also be included in the returned mapping (but the wrapper may override the
         callables from the wrapped instance).
 
-        The callables should raise :exc:`TypedAttributeLookupError` if it is not possible to get the value.
+        The callables should raise :exc:`.TypedAttributeLookupError` if it is not possible to get the value.
         """
         return {}
 
     @overload
-    def extra(self, attribute: T_Attr) -> T_Attr:
-        ...
+    def extra(self, attribute: _T_Attr) -> _T_Attr: ...
 
     @overload
-    def extra(self, attribute: T_Attr, default: T_Default) -> T_Attr | T_Default:
-        ...
+    def extra(self, attribute: _T_Attr, default: _T_Default) -> _T_Attr | _T_Default: ...
 
     @final
     def extra(self, attribute: Any, default: object = undefined) -> object:
         """
         Return the value of the given typed extra attribute.
 
         Parameters:
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_async/__init__.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_async/__init__.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_async/backend/__init__.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_async/endpoints/__init__.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,19 +1,19 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 #
-"""Asynchronous backend engine module"""
+"""Low-level asynchronous endpoints module"""
 
 from __future__ import annotations
 
 __all__ = []  # type: list[str]
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_async/backend/_sniffio_helpers.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_async/backend/_sniffio_helpers.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -18,35 +18,33 @@
 
 __all__ = ["current_async_library", "setup_sniffio_contextvar"]
 
 import contextvars
 import sys
 
 
-def current_async_library() -> str:
-    try:
-        from sniffio import current_async_library
-    except ModuleNotFoundError:
-        current_async_library = _current_async_library_fallback
-    return current_async_library()
-
-
 def _current_async_library_fallback() -> str:
     if "asyncio" in sys.modules:
         import asyncio
 
         try:
             asyncio.get_running_loop()
         except RuntimeError:
             pass
         else:
             return "asyncio"
     raise RuntimeError("unknown async library, or not in async context")
 
 
-def setup_sniffio_contextvar(context: contextvars.Context, library_name: str | None, /) -> None:
-    try:
-        import sniffio
-    except ModuleNotFoundError:
+try:
+    import sniffio
+except ModuleNotFoundError:
+    current_async_library = _current_async_library_fallback
+
+    def setup_sniffio_contextvar(context: contextvars.Context, library_name: str | None, /) -> None:
         pass
-    else:
+
+else:
+    current_async_library = sniffio.current_async_library
+
+    def setup_sniffio_contextvar(context: contextvars.Context, library_name: str | None, /) -> None:
         context.run(sniffio.current_async_library_cvar.set, library_name)
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_async/backend/abc.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_async/backend/abc.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -20,37 +20,37 @@
     "AsyncBackend",
     "CancelScope",
     "ICondition",
     "IEvent",
     "ILock",
     "Task",
     "TaskGroup",
+    "TaskInfo",
     "ThreadsPortal",
 ]
 
-import contextlib
-import contextvars
+import dataclasses
 import math
 from abc import ABCMeta, abstractmethod
-from collections.abc import Awaitable, Callable, Coroutine, Iterator, Mapping, Sequence
+from collections.abc import Awaitable, Callable, Coroutine, Mapping, Sequence
 from contextlib import AbstractContextManager
-from typing import TYPE_CHECKING, Any, Generic, NoReturn, ParamSpec, Protocol, Self, TypeVar
+from typing import TYPE_CHECKING, Any, Generic, NoReturn, ParamSpec, Protocol, Self, TypeVar, TypeVarTuple
 
 if TYPE_CHECKING:
     import concurrent.futures
     import socket as _socket
-    import ssl as _typing_ssl
     from types import TracebackType
 
     from ..transports import abc as transports
 
 
 _P = ParamSpec("_P")
 _T = TypeVar("_T")
 _T_co = TypeVar("_T_co", covariant=True)
+_T_PosArgs = TypeVarTuple("_T_PosArgs")
 
 
 class ILock(Protocol):
     """
     A mutex lock for asynchronous tasks. Not thread-safe.
 
     A lock can be used to guarantee exclusive access to a shared resource.
@@ -72,26 +72,24 @@
         try:
             # access shared state
         finally:
             lock.release()
     """
 
     @abstractmethod
-    async def __aenter__(self) -> Any:
-        ...
+    async def __aenter__(self) -> Any: ...
 
     @abstractmethod
     async def __aexit__(
         self,
         exc_type: type[BaseException] | None,
         exc_val: BaseException | None,
         exc_tb: TracebackType | None,
         /,
-    ) -> bool | None:
-        ...
+    ) -> bool | None: ...
 
     @abstractmethod
     async def acquire(self) -> Any:
         """
         Acquires the lock.
 
         This method waits until the lock is *unlocked*, sets it to *locked*.
@@ -196,14 +194,20 @@
 class Task(Generic[_T_co], metaclass=ABCMeta):
     """
     A :class:`Task` object represents a concurrent "thread" of execution.
     """
 
     __slots__ = ("__weakref__",)
 
+    @property
+    @abstractmethod
+    def info(self) -> TaskInfo:
+        """The task data. Read-only attribute."""
+        raise NotImplementedError
+
     @abstractmethod
     def done(self) -> bool:
         """
         Returns the Task state.
 
         A Task is *done* when the wrapped coroutine either returned a value, raised an exception, or the Task was cancelled.
 
@@ -287,14 +291,36 @@
                     raise
             assert task.done()
             return await task.join()
         """
         raise NotImplementedError
 
 
+@dataclasses.dataclass(eq=False, slots=True, frozen=True)
+class TaskInfo:
+    """
+    Represents an asynchronous task.
+    """
+
+    id: int
+    "The unique identifier of the task"
+    name: str
+    "The description of the task (if any)"
+    coro: Coroutine[Any, Any, Any] | None = dataclasses.field(repr=False)
+    "The coroutine object of the task"
+
+    def __eq__(self, other: object) -> bool:
+        if isinstance(other, TaskInfo):
+            return self.id == other.id
+        return NotImplemented
+
+    def __hash__(self) -> int:
+        return hash(self.id)
+
+
 class CancelScope(metaclass=ABCMeta):
     """
     A temporary scope opened by a task that can be used by other tasks to control its execution time.
 
     Unlike trio's CancelScope, there is no "shielded" scopes; you must use :meth:`AsyncBackend.ignore_cancellation`.
     """
 
@@ -333,15 +359,15 @@
 
     @abstractmethod
     def cancelled_caught(self) -> bool:
         """
         Returns the scope cancellation state.
 
         Returns:
-            :data:`True` if the scope has been is *cancelled*.
+            :data:`True` if the scope has been *cancelled*.
         """
         raise NotImplementedError
 
     @abstractmethod
     def when(self) -> float:
         """
         Returns the current deadline.
@@ -393,16 +419,16 @@
     """
     Groups several asynchronous tasks together.
 
     Example::
 
         async def main():
             async with backend.create_task_group() as tg:
-                task1 = tg.start_soon(some_coro)
-                task2 = tg.start_soon(another_coro)
+                tg.start_soon(some_coro)
+                tg.start_soon(another_coro)
             print("Both tasks have completed now.")
 
     The :keyword:`async with` statement will wait for all tasks in the group to finish.
     While waiting, new tasks may still be added to the group
     (for example, by passing ``tg`` into one of the coroutines and calling ``tg.start_soon()`` in that coroutine).
     Once the last task has finished and the :keyword:`async with` block is exited, no new tasks may be added to the group.
     """
@@ -421,28 +447,49 @@
         exc_tb: TracebackType | None,
     ) -> None:
         raise NotImplementedError
 
     @abstractmethod
     def start_soon(
         self,
-        coro_func: Callable[..., Coroutine[Any, Any, _T]],
+        coro_func: Callable[[*_T_PosArgs], Coroutine[Any, Any, _T]],
+        /,
+        *args: *_T_PosArgs,
+        name: str | None = ...,
+    ) -> None:
+        """
+        Schedules the creation of a new task in this task group.
+
+        Parameters:
+            coro_func: An async function.
+            args: Positional arguments to be passed to `coro_func`. If you need to pass keyword arguments,
+                  then use :func:`functools.partial`.
+            name: Name of the task, for the purposes of introspection and debugging.
+        """
+        raise NotImplementedError
+
+    @abstractmethod
+    async def start(
+        self,
+        coro_func: Callable[[*_T_PosArgs], Coroutine[Any, Any, _T]],
         /,
-        *args: Any,
-        context: contextvars.Context | None = ...,
+        *args: *_T_PosArgs,
+        name: str | None = ...,
     ) -> Task[_T]:
         """
-        Starts a new task in this task group.
+        Starts a new managed task in this task group. Blocks until the event loop starts the task.
+
+        Warning:
+            Unlike ``trio`` and ``anyio``, there is no ``task_status`` parameter.
 
         Parameters:
             coro_func: An async function.
             args: Positional arguments to be passed to `coro_func`. If you need to pass keyword arguments,
                   then use :func:`functools.partial`.
-            context: If given, it must be a :class:`contextvars.Context` instance in which the coroutine should be executed.
-                     If the framework does not support contexts (or does not use them), it must simply ignore this parameter.
+            name: Name of the task, for the purposes of introspection and debugging.
 
         Returns:
             the created task.
         """
         raise NotImplementedError
 
 
@@ -574,16 +621,16 @@
     """
 
     __slots__ = ("__weakref__",)
 
     @abstractmethod
     def bootstrap(
         self,
-        coro_func: Callable[..., Coroutine[Any, Any, _T]],
-        *args: Any,
+        coro_func: Callable[[*_T_PosArgs], Coroutine[Any, Any, _T]],
+        *args: *_T_PosArgs,
         runner_options: Mapping[str, Any] | None = ...,
     ) -> _T:
         """
         Runs an async function, and returns the result.
 
         Calling::
 
@@ -646,15 +693,15 @@
 
         Returns:
             An exception class.
         """
         raise NotImplementedError
 
     @abstractmethod
-    async def ignore_cancellation(self, coroutine: Coroutine[Any, Any, _T_co]) -> _T_co:
+    async def ignore_cancellation(self, coroutine: Awaitable[_T_co]) -> _T_co:
         """
         Protect a :term:`coroutine` from being cancelled.
 
         The statement::
 
             res = await backend.ignore_cancellation(something())
 
@@ -694,15 +741,15 @@
 
         Parameters:
             delay: number of seconds to wait.
 
         Returns:
             a :term:`context manager`
         """
-        return _timeout_after(self, delay)
+        return _timeout_scope(self.move_on_after(delay))
 
     def timeout_at(self, deadline: float) -> AbstractContextManager[CancelScope]:
         """
         Returns a :term:`context manager` that can be used to limit the amount of time spent waiting on something.
 
         This function and :meth:`move_on_at` are similar in that both create a context manager with a given timeout,
         and if the timeout expires then both will cause ``backend.get_cancelled_exc_class()`` to be raised within the scope.
@@ -711,15 +758,15 @@
 
         Parameters:
             deadline: absolute time to stop waiting.
 
         Returns:
             a :term:`context manager`
         """
-        return _timeout_at(self, deadline)
+        return _timeout_scope(self.move_on_at(deadline))
 
     def move_on_after(self, delay: float) -> CancelScope:
         """
         Returns a new :class:`CancelScope` that can be used to limit the amount of time spent waiting on something.
         The deadline is set to now + `delay`.
 
         Example::
@@ -833,70 +880,43 @@
 
         Returns:
             A new task group.
         """
         raise NotImplementedError
 
     @abstractmethod
-    async def create_tcp_connection(
-        self,
-        host: str,
-        port: int,
-        *,
-        local_address: tuple[str, int] | None = ...,
-        happy_eyeballs_delay: float | None = ...,
-    ) -> transports.AsyncStreamTransport:
+    def get_current_task(self) -> TaskInfo:
         """
-        Opens a connection using the TCP/IP protocol.
-
-        Parameters:
-            host: The host IP/domain name.
-            port: Port of connection.
-            local_address: If given, is a ``(local_host, local_port)`` tuple used to bind the socket locally.
-            happy_eyeballs_delay: If given, is the "Connection Attempt Delay" as defined in :rfc:`8305`.
-
-        Raises:
-            ConnectionError: Cannot connect to `host` with the given `port`.
-            OSError: unrelated OS error occurred.
+        Return the current task.
 
         Returns:
-            A stream socket.
+            a representation of the current task.
         """
         raise NotImplementedError
 
     @abstractmethod
-    async def create_ssl_over_tcp_connection(
+    async def create_tcp_connection(
         self,
         host: str,
         port: int,
-        ssl_context: _typing_ssl.SSLContext,
         *,
-        server_hostname: str | None,
-        ssl_handshake_timeout: float,
-        ssl_shutdown_timeout: float,
         local_address: tuple[str, int] | None = ...,
         happy_eyeballs_delay: float | None = ...,
     ) -> transports.AsyncStreamTransport:
         """
-        Opens an SSL/TLS stream connection on top of the TCP/IP protocol.
+        Opens a connection using the TCP/IP protocol.
 
         Parameters:
             host: The host IP/domain name.
             port: Port of connection.
-            ssl_context: TLS connection configuration (see :mod:`ssl` module).
-            server_hostname: sets or overrides the hostname that the target server's certificate will be matched against.
-                             By default, `host` is used.
-            ssl_handshake_timeout: the time in seconds to wait for the TLS handshake to complete.
-            ssl_shutdown_timeout: the time in seconds to wait for the SSL shutdown to complete before aborting the connection.
             local_address: If given, is a ``(local_host, local_port)`` tuple used to bind the socket locally.
             happy_eyeballs_delay: If given, is the "Connection Attempt Delay" as defined in :rfc:`8305`.
 
         Raises:
             ConnectionError: Cannot connect to `host` with the given `port`.
-            ssl.SSLError: Error in the TLS handshake (invalid certificate, ciphers, etc.).
             OSError: unrelated OS error occurred.
 
         Returns:
             A stream socket.
         """
         raise NotImplementedError
 
@@ -918,50 +938,14 @@
 
         Returns:
             A stream socket.
         """
         raise NotImplementedError
 
     @abstractmethod
-    async def wrap_ssl_over_stream_socket_client_side(
-        self,
-        socket: _socket.socket,
-        ssl_context: _typing_ssl.SSLContext,
-        *,
-        server_hostname: str,
-        ssl_handshake_timeout: float,
-        ssl_shutdown_timeout: float,
-    ) -> transports.AsyncStreamTransport:
-        """
-        Wraps an already connected :data:`~socket.SOCK_STREAM` socket into an asynchronous stream socket in a SSL/TLS context.
-
-        Important:
-            The returned stream socket takes the ownership of `socket`.
-
-            You should use :meth:`AsyncStreamTransport.aclose` to close the socket.
-
-        Parameters:
-            socket: The socket to wrap.
-            ssl_context: TLS connection configuration (see :mod:`ssl` module).
-            server_hostname: sets the hostname that the target server's certificate will be matched against.
-            ssl_handshake_timeout: the time in seconds to wait for the TLS handshake to complete.
-            ssl_shutdown_timeout: the time in seconds to wait for the SSL shutdown to complete before aborting the connection.
-
-        Raises:
-            ConnectionError: TLS handshake failed to connect to the remote.
-            ssl.SSLError: Error in the TLS handshake (invalid certificate, ciphers, etc.).
-            OSError: unrelated OS error occurred.
-            ValueError: Invalid socket type or family.
-
-        Returns:
-            A stream socket.
-        """
-        raise NotImplementedError
-
-    @abstractmethod
     async def create_tcp_listeners(
         self,
         host: str | Sequence[str] | None,
         port: int,
         backlog: int,
         *,
         reuse_port: bool = ...,
@@ -991,59 +975,14 @@
 
         Returns:
             A sequence of listener sockets.
         """
         raise NotImplementedError
 
     @abstractmethod
-    async def create_ssl_over_tcp_listeners(
-        self,
-        host: str | Sequence[str] | None,
-        port: int,
-        backlog: int,
-        ssl_context: _typing_ssl.SSLContext,
-        *,
-        ssl_handshake_timeout: float,
-        ssl_shutdown_timeout: float,
-        reuse_port: bool = ...,
-    ) -> Sequence[transports.AsyncListener[transports.AsyncStreamTransport]]:
-        """
-        Opens listener sockets for TCP connections in a SSL/TLS context.
-
-        Parameters:
-            host: Can be set to several types which determine where the server would be listening:
-
-                  * If `host` is a string, the TCP server is bound to a single network interface specified by `host`.
-
-                  * If `host` is a sequence of strings, the TCP server is bound to all network interfaces specified by the sequence.
-
-                  * If `host` is :data:`None`, all interfaces are assumed and a list of multiple sockets will be returned
-                    (most likely one for IPv4 and another one for IPv6).
-            port: specify which port the server should listen on. If the value is ``0``, a random unused port will be selected
-                  (note that if `host` resolves to multiple network interfaces, a different random port will be selected
-                  for each interface).
-            backlog: is the maximum number of queued connections passed to :class:`~socket.socket.listen`.
-            ssl: can be set to an :class:`ssl.SSLContext` instance to enable TLS over the accepted connections.
-            ssl_handshake_timeout: (for a TLS connection) the time in seconds to wait for the TLS handshake to complete
-                                   before aborting the connection. ``60.0`` seconds if :data:`None` (default).
-            ssl_shutdown_timeout: the time in seconds to wait for the SSL shutdown to complete before aborting the connection.
-                                  ``30.0`` seconds if :data:`None` (default).
-            reuse_port: tells the kernel to allow this endpoint to be bound to the same port as other existing endpoints
-                        are bound to, so long as they all set this flag when being created.
-                        This option is not supported on Windows.
-
-        Raises:
-            OSError: unrelated OS error occurred.
-
-        Returns:
-            A sequence of listener sockets.
-        """
-        raise NotImplementedError
-
-    @abstractmethod
     async def create_udp_endpoint(
         self,
         remote_host: str,
         remote_port: int,
         *,
         local_address: tuple[str, int] | None = ...,
         family: int = ...,
@@ -1194,50 +1133,19 @@
             RuntimeError: not called in the event loop thread.
 
         Returns:
             a new thread portal.
         """
         raise NotImplementedError
 
-    @abstractmethod
-    async def wait_future(self, future: concurrent.futures.Future[_T_co]) -> _T_co:
-        """
-        Blocks until the future is done, and returns the result.
-
-        Cancellation handling:
-            In the case of cancellation, the rules follows what :class:`concurrent.futures.Future` defines:
-
-            * :meth:`wait_future` tries to cancel the given `future` (using :meth:`concurrent.futures.Future.cancel`)
-
-            * If the future has been effectively cancelled, the cancellation request is "accepted" and propagated.
-
-            * Otherwise, the cancellation request is "rejected" and discarded.
-              :meth:`wait_future` will block until `future` is done, and will ignore any further cancellation request.
-
-            * A coroutine awaiting a `future` in ``running`` state (:meth:`concurrent.futures.Future.running` returns :data:`True`)
-              cannot be cancelled.
-
-        Parameters:
-            future: The :class:`~concurrent.futures.Future` object to wait for.
-
-        Raises:
-            concurrent.futures.CancelledError: the future has been unexpectedly cancelled by an external code
-                                               (typically :meth:`concurrent.futures.Executor.shutdown`).
-            Exception: If ``future.exception()`` does not return :data:`None`, this exception is raised.
-
-        Returns:
-            Whatever returns ``future.result()``
-        """
-        raise NotImplementedError
-
-
-def _timeout_after(backend: AsyncBackend, delay: float) -> contextlib._GeneratorContextManager[CancelScope]:
-    return _timeout_at(backend, backend.current_time() + delay)
-
-
-@contextlib.contextmanager
-def _timeout_at(backend: AsyncBackend, deadline: float) -> Iterator[CancelScope]:
-    with backend.move_on_at(deadline) as scope:
-        yield scope
 
-    if scope.cancelled_caught():
-        raise TimeoutError("timed out")
+@dataclasses.dataclass(frozen=True, slots=True, weakref_slot=True)
+class _timeout_scope:
+    scope: CancelScope
+
+    def __enter__(self) -> CancelScope:
+        return self.scope.__enter__()
+
+    def __exit__(self, exc_type: type[BaseException] | None, exc_val: BaseException | None, exc_tb: TracebackType | None) -> None:
+        cancelled_caught = self.scope.__exit__(exc_type, exc_val, exc_tb)
+        if cancelled_caught:
+            raise TimeoutError("timed out")
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_async/backend/factory.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_async/backend/factory.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -18,100 +18,96 @@
 
 __all__ = ["AsyncBackendFactory", "current_async_backend"]
 
 import functools
 import threading
 from collections import deque
 from collections.abc import Callable
-from typing import Final, final
+from typing import ClassVar, Final, final
 
-from ....exceptions import UnsupportedOperation
-from ... import _lock
 from ..._final import runtime_final_class
 from . import _sniffio_helpers
 from .abc import AsyncBackend
 
 
 @final
 @runtime_final_class
 class AsyncBackendFactory:
-    __lock: Final[_lock.ForkSafeLock[threading.RLock]] = _lock.ForkSafeLock(threading.RLock)
-    __hooks: Final[deque[Callable[[str], AsyncBackend]]] = deque()
-    __instances: Final[dict[str, AsyncBackend]] = {}
+    __hooks: Final[deque[Callable[[str], AsyncBackend | None]]] = deque()
+    __thread_local_instances: ClassVar[threading.local] = threading.local()
 
     @classmethod
     def current(cls) -> AsyncBackend:
         name: str = _sniffio_helpers.current_async_library()
         return cls.__get_backend(name, error_msg_format="Running library {name!r} misses the backend implementation")
 
     @classmethod
     def get_backend(cls, name: str, /) -> AsyncBackend:
         return cls.__get_backend(name, error_msg_format="Unknown backend {name!r}")
 
     @classmethod
-    def push_factory_hook(cls, factory: Callable[[str], AsyncBackend], /) -> None:
+    def push_factory_hook(cls, factory: Callable[[str], AsyncBackend | None], /) -> None:
         if not callable(factory):
             raise TypeError(f"{factory!r} is not callable")
-        with cls.__lock.get():
-            cls.__hooks.appendleft(factory)
-            cls.invalidate_backends_cache()
+        if factory in cls.__hooks:
+            raise ValueError(f"{factory!r} is already registered")
+        cls.__hooks.appendleft(factory)
+        cls.__thread_local_instances = threading.local()
+
+    @classmethod
+    def remove_factory_hook(cls, factory: Callable[[str], AsyncBackend | None], /) -> None:
+        try:
+            cls.__hooks.remove(factory)
+        except ValueError:
+            pass
+        else:
+            cls.__thread_local_instances = threading.local()
 
     @classmethod
-    def push_backend_factory(cls, backend_name: str, factory: Callable[[], AsyncBackend]) -> None:
+    def backend_factory_hook(cls, backend_name: str, factory: Callable[[], AsyncBackend]) -> Callable[[str], AsyncBackend | None]:
         if not isinstance(backend_name, str):
             raise TypeError("backend_name: Expected a string")
         if backend_name.strip() != backend_name or not backend_name:
             raise ValueError("backend_name: Invalid value")
         if not callable(factory):
             raise TypeError(f"{factory!r} is not callable")
-        return cls.push_factory_hook(functools.partial(cls.__backend_factory_hook, backend_name, factory))
-
-    @classmethod
-    def invalidate_backends_cache(cls) -> None:
-        with cls.__lock.get():
-            cls.__instances.clear()
-
-    @classmethod
-    def remove_installed_hooks(cls) -> None:
-        with cls.__lock.get():
-            if cls.__hooks:
-                cls.invalidate_backends_cache()
-                cls.__hooks.clear()
+        return functools.partial(cls.__backend_factory_hook, backend_name, factory)
 
     @classmethod
     def __get_backend(cls, name: str, error_msg_format: str) -> AsyncBackend:
-        with cls.__lock.get():
-            try:
-                return cls.__instances[name]
-            except KeyError:
-                pass
-
-            backend_instance: AsyncBackend | None = None
-            for factory_hook in cls.__hooks:
-                try:
-                    backend_instance = factory_hook(name)
-                except UnsupportedOperation:
-                    continue
-
-                if not isinstance(backend_instance, AsyncBackend):
-                    raise TypeError(f"{factory_hook!r} did not return an AsyncBackend instance")
-                break
-
-            if backend_instance is None and name == "asyncio":
-                from ...std_asyncio import AsyncIOBackend
-
-                backend_instance = AsyncIOBackend()
-
+        try:
+            return getattr(cls.__thread_local_instances, name)
+        except AttributeError:
+            pass
+
+        backend_instance: AsyncBackend | None = None
+        for factory_hook in cls.__hooks:
+            backend_instance = factory_hook(name)
             if backend_instance is None:
-                raise NotImplementedError(error_msg_format.format(name=name))
+                continue
+            if not isinstance(backend_instance, AsyncBackend):
+                raise TypeError(f"{factory_hook!r} did not return an AsyncBackend instance")
+            break
+
+        if backend_instance is None:
+            match name:
+                case "asyncio":
+                    from ...std_asyncio import AsyncIOBackend
+
+                    backend_instance = AsyncIOBackend()
+                case _:
+                    raise NotImplementedError(error_msg_format.format(name=name))
 
-            cls.__instances[name] = backend_instance
-            return backend_instance
+        setattr(cls.__thread_local_instances, name, backend_instance)
+        return backend_instance
 
     @staticmethod
-    def __backend_factory_hook(backend_name: str, factory: Callable[[], AsyncBackend], name: str, /) -> AsyncBackend:
+    def __backend_factory_hook(backend_name: str, factory: Callable[[], AsyncBackend], name: str, /) -> AsyncBackend | None:
         if name != backend_name:
-            raise UnsupportedOperation(f"{name!r} backend is not implemented")
-        return factory()
+            return None
+        instance = factory()
+        if not isinstance(instance, AsyncBackend):
+            raise TypeError(f"{factory!r} did not return an AsyncBackend instance")
+        return instance
 
 
 current_async_backend = AsyncBackendFactory.current
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_async/backend/futures.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/futures.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -12,34 +12,37 @@
 # limitations under the License.
 #
 #
 """Asynchronous backend engine bindings with concurrent.futures module"""
 
 from __future__ import annotations
 
-__all__ = ["AsyncExecutor"]
+__all__ = ["AsyncExecutor", "unwrap_future"]
 
 import concurrent.futures
+import contextlib
 import contextvars
 import functools
+import threading
 from collections import deque
 from collections.abc import AsyncGenerator, Callable, Iterable
-from typing import TYPE_CHECKING, Any, ParamSpec, Self, TypeVar
+from typing import TYPE_CHECKING, Any, Generic, ParamSpec, Self, TypeVar
 
-from . import _sniffio_helpers
-from .factory import current_async_backend
+from .api_async.backend import _sniffio_helpers
+from .api_async.backend.factory import current_async_backend
 
 if TYPE_CHECKING:
     from types import TracebackType
 
 _P = ParamSpec("_P")
 _T = TypeVar("_T")
+_T_Executor = TypeVar("_T_Executor", bound=concurrent.futures.Executor, covariant=True)
 
 
-class AsyncExecutor:
+class AsyncExecutor(Generic[_T_Executor]):
     """
     Wraps a :class:`concurrent.futures.Executor` instance.
 
 
     For example, this code::
 
         from concurrent.futures import ProcessPoolExecutor, wait
@@ -53,39 +56,39 @@
     can be converted to::
 
         from concurrent.futures import ProcessPoolExecutor
 
         async def main() -> None:
             ...
 
-            async with AsyncExecutor(ProcessPoolExecutor()) as executor:
+            async with AsyncExecutor(ProcessPoolExecutor(), handle_contexts=False) as executor:
                 async with backend.create_task_group() as task_group:
-                    tasks = [task_group.start_soon(executor.run, pow, a, b) for a, b in [(3, 4), (12, 2), (6, 8)]]
+                    tasks = [await task_group.start(executor.run, pow, a, b) for a, b in [(3, 4), (12, 2), (6, 8)]]
                 results = [await t.join() for t in tasks]
     """
 
     __slots__ = ("__executor", "__handle_contexts", "__weakref__")
 
     def __init__(
         self,
-        executor: concurrent.futures.Executor,
+        executor: _T_Executor,
         *,
-        handle_contexts: bool = False,
+        handle_contexts: bool = True,
     ) -> None:
         """
         Parameters:
             executor: The executor instance to wrap.
-            handle_contexts: If :data:`True`, contexts (:class:`contextvars.Context`) are properly propagated to workers.
-                             Defaults to :data:`False` because not all executors support the use of contexts
+            handle_contexts: If :data:`True` (the default), contexts (:class:`contextvars.Context`) are properly propagated to
+                             workers. Set it to :data:`False` if the executor does not support the use of contexts
                              (e.g. :class:`concurrent.futures.ProcessPoolExecutor`).
         """
         if not isinstance(executor, concurrent.futures.Executor):
             raise TypeError("Invalid executor type")
 
-        self.__executor: concurrent.futures.Executor = executor
+        self.__executor: _T_Executor = executor
         self.__handle_contexts: bool = bool(handle_contexts)
 
     async def __aenter__(self) -> Self:
         return self
 
     async def __aexit__(
         self,
@@ -131,15 +134,15 @@
         Returns an asynchronous iterator equivalent to ``map(fn, iter)``.
 
         Example::
 
             def pow_50(x):
                 return x**50
 
-            async with AsyncExecutor(ProcessPoolExecutor()) as executor:
+            async with AsyncExecutor(ProcessPoolExecutor(), handle_contexts=False) as executor:
                 results = [result async for result in executor.map(pow_50, (1, 4, 12))]
 
         Parameters:
             func: A callable that will take as many arguments as there are passed `iterables`.
             iterables: iterables yielding arguments for `func`.
 
         Raises:
@@ -196,17 +199,85 @@
     def _setup_func(self, func: Callable[_P, _T]) -> Callable[_P, _T]:
         if self.__handle_contexts:
             ctx = contextvars.copy_context()
             _sniffio_helpers.setup_sniffio_contextvar(ctx, None)
             func = functools.partial(ctx.run, func)
         return func
 
+    @property
+    def wrapped(self) -> _T_Executor:
+        """The wrapped :class:`~concurrent.futures.Executor` instance. Read-only attribute."""
+        return self.__executor
+
+
+async def unwrap_future(future: concurrent.futures.Future[_T]) -> _T:
+    """
+    Blocks until the future is done, and returns the result.
+
+    Cancellation handling:
+
+        * :meth:`unwrap_future` tries to cancel the given `future` (using :meth:`concurrent.futures.Future.cancel`)
+
+            * If the future has been effectively cancelled, the cancellation request is "accepted" and propagated.
+
+            * Otherwise, the cancellation request is "rejected": :meth:`unwrap_future` will block until `future` is done,
+              and will ignore any further cancellation request.
+
+        * A coroutine awaiting a `future` in ``running`` state (:meth:`concurrent.futures.Future.running` returns :data:`True`)
+          cannot be cancelled.
+
+    Parameters:
+        future: The future object to wait for.
+
+    Raises:
+        concurrent.futures.CancelledError: the future has been unexpectedly cancelled by an external code
+                                           (typically :meth:`concurrent.futures.Executor.shutdown`).
+        Exception: If ``future.exception()`` does not return :data:`None`, this exception is raised.
+
+    Returns:
+        Whatever returns ``future.result()``
+    """
+    try:
+        backend = current_async_backend()
+        event_loop_thread_id = threading.get_ident()
+        done_event = backend.create_event()
+
+        async with backend.create_threads_portal() as portal:
+
+            def on_fut_done(future: concurrent.futures.Future[_T]) -> None:
+                with contextlib.suppress(RuntimeError):
+                    if threading.get_ident() == event_loop_thread_id:
+                        done_event.set()
+                    else:
+                        portal.run_sync_soon(done_event.set)
+
+            future.add_done_callback(on_fut_done)
+
+            try:
+                await done_event.wait()
+            except backend.get_cancelled_exc_class():
+                if future.cancel():
+                    raise
+
+                # If future.cancel() failed, that means future.set_running_or_notify_cancel() has been called
+                # and set future in RUNNING state.
+                # This future cannot be cancelled anymore, therefore it must be awaited.
+                await backend.ignore_cancellation(done_event.wait())
+            else:
+                if future.cancelled():
+                    # Task cancellation prevails over future cancellation
+                    await backend.coro_yield()
+
+        return future.result(timeout=0)
+    finally:
+        del future
+
 
 async def _result_or_cancel(future: concurrent.futures.Future[_T]) -> _T:
     try:
         try:
-            return await current_async_backend().wait_future(future)
+            return await unwrap_future(future)
         finally:
             future.cancel()
     finally:
         # Break a reference cycle with the exception in future._exception
         del future
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_async/endpoints/__init__.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_async/servers/__init__.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,19 +1,19 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 #
-"""Low-level asynchronous endpoints module"""
+"""Low-level asynchronous servers module"""
 
 from __future__ import annotations
 
 __all__ = []  # type: list[str]
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_async/endpoints/datagram.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_async/endpoints/datagram.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -18,21 +18,21 @@
 
 __all__ = ["AsyncDatagramEndpoint"]
 
 from collections.abc import Callable, Mapping
 from typing import Any, Generic, TypeGuard
 
 from .... import protocol as protocol_module
-from ...._typevars import _ReceivedPacketT, _SentPacketT
+from ...._typevars import _T_ReceivedPacket, _T_SentPacket
 from ....exceptions import DatagramProtocolParseError, UnsupportedOperation
 from ... import _utils, typed_attr
 from ..transports import abc as transports
 
 
-class AsyncDatagramEndpoint(typed_attr.TypedAttributeProvider, Generic[_SentPacketT, _ReceivedPacketT]):
+class AsyncDatagramEndpoint(typed_attr.TypedAttributeProvider, Generic[_T_SentPacket, _T_ReceivedPacket]):
     """
     A communication endpoint based on unreliable packets of data.
     """
 
     __slots__ = (
         "__transport",
         "__is_read_transport",
@@ -41,18 +41,18 @@
         "__send_guard",
         "__recv_guard",
         "__weakref__",
     )
 
     def __init__(
         self,
-        transport: transports.AsyncDatagramTransport
-        | transports.AsyncDatagramReadTransport
-        | transports.AsyncDatagramWriteTransport,
-        protocol: protocol_module.DatagramProtocol[_SentPacketT, _ReceivedPacketT],
+        transport: (
+            transports.AsyncDatagramTransport | transports.AsyncDatagramReadTransport | transports.AsyncDatagramWriteTransport
+        ),
+        protocol: protocol_module.DatagramProtocol[_T_SentPacket, _T_ReceivedPacket],
     ) -> None:
         """
         Parameters:
             transport: The data transport to use.
             protocol: The :term:`protocol object` to use.
         """
 
@@ -60,15 +60,15 @@
             raise TypeError(f"Expected an AsyncDatagramTransport object, got {transport!r}")
         if not isinstance(protocol, protocol_module.DatagramProtocol):
             raise TypeError(f"Expected a DatagramProtocol object, got {protocol!r}")
 
         self.__is_read_transport: bool = isinstance(transport, transports.AsyncDatagramReadTransport)
         self.__is_write_transport: bool = isinstance(transport, transports.AsyncDatagramWriteTransport)
         self.__transport: transports.AsyncDatagramReadTransport | transports.AsyncDatagramWriteTransport = transport
-        self.__protocol: protocol_module.DatagramProtocol[_SentPacketT, _ReceivedPacketT] = protocol
+        self.__protocol: protocol_module.DatagramProtocol[_T_SentPacket, _T_ReceivedPacket] = protocol
         self.__send_guard: _utils.ResourceGuard = _utils.ResourceGuard("another task is currently sending data on this endpoint")
         self.__recv_guard: _utils.ResourceGuard = _utils.ResourceGuard("another task is currently receving data on this endpoint")
 
     def is_closing(self) -> bool:
         """
         Checks if the endpoint is closed or in the process of being closed.
 
@@ -79,15 +79,15 @@
 
     async def aclose(self) -> None:
         """
         Closes the endpoint.
         """
         await self.__transport.aclose()
 
-    async def send_packet(self, packet: _SentPacketT) -> None:
+    async def send_packet(self, packet: _T_SentPacket) -> None:
         """
         Sends `packet` to the remote endpoint.
 
         Warning:
             In the case of a cancellation, it is impossible to know if all the packet data has been sent.
 
         Parameters:
@@ -105,15 +105,15 @@
             except Exception as exc:
                 raise RuntimeError("protocol.make_datagram() crashed") from exc
             finally:
                 del packet
 
             await transport.send(datagram)
 
-    async def recv_packet(self) -> _ReceivedPacketT:
+    async def recv_packet(self) -> _T_ReceivedPacket:
         """
         Waits for a new packet from the remote endpoint.
 
         Raises:
             DatagramProtocolParseError: invalid data received.
 
         Returns:
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_async/endpoints/stream.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_sync/endpoints/stream.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,172 +1,230 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 #
-"""Low-level asynchronous endpoints module"""
+"""Low-level endpoints module"""
 
 from __future__ import annotations
 
-__all__ = ["AsyncStreamEndpoint"]
+__all__ = ["StreamEndpoint"]
 
 import dataclasses
+import errno as _errno
+import math
+import warnings
 from collections.abc import Callable, Mapping
-from typing import Any, Generic
+from typing import Any, Generic, Literal, assert_never
 
 from .... import protocol as protocol_module
-from ...._typevars import _ReceivedPacketT, _SentPacketT
+from ...._typevars import _T_ReceivedPacket, _T_SentPacket
 from ....exceptions import UnsupportedOperation
+from ....warnings import ManualBufferAllocationWarning
 from ... import _stream, _utils, typed_attr
 from ..transports import abc as transports
 
 
-class AsyncStreamEndpoint(typed_attr.TypedAttributeProvider, Generic[_SentPacketT, _ReceivedPacketT]):
+class StreamEndpoint(typed_attr.TypedAttributeProvider, Generic[_T_SentPacket, _T_ReceivedPacket]):
     """
     A communication endpoint based on continuous stream data transport.
     """
 
     __slots__ = (
         "__transport",
         "__sender",
         "__receiver",
-        "__send_guard",
-        "__recv_guard",
         "__eof_sent",
         "__weakref__",
     )
 
     def __init__(
         self,
-        transport: transports.AsyncStreamTransport | transports.AsyncStreamReadTransport | transports.AsyncStreamWriteTransport,
-        protocol: protocol_module.StreamProtocol[_SentPacketT, _ReceivedPacketT],
+        transport: transports.StreamTransport | transports.StreamReadTransport | transports.StreamWriteTransport,
+        protocol: protocol_module.StreamProtocol[_T_SentPacket, _T_ReceivedPacket],
         max_recv_size: int,
+        *,
+        manual_buffer_allocation: Literal["try", "no", "force"] = "try",
+        manual_buffer_allocation_warning_stacklevel: int = 2,
     ) -> None:
         """
         Parameters:
             transport: The data transport to use.
             protocol: The :term:`protocol object` to use.
             max_recv_size: Read buffer size.
+            manual_buffer_allocation: Select whether or not to enable the manual buffer allocation system:
+
+                                      * ``"try"``: (the default) will use the buffer API if the transport and protocol support it,
+                                        and fall back to the default implementation otherwise.
+                                        Emits a :exc:`.ManualBufferAllocationWarning` if only the transport does not support it.
+
+                                      * ``"no"``: does not use the buffer API, even if they both support it.
+
+                                      * ``"force"``: requires the buffer API. Raises :exc:`.UnsupportedOperation` if it fails and
+                                        no warnings are emitted.
+            manual_buffer_allocation_warning_stacklevel: ``stacklevel`` parameter of :func:`warnings.warn` when emitting
+                                                         :exc:`.ManualBufferAllocationWarning`.
         """
 
-        if not isinstance(transport, (transports.AsyncStreamReadTransport, transports.AsyncStreamWriteTransport)):
-            raise TypeError(f"Expected an AsyncStreamTransport object, got {transport!r}")
+        if not isinstance(transport, (transports.StreamReadTransport, transports.StreamWriteTransport)):
+            raise TypeError(f"Expected a StreamTransport object, got {transport!r}")
         if not isinstance(max_recv_size, int) or max_recv_size <= 0:
             raise ValueError("'max_recv_size' must be a strictly positive integer")
+        if manual_buffer_allocation not in ("try", "no", "force"):
+            raise ValueError('"manual_buffer_allocation" must be "try", "no" or "force"')
+        manual_buffer_allocation_warning_stacklevel = max(manual_buffer_allocation_warning_stacklevel, 2)
 
-        self.__sender: _DataSenderImpl[_SentPacketT] | None = None
-        if isinstance(transport, transports.AsyncStreamWriteTransport):
+        self.__transport: transports.StreamReadTransport | transports.StreamWriteTransport = transport
+        self.__eof_sent: bool = False
+
+        self.__sender: _DataSenderImpl[_T_SentPacket] | None = None
+        if isinstance(transport, transports.StreamWriteTransport):
             self.__sender = _DataSenderImpl(transport, _stream.StreamDataProducer(protocol))
 
-        self.__receiver: _DataReceiverImpl[_ReceivedPacketT] | _BufferedReceiverImpl[_ReceivedPacketT] | None = None
-        if isinstance(transport, transports.AsyncStreamReadTransport):
-            try:
-                if not isinstance(transport, transports.AsyncBufferedStreamReadTransport):
-                    raise UnsupportedOperation
-                self.__receiver = _BufferedReceiverImpl(transport, _stream.BufferedStreamDataConsumer(protocol, max_recv_size))
-            except UnsupportedOperation:
-                self.__receiver = _DataReceiverImpl(transport, _stream.StreamDataConsumer(protocol), max_recv_size)
-
-        self.__transport: transports.AsyncStreamReadTransport | transports.AsyncStreamWriteTransport = transport
-        self.__send_guard: _utils.ResourceGuard = _utils.ResourceGuard("another task is currently sending data on this endpoint")
-        self.__recv_guard: _utils.ResourceGuard = _utils.ResourceGuard("another task is currently receving data on this endpoint")
-        self.__eof_sent: bool = False
+        self.__receiver: _DataReceiverImpl[_T_ReceivedPacket] | _BufferedReceiverImpl[_T_ReceivedPacket] | None = None
+        if isinstance(transport, transports.StreamReadTransport):
+            match manual_buffer_allocation:
+                case "try" | "force":
+                    try:
+                        buffered_consumer = _stream.BufferedStreamDataConsumer(protocol, max_recv_size)
+                        if not isinstance(transport, transports.BufferedStreamReadTransport):
+                            msg = f"The transport implementation {transport!r} does not implement BufferedStreamReadTransport interface"
+                            if manual_buffer_allocation == "try":
+                                warnings.warn(
+                                    msg,
+                                    category=ManualBufferAllocationWarning,
+                                    stacklevel=manual_buffer_allocation_warning_stacklevel,
+                                )
+                            raise UnsupportedOperation(msg)
+                        self.__receiver = _BufferedReceiverImpl(transport, buffered_consumer)
+                    except UnsupportedOperation:
+                        if manual_buffer_allocation == "force":
+                            raise
+                        self.__receiver = _DataReceiverImpl(transport, _stream.StreamDataConsumer(protocol), max_recv_size)
+                case "no":
+                    self.__receiver = _DataReceiverImpl(transport, _stream.StreamDataConsumer(protocol), max_recv_size)
+                case _:  # pragma: no cover
+                    assert_never(manual_buffer_allocation)
+            if self.__receiver is None:
+                raise AssertionError("self.__receiver should be set.")
+
+    def __del__(self) -> None:
+        try:
+            if not self.__transport.is_closed():
+                self.close()
+        except AttributeError:
+            return
 
-    def is_closing(self) -> bool:
+    def is_closed(self) -> bool:
         """
-        Checks if the endpoint is closed or in the process of being closed.
+        Checks if :meth:`close` has been called.
 
         Returns:
             :data:`True` if the endpoint is closed.
         """
-        return self.__transport.is_closing()
+        return self.__transport.is_closed()
 
-    async def aclose(self) -> None:
+    def close(self) -> None:
         """
         Closes the endpoint.
         """
-        await self.__transport.aclose()
+        self.__transport.close()
         if self.__receiver is not None:
             self.__receiver.clear()
         if self.__sender is not None:
             self.__sender.clear()
 
-    async def send_packet(self, packet: _SentPacketT) -> None:
+    def send_packet(self, packet: _T_SentPacket, *, timeout: float | None = None) -> None:
         """
         Sends `packet` to the remote endpoint.
 
+        If `timeout` is not :data:`None`, the entire send operation will take at most `timeout` seconds.
+
         Warning:
-            In the case of a cancellation, it is impossible to know if all the packet data has been sent.
+            A timeout on a send operation is unusual unless you have a SSL/TLS context.
+
+            In the case of a timeout, it is impossible to know if all the packet data has been sent.
             This would leave the connection in an inconsistent state.
 
         Parameters:
             packet: the Python object to send.
+            timeout: the allowed time (in seconds) for blocking operations.
 
         Raises:
+            TimeoutError: the send operation does not end up after `timeout` seconds.
             RuntimeError: :meth:`send_eof` has been called earlier.
         """
-        with self.__send_guard:
-            if self.__eof_sent:
-                raise RuntimeError("send_eof() has been called earlier")
+        if self.__eof_sent:
+            raise RuntimeError("send_eof() has been called earlier")
 
-            sender = self.__sender
+        sender = self.__sender
 
-            if sender is None:
-                raise UnsupportedOperation("transport does not support sending data")
+        if sender is None:
+            raise UnsupportedOperation("transport does not support sending data")
 
-            return await sender.send(packet)
+        if timeout is None:
+            timeout = math.inf
 
-    async def send_eof(self) -> None:
+        return sender.send(packet, timeout)
+
+    def send_eof(self) -> None:
         """
         Close the write end of the stream after the buffered write data is flushed.
 
         This method does nothing if the endpoint is closed.
 
         Can be safely called multiple times.
         """
-        with self.__send_guard:
-            if self.__eof_sent:
-                return
+        if self.__eof_sent:
+            return
 
-            sender = self.__sender
+        sender = self.__sender
 
-            if sender is None or not isinstance(sender.transport, transports.AsyncStreamTransport):
-                raise UnsupportedOperation("transport does not support sending EOF")
+        if sender is None or not isinstance(sender.transport, transports.StreamTransport):
+            raise UnsupportedOperation("transport does not support sending EOF")
 
-            await sender.transport.send_eof()
-            self.__eof_sent = True
-            sender.clear()
+        sender.transport.send_eof()
+        self.__eof_sent = True
+        sender.clear()
 
-    async def recv_packet(self) -> _ReceivedPacketT:
+    def recv_packet(self, *, timeout: float | None = None) -> _T_ReceivedPacket:
         """
         Waits for a new packet to arrive from the remote endpoint.
 
+        If `timeout` is not :data:`None`, the entire receive operation will take at most `timeout` seconds.
+
+        Parameters:
+            timeout: the allowed time (in seconds) for blocking operations.
+
         Raises:
+            TimeoutError: the receive operation does not end up after `timeout` seconds.
             EOFError: the read end of the stream is closed.
             StreamProtocolParseError: invalid data received.
 
         Returns:
             the received packet.
         """
-        with self.__recv_guard:
-            receiver = self.__receiver
+        receiver = self.__receiver
+
+        if receiver is None:
+            raise UnsupportedOperation("transport does not support receiving data")
 
-            if receiver is None:
-                raise UnsupportedOperation("transport does not support receiving data")
+        if timeout is None:
+            timeout = math.inf
 
-            return await receiver.receive()
+        return receiver.receive(timeout)
 
     @property
     def max_recv_size(self) -> int:
         """Read buffer size. Read-only attribute."""
         receiver = self.__receiver
         if receiver is None:
             return 0
@@ -174,82 +232,101 @@
 
     @property
     def extra_attributes(self) -> Mapping[Any, Callable[[], Any]]:
         return self.__transport.extra_attributes
 
 
 @dataclasses.dataclass(slots=True)
-class _DataSenderImpl(Generic[_SentPacketT]):
-    transport: transports.AsyncStreamWriteTransport
-    producer: _stream.StreamDataProducer[_SentPacketT]
+class _DataSenderImpl(Generic[_T_SentPacket]):
+    transport: transports.StreamWriteTransport
+    producer: _stream.StreamDataProducer[_T_SentPacket]
 
     def clear(self) -> None:
-        self.producer.clear()
+        pass
 
-    async def send(self, packet: _SentPacketT) -> None:
-        self.producer.enqueue(packet)
-        return await self.transport.send_all_from_iterable(self.producer)
+    def send(self, packet: _T_SentPacket, timeout: float) -> None:
+        return self.transport.send_all_from_iterable(self.producer.generate(packet), timeout)
 
 
 @dataclasses.dataclass(slots=True)
-class _DataReceiverImpl(Generic[_ReceivedPacketT]):
-    transport: transports.AsyncStreamReadTransport
-    consumer: _stream.StreamDataConsumer[_ReceivedPacketT]
+class _DataReceiverImpl(Generic[_T_ReceivedPacket]):
+    transport: transports.StreamReadTransport
+    consumer: _stream.StreamDataConsumer[_T_ReceivedPacket]
     max_recv_size: int
     _eof_reached: bool = dataclasses.field(init=False, default=False)
 
     def clear(self) -> None:
         self.consumer.clear()
 
-    async def receive(self) -> _ReceivedPacketT:
-        transport = self.transport
+    def receive(self, timeout: float) -> _T_ReceivedPacket:
         consumer = self.consumer
+        try:
+            return consumer.next(None)
+        except StopIteration:
+            pass
+
+        transport = self.transport
         bufsize: int = self.max_recv_size
 
-        while True:
+        while not self._eof_reached:
+            with _utils.ElapsedTime() as elapsed:
+                chunk: bytes = transport.recv(bufsize, timeout)
+            if not chunk:
+                self._eof_reached = True
+                continue
             try:
-                return next(consumer)
+                return consumer.next(chunk)
             except StopIteration:
-                pass
-            if not self._eof_reached:
-                chunk: bytes = await transport.recv(bufsize)
-                if chunk:
-                    consumer.feed(chunk)
-                else:
-                    self._eof_reached = True
+                if timeout > 0:
+                    timeout = elapsed.recompute_timeout(timeout)
+                elif len(chunk) < bufsize:
+                    break
+            finally:
                 del chunk
-            if self._eof_reached:
-                raise EOFError("end-of-stream")
+        # Loop break
+        if self._eof_reached:
+            raise EOFError("end-of-stream")
+        raise _utils.error_from_errno(_errno.ETIMEDOUT)
 
 
 @dataclasses.dataclass(slots=True)
-class _BufferedReceiverImpl(Generic[_ReceivedPacketT]):
-    transport: transports.AsyncBufferedStreamReadTransport
-    consumer: _stream.BufferedStreamDataConsumer[_ReceivedPacketT]
+class _BufferedReceiverImpl(Generic[_T_ReceivedPacket]):
+    transport: transports.BufferedStreamReadTransport
+    consumer: _stream.BufferedStreamDataConsumer[_T_ReceivedPacket]
     _eof_reached: bool = dataclasses.field(init=False, default=False)
 
     @property
     def max_recv_size(self) -> int:
         # Ensure buffer is allocated
         self.consumer.get_write_buffer()
         return self.consumer.buffer_size
 
     def clear(self) -> None:
         self.consumer.clear()
 
-    async def receive(self) -> _ReceivedPacketT:
-        transport = self.transport
+    def receive(self, timeout: float) -> _T_ReceivedPacket:
         consumer = self.consumer
+        try:
+            return consumer.next(None)
+        except StopIteration:
+            pass
 
-        while True:
+        transport = self.transport
+        while not self._eof_reached:
+            with memoryview(consumer.get_write_buffer()) as buffer:
+                bufsize: int = buffer.nbytes
+                with _utils.ElapsedTime() as elapsed:
+                    nbytes: int = transport.recv_into(buffer, timeout)
+            if not nbytes:
+                self._eof_reached = True
+                continue
             try:
-                return next(consumer)
+                return consumer.next(nbytes)
             except StopIteration:
-                pass
-            if not self._eof_reached:
-                nbytes: int = await transport.recv_into(consumer.get_write_buffer())
-                if nbytes:
-                    consumer.buffer_updated(nbytes)
-                else:
-                    self._eof_reached = True
-            if self._eof_reached:
-                raise EOFError("end-of-stream")
+                if timeout > 0:
+                    timeout = elapsed.recompute_timeout(timeout)
+                elif nbytes < bufsize:
+                    break
+        # Loop break
+        if self._eof_reached:
+            raise EOFError("end-of-stream")
+        raise _utils.error_from_errno(_errno.ETIMEDOUT)
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_async/servers/__init__.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_sync/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,19 +1,19 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 #
-"""Low-level asynchronous servers module"""
+"""Low-level synchronous API module"""
 
 from __future__ import annotations
 
 __all__ = []  # type: list[str]
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_async/servers/datagram.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_async/servers/datagram.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -12,61 +12,65 @@
 # limitations under the License.
 #
 #
 """Low-level asynchronous datagram servers module"""
 
 from __future__ import annotations
 
-__all__ = ["AsyncDatagramServer"]
+__all__ = ["AsyncDatagramServer", "DatagramClientContext"]
 
 import contextlib
 import contextvars
+import dataclasses
 import enum
-import operator
-from collections import Counter, defaultdict, deque
-from collections.abc import AsyncGenerator, AsyncIterator, Callable, Hashable, Iterator, Mapping
-from types import TracebackType
-from typing import Any, Generic, NoReturn, Self, TypeVar, assert_never
+import weakref
+from collections import deque
+from collections.abc import AsyncGenerator, Callable, Hashable, Mapping
+from contextlib import AsyncExitStack, ExitStack
+from typing import Any, Generic, NoReturn, TypeVar
 
 from .... import protocol as protocol_module
-from ...._typevars import _RequestT, _ResponseT
+from ...._typevars import _T_Request, _T_Response
 from ....exceptions import DatagramProtocolParseError
-from ... import _asyncgen, _utils, typed_attr
+from ... import _utils, typed_attr
+from ..._asyncgen import AsyncGenAction, SendAction, ThrowAction
 from ..backend.abc import AsyncBackend, ICondition, ILock, TaskGroup
 from ..backend.factory import current_async_backend
 from ..transports import abc as transports
 
 _T_Address = TypeVar("_T_Address", bound=Hashable)
-_KT = TypeVar("_KT")
-_VT = TypeVar("_VT")
 
 
-class AsyncDatagramServer(typed_attr.TypedAttributeProvider, Generic[_RequestT, _ResponseT, _T_Address]):
+@dataclasses.dataclass(frozen=True, unsafe_hash=True, slots=True, weakref_slot=True)
+class DatagramClientContext(Generic[_T_Response, _T_Address]):
+    address: _T_Address
+    server: AsyncDatagramServer[Any, _T_Response, _T_Address]
+
+
+class AsyncDatagramServer(typed_attr.TypedAttributeProvider, Generic[_T_Request, _T_Response, _T_Address]):
     __slots__ = (
         "__listener",
         "__protocol",
-        "__client_manager",
         "__sendto_lock",
         "__serve_guard",
         "__weakref__",
     )
 
     def __init__(
         self,
         listener: transports.AsyncDatagramListener[_T_Address],
-        protocol: protocol_module.DatagramProtocol[_ResponseT, _RequestT],
+        protocol: protocol_module.DatagramProtocol[_T_Response, _T_Request],
     ) -> None:
         if not isinstance(listener, transports.AsyncDatagramListener):
             raise TypeError(f"Expected an AsyncDatagramListener object, got {listener!r}")
         if not isinstance(protocol, protocol_module.DatagramProtocol):
             raise TypeError(f"Expected a DatagramProtocol object, got {protocol!r}")
 
         self.__listener: transports.AsyncDatagramListener[_T_Address] = listener
-        self.__protocol: protocol_module.DatagramProtocol[_ResponseT, _RequestT] = protocol
-        self.__client_manager: _ClientManager[_T_Address] = _ClientManager(current_async_backend())
+        self.__protocol: protocol_module.DatagramProtocol[_T_Response, _T_Request] = protocol
         self.__sendto_lock: ILock = current_async_backend().create_lock()
         self.__serve_guard: _utils.ResourceGuard = _utils.ResourceGuard("another task is currently receiving datagrams")
 
     def is_closing(self) -> bool:
         """
         Checks if the server is closed or in the process of being closed.
 
@@ -77,286 +81,263 @@
 
     async def aclose(self) -> None:
         """
         Closes the server.
         """
         await self.__listener.aclose()
 
-    async def send_packet_to(self, packet: _ResponseT, address: _T_Address) -> None:
+    async def send_packet_to(self, packet: _T_Response, address: _T_Address) -> None:
         """
         Sends `packet` to the remote endpoint `address`.
 
         Warning:
             In the case of a cancellation, it is impossible to know if all the packet data has been sent.
 
         Parameters:
             packet: the Python object to send.
             address: the remote endpoint address.
         """
-        with self.__client_manager.send_guard(address):
-            async with self.__sendto_lock:
-                listener = self.__listener
-                protocol = self.__protocol
-
-                try:
-                    datagram: bytes = protocol.make_datagram(packet)
-                except Exception as exc:
-                    raise RuntimeError("protocol.make_datagram() crashed") from exc
-                finally:
-                    del packet
-                try:
-                    await listener.send_to(datagram, address)
-                finally:
-                    del datagram
+        try:
+            datagram: bytes = self.__protocol.make_datagram(packet)
+        except Exception as exc:
+            raise RuntimeError("protocol.make_datagram() crashed") from exc
+        async with self.__sendto_lock:
+            await self.__listener.send_to(datagram, address)
 
     async def serve(
         self,
-        datagram_received_cb: Callable[[_T_Address, Self], AsyncGenerator[None, _RequestT]],
-        task_group: TaskGroup,
+        datagram_received_cb: Callable[
+            [DatagramClientContext[_T_Response, _T_Address]], AsyncGenerator[float | None, _T_Request]
+        ],
+        task_group: TaskGroup | None = None,
     ) -> NoReturn:
         with self.__serve_guard:
-            client_coroutine = self.__client_coroutine
-            client_manager = self.__client_manager
-            backend = current_async_backend()
             listener = self.__listener
+            backend = current_async_backend()
+            client_cache: weakref.WeakValueDictionary[_T_Address, _ClientToken[_T_Response, _T_Address]]
+            client_cache = weakref.WeakValueDictionary()
+            default_context = contextvars.copy_context()
+
+            async with AsyncExitStack() as stack:
+                if task_group is None:
+                    task_group = await stack.enter_async_context(backend.create_task_group())
 
-            async def handler(datagram: bytes, address: _T_Address, /) -> None:
-                with client_manager.datagram_queue(address) as datagram_queue:
-                    datagram_queue.append(datagram)
-
-                    # client_coroutine() is running (or will be run) if datagram_queue was not empty
-                    # Therefore, start a new task only if there was no previous datagrams
-                    if len(datagram_queue) > 1:
-                        return
+                # The responsibility for ordering datagram reception is shifted to the listener.
+                async def handler(datagram: bytes, address: _T_Address, /) -> None:
+                    try:
+                        client = client_cache[address]
+                    except KeyError:
+                        client_cache[address] = client = _ClientToken(DatagramClientContext(address, self), _ClientData(backend))
+                        new_client_task = True
+                    else:
+                        new_client_task = client.data.state is None
+
+                    if new_client_task:
+                        client.data.mark_pending()
+                        await self.__client_coroutine(datagram_received_cb, datagram, client, task_group, default_context)
+                    else:
+                        await client.data.push_datagram(datagram)
 
-                del datagram_queue, datagram
+                await listener.serve(handler, task_group)
 
-                client_state = client_manager.client_state(address)
-                match client_state:
-                    case None:
-                        # Start a new task
-                        await client_coroutine(datagram_received_cb, address, task_group)
-                    case _ClientState.TASK_WAITING:
-                        # Wake up the idle task
-                        async with client_manager.lock(address) as condition:
-                            condition.notify()
-                    case _ClientState.TASK_RUNNING:
-                        # Do nothing
-                        pass
-                    case _:  # pragma: no cover
-                        assert_never(client_state)
-
-            while True:
-                datagram, address = await listener.recv_from()
-                task_group.start_soon(handler, datagram, address)
-                del datagram, address
-                await backend.cancel_shielded_coro_yield()
+            raise AssertionError("Expected code to be unreachable.")
 
     async def __client_coroutine(
         self,
-        datagram_received_cb: Callable[[_T_Address, Self], AsyncGenerator[None, _RequestT]],
-        address: _T_Address,
+        datagram_received_cb: Callable[
+            [DatagramClientContext[_T_Response, _T_Address]], AsyncGenerator[float | None, _T_Request]
+        ],
+        datagram: bytes,
+        client: _ClientToken[_T_Response, _T_Address],
         task_group: TaskGroup,
+        default_context: contextvars.Context,
     ) -> None:
-        client_manager = self.__client_manager
-
-        async with contextlib.AsyncExitStack() as client_exit_stack:
-            condition = await client_exit_stack.enter_async_context(client_manager.lock(address))
-
-            datagram_queue: deque[bytes] = client_exit_stack.enter_context(client_manager.datagram_queue(address))
-            self.__check_datagram_queue_not_empty(datagram_queue)
-
-            # This block must not have any asynchronous function calls or add any asynchronous callbacks/contexts to the exit stack.
-            client_exit_stack.enter_context(client_manager.set_client_state(address, _ClientState.TASK_RUNNING))
-            client_exit_stack.callback(
-                self.__enqueue_task_at_end,
-                datagram_received_cb=datagram_received_cb,
-                address=address,
-                task_group=task_group,
-                datagram_queue=datagram_queue,
-                default_context=contextvars.copy_context(),
-            )
-            client_exit_stack.push(_utils.prepend_argument(datagram_queue)(self.__clear_queue_on_error))
-            ########################################################################################################################
+        client_data = client.data
+        async with client_data.task_lock:
+            with ExitStack() as exit_stack:
+                #####################################################################################################
+                # CRITICAL SECTION
+                # This block must not have any asynchronous function calls
+                # or add any asynchronous callbacks/contexts to the exit stack.
+                client_data.mark_running()
+                exit_stack.callback(
+                    self.__on_task_done,
+                    datagram_received_cb=datagram_received_cb,
+                    client=client,
+                    task_group=task_group,
+                    default_context=default_context,
+                )
+                #####################################################################################################
 
-            request_handler_generator = datagram_received_cb(address, self)
+                request_handler_generator = datagram_received_cb(client.ctx)
+                timeout: float | None
 
-            del client_exit_stack, datagram_received_cb
-
-            async with contextlib.aclosing(request_handler_generator):
                 try:
+                    # Ignore sent timeout here, we already have the datagram.
                     await anext(request_handler_generator)
                 except StopAsyncIteration:
-                    del datagram_queue[0]
                     return
-
-                protocol = self.__protocol
-                action: _asyncgen.AsyncGenAction[None, _RequestT]
-                while True:
+                else:
+                    action: AsyncGenAction[_T_Request] = self.__parse_datagram(datagram, self.__protocol)
                     try:
-                        if not datagram_queue:
-                            with client_manager.set_client_state(address, _ClientState.TASK_WAITING):
-                                await condition.wait()
-                            self.__check_datagram_queue_not_empty(datagram_queue)
-                        datagram = datagram_queue.popleft()
-                        try:
-                            request = protocol.build_packet_from_datagram(datagram)
-                        except DatagramProtocolParseError:
-                            raise
-                        except Exception as exc:
-                            raise RuntimeError("protocol.build_packet_from_datagram() crashed") from exc
-                        else:
-                            action = _asyncgen.SendAction(request)
-                            del request
-                        finally:
-                            del datagram
-                    except BaseException as exc:
-                        action = _asyncgen.ThrowAction(exc)
-                    try:
-                        await action.asend(request_handler_generator)
+                        timeout = await action.asend(request_handler_generator)
                     except StopAsyncIteration:
-                        break
+                        return
                     finally:
                         del action
 
-    def __enqueue_task_at_end(
+                    del datagram
+                    while True:
+                        try:
+                            with contextlib.nullcontext() if timeout is None else client_data.backend.timeout(timeout):
+                                datagram = await client_data.pop_datagram()
+
+                            action = self.__parse_datagram(datagram, self.__protocol)
+                            del datagram
+                        except BaseException as exc:
+                            action = ThrowAction(exc)
+                        try:
+                            timeout = await action.asend(request_handler_generator)
+                        except StopAsyncIteration:
+                            break
+                        finally:
+                            del action
+                finally:
+                    await request_handler_generator.aclose()
+
+    def __on_task_done(
         self,
-        datagram_received_cb: Callable[[_T_Address, Self], AsyncGenerator[None, _RequestT]],
-        address: _T_Address,
+        datagram_received_cb: Callable[
+            [DatagramClientContext[_T_Response, _T_Address]], AsyncGenerator[float | None, _T_Request]
+        ],
+        client: _ClientToken[_T_Response, _T_Address],
         task_group: TaskGroup,
-        datagram_queue: deque[bytes],
         default_context: contextvars.Context,
     ) -> None:
-        if datagram_queue:
-            task_group.start_soon(
-                self.__client_coroutine,
-                datagram_received_cb,
-                address,
-                task_group,
-                context=default_context,
-            )
-
-    def __clear_queue_on_error(
-        self,
-        datagram_queue: deque[bytes],
-        exc_type: type[BaseException] | None,
-        exc_val: BaseException | None,
-        exc_tb: TracebackType | None,
-        /,
-    ) -> None:
-        if exc_type is not None:
-            datagram_queue.clear()
+        client.data.mark_done()
+        try:
+            pending_datagram = client.data.pop_datagram_no_wait()
+        except IndexError:
+            return
+
+        client.data.mark_pending()
+        default_context.run(
+            task_group.start_soon,
+            self.__client_coroutine,
+            datagram_received_cb,
+            pending_datagram,
+            client,
+            task_group,
+            default_context,
+        )
 
     @staticmethod
-    def __check_datagram_queue_not_empty(datagram_queue: deque[bytes]) -> None:
-        if len(datagram_queue) == 0:
-            _ClientManager.handle_inconsistent_state_error()  # pragma: no cover
+    def __parse_datagram(
+        datagram: bytes,
+        protocol: protocol_module.DatagramProtocol[_T_Response, _T_Request],
+    ) -> AsyncGenAction[_T_Request]:
+        try:
+            try:
+                request = protocol.build_packet_from_datagram(datagram)
+            except DatagramProtocolParseError:
+                raise
+            except Exception as exc:
+                raise RuntimeError("protocol.build_packet_from_datagram() crashed") from exc
+        except BaseException as exc:
+            return ThrowAction(exc)
+        else:
+            return SendAction(request)
 
     @property
     def extra_attributes(self) -> Mapping[Any, Callable[[], Any]]:
         return self.__listener.extra_attributes
 
 
 @enum.unique
 class _ClientState(enum.Enum):
+    TASK_PENDING = enum.auto()
     TASK_RUNNING = enum.auto()
-    TASK_WAITING = enum.auto()
 
 
-class _ClientManager(Generic[_T_Address]):
+@dataclasses.dataclass(slots=True, weakref_slot=True)
+class _ClientToken(Generic[_T_Response, _T_Address]):
+    ctx: DatagramClientContext[_T_Response, _T_Address]
+    data: _ClientData
+
+
+class _ClientData:
     __slots__ = (
-        "__client_lock",
-        "__client_queue",
-        "__client_state",
-        "__send_guard",
-        "__weakref__",
+        "__backend",
+        "__task_lock",
+        "__state",
+        "_queue_condition",
+        "_datagram_queue",
     )
 
     def __init__(self, backend: AsyncBackend) -> None:
-        super().__init__()
+        self.__backend: AsyncBackend = backend
+        self.__task_lock: ILock = backend.create_lock()
+        self.__state: _ClientState | None = None
+        self._queue_condition: ICondition | None = None
+        self._datagram_queue: deque[bytes] | None = None
 
-        self.__client_lock: _TemporaryValue[_T_Address, ICondition] = _TemporaryValue(backend.create_condition_var)
-        self.__client_queue: _TemporaryValue[_T_Address, deque[bytes]] = _TemporaryValue(
-            deque,
-            must_delete_value=operator.not_,  # delete if and only if the deque is empty
-        )
-        self.__client_state: dict[_T_Address, _ClientState] = {}
-        self.__send_guard: _TemporaryValue[_T_Address, _utils.ResourceGuard] = _TemporaryValue(
-            lambda: _utils.ResourceGuard("another task is currently sending data for this address")
-        )
+    @property
+    def backend(self) -> AsyncBackend:
+        return self.__backend
 
-    def client_state(self, address: _T_Address) -> _ClientState | None:
-        return self.__client_state.get(address)
+    @property
+    def task_lock(self) -> ILock:
+        return self.__task_lock
 
-    @contextlib.contextmanager
-    def set_client_state(self, address: _T_Address, state: _ClientState) -> Iterator[None]:
-        old_state: _ClientState | None = self.__client_state.get(address)
-        match state:
-            case _ClientState.TASK_RUNNING if old_state is None:
-                pass
-            case _ClientState.TASK_WAITING if old_state is _ClientState.TASK_RUNNING:
-                pass
-            case _:
-                self.handle_inconsistent_state_error()
+    @property
+    def state(self) -> _ClientState | None:
+        return self.__state
 
-        self.__client_state[address] = state
-        try:
-            yield
-        finally:
-            assert self.__client_state[address] is state  # nosec assert_used
-            if old_state is None:
-                del self.__client_state[address]
-            else:
-                self.__client_state[address] = old_state
-
-    @contextlib.asynccontextmanager
-    async def lock(self, address: _T_Address) -> AsyncIterator[ICondition]:
-        with self.__client_lock.get(address) as condition:
-            async with condition:
-                yield condition
-
-    @contextlib.contextmanager
-    def datagram_queue(self, address: _T_Address) -> Iterator[deque[bytes]]:
-        with self.__client_queue.get(address) as datagram_queue:
-            yield datagram_queue
-
-    @contextlib.contextmanager
-    def send_guard(self, address: _T_Address) -> Iterator[None]:
-        with self.__send_guard.get(address) as send_guard:
-            with send_guard:
-                yield
+    async def push_datagram(self, datagram: bytes) -> None:
+        self.__ensure_queue().append(datagram)
+        if (queue_condition := self._queue_condition) is not None:
+            async with queue_condition:
+                queue_condition.notify()
+
+    def pop_datagram_no_wait(self) -> bytes:
+        if not (queue := self._datagram_queue):
+            raise IndexError("pop from an empty deque")
+        return queue.popleft()
+
+    async def pop_datagram(self) -> bytes:
+        queue_condition = self.__ensure_queue_condition_var()
+        async with queue_condition:
+            queue = self.__ensure_queue()
+            while not queue:
+                await queue_condition.wait()
+            return queue.popleft()
+
+    def mark_pending(self) -> None:
+        if self.__state is not None:
+            self.handle_inconsistent_state_error()
+        self.__state = _ClientState.TASK_PENDING
+
+    def mark_done(self) -> None:
+        if self.__state is not _ClientState.TASK_RUNNING:
+            self.handle_inconsistent_state_error()
+        self.__state = None
+
+    def mark_running(self) -> None:
+        if self.__state is not _ClientState.TASK_PENDING:
+            self.handle_inconsistent_state_error()
+        self.__state = _ClientState.TASK_RUNNING
+
+    def __ensure_queue(self) -> deque[bytes]:
+        if (queue := self._datagram_queue) is None:
+            self._datagram_queue = queue = deque()
+        return queue
+
+    def __ensure_queue_condition_var(self) -> ICondition:
+        if (cond := self._queue_condition) is None:
+            self._queue_condition = cond = self.__backend.create_condition_var()
+        return cond
 
     @staticmethod
     def handle_inconsistent_state_error() -> NoReturn:
         msg = "The server has created too many tasks and ends up in an inconsistent state."
         note = "Please fill an issue (https://github.com/francis-clairicia/EasyNetwork/issues)"
         raise _utils.exception_with_notes(RuntimeError(msg), note)
-
-
-class _TemporaryValue(Generic[_KT, _VT]):
-    __slots__ = ("__values", "__counter", "__must_delete_value")
-
-    def __init__(self, value_factory: Callable[[], _VT], must_delete_value: Callable[[_VT], bool] | None = None) -> None:
-        super().__init__()
-
-        if must_delete_value is None:
-            must_delete_value = lambda _: True
-
-        self.__values: defaultdict[_KT, _VT] = defaultdict(value_factory)
-        self.__counter: Counter[_KT] = Counter()
-        self.__must_delete_value: Callable[[_VT], bool] = must_delete_value
-
-    def __contains__(self, obj: _KT, /) -> bool:  # pragma: no cover  # This method exists for testing purposes
-        return obj in self.__values
-
-    @contextlib.contextmanager
-    def get(self, key: _KT) -> Iterator[_VT]:
-        value: _VT = self.__values[key]
-        self.__counter[key] += 1
-        try:
-            yield value
-        finally:
-            self.__counter[key] -= 1
-            assert self.__counter[key] >= 0, f"{self.__counter[key]=}"  # nosec assert_used
-            if self.__counter[key] == 0 and self.__must_delete_value(value):
-                del self.__counter[key], self.__values[key]
-            del key, value
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_async/servers/stream.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_async/servers/stream.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -15,38 +15,49 @@
 """Low-level asynchronous stream servers module"""
 
 from __future__ import annotations
 
 __all__ = ["AsyncStreamClient", "AsyncStreamServer"]
 
 import contextlib
+import dataclasses
+import warnings
 from collections.abc import AsyncGenerator, Callable, Mapping
-from typing import Any, Generic, NoReturn, Self
+from typing import Any, Generic, Literal, NoReturn, assert_never
 
 from .... import protocol as protocol_module
-from ...._typevars import _RequestT, _ResponseT
+from ...._typevars import _T_Request, _T_Response
 from ....exceptions import UnsupportedOperation
-from ... import _asyncgen, _stream, _utils, typed_attr
-from ..backend.abc import TaskGroup
+from ....warnings import ManualBufferAllocationWarning
+from ... import _stream, _utils, typed_attr
+from ..._asyncgen import AsyncGenAction, SendAction, ThrowAction
+from ..backend.abc import AsyncBackend, TaskGroup
 from ..transports import abc as transports, utils as transports_utils
 
 
-class AsyncStreamClient(typed_attr.TypedAttributeProvider, Generic[_ResponseT]):
+class AsyncStreamClient(typed_attr.TypedAttributeProvider, Generic[_T_Response]):
     __slots__ = (
         "__transport",
         "__producer",
+        "__exit_stack",
         "__send_guard",
         "__weakref__",
     )
 
-    def __init__(self, transport: transports.AsyncStreamWriteTransport, producer: _stream.StreamDataProducer[_ResponseT]) -> None:
+    def __init__(
+        self,
+        transport: transports.AsyncStreamWriteTransport,
+        producer: _stream.StreamDataProducer[_T_Response],
+        exit_stack: contextlib.AsyncExitStack,
+    ) -> None:
         super().__init__()
 
         self.__transport: transports.AsyncStreamWriteTransport = transport
-        self.__producer: _stream.StreamDataProducer[_ResponseT] = producer
+        self.__producer: _stream.StreamDataProducer[_T_Response] = producer
+        self.__exit_stack: contextlib.AsyncExitStack = exit_stack
         self.__send_guard: _utils.ResourceGuard = _utils.ResourceGuard("another task is currently sending data on this endpoint")
 
     def is_closing(self) -> bool:
         """
         Checks if the endpoint is closed or in the process of being closed.
 
         Returns:
@@ -55,66 +66,67 @@
         return self.__transport.is_closing()
 
     async def aclose(self) -> None:
         """
         Closes the endpoint.
         """
         await self.__transport.aclose()
-        self.__producer.clear()
+        await self.__exit_stack.aclose()
 
-    async def send_packet(self, packet: _ResponseT) -> None:
+    async def send_packet(self, packet: _T_Response) -> None:
         """
         Sends `packet` to the remote endpoint.
 
         Warning:
             In the case of a cancellation, it is impossible to know if all the packet data has been sent.
             This would leave the connection in an inconsistent state.
 
         Parameters:
             packet: the Python object to send.
         """
         with self.__send_guard:
-            transport = self.__transport
-            producer = self.__producer
-
-            producer.enqueue(packet)
-            del packet
-            await transport.send_all_from_iterable(producer)
+            await self.__transport.send_all_from_iterable(self.__producer.generate(packet))
 
     @property
     def extra_attributes(self) -> Mapping[Any, Callable[[], Any]]:
         return self.__transport.extra_attributes
 
 
-class AsyncStreamServer(typed_attr.TypedAttributeProvider, Generic[_RequestT, _ResponseT]):
+class AsyncStreamServer(typed_attr.TypedAttributeProvider, Generic[_T_Request, _T_Response]):
     __slots__ = (
         "__listener",
         "__protocol",
         "__max_recv_size",
         "__serve_guard",
+        "__manual_buffer_allocation",
         "__weakref__",
     )
 
     def __init__(
         self,
         listener: transports.AsyncListener[transports.AsyncStreamTransport],
-        protocol: protocol_module.StreamProtocol[_ResponseT, _RequestT],
+        protocol: protocol_module.StreamProtocol[_T_Response, _T_Request],
         max_recv_size: int,
+        *,
+        manual_buffer_allocation: Literal["try", "no", "force"] = "try",
     ) -> None:
         if not isinstance(listener, transports.AsyncListener):
             raise TypeError(f"Expected an AsyncListener object, got {listener!r}")
         if not isinstance(protocol, protocol_module.StreamProtocol):
             raise TypeError(f"Expected a StreamProtocol object, got {protocol!r}")
         if not isinstance(max_recv_size, int) or max_recv_size <= 0:
             raise ValueError("'max_recv_size' must be a strictly positive integer")
+        if manual_buffer_allocation not in ("try", "no", "force"):
+            raise ValueError('"manual_buffer_allocation" must be "try", "no" or "force"')
 
         self.__listener: transports.AsyncListener[transports.AsyncStreamTransport] = listener
-        self.__protocol: protocol_module.StreamProtocol[_ResponseT, _RequestT] = protocol
+        self.__protocol: protocol_module.StreamProtocol[_T_Response, _T_Request] = protocol
         self.__max_recv_size: int = max_recv_size
         self.__serve_guard: _utils.ResourceGuard = _utils.ResourceGuard("another task is currently accepting new connections")
+        self.__manual_buffer_allocation: Literal["try", "no", "force"] = manual_buffer_allocation
 
     def is_closing(self) -> bool:
         """
         Checks if the server is closed or in the process of being closed.
 
         Returns:
             :data:`True` if the server is closed.
@@ -125,133 +137,162 @@
         """
         Closes the server.
         """
         await self.__listener.aclose()
 
     async def serve(
         self,
-        client_connected_cb: Callable[[AsyncStreamClient[_ResponseT]], AsyncGenerator[None, _RequestT]],
-        task_group: TaskGroup,
+        client_connected_cb: Callable[[AsyncStreamClient[_T_Response]], AsyncGenerator[float | None, _T_Request]],
+        task_group: TaskGroup | None = None,
     ) -> NoReturn:
         with self.__serve_guard:
-            handler = _utils.prepend_argument(client_connected_cb)(self.__client_coroutine)
+            handler = _utils.prepend_argument(client_connected_cb, self.__client_coroutine)
             await self.__listener.serve(handler, task_group)
 
     async def __client_coroutine(
         self,
-        client_connected_cb: Callable[[AsyncStreamClient[_ResponseT]], AsyncGenerator[None, _RequestT]],
+        client_connected_cb: Callable[[AsyncStreamClient[_T_Response]], AsyncGenerator[float | None, _T_Request]],
         transport: transports.AsyncStreamTransport,
     ) -> None:
         if not isinstance(transport, transports.AsyncStreamTransport):
             raise TypeError(f"Expected an AsyncStreamTransport object, got {transport!r}")
 
-        async with contextlib.AsyncExitStack() as client_exit_stack:
-            client_exit_stack.push_async_callback(transports_utils.aclose_forcefully, transport)
+        from ..backend.factory import current_async_backend
 
-            producer = _stream.StreamDataProducer(self.__protocol)
-            consumer: _stream.StreamDataConsumer[_RequestT] | _stream.BufferedStreamDataConsumer[_RequestT]
+        async with contextlib.AsyncExitStack() as task_exit_stack:
+            task_exit_stack.push_async_callback(transports_utils.aclose_forcefully, transport)
 
-            request_receiver: _RequestReceiver[_RequestT] | _BufferedRequestReceiver[_RequestT]
-            try:
-                if not isinstance(transport, transports.AsyncBufferedStreamReadTransport):
-                    raise UnsupportedOperation
-                consumer = _stream.BufferedStreamDataConsumer(self.__protocol, self.__max_recv_size)
-                request_receiver = _BufferedRequestReceiver(transport, consumer)
-            except UnsupportedOperation:
-                consumer = _stream.StreamDataConsumer(self.__protocol)
-                request_receiver = _RequestReceiver(transport, consumer, self.__max_recv_size)
+            producer = _stream.StreamDataProducer(self.__protocol)
+            consumer: _stream.StreamDataConsumer[_T_Request] | _stream.BufferedStreamDataConsumer[_T_Request]
 
-            client = AsyncStreamClient(transport, producer)
+            request_receiver: _RequestReceiver[_T_Request] | _BufferedRequestReceiver[_T_Request]
+            match self.__manual_buffer_allocation:
+                case "try" | "force" as manual_buffer_allocation:
+                    try:
+                        consumer = _stream.BufferedStreamDataConsumer(self.__protocol, self.__max_recv_size)
+                        if not isinstance(transport, transports.AsyncBufferedStreamReadTransport):
+                            msg = f"The transport implementation {transport!r} does not implement AsyncBufferedStreamReadTransport interface"
+                            if manual_buffer_allocation == "try":
+                                warnings.warn(msg, category=ManualBufferAllocationWarning, stacklevel=1)
+                            raise UnsupportedOperation(msg)
+                        request_receiver = _BufferedRequestReceiver(
+                            backend=current_async_backend(),
+                            transport=transport,
+                            consumer=consumer,
+                        )
+                    except UnsupportedOperation:
+                        if manual_buffer_allocation == "force":
+                            raise
+                        consumer = _stream.StreamDataConsumer(self.__protocol)
+                        request_receiver = _RequestReceiver(
+                            backend=current_async_backend(),
+                            transport=transport,
+                            consumer=consumer,
+                            max_recv_size=self.__max_recv_size,
+                        )
+                case "no":
+                    consumer = _stream.StreamDataConsumer(self.__protocol)
+                    request_receiver = _RequestReceiver(
+                        backend=current_async_backend(),
+                        transport=transport,
+                        consumer=consumer,
+                        max_recv_size=self.__max_recv_size,
+                    )
+                case manual_buffer_allocation:  # pragma: no cover
+                    assert_never(manual_buffer_allocation)
 
+            client_exit_stack = await task_exit_stack.enter_async_context(contextlib.AsyncExitStack())
             client_exit_stack.callback(consumer.clear)
-            client_exit_stack.callback(producer.clear)
 
-            request_handler_generator = client_connected_cb(client)
+            client = AsyncStreamClient(transport, producer, client_exit_stack)
 
-            del client_exit_stack, client_connected_cb, client
+            request_handler_generator = client_connected_cb(client)
 
-            async with contextlib.aclosing(request_handler_generator):
-                try:
-                    await anext(request_handler_generator)
-                except StopAsyncIteration:
-                    return
+            del client_exit_stack, task_exit_stack, client_connected_cb, client, current_async_backend
 
-                async for action in request_receiver:
+            timeout: float | None
+            try:
+                timeout = await anext(request_handler_generator)
+            except StopAsyncIteration:
+                return
+            else:
+                while True:
+                    try:
+                        action = await request_receiver.next(timeout)
+                    except StopAsyncIteration:
+                        break
                     try:
-                        await action.asend(request_handler_generator)
+                        timeout = await action.asend(request_handler_generator)
                     except StopAsyncIteration:
                         break
                     finally:
                         del action
+            finally:
+                await request_handler_generator.aclose()
 
     @property
     def extra_attributes(self) -> Mapping[Any, Callable[[], Any]]:
         return self.__listener.extra_attributes
 
 
-class _RequestReceiver(Generic[_RequestT]):
-    __slots__ = ("__consumer", "__transport", "__max_recv_size")
-
-    def __init__(
-        self,
-        transport: transports.AsyncStreamReadTransport,
-        consumer: _stream.StreamDataConsumer[_RequestT],
-        max_recv_size: int,
-    ) -> None:
-        assert max_recv_size > 0, f"{max_recv_size=}"  # nosec assert_used
-        self.__transport: transports.AsyncStreamReadTransport = transport
-        self.__consumer: _stream.StreamDataConsumer[_RequestT] = consumer
-        self.__max_recv_size: int = max_recv_size
+@dataclasses.dataclass(kw_only=True, eq=False, frozen=True, slots=True)
+class _RequestReceiver(Generic[_T_Request]):
+    backend: AsyncBackend
+    transport: transports.AsyncStreamReadTransport
+    consumer: _stream.StreamDataConsumer[_T_Request]
+    max_recv_size: int
+    __null_timeout_ctx: contextlib.nullcontext[None] = dataclasses.field(init=False, default_factory=contextlib.nullcontext)
 
-    def __aiter__(self) -> Self:
-        return self
+    def __post_init__(self) -> None:
+        assert self.max_recv_size > 0, f"{self.max_recv_size=}"  # nosec assert_used
 
-    async def __anext__(self) -> _asyncgen.AsyncGenAction[None, _RequestT]:
-        transport: transports.AsyncStreamReadTransport = self.__transport
-        consumer: _stream.StreamDataConsumer[_RequestT] = self.__consumer
-        bufsize: int = self.__max_recv_size
+    async def next(self, timeout: float | None) -> AsyncGenAction[_T_Request]:
         try:
-            while not transport.is_closing():
-                try:
-                    return _asyncgen.SendAction(next(consumer))
-                except StopIteration:
-                    pass
-                data: bytes = await transport.recv(bufsize)
-                if not data:  # Closed connection (EOF)
-                    break
-                consumer.feed(data)
-                del data
+            consumer = self.consumer
+            try:
+                request = consumer.next(None)
+            except StopIteration:
+                pass
+            else:
+                return SendAction(request)
+
+            with self.__null_timeout_ctx if timeout is None else self.backend.timeout(timeout):
+                while data := await self.transport.recv(self.max_recv_size):
+                    try:
+                        request = consumer.next(data)
+                    except StopIteration:
+                        continue
+                    finally:
+                        del data
+                    return SendAction(request)
         except BaseException as exc:
-            return _asyncgen.ThrowAction(exc)
+            return ThrowAction(exc)
         raise StopAsyncIteration
 
 
-class _BufferedRequestReceiver(Generic[_RequestT]):
-    __slots__ = ("__consumer", "__transport")
+@dataclasses.dataclass(kw_only=True, eq=False, frozen=True, slots=True)
+class _BufferedRequestReceiver(Generic[_T_Request]):
+    backend: AsyncBackend
+    transport: transports.AsyncBufferedStreamReadTransport
+    consumer: _stream.BufferedStreamDataConsumer[_T_Request]
+    __null_timeout_ctx: contextlib.nullcontext[None] = dataclasses.field(init=False, default_factory=contextlib.nullcontext)
 
-    def __init__(
-        self,
-        transport: transports.AsyncBufferedStreamReadTransport,
-        consumer: _stream.BufferedStreamDataConsumer[_RequestT],
-    ) -> None:
-        self.__transport: transports.AsyncBufferedStreamReadTransport = transport
-        self.__consumer: _stream.BufferedStreamDataConsumer[_RequestT] = consumer
-
-    def __aiter__(self) -> Self:
-        return self
-
-    async def __anext__(self) -> _asyncgen.AsyncGenAction[None, _RequestT]:
-        transport: transports.AsyncBufferedStreamReadTransport = self.__transport
-        consumer: _stream.BufferedStreamDataConsumer[_RequestT] = self.__consumer
+    async def next(self, timeout: float | None) -> AsyncGenAction[_T_Request]:
         try:
-            while not transport.is_closing():
-                try:
-                    return _asyncgen.SendAction(next(consumer))
-                except StopIteration:
-                    pass
-                nbytes: int = await transport.recv_into(consumer.get_write_buffer())
-                if not nbytes:
-                    break
-                consumer.buffer_updated(nbytes)
+            consumer = self.consumer
+            try:
+                request = consumer.next(None)
+            except StopIteration:
+                pass
+            else:
+                return SendAction(request)
+
+            with self.__null_timeout_ctx if timeout is None else self.backend.timeout(timeout):
+                while nbytes := await self.transport.recv_into(consumer.get_write_buffer()):
+                    try:
+                        request = consumer.next(nbytes)
+                    except StopIteration:
+                        continue
+                    return SendAction(request)
         except BaseException as exc:
-            return _asyncgen.ThrowAction(exc)
+            return ThrowAction(exc)
         raise StopAsyncIteration
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_async/transports/__init__.py` & `easynetwork-1.0.0rc8/src/easynetwork/clients/__init__.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,19 +1,29 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 #
-"""Low-level asynchronous transports module"""
+"""Network client module"""
 
 from __future__ import annotations
 
-__all__ = []  # type: list[str]
+__all__ = [
+    "AsyncTCPNetworkClient",
+    "AsyncUDPNetworkClient",
+    "TCPNetworkClient",
+    "UDPNetworkClient",
+]
+
+from .async_tcp import AsyncTCPNetworkClient
+from .async_udp import AsyncUDPNetworkClient
+from .tcp import TCPNetworkClient
+from .udp import UDPNetworkClient
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_async/transports/abc.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_async/transports/abc.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -218,15 +218,15 @@
     """
     An interface for objects that let you accept incoming connections.
     """
 
     __slots__ = ()
 
     @abstractmethod
-    async def serve(self, handler: Callable[[_T_co], Coroutine[Any, Any, None]], task_group: TaskGroup) -> NoReturn:
+    async def serve(self, handler: Callable[[_T_co], Coroutine[Any, Any, None]], task_group: TaskGroup | None = None) -> NoReturn:
         """
         Accept incoming connections as they come in and start tasks to handle them.
 
         Parameters:
             handler: a callable that will be used to handle each accepted connection.
             task_group: the task group that will be used to start tasks for handling each accepted connection.
         """
@@ -237,20 +237,28 @@
     """
     An interface specialized for objects that let you handle incoming datagrams from anywhere.
     """
 
     __slots__ = ()
 
     @abstractmethod
-    async def recv_from(self) -> tuple[bytes, _T_Address]:
+    async def serve(
+        self,
+        handler: Callable[[bytes, _T_Address], Coroutine[Any, Any, None]],
+        task_group: TaskGroup | None = None,
+    ) -> NoReturn:
         """
-        Receive incoming datagrams as they come.
+        Receive incoming datagrams as they come in and start tasks to handle them.
 
-        Returns:
-            a pair with the datagram packet and the sender address.
+        Important:
+            The implementation must ensure that datagrams are processed in the order in which they are received.
+
+        Parameters:
+            handler: a callable that will be used to handle each received datagram.
+            task_group: the task group that will be used to start tasks for handling each received datagram.
         """
         raise NotImplementedError
 
     @abstractmethod
     async def send_to(self, data: bytes | bytearray | memoryview, address: _T_Address) -> None:
         """
         Send the `data` bytes to the remote peer `address`.
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_async/transports/utils.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_async/transports/utils.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_sync/endpoints/__init__.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/std_asyncio/__init__.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,19 +1,22 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 #
-"""Low-level endpoints module"""
+"""asyncio engine for easynetwork.api_async
+"""
 
 from __future__ import annotations
 
-__all__ = []  # type: list[str]
+__all__ = ["AsyncIOBackend"]  # type: list[str]
+
+from .backend import AsyncIOBackend
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_sync/endpoints/datagram.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_sync/endpoints/datagram.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -19,21 +19,21 @@
 __all__ = ["DatagramEndpoint"]
 
 import math
 from collections.abc import Callable, Mapping
 from typing import Any, Generic, TypeGuard
 
 from .... import protocol as protocol_module
-from ...._typevars import _ReceivedPacketT, _SentPacketT
+from ...._typevars import _T_ReceivedPacket, _T_SentPacket
 from ....exceptions import DatagramProtocolParseError, UnsupportedOperation
 from ... import typed_attr
 from ..transports import abc as transports
 
 
-class DatagramEndpoint(typed_attr.TypedAttributeProvider, Generic[_SentPacketT, _ReceivedPacketT]):
+class DatagramEndpoint(typed_attr.TypedAttributeProvider, Generic[_T_SentPacket, _T_ReceivedPacket]):
     """
     A communication endpoint based on unreliable packets of data.
     """
 
     __slots__ = (
         "__transport",
         "__is_read_transport",
@@ -41,15 +41,15 @@
         "__protocol",
         "__weakref__",
     )
 
     def __init__(
         self,
         transport: transports.DatagramTransport | transports.DatagramReadTransport | transports.DatagramWriteTransport,
-        protocol: protocol_module.DatagramProtocol[_SentPacketT, _ReceivedPacketT],
+        protocol: protocol_module.DatagramProtocol[_T_SentPacket, _T_ReceivedPacket],
     ) -> None:
         """
         Parameters:
             transport: The data transport to use.
             protocol: The :term:`protocol object` to use.
         """
 
@@ -57,17 +57,17 @@
             raise TypeError(f"Expected a DatagramTransport object, got {transport!r}")
         if not isinstance(protocol, protocol_module.DatagramProtocol):
             raise TypeError(f"Expected a DatagramProtocol object, got {protocol!r}")
 
         self.__is_read_transport: bool = isinstance(transport, transports.DatagramReadTransport)
         self.__is_write_transport: bool = isinstance(transport, transports.DatagramWriteTransport)
         self.__transport: transports.DatagramReadTransport | transports.DatagramWriteTransport = transport
-        self.__protocol: protocol_module.DatagramProtocol[_SentPacketT, _ReceivedPacketT] = protocol
+        self.__protocol: protocol_module.DatagramProtocol[_T_SentPacket, _T_ReceivedPacket] = protocol
 
-    def __del__(self) -> None:  # pragma: no cover
+    def __del__(self) -> None:
         try:
             if not self.__transport.is_closed():
                 self.close()
         except AttributeError:
             return
 
     def is_closed(self) -> bool:
@@ -81,15 +81,15 @@
 
     def close(self) -> None:
         """
         Closes the endpoint.
         """
         self.__transport.close()
 
-    def send_packet(self, packet: _SentPacketT, *, timeout: float | None = None) -> None:
+    def send_packet(self, packet: _T_SentPacket, *, timeout: float | None = None) -> None:
         """
         Sends `packet` to the remote endpoint.
 
         If `timeout` is not :data:`None`, the entire send operation will take at most `timeout` seconds.
 
         Warning:
             A timeout on a send operation is unusual.
@@ -117,15 +117,15 @@
         except Exception as exc:
             raise RuntimeError("protocol.make_datagram() crashed") from exc
         finally:
             del packet
 
         transport.send(datagram, timeout)
 
-    def recv_packet(self, *, timeout: float | None = None) -> _ReceivedPacketT:
+    def recv_packet(self, *, timeout: float | None = None) -> _T_ReceivedPacket:
         """
         Waits for a new packet from the remote endpoint.
 
         If `timeout` is not :data:`None`, the entire receive operation will take at most `timeout` seconds.
 
         Parameters:
             timeout: the allowed time (in seconds) for blocking operations.
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_sync/endpoints/stream.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_async/transports/tls.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,303 +1,343 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 #
-"""Low-level endpoints module"""
+"""Low-level asynchronous transports module"""
 
 from __future__ import annotations
 
-__all__ = ["StreamEndpoint"]
+__all__ = ["AsyncTLSListener", "AsyncTLSStreamTransport"]
 
+import contextlib
 import dataclasses
-import errno as _errno
-import math
-from collections.abc import Callable, Mapping
-from typing import Any, Generic
+import errno
+import functools
+import logging
+from collections.abc import Callable, Coroutine, Mapping
+from typing import TYPE_CHECKING, Any, Final, NoReturn, Self, TypeVar, TypeVarTuple
+
+try:
+    import ssl as _ssl
+except ImportError:  # pragma: no cover
+    _ssl_module = None
+else:
+    _ssl_module = _ssl
+    del _ssl
 
-from .... import protocol as protocol_module
-from ...._typevars import _ReceivedPacketT, _SentPacketT
 from ....exceptions import UnsupportedOperation
-from ... import _stream, _utils, typed_attr
-from ..transports import abc as transports
+from ... import _utils, constants, socket as socket_tools
+from ..backend.abc import TaskGroup
+from ..backend.factory import current_async_backend
+from . import abc as transports
+from .utils import aclose_forcefully
+
+if TYPE_CHECKING:
+    import ssl as _typing_ssl
+
+    from _typeshed import WriteableBuffer
+
+_T_PosArgs = TypeVarTuple("_T_PosArgs")
+_T_Return = TypeVar("_T_Return")
+
+
+@dataclasses.dataclass(repr=False, eq=False, slots=True, kw_only=True)
+class AsyncTLSStreamTransport(transports.AsyncStreamTransport, transports.AsyncBufferedStreamReadTransport):
+    _transport: transports.AsyncStreamTransport
+    _standard_compatible: bool
+    _shutdown_timeout: float
+    _ssl_object: _typing_ssl.SSLObject
+    _read_bio: _typing_ssl.MemoryBIO
+    _write_bio: _typing_ssl.MemoryBIO
+    __incoming_reader: _IncomingDataReader = dataclasses.field(init=False)
+    __closing: bool = dataclasses.field(init=False, default=False)
+
+    def __post_init__(self) -> None:
+        if isinstance(self._transport, transports.AsyncBufferedStreamReadTransport):
+            self.__incoming_reader = _BufferedIncomingDataReader(transport=self._transport)
+        else:
+            self.__incoming_reader = _IncomingDataReader(transport=self._transport)
+
+    @classmethod
+    async def wrap(
+        cls,
+        transport: transports.AsyncStreamTransport,
+        ssl_context: _typing_ssl.SSLContext,
+        *,
+        handshake_timeout: float | None = None,
+        shutdown_timeout: float | None = None,
+        server_side: bool | None = None,
+        server_hostname: str | None = None,
+        standard_compatible: bool = True,
+        session: _typing_ssl.SSLSession | None = None,
+    ) -> Self:
+        assert _ssl_module is not None, "stdlib ssl module not available"  # nosec assert_used
+
+        if server_side is None:
+            server_side = not server_hostname
+
+        if handshake_timeout is None:
+            handshake_timeout = constants.SSL_HANDSHAKE_TIMEOUT
+        if shutdown_timeout is None:
+            shutdown_timeout = constants.SSL_SHUTDOWN_TIMEOUT
+
+        read_bio = _ssl_module.MemoryBIO()
+        write_bio = _ssl_module.MemoryBIO()
+        ssl_object = ssl_context.wrap_bio(
+            read_bio,
+            write_bio,
+            server_side=server_side,
+            server_hostname=server_hostname,
+            session=session,
+        )
+
+        self = cls(
+            _transport=transport,
+            _standard_compatible=bool(standard_compatible),
+            _shutdown_timeout=float(shutdown_timeout),
+            _ssl_object=ssl_object,
+            _read_bio=read_bio,
+            _write_bio=write_bio,
+        )
 
+        try:
+            with current_async_backend().timeout(handshake_timeout):
+                await self._retry_ssl_method(ssl_object.do_handshake)
 
-class StreamEndpoint(typed_attr.TypedAttributeProvider, Generic[_SentPacketT, _ReceivedPacketT]):
-    """
-    A communication endpoint based on continuous stream data transport.
-    """
-
-    __slots__ = (
-        "__transport",
-        "__sender",
-        "__receiver",
-        "__eof_sent",
-        "__weakref__",
-    )
+            _ = ssl_object.getpeercert()
+        except BaseException:
+            await aclose_forcefully(transport)
+            raise
+        return self
+
+    @_utils.inherit_doc(transports.AsyncStreamTransport)
+    def is_closing(self) -> bool:
+        return self.__closing
+
+    @_utils.inherit_doc(transports.AsyncStreamTransport)
+    async def aclose(self) -> None:
+        with contextlib.ExitStack() as stack:
+            stack.callback(self.__incoming_reader.close)
+
+            self.__closing = True
+            if self._standard_compatible:
+                with current_async_backend().move_on_after(self._shutdown_timeout) as shutdown_timeout_scope:
+                    try:
+                        await self._retry_ssl_method(self._ssl_object.unwrap)
+                        self._read_bio.write_eof()
+                        self._write_bio.write_eof()
+                    except BaseException:
+                        await aclose_forcefully(self._transport)
+                        raise
+                if shutdown_timeout_scope.cancelled_caught():
+                    return
+
+            await self._transport.aclose()
+
+    @_utils.inherit_doc(transports.AsyncStreamTransport)
+    async def recv(self, bufsize: int) -> bytes:
+        assert _ssl_module is not None, "stdlib ssl module not available"  # nosec assert_used
+        try:
+            return await self._retry_ssl_method(self._ssl_object.read, bufsize)
+        except _ssl_module.SSLZeroReturnError:
+            return b""
+        except _ssl_module.SSLError as exc:
+            if _utils.is_ssl_eof_error(exc):
+                if not self._standard_compatible:
+                    return b""
+            raise
+
+    @_utils.inherit_doc(transports.AsyncBufferedStreamReadTransport)
+    async def recv_into(self, buffer: WriteableBuffer) -> int:
+        assert _ssl_module is not None, "stdlib ssl module not available"  # nosec assert_used
+        nbytes = memoryview(buffer).nbytes or 1024
+        try:
+            return await self._retry_ssl_method(self._ssl_object.read, nbytes, buffer)  # type: ignore[arg-type]
+        except _ssl_module.SSLZeroReturnError:
+            return 0
+        except _ssl_module.SSLError as exc:
+            if _utils.is_ssl_eof_error(exc):
+                if not self._standard_compatible:
+                    return 0
+            raise
+
+    @_utils.inherit_doc(transports.AsyncStreamTransport)
+    async def send_all(self, data: bytes | bytearray | memoryview) -> None:
+        assert _ssl_module is not None, "stdlib ssl module not available"  # nosec assert_used
+        try:
+            await self._retry_ssl_method(self._ssl_object.write, data)
+        except _ssl_module.SSLZeroReturnError as exc:
+            raise _utils.error_from_errno(errno.ECONNRESET) from exc
+
+    @_utils.inherit_doc(transports.AsyncStreamTransport)
+    async def send_eof(self) -> None:
+        raise UnsupportedOperation("SSL/TLS API does not support sending EOF.")
 
-    def __init__(
+    async def _retry_ssl_method(
         self,
-        transport: transports.StreamTransport | transports.StreamReadTransport | transports.StreamWriteTransport,
-        protocol: protocol_module.StreamProtocol[_SentPacketT, _ReceivedPacketT],
-        max_recv_size: int,
-    ) -> None:
-        """
-        Parameters:
-            transport: The data transport to use.
-            protocol: The :term:`protocol object` to use.
-            max_recv_size: Read buffer size.
-        """
-
-        if not isinstance(transport, (transports.StreamReadTransport, transports.StreamWriteTransport)):
-            raise TypeError(f"Expected a StreamTransport object, got {transport!r}")
-        if not isinstance(max_recv_size, int) or max_recv_size <= 0:
-            raise ValueError("'max_recv_size' must be a strictly positive integer")
-
-        self.__sender: _DataSenderImpl[_SentPacketT] | None = None
-        if isinstance(transport, transports.StreamWriteTransport):
-            self.__sender = _DataSenderImpl(transport, _stream.StreamDataProducer(protocol))
-
-        self.__receiver: _DataReceiverImpl[_ReceivedPacketT] | _BufferedReceiverImpl[_ReceivedPacketT] | None = None
-        if isinstance(transport, transports.StreamReadTransport):
+        ssl_object_method: Callable[[*_T_PosArgs], _T_Return],
+        *args: *_T_PosArgs,
+    ) -> _T_Return:
+        assert _ssl_module is not None, "stdlib ssl module not available"  # nosec assert_used
+        while True:
             try:
-                if not isinstance(transport, transports.BufferedStreamReadTransport):
-                    raise UnsupportedOperation
-                self.__receiver = _BufferedReceiverImpl(transport, _stream.BufferedStreamDataConsumer(protocol, max_recv_size))
-            except UnsupportedOperation:
-                self.__receiver = _DataReceiverImpl(transport, _stream.StreamDataConsumer(protocol), max_recv_size)
-
-        self.__transport: transports.StreamReadTransport | transports.StreamWriteTransport = transport
-        self.__eof_sent: bool = False
-
-    def __del__(self) -> None:  # pragma: no cover
-        try:
-            if not self.__transport.is_closed():
-                self.close()
-        except AttributeError:
-            return
-
-    def is_closed(self) -> bool:
-        """
-        Checks if :meth:`close` has been called.
-
-        Returns:
-            :data:`True` if the endpoint is closed.
-        """
-        return self.__transport.is_closed()
-
-    def close(self) -> None:
-        """
-        Closes the endpoint.
-        """
-        self.__transport.close()
-        if self.__receiver is not None:
-            self.__receiver.clear()
-        if self.__sender is not None:
-            self.__sender.clear()
-
-    def send_packet(self, packet: _SentPacketT, *, timeout: float | None = None) -> None:
-        """
-        Sends `packet` to the remote endpoint.
-
-        If `timeout` is not :data:`None`, the entire send operation will take at most `timeout` seconds.
-
-        Warning:
-            A timeout on a send operation is unusual unless you have a SSL/TLS context.
-
-            In the case of a timeout, it is impossible to know if all the packet data has been sent.
-            This would leave the connection in an inconsistent state.
-
-        Parameters:
-            packet: the Python object to send.
-            timeout: the allowed time (in seconds) for blocking operations.
-
-        Raises:
-            TimeoutError: the send operation does not end up after `timeout` seconds.
-            RuntimeError: :meth:`send_eof` has been called earlier.
-        """
-        if self.__eof_sent:
-            raise RuntimeError("send_eof() has been called earlier")
-
-        sender = self.__sender
-
-        if sender is None:
-            raise UnsupportedOperation("transport does not support sending data")
-
-        if timeout is None:
-            timeout = math.inf
-
-        return sender.send(packet, timeout)
-
-    def send_eof(self) -> None:
-        """
-        Close the write end of the stream after the buffered write data is flushed.
-
-        This method does nothing if the endpoint is closed.
-
-        Can be safely called multiple times.
-        """
-        if self.__eof_sent:
-            return
-
-        sender = self.__sender
-
-        if sender is None or not isinstance(sender.transport, transports.StreamTransport):
-            raise UnsupportedOperation("transport does not support sending EOF")
-
-        sender.transport.send_eof()
-        self.__eof_sent = True
-        sender.clear()
+                result = ssl_object_method(*args)
+            except _ssl_module.SSLWantReadError:
+                try:
+                    # Flush any pending writes first
+                    if self._write_bio.pending:
+                        await self._transport.send_all(self._write_bio.read())
+
+                    await self.__incoming_reader.readinto(self._read_bio)
+                except OSError:
+                    self._read_bio.write_eof()
+                    self._write_bio.write_eof()
+                    raise
+            except _ssl_module.SSLWantWriteError:
+                await self._transport.send_all(self._write_bio.read())
+            except _ssl_module.SSLError:
+                self._read_bio.write_eof()
+                self._write_bio.write_eof()
+                raise
+            else:
+                # Flush any pending writes first
+                if self._write_bio.pending:
+                    await self._transport.send_all(self._write_bio.read())
 
-    def recv_packet(self, *, timeout: float | None = None) -> _ReceivedPacketT:
-        """
-        Waits for a new packet to arrive from the remote endpoint.
-
-        If `timeout` is not :data:`None`, the entire receive operation will take at most `timeout` seconds.
-
-        Parameters:
-            timeout: the allowed time (in seconds) for blocking operations.
-
-        Raises:
-            TimeoutError: the receive operation does not end up after `timeout` seconds.
-            EOFError: the read end of the stream is closed.
-            StreamProtocolParseError: invalid data received.
-
-        Returns:
-            the received packet.
-        """
-        receiver = self.__receiver
-
-        if receiver is None:
-            raise UnsupportedOperation("transport does not support receiving data")
-
-        if timeout is None:
-            timeout = math.inf
-
-        return receiver.receive(timeout)
-
-    @property
-    def max_recv_size(self) -> int:
-        """Read buffer size. Read-only attribute."""
-        receiver = self.__receiver
-        if receiver is None:
-            return 0
-        return receiver.max_recv_size
+                return result
 
     @property
+    @_utils.inherit_doc(transports.AsyncStreamTransport)
     def extra_attributes(self) -> Mapping[Any, Callable[[], Any]]:
-        return self.__transport.extra_attributes
-
-
-@dataclasses.dataclass(slots=True)
-class _DataSenderImpl(Generic[_SentPacketT]):
-    transport: transports.StreamWriteTransport
-    producer: _stream.StreamDataProducer[_SentPacketT]
-
-    def clear(self) -> None:
-        self.producer.clear()
-
-    def send(self, packet: _SentPacketT, timeout: float) -> None:
-        self.producer.enqueue(packet)
-        return self.transport.send_all_from_iterable(self.producer, timeout)
-
-
-@dataclasses.dataclass(slots=True)
-class _DataReceiverImpl(Generic[_ReceivedPacketT]):
-    transport: transports.StreamReadTransport
-    consumer: _stream.StreamDataConsumer[_ReceivedPacketT]
-    max_recv_size: int
-    _eof_reached: bool = dataclasses.field(init=False, default=False)
-
-    def clear(self) -> None:
-        self.consumer.clear()
+        return {
+            **self._transport.extra_attributes,
+            **socket_tools._get_tls_extra(self._ssl_object),
+            socket_tools.TLSAttribute.standard_compatible: lambda: self._standard_compatible,
+        }
 
-    def receive(self, timeout: float) -> _ReceivedPacketT:
-        transport = self.transport
-        consumer = self.consumer
 
-        try:
-            return next(consumer)  # If there is enough data from last call to create a packet, return immediately
-        except StopIteration:
-            pass
+class AsyncTLSListener(transports.AsyncListener[AsyncTLSStreamTransport]):
+    __slots__ = (
+        "__listener",
+        "__ssl_context",
+        "__standard_compatible",
+        "__handshake_timeout",
+        "__shutdown_timeout",
+    )
 
-        if self._eof_reached:
-            raise EOFError("end-of-stream")
+    def __init__(
+        self,
+        listener: transports.AsyncListener[transports.AsyncStreamTransport],
+        ssl_context: _typing_ssl.SSLContext,
+        *,
+        handshake_timeout: float | None = None,
+        shutdown_timeout: float | None = None,
+        standard_compatible: bool = True,
+    ) -> None:
+        super().__init__()
+        self.__listener: transports.AsyncListener[transports.AsyncStreamTransport] = listener
+        self.__ssl_context: _typing_ssl.SSLContext = ssl_context
+        self.__handshake_timeout: float | None = handshake_timeout
+        self.__shutdown_timeout: float | None = shutdown_timeout
+        self.__standard_compatible: bool = standard_compatible
+
+    @_utils.inherit_doc(transports.AsyncListener)
+    def is_closing(self) -> bool:
+        return self.__listener.is_closing()
+
+    @_utils.inherit_doc(transports.AsyncListener)
+    async def aclose(self) -> None:
+        return await self.__listener.aclose()
 
-        bufsize: int = self.max_recv_size
+    @_utils.inherit_doc(transports.AsyncListener)
+    async def serve(
+        self,
+        handler: Callable[[AsyncTLSStreamTransport], Coroutine[Any, Any, None]],
+        task_group: TaskGroup | None = None,
+    ) -> NoReturn:
+        listener = self.__listener
+        logger = logging.getLogger(__name__)
 
-        while True:
-            with _utils.ElapsedTime() as elapsed:
-                chunk: bytes = transport.recv(bufsize, timeout)
-            if not chunk:
-                self._eof_reached = True
-                raise EOFError("end-of-stream")
-            consumer.feed(chunk)
-            buffer_not_full: bool = len(chunk) < bufsize
-            del chunk
+        @functools.wraps(handler)
+        async def tls_handler_wrapper(stream: transports.AsyncStreamTransport, /) -> None:
             try:
-                return next(consumer)
-            except StopIteration:
-                if timeout > 0:
-                    timeout = elapsed.recompute_timeout(timeout)
-                elif buffer_not_full:
-                    break
-        # Loop break
-        raise _utils.error_from_errno(_errno.ETIMEDOUT)
+                stream = await AsyncTLSStreamTransport.wrap(
+                    stream,
+                    self.__ssl_context,
+                    server_side=True,
+                    handshake_timeout=self.__handshake_timeout,
+                    shutdown_timeout=self.__shutdown_timeout,
+                    standard_compatible=self.__standard_compatible,
+                )
+            except current_async_backend().get_cancelled_exc_class():
+                await aclose_forcefully(stream)
+                raise
+            except BaseException as exc:
+                await aclose_forcefully(stream)
+
+                logger.error("Error in client task (during TLS handshake)", exc_info=exc)
+
+                # Only reraise base exceptions
+                if not isinstance(exc, Exception):
+                    raise
+            else:
+                await handler(stream)
 
-
-@dataclasses.dataclass(slots=True)
-class _BufferedReceiverImpl(Generic[_ReceivedPacketT]):
-    transport: transports.BufferedStreamReadTransport
-    consumer: _stream.BufferedStreamDataConsumer[_ReceivedPacketT]
-    _eof_reached: bool = dataclasses.field(init=False, default=False)
+        await listener.serve(tls_handler_wrapper, task_group)
 
     @property
-    def max_recv_size(self) -> int:
-        # Ensure buffer is allocated
-        self.consumer.get_write_buffer()
-        return self.consumer.buffer_size
-
-    def clear(self) -> None:
-        self.consumer.clear()
-
-    def receive(self, timeout: float) -> _ReceivedPacketT:
-        transport = self.transport
-        consumer = self.consumer
+    @_utils.inherit_doc(transports.AsyncListener)
+    def extra_attributes(self) -> Mapping[Any, Callable[[], Any]]:
+        return {
+            **self.__listener.extra_attributes,
+            socket_tools.TLSAttribute.sslcontext: lambda: self.__ssl_context,
+            socket_tools.TLSAttribute.standard_compatible: lambda: self.__standard_compatible,
+        }
+
+
+@dataclasses.dataclass(kw_only=True, eq=False, slots=True)
+class _IncomingDataReader:
+    transport: transports.AsyncStreamReadTransport
+    max_size: Final[int] = 256 * 1024  # 256KiB
+
+    async def readinto(self, read_bio: _typing_ssl.MemoryBIO) -> int:
+        data = await self.transport.recv(self.max_size)
+        if data:
+            return read_bio.write(data)
+        read_bio.write_eof()
+        return 0
 
-        try:
-            return next(consumer)  # If there is enough data from last call to create a packet, return immediately
-        except StopIteration:
-            pass
+    def close(self) -> None:
+        pass
 
-        if self._eof_reached:
-            raise EOFError("end-of-stream")
 
-        while True:
-            with memoryview(consumer.get_write_buffer()) as buffer:
-                bufsize: int = buffer.nbytes
-                with _utils.ElapsedTime() as elapsed:
-                    nbytes: int = transport.recv_into(buffer, timeout)
-            del buffer
-            if not nbytes:
-                self._eof_reached = True
-                raise EOFError("end-of-stream")
-            consumer.buffer_updated(nbytes)
-            buffer_not_full: bool = nbytes < bufsize
-            try:
-                return next(consumer)
-            except StopIteration:
-                if timeout > 0:
-                    timeout = elapsed.recompute_timeout(timeout)
-                elif buffer_not_full:
-                    break
-        # Loop break
-        raise _utils.error_from_errno(_errno.ETIMEDOUT)
+@dataclasses.dataclass(kw_only=True, eq=False, slots=True)
+class _BufferedIncomingDataReader(_IncomingDataReader):
+    transport: transports.AsyncBufferedStreamReadTransport
+    buffer: bytearray | None = dataclasses.field(init=False)
+    buffer_view: memoryview = dataclasses.field(init=False)
+
+    def __post_init__(self) -> None:
+        self.buffer = bytearray(self.max_size)
+        self.buffer_view = memoryview(self.buffer)
+
+    async def readinto(self, read_bio: _typing_ssl.MemoryBIO) -> int:
+        buffer = self.buffer_view
+        nbytes = await self.transport.recv_into(buffer)
+        if nbytes:
+            return read_bio.write(buffer[:nbytes])
+        read_bio.write_eof()
+        return 0
+
+    def close(self) -> None:
+        self.buffer_view.release()
+        self.buffer = None
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_sync/transports/abc.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_sync/transports/abc.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_sync/transports/base_selector.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_sync/transports/base_selector.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -38,15 +38,15 @@
 
 from ... import _utils
 from . import abc as transports
 
 if TYPE_CHECKING:
     from _typeshed import WriteableBuffer
 
-_R = TypeVar("_R")
+_T_Return = TypeVar("_T_Return")
 
 
 class WouldBlockOnRead(Exception):
     """The operation would block when reading the pipe."""
 
     def __init__(self, fileno: int) -> None:
         super().__init__(fileno)
@@ -95,24 +95,24 @@
 
         self._retry_interval: float = _utils.validate_timeout_delay(retry_interval, positive_check=False)
         if self._retry_interval <= 0:
             raise ValueError("retry_interval must be a strictly positive float")
 
     def _retry(
         self,
-        callback: Callable[[], _R],
+        callback: Callable[[], _T_Return],
         timeout: float,
-    ) -> _R:
+    ) -> tuple[_T_Return, float]:
         timeout = _utils.validate_timeout_delay(timeout, positive_check=True)
         retry_interval = self._retry_interval
         event: int
         fileno: int
         while True:
             try:
-                return callback()
+                return callback(), timeout
             except WouldBlockOnRead as exc:
                 event = selectors.EVENT_READ
                 fileno = exc.fileno
             except WouldBlockOnWrite as exc:
                 event = selectors.EVENT_WRITE
                 fileno = exc.fileno
             if timeout <= 0:
@@ -173,15 +173,15 @@
 
     def recv(self, bufsize: int, timeout: float) -> bytes:
         """
         Read and return up to `bufsize` bytes.
 
         The default implementation will retry to call :meth:`recv_noblock` until it succeeds under the given `timeout`.
         """
-        return self._retry(lambda: self.recv_noblock(bufsize), timeout)
+        return self._retry(lambda: self.recv_noblock(bufsize), timeout)[0]
 
 
 class SelectorBufferedStreamReadTransport(SelectorStreamReadTransport, transports.BufferedStreamReadTransport):
     """
     A continuous stream data reader transport using the :mod:`selectors` module for blocking operations polling
     that supports externally allocated buffers.
     """
@@ -209,15 +209,15 @@
 
     def recv_into(self, buffer: WriteableBuffer, timeout: float) -> int:
         """
         Read into the given `buffer`.
 
         The default implementation will retry to call :meth:`recv_noblock_into` until it succeeds under the given `timeout`.
         """
-        return self._retry(lambda: self.recv_noblock_into(buffer), timeout)
+        return self._retry(lambda: self.recv_noblock_into(buffer), timeout)[0]
 
 
 class SelectorStreamWriteTransport(SelectorBaseTransport, transports.StreamWriteTransport):
     """
     A continuous stream data writer transport using the :mod:`selectors` module for blocking operations polling.
     """
 
@@ -239,15 +239,15 @@
 
     def send(self, data: bytes | bytearray | memoryview, timeout: float) -> int:
         """
         Send the `data` bytes to the remote peer.
 
         The default implementation will retry to call :meth:`send_noblock` until it succeeds under the given `timeout`.
         """
-        return self._retry(lambda: self.send_noblock(data), timeout)
+        return self._retry(lambda: self.send_noblock(data), timeout)[0]
 
 
 class SelectorStreamTransport(SelectorStreamWriteTransport, SelectorStreamReadTransport, transports.StreamTransport):
     """
     A continuous stream data transport using the :mod:`selectors` module for blocking operations polling.
     """
 
@@ -277,15 +277,15 @@
 
     def recv(self, timeout: float) -> bytes:
         """
         Read and return the next available packet.
 
         The default implementation will retry to call :meth:`recv_noblock` until it succeeds under the given `timeout`.
         """
-        return self._retry(lambda: self.recv_noblock(), timeout)
+        return self._retry(lambda: self.recv_noblock(), timeout)[0]
 
 
 class SelectorDatagramWriteTransport(SelectorBaseTransport, transports.DatagramWriteTransport):
     """
     A writer transport of unreliable packets of data using the :mod:`selectors` module for blocking operations polling.
     """
 
@@ -308,15 +308,15 @@
 
     def send(self, data: bytes | bytearray | memoryview, timeout: float) -> None:
         """
         Send the `data` bytes to the remote peer.
 
         The default implementation will retry to call :meth:`send_noblock` until it succeeds under the given `timeout`.
         """
-        return self._retry(lambda: self.send_noblock(data), timeout)
+        return self._retry(lambda: self.send_noblock(data), timeout)[0]
 
 
 class SelectorDatagramTransport(SelectorDatagramWriteTransport, SelectorDatagramReadTransport, transports.DatagramTransport):
     """
     A transport of unreliable packets of data using the :mod:`selectors` module for blocking operations polling.
     """
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/lowlevel/api_sync/transports/socket.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/api_sync/transports/socket.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -18,14 +18,15 @@
 
 __all__ = [
     "SSLStreamTransport",
     "SocketDatagramTransport",
     "SocketStreamTransport",
 ]
 
+import errno
 import itertools
 import selectors
 import socket
 from collections import ChainMap, deque
 from collections.abc import Callable, Iterable, Mapping
 from typing import TYPE_CHECKING, Any, ParamSpec, TypeVar
 
@@ -43,15 +44,15 @@
 
 if TYPE_CHECKING:
     import ssl as _typing_ssl
 
     from _typeshed import WriteableBuffer
 
 _P = ParamSpec("_P")
-_R = TypeVar("_R")
+_T_Return = TypeVar("_T_Return")
 
 
 def _close_stream_socket(sock: socket.socket) -> None:
     try:
         sock.shutdown(socket.SHUT_RDWR)
     except OSError:
         pass
@@ -121,18 +122,16 @@
         def try_sendmsg() -> int:
             try:
                 return sock_sendmsg(itertools.islice(buffers, constants.SC_IOV_MAX))
             except (BlockingIOError, InterruptedError):
                 raise base_selector.WouldBlockOnWrite(self.__socket.fileno()) from None
 
         while buffers:
-            with _utils.ElapsedTime() as elapsed:
-                sent: int = self._retry(try_sendmsg, timeout)
+            sent, timeout = self._retry(try_sendmsg, timeout)
             _utils.adjust_leftover_buffer(buffers, sent)
-            timeout = elapsed.recompute_timeout(timeout)
 
     @_utils.inherit_doc(base_selector.SelectorStreamTransport)
     def send_eof(self) -> None:
         if self.__socket.fileno() < 0:
             return
         try:
             self.__socket.shutdown(socket.SHUT_WR)
@@ -216,30 +215,39 @@
         except (OSError, ValueError):
             pass
         finally:
             _close_stream_socket(self.__socket)
 
     @_utils.inherit_doc(base_selector.SelectorStreamTransport)
     def recv_noblock(self, bufsize: int) -> bytes:
-        return self._try_ssl_method(self.__socket.recv, bufsize)
+        try:
+            return self._try_ssl_method(self.__socket.recv, bufsize)
+        except _ssl_module.SSLZeroReturnError if _ssl_module else ():
+            return b""
 
     @_utils.inherit_doc(base_selector.SelectorBufferedStreamReadTransport)
     def recv_noblock_into(self, buffer: WriteableBuffer) -> int:
-        return self._try_ssl_method(self.__socket.recv_into, buffer)
+        try:
+            return self._try_ssl_method(self.__socket.recv_into, buffer)
+        except _ssl_module.SSLZeroReturnError if _ssl_module else ():
+            return 0
 
     @_utils.inherit_doc(base_selector.SelectorStreamTransport)
     def send_noblock(self, data: bytes | bytearray | memoryview) -> int:
-        return self._try_ssl_method(self.__socket.send, data)
+        try:
+            return self._try_ssl_method(self.__socket.send, data)
+        except _ssl_module.SSLZeroReturnError if _ssl_module else () as exc:
+            raise _utils.error_from_errno(errno.ECONNRESET) from exc
 
     @_utils.inherit_doc(base_selector.SelectorStreamTransport)
     def send_eof(self) -> None:
         # ssl.SSLSocket.shutdown() would close both read and write streams
         raise UnsupportedOperation("SSL/TLS API does not support sending EOF.")
 
-    def _try_ssl_method(self, socket_method: Callable[_P, _R], /, *args: _P.args, **kwargs: _P.kwargs) -> _R:
+    def _try_ssl_method(self, socket_method: Callable[_P, _T_Return], /, *args: _P.args, **kwargs: _P.kwargs) -> _T_Return:
         if _ssl_module is None:
             raise RuntimeError("stdlib ssl module not available")
         try:
             return socket_method(*args, **kwargs)
         except (_ssl_module.SSLWantReadError, _ssl_module.SSLSyscallError):
             raise base_selector.WouldBlockOnRead(self.__socket.fileno()) from None
         except _ssl_module.SSLWantWriteError:
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/lowlevel/std_asyncio/_asyncio_utils.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/std_asyncio/_asyncio_utils.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -14,27 +14,32 @@
 #
 """asyncio engine for easynetwork.api_async
 """
 
 from __future__ import annotations
 
 __all__ = [
+    "add_flowcontrol_defaults",
     "create_connection",
     "create_datagram_connection",
+    "ensure_resolved",
     "open_listener_sockets_from_getaddrinfo_result",
+    "resolve_local_addresses",
     "wait_until_readable",
     "wait_until_writable",
 ]
 
 import asyncio
 import contextlib
 import itertools
+import math
 import socket as _socket
+from collections import OrderedDict
 from collections.abc import Iterable, Sequence
-from typing import Any
+from typing import Any, cast
 
 from .. import _utils
 
 
 async def ensure_resolved(
     host: str | None,
     port: int,
@@ -140,19 +145,101 @@
     assert errors  # nosec assert_used
     try:
         raise ExceptionGroup("create_connection() failed", errors)
     finally:
         errors.clear()
 
 
+# Taken from asyncio library (https://github.com/python/cpython/tree/v3.12.0/Lib/asyncio)
+def _interleave_addrinfos(
+    addrinfos: Sequence[tuple[int, int, int, str, tuple[Any, ...]]]
+) -> list[tuple[int, int, int, str, tuple[Any, ...]]]:
+    """Interleave list of addrinfo tuples by family."""
+    # Group addresses by family
+    addrinfos_by_family: OrderedDict[int, list[tuple[Any, ...]]] = OrderedDict()
+    for addr in addrinfos:
+        family = addr[0]
+        if family not in addrinfos_by_family:
+            addrinfos_by_family[family] = []
+        addrinfos_by_family[family].append(addr)
+    addrinfos_lists = list(addrinfos_by_family.values())
+    return [addr for addr in itertools.chain.from_iterable(itertools.zip_longest(*addrinfos_lists)) if addr is not None]
+
+
+# Taken from anyio project (https://github.com/agronholm/anyio/tree/4.2.0)
+def _prioritize_ipv6_over_ipv4(
+    addrinfos: Sequence[tuple[int, int, int, str, tuple[Any, ...]]]
+) -> list[tuple[int, int, int, str, tuple[Any, ...]]]:
+    # Organize the list so that the first address is an IPv6 address (if available)
+    # and the second one is an IPv4 addresses. The rest can be in whatever order.
+    v6_found = v4_found = False
+    reordered: list[tuple[int, int, int, str, tuple[Any, ...]]] = []
+    for addr in addrinfos:
+        family = addr[0]
+        if family == _socket.AF_INET6 and not v6_found:
+            v6_found = True
+            reordered.insert(0, addr)
+        elif family == _socket.AF_INET and not v4_found and v6_found:
+            v4_found = True
+            reordered.insert(1, addr)
+        else:
+            reordered.append(addr)
+    return reordered
+
+
+async def _staggered_race_connection_impl(
+    *,
+    loop: asyncio.AbstractEventLoop,
+    remote_addrinfo: Sequence[tuple[int, int, int, str, tuple[Any, ...]]],
+    local_addrinfo: Sequence[tuple[int, int, int, str, tuple[Any, ...]]] | None,
+    happy_eyeballs_delay: float,
+) -> _socket.socket:
+    from .tasks import CancelScope
+
+    remote_addrinfo = _interleave_addrinfos(_prioritize_ipv6_over_ipv4(remote_addrinfo))
+    winner: _socket.socket | None = cast(_socket.socket | None, None)
+    errors: list[OSError | BaseExceptionGroup[OSError]] = []
+
+    async def try_connect(addr: tuple[int, int, int, str, tuple[Any, ...]]) -> None:
+        nonlocal winner
+        try:
+            socket = await _create_connection_impl(loop=loop, remote_addrinfo=[addr], local_addrinfo=local_addrinfo)
+        except* OSError as excgrp:
+            errors.extend(excgrp.exceptions)
+        else:
+            if winner is None:
+                winner = socket
+                connection_scope.cancel()
+            else:
+                socket.close()
+
+    try:
+        with CancelScope() as connection_scope:
+            async with asyncio.TaskGroup() as task_group:
+                for addr in remote_addrinfo:
+                    await asyncio.wait({task_group.create_task(try_connect(addr))}, timeout=happy_eyeballs_delay)
+
+        if winner is None:
+            raise BaseExceptionGroup("create_connection() failed", errors)
+        return winner
+    except BaseException:
+        if winner is not None:
+            winner.close()
+        raise
+    finally:
+        errors.clear()
+
+
 async def create_connection(
     host: str,
     port: int,
     loop: asyncio.AbstractEventLoop,
     local_address: tuple[str, int] | None = None,
+    *,
+    happy_eyeballs_delay: float = math.inf,
 ) -> _socket.socket:
     remote_addrinfo: Sequence[tuple[int, int, int, str, tuple[Any, ...]]] = await ensure_resolved(
         host,
         port,
         family=_socket.AF_UNSPEC,
         type=_socket.SOCK_STREAM,
         loop=loop,
@@ -164,18 +251,19 @@
             local_host,
             local_port,
             family=_socket.AF_UNSPEC,
             type=_socket.SOCK_STREAM,
             loop=loop,
         )
 
-    return await _create_connection_impl(
+    return await _staggered_race_connection_impl(
         loop=loop,
         remote_addrinfo=remote_addrinfo,
         local_addrinfo=local_addrinfo,
+        happy_eyeballs_delay=happy_eyeballs_delay,
     )
 
 
 async def create_datagram_connection(
     host: str,
     port: int,
     loop: asyncio.AbstractEventLoop,
@@ -290,7 +378,28 @@
         if not f.done():
             f.set_result(None)
 
     f = loop.create_future()
     loop.add_writer(sock, wakeup, f)
     f.add_done_callback(on_fut_done)
     return f
+
+
+# Taken from asyncio library (https://github.com/python/cpython/tree/v3.12.0/Lib/asyncio)
+def add_flowcontrol_defaults(high: int | None, low: int | None, kb: int) -> tuple[int, int]:  # pragma: no cover
+    if high is None:
+        if low is None:
+            hi = kb * 1024
+        else:
+            lo = low
+            hi = 4 * lo
+    else:
+        hi = high
+    if low is None:
+        lo = hi // 4
+    else:
+        lo = low
+
+    if not hi >= lo >= 0:
+        raise ValueError(f"high ({hi!r}) must be >= low ({lo!r}) must be >= 0")
+
+    return hi, lo
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/lowlevel/std_asyncio/socket.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/std_asyncio/socket.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -24,15 +24,14 @@
 import contextlib
 import errno as _errno
 import itertools
 import socket as _socket
 from collections import deque
 from collections.abc import Iterable, Iterator
 from typing import TYPE_CHECKING, Literal, Self, TypeAlias, cast
-from weakref import WeakSet
 
 from ...exceptions import UnsupportedOperation
 from .. import _utils, constants
 from . import _asyncio_utils
 from .tasks import CancelScope, TaskUtils
 
 if TYPE_CHECKING:
@@ -60,19 +59,19 @@
 
         _utils.check_socket_no_ssl(socket)
         socket.setblocking(False)
 
         self.__socket: _socket.socket | None = socket
         self.__trsock: asyncio.trsock.TransportSocket = asyncio.trsock.TransportSocket(socket)
         self.__loop: asyncio.AbstractEventLoop = loop
-        self.__scopes: WeakSet[CancelScope] = WeakSet()
+        self.__scopes: set[CancelScope] = set()
         self.__waiters: dict[_SocketTaskId, asyncio.Future[None]] = {}
         self.__close_waiter: asyncio.Future[None] = loop.create_future()
 
-    def __del__(self) -> None:  # pragma: no cover
+    def __del__(self) -> None:
         try:
             socket: _socket.socket | None = self.__socket
         except AttributeError:
             return
         if socket is not None:
             socket.close()
 
@@ -107,15 +106,15 @@
             for scope in list(self.__scopes):
                 scope.cancel()
                 del scope
             futures_to_wait_for_completion: set[asyncio.Future[None]] = set(self.__waiters.values())
             if futures_to_wait_for_completion:
                 await asyncio.wait(futures_to_wait_for_completion, return_when=asyncio.ALL_COMPLETED)
 
-        await asyncio.sleep(0)
+        await TaskUtils.coro_yield()
 
     async def accept(self) -> _socket.socket:
         listener_socket = self.__check_not_closed()
         with self.__conflict_detection("accept"):
             client_socket, _ = await self.__loop.sock_accept(listener_socket)
             return client_socket
 
@@ -153,34 +152,39 @@
         socket = self.__check_not_closed()
         with self.__conflict_detection("recv"):
             return await self.__loop.sock_recv(socket, bufsize)
 
     async def recv_into(self, buffer: WriteableBuffer, /) -> int:
         socket = self.__check_not_closed()
         with self.__conflict_detection("recv"):
+            # NOTE: Workaround for an issue in asyncio.ProactorEventLoop which occurs because a call to Overlapped.WSARecvInto()
+            # does not release the exported buffer (using PyBuffer_Release()) at the end of the function unless the
+            # garbage collector clears the object.
+            buffer = memoryview(buffer)
+
             return await self.__loop.sock_recv_into(socket, buffer)
 
     async def recvfrom(self, bufsize: int, /) -> tuple[bytes, _socket._RetAddress]:
+        socket = self.__check_not_closed()
         with self.__conflict_detection("recv"):
-            socket = self.__check_not_closed()
             return await self.__loop.sock_recvfrom(socket, bufsize)
 
     async def shutdown(self, how: int, /) -> None:
         TaskUtils.check_current_event_loop(self.__loop)
 
         if how in {_socket.SHUT_RDWR, _socket.SHUT_WR}:
             while (waiter := self.__waiters.get("send")) is not None:
                 try:
                     await asyncio.shield(waiter)
                 finally:
                     waiter = None  # Break cyclic reference with raised exception
 
         socket: _socket.socket = self.__check_not_closed()
         socket.shutdown(how)
-        await asyncio.sleep(0)
+        await TaskUtils.coro_yield()
 
     @contextlib.contextmanager
     def __conflict_detection(self, task_id: _SocketTaskId) -> Iterator[None]:
         if task_id in self.__waiters:
             raise _utils.error_from_errno(_errno.EBUSY)
 
         TaskUtils.check_current_event_loop(self.__loop)
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/lowlevel/std_asyncio/tasks.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/std_asyncio/tasks.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -18,29 +18,38 @@
 from __future__ import annotations
 
 __all__ = ["CancelScope", "Task", "TaskGroup", "TaskUtils"]
 
 import asyncio
 import contextvars
 import enum
+import inspect
 import math
+import types
 from collections import deque
-from collections.abc import Callable, Coroutine, Iterable, Iterator
-from typing import TYPE_CHECKING, Any, NamedTuple, Self, TypeVar, final
+from collections.abc import Awaitable, Callable, Coroutine, Generator, Iterable, Iterator
+from typing import TYPE_CHECKING, Any, ClassVar, NamedTuple, Self, TypeVar, TypeVarTuple, cast, final
 from weakref import WeakKeyDictionary
 
+from .. import _utils
 from .._final import runtime_final_class
-from ..api_async.backend.abc import CancelScope as AbstractCancelScope, Task as AbstractTask, TaskGroup as AbstractTaskGroup
+from ..api_async.backend.abc import (
+    CancelScope as AbstractCancelScope,
+    Task as AbstractTask,
+    TaskGroup as AbstractTaskGroup,
+    TaskInfo,
+)
 
 if TYPE_CHECKING:
     from types import TracebackType
 
 
 _T = TypeVar("_T")
 _T_co = TypeVar("_T_co", covariant=True)
+_T_PosArgs = TypeVarTuple("_T_PosArgs")
 
 
 @final
 @runtime_final_class
 class Task(AbstractTask[_T_co]):
     __slots__ = ("__t", "__h")
 
@@ -57,26 +66,44 @@
         return h
 
     def __eq__(self, other: object, /) -> bool:
         if not isinstance(other, Task):
             return NotImplemented
         return self.__t == other.__t
 
+    @property
+    def info(self) -> TaskInfo:
+        return TaskUtils.create_task_info(self.__t)
+
     def done(self) -> bool:
         return self.__t.done()
 
     def cancel(self) -> bool:
         return self.__t.cancel()
 
     def cancelled(self) -> bool:
         return self.__t.cancelled()
 
     async def wait(self) -> None:
         task = self.__t
-        await asyncio.wait({task})
+        if task.done():
+            return
+        try:
+            waiter = asyncio.Event()
+
+            def on_task_done(task: asyncio.Task[Any]) -> None:
+                waiter.set()
+
+            task.add_done_callback(on_task_done)
+            try:
+                await waiter.wait()
+            finally:
+                task.remove_done_callback(on_task_done)
+        finally:
+            del task, self  # This is needed to avoid circular reference with raised exception
 
     async def join(self) -> _T_co:
         task = self.__t
         try:
             return await asyncio.shield(task)
         finally:
             del task, self  # This is needed to avoid circular reference with raised exception
@@ -113,20 +140,54 @@
         try:
             await type(asyncio_tg).__aexit__(asyncio_tg, exc_type, exc_val, exc_tb)
         finally:
             del exc_val, exc_tb, self
 
     def start_soon(
         self,
-        coro_func: Callable[..., Coroutine[Any, Any, _T]],
+        coro_func: Callable[[*_T_PosArgs], Coroutine[Any, Any, _T]],
         /,
-        *args: Any,
-        context: contextvars.Context | None = None,
+        *args: *_T_PosArgs,
+        name: str | None = None,
+    ) -> None:
+        _ = self.__asyncio_tg.create_task(coro_func(*args), name=name)
+
+    async def start(
+        self,
+        coro_func: Callable[[*_T_PosArgs], Coroutine[Any, Any, _T]],
+        /,
+        *args: *_T_PosArgs,
+        name: str | None = None,
     ) -> AbstractTask[_T]:
-        return Task(self.__asyncio_tg.create_task(coro_func(*args), context=context))
+        loop = asyncio.get_running_loop()
+        waiter: asyncio.Future[None] = loop.create_future()
+
+        task = Task(self.__asyncio_tg.create_task(self.__task_coroutine(coro_func, args, waiter), name=name))
+
+        try:
+            await waiter
+        finally:
+            del waiter
+        return task
+
+    @staticmethod
+    async def __task_coroutine(
+        coro_func: Callable[[*_T_PosArgs], Coroutine[Any, Any, _T]],
+        args: tuple[*_T_PosArgs],
+        waiter: asyncio.Future[None],
+    ) -> _T:
+        if not waiter.done():
+            waiter.set_result(None)
+        del waiter
+
+        try:
+            return await coro_func(*args)
+        except BaseException as exc:
+            _utils.remove_traceback_frames_in_place(exc, 1)
+            raise
 
 
 class _ScopeState(enum.Enum):
     CREATED = "created"
     ENTERED = "entered"
     EXITED = "exited"
 
@@ -137,33 +198,39 @@
 
 
 @final
 @runtime_final_class
 class CancelScope(AbstractCancelScope):
     __slots__ = (
         "__host_task",
+        "__host_task_cancelling",
+        "__host_task_cancel_calls",
         "__state",
         "__cancel_called",
         "__cancelled_caught",
         "__deadline",
         "__timeout_handle",
+        "__cancel_handle",
         "__delayed_cancellation_on_enter",
     )
 
-    __current_task_scope_dict: WeakKeyDictionary[asyncio.Task[Any], deque[CancelScope]] = WeakKeyDictionary()
-    __delayed_task_cancel_dict: WeakKeyDictionary[asyncio.Task[Any], _DelayedCancel] = WeakKeyDictionary()
+    __current_task_scope_dict: ClassVar[WeakKeyDictionary[asyncio.Task[Any], deque[CancelScope]]] = WeakKeyDictionary()
+    __delayed_task_cancel_dict: ClassVar[WeakKeyDictionary[asyncio.Task[Any], _DelayedCancel]] = WeakKeyDictionary()
 
     def __init__(self, *, deadline: float = math.inf) -> None:
         super().__init__()
         self.__host_task: asyncio.Task[Any] | None = None
+        self.__host_task_cancelling: int = 0
+        self.__host_task_cancel_calls: int = 0
         self.__state: _ScopeState = _ScopeState.CREATED
         self.__cancel_called: bool = False
         self.__cancelled_caught: bool = False
         self.__deadline: float = math.inf
         self.__timeout_handle: asyncio.Handle | None = None
+        self.__cancel_handle: asyncio.Handle | None = None
         self.reschedule(deadline)
 
     def __repr__(self) -> str:
         active = self.__state is _ScopeState.ENTERED
         cancel_called = self.__cancel_called
         cancelled_caught = self.__cancelled_caught
         host_task = self.__host_task
@@ -173,27 +240,28 @@
         return f"<{self.__class__.__name__}({info})>"
 
     def __enter__(self) -> Self:
         if self.__state is not _ScopeState.CREATED:
             raise RuntimeError("CancelScope entered twice")
 
         self.__host_task = current_task = TaskUtils.current_asyncio_task()
+        self.__host_task_cancelling = current_task.cancelling()
 
         current_task_scope = self.__current_task_scope_dict
         if current_task not in current_task_scope:
             current_task_scope[current_task] = deque()
             current_task.add_done_callback(current_task_scope.pop)
         current_task_scope[current_task].appendleft(self)
 
         self.__state = _ScopeState.ENTERED
 
         if self.__cancel_called:
             self.__deliver_cancellation()
         else:
-            self.__timeout()
+            self.__setup_cancellation_by_timeout()
         return self
 
     def __exit__(self, exc_type: type[BaseException] | None, exc_val: BaseException | None, exc_tb: TracebackType | None) -> bool:
         if self.__state is not _ScopeState.ENTERED:
             raise RuntimeError("This cancel scope is not active")
 
         if TaskUtils.current_asyncio_task() is not self.__host_task:
@@ -204,48 +272,67 @@
 
         self.__state = _ScopeState.EXITED
 
         if self.__timeout_handle:
             self.__timeout_handle.cancel()
             self.__timeout_handle = None
 
+        if self.__cancel_handle:
+            self.__cancel_handle.cancel()
+            self.__cancel_handle = None
+
         host_task, self.__host_task = self.__host_task, None
         self.__current_task_scope_dict[host_task].popleft()
 
         if self.__cancel_called:
-            task_cancelling = host_task.uncancel()
-            if isinstance(exc_val, asyncio.CancelledError):
-                self.__cancelled_caught = self.__cancellation_id() in exc_val.args
+            if exc_val is not None:
+                self.__cancelled_caught = any(
+                    self.__uncancel_task(host_task, exc)
+                    for exc in _utils.iterate_exceptions(exc_val)
+                    if isinstance(exc, asyncio.CancelledError)
+                )
 
-            delayed_task_cancel = self.__delayed_task_cancel_dict.get(host_task, None)
+            delayed_task_cancel: _DelayedCancel | None = self.__delayed_task_cancel_dict.get(host_task, None)
             if delayed_task_cancel is not None and delayed_task_cancel.message == self.__cancellation_id():
                 del self.__delayed_task_cancel_dict[host_task]
                 delayed_task_cancel.handle.cancel()
                 delayed_task_cancel = None
 
-            if delayed_task_cancel is None:
-                for cancel_scope in self._inner_to_outer_task_scopes(host_task):
-                    if cancel_scope.__cancel_called:
-                        self._reschedule_delayed_task_cancel(host_task, cancel_scope.__cancellation_id())
-                        break
-                else:
-                    if task_cancelling > 0:
-                        self._reschedule_delayed_task_cancel(host_task, None)
+        for parent_scope in self._inner_to_outer_task_scopes(host_task):
+            if parent_scope.__cancel_called:
+                if parent_scope.__cancel_handle is None:
+                    parent_scope.__deliver_cancellation()
+                break
 
         return self.__cancelled_caught
 
+    def __uncancel_task(self, host_task: asyncio.Task[Any], exc: asyncio.CancelledError) -> bool:
+        while self.__host_task_cancel_calls:
+            self.__host_task_cancel_calls -= 1
+            if host_task.uncancel() <= self.__host_task_cancelling:
+                return True
+        return self.__cancellation_id() in exc.args
+
     def __deliver_cancellation(self) -> None:
         if self.__host_task is None:
             # Scope not active.
             return
-        try:
-            self.__delayed_task_cancel_dict.pop(self.__host_task).handle.cancel()
-        except KeyError:
-            pass
-        self.__host_task.cancel(msg=self.__cancellation_id())
+
+        should_retry: bool = False
+
+        if not self.__task_must_cancel(self.__host_task) and self.__host_task not in self.__delayed_task_cancel_dict:
+            should_retry = True
+            if self.__host_task is not asyncio.current_task():
+                self.__host_task.cancel(msg=self.__cancellation_id())
+                self.__host_task_cancel_calls += 1
+
+        if should_retry:
+            self.__cancel_handle = self.__host_task.get_loop().call_soon(self.__deliver_cancellation)
+        else:
+            self.__cancel_handle = None
 
     def __cancellation_id(self) -> str:
         return f"Cancelled by cancel scope {id(self):x}"
 
     def cancel(self) -> None:
         if not self.__cancel_called:
             self.__cancel_called = True
@@ -267,22 +354,22 @@
         if math.isnan(when):
             raise ValueError("deadline is NaN")
         self.__deadline = max(when, 0)
         if self.__timeout_handle:
             self.__timeout_handle.cancel()
             self.__timeout_handle = None
         if self.__state is _ScopeState.ENTERED and not self.__cancel_called:
-            self.__timeout()
+            self.__setup_cancellation_by_timeout()
 
-    def __timeout(self) -> None:
+    def __setup_cancellation_by_timeout(self) -> None:
         if self.__deadline != math.inf:
             assert self.__host_task is not None  # nosec assert_used
             loop = self.__host_task.get_loop()
             if loop.time() >= self.__deadline:
-                self.__timeout_handle = loop.call_soon(self.cancel)
+                self.cancel()
             else:
                 self.__timeout_handle = loop.call_at(self.__deadline, self.cancel)
 
     @classmethod
     def _current_task_scope(cls, task: asyncio.Task[Any]) -> CancelScope | None:
         try:
             return cls.__current_task_scope_dict[task][0]
@@ -294,27 +381,31 @@
         if cls._current_task_scope(task) is None:
             return iter(())
         return iter(cls.__current_task_scope_dict[task])
 
     @classmethod
     def _reschedule_delayed_task_cancel(cls, task: asyncio.Task[Any], cancel_msg: str | None) -> asyncio.Handle:
         if task in cls.__delayed_task_cancel_dict:
-            raise RuntimeError("CancelScope issue.")  # pragma: no cover
+            raise AssertionError("_reschedule_delayed_task_cancel() called too many times.")
         task_cancel_handle = task.get_loop().call_soon(cls.__cancel_task_unless_done, task, cancel_msg)
         cls.__delayed_task_cancel_dict[task] = _DelayedCancel(task_cancel_handle, cancel_msg)
         task.get_loop().call_soon(cls.__delayed_task_cancel_dict.pop, task, None)
         return task_cancel_handle
 
     @staticmethod
     def __cancel_task_unless_done(task: asyncio.Task[Any], cancel_msg: str | None) -> None:
         if task.done():
             return
         task.uncancel()
         task.cancel(cancel_msg)
 
+    @staticmethod
+    def __task_must_cancel(task: asyncio.Task[Any]) -> bool:
+        return getattr(task, "_must_cancel", False)
+
 
 @final
 @runtime_final_class
 class TaskUtils:
     @staticmethod
     def check_current_event_loop(loop: asyncio.AbstractEventLoop) -> None:
         running_loop = asyncio.get_running_loop()
@@ -324,72 +415,151 @@
     @staticmethod
     def current_asyncio_task(loop: asyncio.AbstractEventLoop | None = None) -> asyncio.Task[Any]:
         t: asyncio.Task[Any] | None = asyncio.current_task(loop=loop)
         if t is None:
             raise RuntimeError("This function should be called within a task.")
         return t
 
+    @staticmethod
+    @types.coroutine
+    def coro_yield() -> Generator[Any, Any, None]:
+        yield
+
     @classmethod
-    async def cancel_shielded_wait_asyncio_futures(
-        cls,
-        fs: Iterable[asyncio.Future[Any]],
-        *,
-        abort_func: Callable[[], bool] | None = None,
-    ) -> asyncio.Handle | None:
+    async def cancel_shielded_wait_asyncio_futures(cls, fs: Iterable[asyncio.Future[Any]]) -> asyncio.Handle | None:
         fs = set(fs)
         current_task: asyncio.Task[Any] = cls.current_asyncio_task()
-        abort: bool | None = None
         task_cancelled: bool = False
         task_cancel_msg: str | None = None
 
         try:
             _schedule_task_discard(fs)
-            while fs:
-                try:
-                    await asyncio.wait(fs)
-                except asyncio.CancelledError as exc:
-                    if abort is None:
-                        if abort_func is None:
-                            abort = False
-                        else:
-                            abort = bool(abort_func())
-                    if abort:
-                        raise
-                    task_cancelled = True
-                    task_cancel_msg = _get_cancelled_error_message(exc)
+            # Open a scope in order to check pending cancellations at the end.
+            with CancelScope():
+                while fs:
+                    try:
+                        await asyncio.wait(fs)
+                    except asyncio.CancelledError as exc:
+                        task_cancelled = True
+                        task_cancel_msg = _get_cancelled_error_message(exc)
 
-            if task_cancelled:
-                return CancelScope._reschedule_delayed_task_cancel(current_task, task_cancel_msg)
+                if task_cancelled:
+                    return CancelScope._reschedule_delayed_task_cancel(current_task, task_cancel_msg)
             return None
         finally:
-            del current_task, fs, abort_func
+            del current_task, fs
+            task_cancel_msg = None
+
+    @classmethod
+    async def cancel_shielded_await_future(cls, future: asyncio.Future[_T_co]) -> _T_co:
+        loop = asyncio.get_running_loop()
+        current_task: asyncio.Task[Any] = cls.current_asyncio_task(loop)
+        task_cancelled: bool = False
+        task_cancel_msg: str | None = None
+        try:
+            done_event = asyncio.Event()
+            future.add_done_callback(lambda _: done_event.set())
+
+            # Open a scope in order to check pending cancellations at the end.
+            with CancelScope():
+                while not done_event.is_set():
+                    try:
+                        await done_event.wait()
+                    except asyncio.CancelledError as exc:
+                        task_cancelled = True
+                        task_cancel_msg = _get_cancelled_error_message(exc)
+
+                if task_cancelled:
+                    CancelScope._reschedule_delayed_task_cancel(current_task, task_cancel_msg)
+                    if future.cancelled():
+                        await cls.coro_yield()
+            return future.result()
+        finally:
+            del current_task, future
             task_cancel_msg = None
 
     @classmethod
     async def cancel_shielded_coro_yield(cls) -> None:
-        current_task: asyncio.Task[Any] = cls.current_asyncio_task()
         try:
-            await asyncio.sleep(0)
+            await cls.coro_yield()
         except asyncio.CancelledError as exc:
-            CancelScope._reschedule_delayed_task_cancel(current_task, _get_cancelled_error_message(exc))
-        finally:
-            del current_task
+            CancelScope._reschedule_delayed_task_cancel(cls.current_asyncio_task(), _get_cancelled_error_message(exc))
 
     @classmethod
-    async def cancel_shielded_await_task(cls, task: asyncio.Task[_T_co]) -> _T_co:
-        # This task must be unregistered in order not to be cancelled by runner at event loop shutdown
-        asyncio._unregister_task(task)
+    async def cancel_shielded_await(cls, coroutine: Awaitable[_T_co]) -> _T_co:
+        coroutine = cls.wrap_awaitable(coroutine)
+        loop = asyncio.get_running_loop()
+        current_task_context = cls.get_task_context(cls.current_asyncio_task(loop))
+
+        def copy_future_state(waiter: asyncio.Future[_T_co], task: asyncio.Task[_T_co]) -> None:
+            if waiter.done():
+                raise AssertionError("waiter should not be done")
+            if task.cancelled():
+                waiter.cancel()
+            elif (exc := task.exception()) is not None:
+                waiter.set_exception(exc)
+            else:
+                waiter.set_result(task.result())
 
+        def schedule_task(coroutine: Coroutine[Any, Any, _T_co], waiter: asyncio.Future[_T_co]) -> None:
+            try:
+                task = loop.create_task(coroutine, context=current_task_context)
+            except BaseException as exc:
+                waiter.set_exception(exc)
+                coroutine.close()
+            else:
+                if task.done():  # eager task done
+                    copy_future_state(waiter, task)
+                else:
+                    task.add_done_callback(_utils.prepend_argument(waiter, copy_future_state))
+                    # This task must be unregistered in order not to be cancelled by runner at event loop shutdown
+                    asyncio._unregister_task(task)
+
+        waiter: asyncio.Future[_T_co] = loop.create_future()
+        loop.call_soon(schedule_task, coroutine, waiter)
+        del coroutine
         try:
-            current_task_cancel_handle = await cls.cancel_shielded_wait_asyncio_futures({task})
-            if current_task_cancel_handle is not None and task.cancelled():
-                current_task_cancel_handle.cancel()
-            return task.result()
+            return await cls.cancel_shielded_await_future(waiter)
         finally:
-            del task
+            del waiter
+
+    @classmethod
+    def wrap_awaitable(cls, coroutine: Awaitable[_T]) -> Coroutine[Any, Any, _T]:
+        if not inspect.isawaitable(coroutine):
+            raise TypeError("Expected an awaitable object")
+
+        if not isinstance(coroutine, Coroutine):
+
+            async def coro_wrapper(coroutine: Awaitable[_T]) -> _T:
+                try:
+                    return await coroutine
+                except BaseException as exc:
+                    _utils.remove_traceback_frames_in_place(exc, 1)
+                    raise
+
+            coroutine = coro_wrapper(coroutine)
+            del coro_wrapper
+
+        return coroutine
+
+    @classmethod
+    def create_task_info(cls, task: asyncio.Task[Any]) -> TaskInfo:
+        return TaskInfo(id(task), task.get_name(), cast(Coroutine[Any, Any, Any] | None, task.get_coro()))
+
+    @classmethod
+    def compute_task_name_from_func(cls, func: Callable[..., Any]) -> str:
+        return _utils.get_callable_name(func) or repr(func)
+
+    @classmethod
+    def get_task_context(cls, task: asyncio.Task[Any]) -> contextvars.Context | None:
+        try:
+            get_context: Callable[[], contextvars.Context] = getattr(task, "get_context")
+        except AttributeError:
+            return None
+        return get_context()
 
 
 def _get_cancelled_error_message(exc: asyncio.CancelledError) -> str | None:
     msg: str | None
     if exc.args:
         msg = exc.args[0]
     else:
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/lowlevel/std_asyncio/threads.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/std_asyncio/threads.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -78,52 +78,57 @@
     def run_coroutine_soon(
         self,
         coro_func: Callable[_P, Awaitable[_T]],
         /,
         *args: _P.args,
         **kwargs: _P.kwargs,
     ) -> concurrent.futures.Future[_T]:
-        def schedule_task() -> concurrent.futures.Future[_T]:
-            future: concurrent.futures.Future[_T] = concurrent.futures.Future()
+        future: concurrent.futures.Future[_T] = concurrent.futures.Future()
+
+        def on_fut_done(task: asyncio.Task[None], future: concurrent.futures.Future[_T]) -> None:
+            if future.cancelled():
+                with contextlib.suppress(RuntimeError):
+                    loop = task.get_loop()
+                    loop.call_soon_threadsafe(task.cancel)
+
+        async def coroutine(waiter: asyncio.Future[None]) -> None:
+            try:
+                waiter.set_result(None)
+                del waiter
+
+                future.add_done_callback(_utils.prepend_argument(TaskUtils.current_asyncio_task(), on_fut_done))
 
-            async def coroutine() -> None:
-                def on_fut_done(future: concurrent.futures.Future[_T]) -> None:
-                    if future.cancelled():
-                        if self.__is_in_this_loop_thread(loop):
-                            if not cancelling:
-                                task.cancel()
-                            return
-                        with contextlib.suppress(RuntimeError):
-                            self.run_sync(task.cancel)
-
-                task = TaskUtils.current_asyncio_task()
-                loop = task.get_loop()
-                cancelling: bool = False
-                try:
-                    future.add_done_callback(on_fut_done)
-                    result = await coro_func(*args, **kwargs)
-                except asyncio.CancelledError:
-                    cancelling = True
-                    future.cancel()
-                    future.set_running_or_notify_cancel()
+                result = await coro_func(*args, **kwargs)
+            except asyncio.CancelledError:
+                future.cancel()
+                future.set_running_or_notify_cancel()
+                raise
+            except BaseException as exc:
+                if future.set_running_or_notify_cancel():
+                    future.set_exception(exc)
+                if not isinstance(exc, Exception):
                     raise
-                except BaseException as exc:
-                    if future.set_running_or_notify_cancel():
-                        future.set_exception(exc)
-                    if not isinstance(exc, Exception):
-                        raise  # pragma: no cover
-                else:
-                    if future.set_running_or_notify_cancel():
-                        future.set_result(result)
-
-            task = self.__task_group.create_task(coroutine())
-            loop = task.get_loop()
-            del task
-            with self.__lock.get():
-                loop.call_soon(self.__register_waiter(self.__call_soon_waiters, loop).set_result, None)
+                elif future.cancelled():
+                    loop = asyncio.get_running_loop()
+                    loop.call_soon(
+                        loop.call_exception_handler,
+                        {
+                            "message": "Task exception was not retrieved because future object is cancelled",
+                            "exception": exc,
+                            "task": TaskUtils.current_asyncio_task(loop),
+                        },
+                    )
+            else:
+                if future.set_running_or_notify_cancel():
+                    future.set_result(result)
+
+        def schedule_task() -> concurrent.futures.Future[_T]:
+            loop = asyncio.get_running_loop()
+            waiter = self.__register_waiter(self.__call_soon_waiters, loop)
+            _ = self.__task_group.create_task(coroutine(waiter), name=TaskUtils.compute_task_name_from_func(coro_func))
             return future
 
         return self.run_sync(schedule_task)
 
     def run_sync_soon(self, func: Callable[_P, _T], /, *args: _P.args, **kwargs: _P.kwargs) -> concurrent.futures.Future[_T]:
         def callback() -> None:
             waiter.set_result(None)
@@ -134,16 +139,16 @@
                 if inspect.iscoroutine(result):
                     result.close()  # Prevent ResourceWarnings
                     msg = "func is a coroutine function."
                     note = "You should use run_coroutine() or run_coroutine_soon() instead."
                     raise _utils.exception_with_notes(TypeError(msg), note)
             except BaseException as exc:
                 future.set_exception(exc)
-                if isinstance(exc, (SystemExit, KeyboardInterrupt)):
-                    raise  # pragma: no cover
+                if not isinstance(exc, Exception):
+                    raise
             else:
                 future.set_result(result)
 
         with self.__lock.get():
             loop = self.__check_loop()
             waiter = self.__register_waiter(self.__call_soon_waiters, loop)
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/lowlevel/std_asyncio/datagram/__init__.py` & `easynetwork-1.0.0rc8/src/easynetwork/serializers/wrapper/__init__.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,20 +1,26 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 #
-"""asyncio engine for easynetwork.async
-"""
+"""EasyNetwork's packet serializer module"""
 
 from __future__ import annotations
 
-__all__ = []  # type: list[str]
+__all__ = [
+    "BZ2CompressorSerializer",
+    "Base64EncoderSerializer",
+    "ZlibCompressorSerializer",
+]
+
+from .base64 import Base64EncoderSerializer
+from .compressor import BZ2CompressorSerializer, ZlibCompressorSerializer
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/lowlevel/std_asyncio/datagram/endpoint.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/std_asyncio/datagram/endpoint.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -21,43 +21,42 @@
     "DatagramEndpoint",
     "DatagramEndpointProtocol",
     "create_datagram_endpoint",
 ]
 
 import asyncio
 import asyncio.base_events
-import collections
 import errno as _errno
 import socket as _socket
 from typing import TYPE_CHECKING, Any, final
 
 from ... import _utils
-from ..tasks import TaskUtils
+from .._flow_control import WriteFlowControl
 
 if TYPE_CHECKING:
     import asyncio.trsock
 
 
 async def create_datagram_endpoint(
     *,
     family: int = 0,
     local_addr: tuple[str, int] | None = None,
     remote_addr: tuple[str, int] | None = None,
-    reuse_port: bool = False,
+    reuse_port: bool | None = None,
     sock: _socket.socket | None = None,
 ) -> DatagramEndpoint:
     loop = asyncio.get_running_loop()
     recv_queue: asyncio.Queue[tuple[bytes, tuple[Any, ...]] | None] = asyncio.Queue()
     exception_queue: asyncio.Queue[Exception] = asyncio.Queue()
     flags: int = 0
     if remote_addr is not None:
         flags |= _socket.AI_PASSIVE
 
     transport, protocol = await loop.create_datagram_endpoint(
-        lambda: DatagramEndpointProtocol(loop=loop, recv_queue=recv_queue, exception_queue=exception_queue),
+        _utils.make_callback(DatagramEndpointProtocol, loop=loop, recv_queue=recv_queue, exception_queue=exception_queue),
         family=family,
         local_addr=local_addr,
         remote_addr=remote_addr,
         reuse_port=reuse_port,
         sock=sock,
         flags=flags,
     )
@@ -88,48 +87,44 @@
         self.__exception_queue: asyncio.Queue[Exception] = exception_queue
         self.__transport: asyncio.DatagramTransport = transport
         self.__protocol: DatagramEndpointProtocol = protocol
 
     async def aclose(self) -> None:
         if self.__transport.is_closing():
             # Only wait for it.
-            await self.__protocol._get_close_waiter()
+            await asyncio.shield(self.__protocol._get_close_waiter())
             return
 
         self.__transport.close()
-        await self.__protocol._get_close_waiter()
+        await asyncio.shield(self.__protocol._get_close_waiter())
 
     def is_closing(self) -> bool:
         return self.__transport.is_closing()
 
     async def recvfrom(self) -> tuple[bytes, tuple[Any, ...]]:
-        self.__check_exceptions()
         if self.__transport.is_closing():
             try:
                 data_and_address = self.__recv_queue.get_nowait()
             except asyncio.QueueEmpty:
                 data_and_address = None
             if data_and_address is None:
+                self.__check_exceptions()
                 raise _utils.error_from_errno(_errno.ECONNABORTED)
-            await TaskUtils.cancel_shielded_coro_yield()
         else:
             data_and_address = await self.__recv_queue.get()
             if data_and_address is None:
                 # Woken up because an error occurred ?
                 self.__check_exceptions()
 
                 # Connection lost otherwise
                 assert self.__transport.is_closing()  # nosec assert_used
                 raise _utils.error_from_errno(_errno.ECONNABORTED)
         return data_and_address
 
     async def sendto(self, data: bytes | bytearray | memoryview, address: tuple[Any, ...] | None = None, /) -> None:
-        self.__check_exceptions()
-        if self.__transport.is_closing():
-            raise _utils.error_from_errno(_errno.ECONNABORTED)
         self.__transport.sendto(data, address)
         await self.__protocol._drain_helper()
 
     def get_extra_info(self, name: str, default: Any = None) -> Any:
         return self.__transport.get_extra_info(name, default)
 
     def __check_exceptions(self) -> None:
@@ -139,29 +134,23 @@
             pass
         else:
             try:
                 raise exc
             finally:
                 del exc
 
-    @property
-    @final
-    def transport(self) -> asyncio.DatagramTransport:
-        return self.__transport
-
 
 class DatagramEndpointProtocol(asyncio.DatagramProtocol):
     __slots__ = (
         "__loop",
         "__recv_queue",
         "__exception_queue",
         "__transport",
+        "__write_flow",
         "__closed",
-        "__drain_waiters",
-        "__write_paused",
         "__connection_lost",
     )
 
     def __init__(
         self,
         *,
         loop: asyncio.AbstractEventLoop | None = None,
@@ -172,102 +161,64 @@
         if loop is None:
             loop = asyncio.get_running_loop()
         self.__loop: asyncio.AbstractEventLoop = loop
         self.__recv_queue: asyncio.Queue[tuple[bytes, tuple[Any, ...]] | None] = recv_queue
         self.__exception_queue: asyncio.Queue[Exception] = exception_queue
         self.__transport: asyncio.DatagramTransport | None = None
         self.__closed: asyncio.Future[None] = loop.create_future()
-        self.__drain_waiters: collections.deque[asyncio.Future[None]] = collections.deque()
-        self.__write_paused: bool = False
+        self.__write_flow: WriteFlowControl
         self.__connection_lost: bool = False
 
-    def __del__(self) -> None:  # pragma: no cover
-        # Prevent reports about unhandled exceptions.
-        try:
-            closed = self.__closed
-        except AttributeError:
-            pass
-        else:
-            if closed.done() and not closed.cancelled():
-                closed.exception()
-
     def connection_made(self, transport: asyncio.DatagramTransport) -> None:  # type: ignore[override]
+        assert not self.__connection_lost, "connection_lost() was called"  # nosec assert_used
         assert self.__transport is None, "Transport already set"  # nosec assert_used
         self.__transport = transport
-        self.__connection_lost = False
+        self.__write_flow = WriteFlowControl(self.__transport, self.__loop)
         _monkeypatch_transport(transport, self.__loop)
 
     def connection_lost(self, exc: Exception | None) -> None:
         self.__connection_lost = True
 
         if not self.__closed.done():
             self.__closed.set_result(None)
 
-        for waiter in self.__drain_waiters:
-            if not waiter.done():
-                if exc is None:
-                    waiter.set_result(None)
-                else:
-                    waiter.set_exception(exc)
+        self.__write_flow.connection_lost(exc)
 
         if self.__transport is not None:
+            self.__transport = None
             self.__recv_queue.put_nowait(None)  # Wake up endpoint
             if exc is not None:
                 self.__exception_queue.put_nowait(exc)
-            self.__transport.close()
-            self.__transport = None
-
-        super().connection_lost(exc)
 
     def datagram_received(self, data: bytes, addr: tuple[Any, ...]) -> None:
         if self.__transport is not None:
             self.__recv_queue.put_nowait((data, addr))
 
     def error_received(self, exc: Exception) -> None:
         if self.__transport is not None:
             self.__exception_queue.put_nowait(exc)
             self.__recv_queue.put_nowait(None)  # Wake up endpoint
 
     def pause_writing(self) -> None:
-        assert not self.__write_paused  # nosec assert_used
-        self.__write_paused = True
-
-        super().pause_writing()
+        self.__write_flow.pause_writing()
 
     def resume_writing(self) -> None:
-        assert self.__write_paused  # nosec assert_used
-        self.__write_paused = False
-
-        for waiter in self.__drain_waiters:
-            if not waiter.done():
-                waiter.set_result(None)
-
-        super().resume_writing()
+        self.__write_flow.resume_writing()
 
     async def _drain_helper(self) -> None:
-        if self.__connection_lost:
-            raise _utils.error_from_errno(_errno.ECONNABORTED)
-        if not self.__write_paused:
-            return
-        waiter = self.__loop.create_future()
-        self.__drain_waiters.append(waiter)
-        try:
-            await waiter
-        finally:
-            self.__drain_waiters.remove(waiter)
-            del waiter
+        await self.__write_flow.drain()
 
     def _get_close_waiter(self) -> asyncio.Future[None]:
         return self.__closed
 
     def _get_loop(self) -> asyncio.AbstractEventLoop:
         return self.__loop
 
     def _writing_paused(self) -> bool:
-        return self.__write_paused
+        return self.__write_flow.writing_paused()
 
 
 def _monkeypatch_transport(transport: asyncio.DatagramTransport, loop: asyncio.AbstractEventLoop) -> None:
     if isinstance(loop, asyncio.base_events.BaseEventLoop) and hasattr(transport, "_address"):
         # There is an asyncio issue where the private address attribute is not updated with the actual remote address
         # if the transport is instanciated with an external socket:
         #     await loop.create_datagram_endpoint(sock=my_socket)
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/lowlevel/std_asyncio/datagram/listener.py` & `easynetwork-1.0.0rc8/src/easynetwork/lowlevel/std_asyncio/datagram/socket.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -13,34 +13,31 @@
 #
 #
 """asyncio engine for easynetwork.async
 """
 
 from __future__ import annotations
 
-__all__ = ["AsyncioTransportDatagramListenerSocketAdapter", "RawDatagramListenerSocketAdapter"]
+__all__ = ["AsyncioTransportDatagramSocketAdapter"]
 
 import asyncio
-import asyncio.streams
-import socket as _socket
 from collections.abc import Callable, Mapping
 from typing import TYPE_CHECKING, Any, final
 
-from ... import constants, socket as socket_tools
+from ... import socket as socket_tools
 from ...api_async.transports import abc as transports
-from ..socket import AsyncSocket
 
 if TYPE_CHECKING:
     import asyncio.trsock
 
     from .endpoint import DatagramEndpoint
 
 
 @final
-class AsyncioTransportDatagramListenerSocketAdapter(transports.AsyncDatagramListener[tuple[Any, ...]]):
+class AsyncioTransportDatagramSocketAdapter(transports.AsyncDatagramTransport):
     __slots__ = (
         "__endpoint",
         "__socket",
         "__closing",
     )
 
     def __init__(self, endpoint: DatagramEndpoint) -> None:
@@ -52,54 +49,25 @@
 
         self.__socket: asyncio.trsock.TransportSocket = socket
         # asyncio.DatagramTransport.is_closing() can suddently become true if there is something wrong with the socket
         # even if transport.close() was never called.
         # To bypass this side effect, we use our own flag.
         self.__closing: bool = False
 
-    def is_closing(self) -> bool:
-        return self.__closing
-
     async def aclose(self) -> None:
         self.__closing = True
         await self.__endpoint.aclose()
 
-    async def recv_from(self) -> tuple[bytes, tuple[Any, ...]]:
-        return await self.__endpoint.recvfrom()
-
-    async def send_to(self, data: bytes | bytearray | memoryview, address: tuple[Any, ...]) -> None:
-        await self.__endpoint.sendto(data, address)
-
-    @property
-    def extra_attributes(self) -> Mapping[Any, Callable[[], Any]]:
-        socket = self.__socket
-        return socket_tools._get_socket_extra(socket, wrap_in_proxy=False)
-
-
-@final
-class RawDatagramListenerSocketAdapter(transports.AsyncDatagramListener[tuple[Any, ...]]):
-    __slots__ = ("__socket",)
-
-    def __init__(self, socket: _socket.socket, loop: asyncio.AbstractEventLoop) -> None:
-        super().__init__()
-
-        if socket.type != _socket.SOCK_DGRAM:
-            raise ValueError("A 'SOCK_DGRAM' socket is expected")
-
-        self.__socket: AsyncSocket = AsyncSocket(socket, loop)
-
     def is_closing(self) -> bool:
-        return self.__socket.is_closing()
-
-    async def aclose(self) -> None:
-        return await self.__socket.aclose()
+        return self.__closing
 
-    async def recv_from(self) -> tuple[bytes, tuple[Any, ...]]:
-        return await self.__socket.recvfrom(constants.MAX_DATAGRAM_BUFSIZE)
+    async def recv(self) -> bytes:
+        data, _ = await self.__endpoint.recvfrom()
+        return data
 
-    async def send_to(self, data: bytes | bytearray | memoryview, address: tuple[Any, ...]) -> None:
-        await self.__socket.sendto(data, address)
+    async def send(self, data: bytes | bytearray | memoryview) -> None:
+        await self.__endpoint.sendto(data, None)
 
     @property
     def extra_attributes(self) -> Mapping[Any, Callable[[], Any]]:
-        socket = self.__socket.socket
+        socket = self.__socket
         return socket_tools._get_socket_extra(socket, wrap_in_proxy=False)
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/serializers/__init__.py` & `easynetwork-1.0.0rc8/src/easynetwork/serializers/__init__.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -13,32 +13,24 @@
 #
 #
 """EasyNetwork's packet serializers module"""
 
 from __future__ import annotations
 
 __all__ = [
-    "AbstractStructSerializer",
-    "CBORDecoderConfig",
-    "CBOREncoderConfig",
     "CBORSerializer",
-    "JSONDecoderConfig",
-    "JSONEncoderConfig",
     "JSONSerializer",
     "MessagePackSerializer",
-    "MessagePackerConfig",
-    "MessageUnpackerConfig",
     "NamedTupleStructSerializer",
     "PickleSerializer",
-    "PicklerConfig",
     "StringLineSerializer",
-    "UnpicklerConfig",
+    "StructSerializer",
 ]
 
 
 ############ Package initialization ############
-from .cbor import *
-from .json import *
-from .line import *
-from .msgpack import *
-from .pickle import *
-from .struct import *
+from .cbor import CBORSerializer
+from .json import JSONSerializer
+from .line import StringLineSerializer
+from .msgpack import MessagePackSerializer
+from .pickle import PickleSerializer
+from .struct import NamedTupleStructSerializer, StructSerializer
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/serializers/abc.py` & `easynetwork-1.0.0rc8/src/easynetwork/serializers/abc.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -22,45 +22,45 @@
     "BufferedIncrementalPacketSerializer",
 ]
 
 from abc import ABCMeta, abstractmethod
 from collections.abc import Generator
 from typing import TYPE_CHECKING, Generic
 
-from .._typevars import _BufferT, _ReceivedDTOPacketT, _SentDTOPacketT
+from .._typevars import _T_Buffer, _T_ReceivedDTOPacket, _T_SentDTOPacket
 from ..exceptions import DeserializeError
 
 if TYPE_CHECKING:
     from _typeshed import ReadableBuffer
 
 
-class AbstractPacketSerializer(Generic[_SentDTOPacketT, _ReceivedDTOPacketT], metaclass=ABCMeta):
+class AbstractPacketSerializer(Generic[_T_SentDTOPacket, _T_ReceivedDTOPacket], metaclass=ABCMeta):
     """
     The base class for implementing a :term:`serializer`.
 
     Implementing this interface would create a :term:`one-shot serializer`.
     """
 
     __slots__ = ("__weakref__",)
 
     @abstractmethod
-    def serialize(self, packet: _SentDTOPacketT, /) -> bytes:
+    def serialize(self, packet: _T_SentDTOPacket, /) -> bytes:
         """
         Returns the byte representation of the Python object `packet`.
 
         Parameters:
             packet: The Python object to serialize.
 
         Returns:
             a byte sequence.
         """
         raise NotImplementedError
 
     @abstractmethod
-    def deserialize(self, data: bytes, /) -> _ReceivedDTOPacketT:
+    def deserialize(self, data: bytes, /) -> _T_ReceivedDTOPacket:
         """
         Creates a Python object representing the raw :term:`packet` from `data`.
 
         Parameters:
             data: The byte sequence to deserialize.
 
         Raises:
@@ -68,23 +68,23 @@
 
         Returns:
             the deserialized Python object.
         """
         raise NotImplementedError
 
 
-class AbstractIncrementalPacketSerializer(AbstractPacketSerializer[_SentDTOPacketT, _ReceivedDTOPacketT]):
+class AbstractIncrementalPacketSerializer(AbstractPacketSerializer[_T_SentDTOPacket, _T_ReceivedDTOPacket]):
     """
     The base class for implementing an :term:`incremental serializer`.
     """
 
     __slots__ = ()
 
     @abstractmethod
-    def incremental_serialize(self, packet: _SentDTOPacketT, /) -> Generator[bytes, None, None]:
+    def incremental_serialize(self, packet: _T_SentDTOPacket, /) -> Generator[bytes, None, None]:
         """
         Returns the byte representation of the Python object `packet`.
 
         The generator should :keyword:`yield` non-empty byte sequences.
 
         The main purpose of this method is to add metadata that could not be included in the output of :meth:`serialize`,
         such as headers, separators, and so on. It is used in the :meth:`incremental_deserialize` method.
@@ -94,15 +94,15 @@
 
         Yields:
             all the parts of the :term:`packet`.
         """
         raise NotImplementedError
 
     @abstractmethod
-    def incremental_deserialize(self) -> Generator[None, bytes, tuple[_ReceivedDTOPacketT, bytes]]:
+    def incremental_deserialize(self) -> Generator[None, bytes, tuple[_T_ReceivedDTOPacket, bytes]]:
         """
         Creates a Python object representing the raw :term:`packet`.
 
         Raises:
             IncrementalDeserializeError: An unrelated deserialization error occurred.
 
         Yields:
@@ -112,29 +112,29 @@
             from the remote endpoint.
 
         Returns:
             a tuple with the deserialized Python object and the unused trailing data.
         """
         raise NotImplementedError
 
-    def serialize(self, packet: _SentDTOPacketT, /) -> bytes:
+    def serialize(self, packet: _T_SentDTOPacket, /) -> bytes:
         """
         Returns the byte representation of the Python object `packet`.
 
         The default implementation concatenates and returns the parts sent by :meth:`incremental_serialize`.
 
         Parameters:
             packet: The Python object to serialize.
 
         Returns:
             a byte sequence.
         """
         return b"".join(self.incremental_serialize(packet))
 
-    def deserialize(self, data: bytes, /) -> _ReceivedDTOPacketT:
+    def deserialize(self, data: bytes, /) -> _T_ReceivedDTOPacket:
         """
         Creates a Python object representing the raw :term:`packet` from `data`.
 
         The default implementation uses :meth:`incremental_deserialize` and expects it to deserialize ``data`` at once.
 
         Parameters:
             data: The byte sequence to deserialize.
@@ -142,45 +142,45 @@
         Raises:
             DeserializeError: Too little or too much data to parse.
             DeserializeError: An unrelated deserialization error occurred.
 
         Returns:
             the deserialized Python object.
         """
-        consumer: Generator[None, bytes, tuple[_ReceivedDTOPacketT, bytes]] = self.incremental_deserialize()
+        consumer: Generator[None, bytes, tuple[_T_ReceivedDTOPacket, bytes]] = self.incremental_deserialize()
         try:
             next(consumer)
         except StopIteration:
             raise RuntimeError("self.incremental_deserialize() generator did not yield") from None
-        packet: _ReceivedDTOPacketT
+        packet: _T_ReceivedDTOPacket
         remaining: bytes
         try:
             consumer.send(data)
         except StopIteration as exc:
             packet, remaining = exc.value
         else:
             consumer.close()
             raise DeserializeError("Missing data to create packet", error_info={"data": data}) from None
         if remaining:
             raise DeserializeError("Extra data caught", error_info={"packet": packet, "extra": remaining})
         return packet
 
 
 class BufferedIncrementalPacketSerializer(
-    AbstractIncrementalPacketSerializer[_SentDTOPacketT, _ReceivedDTOPacketT],
-    Generic[_SentDTOPacketT, _ReceivedDTOPacketT, _BufferT],
+    AbstractIncrementalPacketSerializer[_T_SentDTOPacket, _T_ReceivedDTOPacket],
+    Generic[_T_SentDTOPacket, _T_ReceivedDTOPacket, _T_Buffer],
 ):
     """
     The base class for implementing an :term:`incremental serializer` with manual control of the receive buffer.
     """
 
     __slots__ = ()
 
     @abstractmethod
-    def create_deserializer_buffer(self, sizehint: int, /) -> _BufferT:
+    def create_deserializer_buffer(self, sizehint: int, /) -> _T_Buffer:
         """
         Called to allocate a new receive buffer.
 
         Parameters:
             sizehint: the recommended size (in bytes) for the returned buffer.
                       It is acceptable to return smaller or larger buffers than what `sizehint` suggests.
 
@@ -188,17 +188,17 @@
             an object implementing the :ref:`buffer protocol <bufferobjects>`. It is an error to return a buffer with a zero size.
         """
         raise NotImplementedError
 
     @abstractmethod
     def buffered_incremental_deserialize(
         self,
-        buffer: _BufferT,
+        buffer: _T_Buffer,
         /,
-    ) -> Generator[int | None, int, tuple[_ReceivedDTOPacketT, ReadableBuffer]]:
+    ) -> Generator[int | None, int, tuple[_T_ReceivedDTOPacket, ReadableBuffer]]:
         """
         Creates a Python object representing the raw :term:`packet`.
 
         Parameters:
             buffer: The buffer allocated by :meth:`create_deserializer_buffer`.
 
         Raises:
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/serializers/base_stream.py` & `easynetwork-1.0.0rc8/src/easynetwork/serializers/base_stream.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -18,42 +18,43 @@
 
 __all__ = [
     "AutoSeparatedPacketSerializer",
     "FileBasedPacketSerializer",
     "FixedSizePacketSerializer",
 ]
 
+import contextlib
 from abc import abstractmethod
-from collections.abc import Generator
+from collections.abc import Callable, Generator
 from io import BytesIO
 from typing import IO, TYPE_CHECKING, Any, final
 
-from .._typevars import _ReceivedDTOPacketT, _SentDTOPacketT
-from ..exceptions import DeserializeError, IncrementalDeserializeError
-from ..lowlevel.constants import _DEFAULT_LIMIT
-from .abc import AbstractIncrementalPacketSerializer, BufferedIncrementalPacketSerializer
-from .tools import GeneratorStreamReader, _wrap_generic_buffered_incremental_deserialize, _wrap_generic_incremental_deserialize
+from .._typevars import _T_ReceivedDTOPacket, _T_SentDTOPacket
+from ..exceptions import DeserializeError, IncrementalDeserializeError, LimitOverrunError
+from ..lowlevel.constants import DEFAULT_SERIALIZER_LIMIT
+from .abc import BufferedIncrementalPacketSerializer
+from .tools import GeneratorStreamReader
 
 if TYPE_CHECKING:
     from _typeshed import ReadableBuffer
 
 
-class AutoSeparatedPacketSerializer(AbstractIncrementalPacketSerializer[_SentDTOPacketT, _ReceivedDTOPacketT]):
+class AutoSeparatedPacketSerializer(BufferedIncrementalPacketSerializer[_T_SentDTOPacket, _T_ReceivedDTOPacket, bytearray]):
     """
     Base class for stream protocols that separates sent information by a byte sequence.
     """
 
     __slots__ = ("__separator", "__limit", "__incremental_serialize_check_separator", "__debug")
 
     def __init__(
         self,
         separator: bytes,
         *,
         incremental_serialize_check_separator: bool = True,
-        limit: int = _DEFAULT_LIMIT,
+        limit: int = DEFAULT_SERIALIZER_LIMIT,
         debug: bool = False,
         **kwargs: Any,
     ) -> None:
         """
         Parameters:
             separator: Byte sequence that indicates the end of the token.
             incremental_serialize_check_separator: If :data:`True` (the default), checks that the data returned by
@@ -76,22 +77,22 @@
             raise ValueError("limit must be a positive integer")
         self.__separator: bytes = separator
         self.__limit: int = limit
         self.__incremental_serialize_check_separator = bool(incremental_serialize_check_separator)
         self.__debug: bool = bool(debug)
 
     @abstractmethod
-    def serialize(self, packet: _SentDTOPacketT, /) -> bytes:
+    def serialize(self, packet: _T_SentDTOPacket, /) -> bytes:
         """
         See :meth:`.AbstractPacketSerializer.serialize` documentation.
         """
         raise NotImplementedError
 
     @final
-    def incremental_serialize(self, packet: _SentDTOPacketT, /) -> Generator[bytes, None, None]:
+    def incremental_serialize(self, packet: _T_SentDTOPacket, /) -> Generator[bytes, None, None]:
         """
         Yields the data returned by :meth:`serialize` and appends `separator`.
 
         See :meth:`.AbstractIncrementalPacketSerializer.incremental_serialize` documentation for details.
 
         Raises:
             ValueError: If `incremental_serialize_check_separator` is `True` and `separator` is in the returned data.
@@ -100,33 +101,40 @@
         data: bytes = self.serialize(packet)
         separator: bytes = self.__separator
         if self.__incremental_serialize_check_separator:
             while data.endswith(separator):
                 data = data.removesuffix(separator)
             if separator in data:
                 raise ValueError(f"{separator!r} separator found in serialized packet {packet!r} which was not at the end")
+        elif data.endswith(separator):
+            yield data
+            return
         if not data:
             return
-        if len(data) + len(separator) <= self.__limit // 2:
-            data += separator
-            yield data
-        else:
-            yield data
-            del data
-            yield separator
+        data += separator
+        yield data
 
     @abstractmethod
-    def deserialize(self, data: bytes, /) -> _ReceivedDTOPacketT:
+    def deserialize(self, data: bytes, /) -> _T_ReceivedDTOPacket:
         """
         See :meth:`.AbstractPacketSerializer.deserialize` documentation.
         """
         raise NotImplementedError
 
+    def deserialize_from_buffer(self, data: ReadableBuffer, /) -> _T_ReceivedDTOPacket:
+        """
+        Called by :meth:`buffered_incremental_deserialize` and must have the same behavior as :meth:`deserialize`.
+
+        The default implementation creates a :class:`bytes` object from `data` and calls :meth:`deserialize`.
+        """
+        data = bytes(data)
+        return self.deserialize(data)
+
     @final
-    def incremental_deserialize(self) -> Generator[None, bytes, tuple[_ReceivedDTOPacketT, bytes]]:
+    def incremental_deserialize(self) -> Generator[None, bytes, tuple[_T_ReceivedDTOPacket, bytes]]:
         """
         Yields until `separator` is found and calls :meth:`deserialize` **without** `separator`.
 
         See :meth:`.AbstractIncrementalPacketSerializer.incremental_deserialize` documentation for details.
 
         Raises:
             LimitOverrunError: Reached buffer size limit.
@@ -145,14 +153,51 @@
                 remaining_data=remainder,
                 error_info=exc.error_info,
             ) from exc
         finally:
             del data
         return packet, remainder
 
+    @final
+    def create_deserializer_buffer(self, sizehint: int) -> bytearray:
+        """
+        See :meth:`.BufferedIncrementalPacketSerializer.buffered_incremental_deserialize` documentation for details.
+        """
+
+        # Ignore sizehint, we have our own limit
+        return bytearray(self.__limit)
+
+    @final
+    def buffered_incremental_deserialize(self, buffer: bytearray) -> Generator[int, int, tuple[_T_ReceivedDTOPacket, memoryview]]:
+        """
+        Yields until `separator` is found and calls :meth:`deserialize_from_buffer` **without** `separator`.
+
+        See :meth:`.BufferedIncrementalPacketSerializer.buffered_incremental_deserialize` documentation for details.
+
+        Raises:
+            LimitOverrunError: Reached buffer size limit.
+            IncrementalDeserializeError: :meth:`deserialize_from_buffer` raised :exc:`.DeserializeError`.
+            Exception: Any error raised by :meth:`deserialize_from_buffer`.
+        """
+        with memoryview(buffer) as buffer_view:
+            sepidx, offset, buflen = yield from _buffered_readuntil(buffer, self.__separator)
+            del buffer
+
+            remainder: memoryview = buffer_view[offset:buflen]
+            with buffer_view[:sepidx] as data:
+                try:
+                    packet = self.deserialize_from_buffer(data)
+                except DeserializeError as exc:
+                    raise IncrementalDeserializeError(
+                        f"Error when deserializing data: {exc}",
+                        remaining_data=remainder,
+                        error_info=exc.error_info,
+                    ) from exc
+            return packet, remainder
+
     @property
     @final
     def separator(self) -> bytes:
         """
         Byte sequence that indicates the end of the token. Read-only attribute.
         """
         return self.__separator
@@ -161,16 +206,24 @@
     @final
     def debug(self) -> bool:
         """
         The debug mode flag. Read-only attribute.
         """
         return self.__debug
 
+    @property
+    @final
+    def buffer_limit(self) -> int:
+        """
+        Maximum buffer size. Read-only attribute.
+        """
+        return self.__limit
+
 
-class FixedSizePacketSerializer(BufferedIncrementalPacketSerializer[_SentDTOPacketT, _ReceivedDTOPacketT, memoryview]):
+class FixedSizePacketSerializer(BufferedIncrementalPacketSerializer[_T_SentDTOPacket, _T_ReceivedDTOPacket, memoryview]):
     """
     A base class for stream protocols in which the packets are of a fixed size.
     """
 
     __slots__ = ("__size", "__debug")
 
     def __init__(self, size: int, *, debug: bool = False, **kwargs: Any) -> None:
@@ -188,22 +241,22 @@
         size = int(size)
         if size <= 0:
             raise ValueError("size must be a positive integer")
         self.__size: int = size
         self.__debug: bool = bool(debug)
 
     @abstractmethod
-    def serialize(self, packet: _SentDTOPacketT, /) -> bytes:
+    def serialize(self, packet: _T_SentDTOPacket, /) -> bytes:
         """
         See :meth:`.AbstractPacketSerializer.serialize` documentation.
         """
         raise NotImplementedError
 
     @final
-    def incremental_serialize(self, packet: _SentDTOPacketT, /) -> Generator[bytes, None, None]:
+    def incremental_serialize(self, packet: _T_SentDTOPacket, /) -> Generator[bytes, None, None]:
         """
         Yields the data returned by :meth:`serialize`.
 
         See :meth:`.AbstractIncrementalPacketSerializer.incremental_serialize` documentation for details.
 
         Raises:
             ValueError: If the returned data size is not equal to `packet_size`.
@@ -211,25 +264,31 @@
         """
         data = self.serialize(packet)
         if len(data) != self.__size:
             raise ValueError("serialized data size does not meet expectation")
         yield data
 
     @abstractmethod
-    def deserialize(self, data: bytes | memoryview, /) -> _ReceivedDTOPacketT:
+    def deserialize(self, data: bytes, /) -> _T_ReceivedDTOPacket:
         """
         See :meth:`.AbstractPacketSerializer.deserialize` documentation.
-
-        Warning:
-            `data` can be a :class:`memoryview`.
         """
         raise NotImplementedError
 
+    def deserialize_from_buffer(self, data: ReadableBuffer, /) -> _T_ReceivedDTOPacket:
+        """
+        Called by :meth:`buffered_incremental_deserialize` and must have the same behavior as :meth:`deserialize`.
+
+        The default implementation creates a :class:`bytes` object from `data` and calls :meth:`deserialize`.
+        """
+        data = bytes(data)
+        return self.deserialize(data)
+
     @final
-    def incremental_deserialize(self) -> Generator[None, bytes, tuple[_ReceivedDTOPacketT, bytes]]:
+    def incremental_deserialize(self) -> Generator[None, bytes, tuple[_T_ReceivedDTOPacket, bytes]]:
         """
         Yields until there is enough data and calls :meth:`deserialize`.
 
         See :meth:`.AbstractIncrementalPacketSerializer.incremental_deserialize` documentation for details.
 
         Raises:
             IncrementalDeserializeError: :meth:`deserialize` raised :exc:`.DeserializeError`.
@@ -260,37 +319,37 @@
         return memoryview(bytearray(bufsize))
 
     @final
     def buffered_incremental_deserialize(
         self,
         buffer: memoryview,
         /,
-    ) -> Generator[int, int, tuple[_ReceivedDTOPacketT, memoryview]]:
+    ) -> Generator[int, int, tuple[_T_ReceivedDTOPacket, memoryview]]:
         """
-        Yields until there is enough data and calls :meth:`deserialize`.
+        Yields until there is enough data and calls :meth:`deserialize_from_buffer`.
 
         See :meth:`.BufferedIncrementalPacketSerializer.buffered_incremental_deserialize` documentation for details.
 
         Raises:
-            IncrementalDeserializeError: :meth:`deserialize` raised :exc:`.DeserializeError`.
-            Exception: Any error raised by :meth:`deserialize`.
+            IncrementalDeserializeError: :meth:`deserialize_from_buffer` raised :exc:`.DeserializeError`.
+            Exception: Any error raised by :meth:`deserialize_from_buffer`.
         """
         packet_size: int = self.__size
         assert len(buffer) >= packet_size  # nosec assert_used
 
         nread: int = 0
         while nread < packet_size:
             nread += yield nread
 
         data = buffer[:packet_size]
         remainder = buffer[packet_size:nread]
         del buffer
 
         try:
-            packet = self.deserialize(data)
+            packet = self.deserialize_from_buffer(data)
         except DeserializeError as exc:
             raise IncrementalDeserializeError(
                 f"Error when deserializing data: {exc}",
                 remaining_data=remainder,
                 error_info=exc.error_info,
             ) from exc
         return packet, remainder
@@ -308,15 +367,15 @@
     def debug(self) -> bool:
         """
         The debug mode flag. Read-only attribute.
         """
         return self.__debug
 
 
-class FileBasedPacketSerializer(BufferedIncrementalPacketSerializer[_SentDTOPacketT, _ReceivedDTOPacketT, memoryview]):
+class FileBasedPacketSerializer(BufferedIncrementalPacketSerializer[_T_SentDTOPacket, _T_ReceivedDTOPacket, memoryview]):
     """
     Base class for APIs requiring a :std:term:`file object` for serialization/deserialization.
     """
 
     __slots__ = ("__expected_errors", "__debug")
 
     def __init__(
@@ -337,26 +396,26 @@
         if not isinstance(expected_load_error, tuple):
             expected_load_error = (expected_load_error,)
         assert all(issubclass(e, Exception) for e in expected_load_error)  # nosec assert_used
         self.__expected_errors: tuple[type[Exception], ...] = expected_load_error
         self.__debug: bool = bool(debug)
 
     @abstractmethod
-    def dump_to_file(self, packet: _SentDTOPacketT, file: IO[bytes], /) -> None:
+    def dump_to_file(self, packet: _T_SentDTOPacket, file: IO[bytes], /) -> None:
         """
         Write the serialized `packet` to `file`.
 
         Parameters:
             packet: The Python object to serialize.
             file: The :std:term:`binary file` to write to.
         """
         raise NotImplementedError
 
     @abstractmethod
-    def load_from_file(self, file: IO[bytes], /) -> _ReceivedDTOPacketT:
+    def load_from_file(self, file: IO[bytes], /) -> _T_ReceivedDTOPacket:
         """
         Read from `file` to deserialize the raw :term:`packet`.
 
         Parameters:
             file: The :std:term:`binary file` to read from.
 
         Raises:
@@ -365,43 +424,43 @@
 
         Returns:
             the deserialized Python object.
         """
         raise NotImplementedError
 
     @final
-    def serialize(self, packet: _SentDTOPacketT, /) -> bytes:
+    def serialize(self, packet: _T_SentDTOPacket, /) -> bytes:
         """
         Calls :meth:`dump_to_file` and returns the result.
 
         See :meth:`.AbstractPacketSerializer.serialize` documentation for details.
 
         Raises:
             Exception: Any error raised by :meth:`dump_to_file`.
         """
         with BytesIO() as buffer:
             self.dump_to_file(packet, buffer)
             return buffer.getvalue()
 
     @final
-    def deserialize(self, data: bytes, /) -> _ReceivedDTOPacketT:
+    def deserialize(self, data: bytes, /) -> _T_ReceivedDTOPacket:
         """
         Calls :meth:`load_from_file` and returns the result.
 
         See :meth:`.AbstractPacketSerializer.deserialize` documentation for details.
 
         Raises:
             DeserializeError: :meth:`load_from_file` raised :class:`EOFError`.
             DeserializeError: :meth:`load_from_file` does not read until EOF (unused trailing data).
             DeserializeError: :meth:`load_from_file` raised an error that matches `expected_load_error`.
             Exception: Any other error raised by :meth:`load_from_file`.
         """
         with BytesIO(data) as buffer:
             try:
-                packet: _ReceivedDTOPacketT = self.load_from_file(buffer)
+                packet: _T_ReceivedDTOPacket = self.load_from_file(buffer)
             except EOFError as exc:
                 msg = "Missing data to create packet"
                 if self.debug:
                     raise DeserializeError(msg, error_info={"data": data}) from exc
                 raise DeserializeError(msg) from exc
             except self.__expected_errors as exc:
                 msg = str(exc)
@@ -414,15 +473,15 @@
                 msg = "Extra data caught"
                 if self.debug:
                     raise DeserializeError(msg, error_info={"packet": packet, "extra": extra})
                 raise DeserializeError(msg)
         return packet
 
     @final
-    def incremental_serialize(self, packet: _SentDTOPacketT, /) -> Generator[bytes, None, None]:
+    def incremental_serialize(self, packet: _T_SentDTOPacket, /) -> Generator[bytes, None, None]:
         """
         Calls :meth:`dump_to_file` and yields the result.
 
         See :meth:`.AbstractIncrementalPacketSerializer.incremental_serialize` documentation for details.
 
         Raises:
             Exception: Any error raised by :meth:`dump_to_file`.
@@ -431,15 +490,15 @@
             self.dump_to_file(packet, buffer)
             if buffer.getbuffer().nbytes == 0:
                 return
             data = buffer.getvalue()
         yield data
 
     @final
-    def incremental_deserialize(self) -> Generator[None, bytes, tuple[_ReceivedDTOPacketT, bytes]]:
+    def incremental_deserialize(self) -> Generator[None, bytes, tuple[_T_ReceivedDTOPacket, bytes]]:
         """
         Calls :meth:`load_from_file` and returns the result.
 
         See :meth:`.AbstractIncrementalPacketSerializer.incremental_deserialize` documentation for details.
 
         Note:
             The generator will always :keyword:`yield` if :meth:`load_from_file` raises :class:`EOFError`.
@@ -458,38 +517,38 @@
         return memoryview(bytearray(sizehint))
 
     @final
     def buffered_incremental_deserialize(
         self,
         buffer: memoryview,
         /,
-    ) -> Generator[None, int, tuple[_ReceivedDTOPacketT, ReadableBuffer]]:
+    ) -> Generator[None, int, tuple[_T_ReceivedDTOPacket, ReadableBuffer]]:
         """
         Calls :meth:`load_from_file` and returns the result.
 
         See :meth:`.BufferedIncrementalPacketSerializer.buffered_incremental_deserialize` documentation for details.
 
         Note:
             The generator will always :keyword:`yield` if :meth:`load_from_file` raises :class:`EOFError`.
 
         Raises:
             IncrementalDeserializeError: :meth:`load_from_file` raised an error that matches `expected_load_error`.
             Exception: Any other error raised by :meth:`load_from_file`.
         """
         return (yield from _wrap_generic_buffered_incremental_deserialize(buffer, self.__generic_incremental_deserialize))
 
-    def __generic_incremental_deserialize(self) -> Generator[None, ReadableBuffer, tuple[_ReceivedDTOPacketT, ReadableBuffer]]:
+    def __generic_incremental_deserialize(self) -> Generator[None, ReadableBuffer, tuple[_T_ReceivedDTOPacket, ReadableBuffer]]:
         with BytesIO((yield)) as buffer:
             initial: bool = True
             while True:
                 if not initial:
                     buffer.write((yield))
                     buffer.seek(0)
                 try:
-                    packet: _ReceivedDTOPacketT = self.load_from_file(buffer)
+                    packet: _T_ReceivedDTOPacket = self.load_from_file(buffer)
                 except EOFError:
                     continue
                 except self.__expected_errors as exc:
                     msg = f"Deserialize error: {exc}"
                     if self.debug:
                         raise IncrementalDeserializeError(
                             msg,
@@ -505,7 +564,57 @@
     @property
     @final
     def debug(self) -> bool:
         """
         The debug mode flag. Read-only attribute.
         """
         return self.__debug
+
+
+def _wrap_generic_incremental_deserialize(
+    func: Callable[[], Generator[None, ReadableBuffer, tuple[_T_ReceivedDTOPacket, ReadableBuffer]]],
+) -> Generator[None, bytes, tuple[_T_ReceivedDTOPacket, bytes]]:
+    packet, remainder = yield from func()
+    # remainder is not copied if it is already a "bytes" object
+    remainder = bytes(remainder)
+    return packet, remainder
+
+
+def _wrap_generic_buffered_incremental_deserialize(
+    buffer: memoryview,
+    func: Callable[[], Generator[None, ReadableBuffer, tuple[_T_ReceivedDTOPacket, ReadableBuffer]]],
+) -> Generator[None, int, tuple[_T_ReceivedDTOPacket, ReadableBuffer]]:
+    next(gen := func())
+    with contextlib.closing(gen):
+        while True:
+            nbytes: int = yield
+            try:
+                gen.send(buffer[:nbytes])
+            except StopIteration as exc:
+                return exc.value
+
+
+def _buffered_readuntil(
+    buffer: bytearray,
+    separator: bytes,
+) -> Generator[int, int, tuple[int, int, int]]:
+    last_idx = len(buffer) - 1
+    seplen: int = len(separator)
+    limit = last_idx - seplen
+    buflen: int = yield 0
+
+    offset: int = 0
+    sepidx: int = -1
+    while True:
+        if buflen - offset >= seplen:
+            sepidx = buffer.find(separator, offset, buflen)
+
+            if sepidx != -1:
+                offset = sepidx + seplen
+                return sepidx, offset, buflen
+
+            offset = buflen + 1 - seplen
+            if offset > limit:
+                msg = "Separator is not found, and chunk exceed the limit"
+                raise LimitOverrunError(msg, buffer, offset, separator)
+
+        buflen += yield buflen
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/serializers/cbor.py` & `easynetwork-1.0.0rc8/src/easynetwork/serializers/cbor.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -23,26 +23,27 @@
 ]
 
 from collections.abc import Callable
 from dataclasses import asdict as dataclass_asdict, dataclass
 from functools import partial
 from typing import IO, TYPE_CHECKING, Any, final
 
+from ..lowlevel import _utils
 from .base_stream import FileBasedPacketSerializer
 
 if TYPE_CHECKING:
     import datetime
 
 
 @dataclass(kw_only=True)
 class CBOREncoderConfig:
     """
     A dataclass with the CBOR encoder options.
 
-    See :class:`cbor2.encoder.CBOREncoder` for details.
+    See :class:`cbor2.CBOREncoder` for details.
     """
 
     datetime_as_timestamp: bool = False
     timezone: datetime.tzinfo | None = None
     value_sharing: bool = False
     default: Callable[..., Any] | None = None
     canonical: bool = False
@@ -51,15 +52,15 @@
 
 
 @dataclass(kw_only=True)
 class CBORDecoderConfig:
     """
     A dataclass with the CBOR decoder options.
 
-    See :class:`cbor2.decoder.CBORDecoder` for details.
+    See :class:`cbor2.CBORDecoder` for details.
     """
 
     object_hook: Callable[..., Any] | None = None
     tag_hook: Callable[..., Any] | None = None
     str_errors: str = "strict"
 
 
@@ -83,16 +84,16 @@
         Parameters:
             encoder_config: Parameter object to configure the :class:`~cbor.encoder.CBOREncoder`.
             decoder_config: Parameter object to configure the :class:`~cbor.decoder.CBORDecoder`.
             debug: If :data:`True`, add information to :exc:`.DeserializeError` via the ``error_info`` attribute.
         """
         try:
             import cbor2
-        except ModuleNotFoundError as exc:  # pragma: no cover
-            raise ModuleNotFoundError("cbor dependencies are missing. Consider adding 'cbor' extra") from exc
+        except ModuleNotFoundError as exc:
+            raise _utils.missing_extra_deps("cbor") from exc
 
         super().__init__(expected_load_error=(cbor2.CBORDecodeError, UnicodeError), debug=debug)
         self.__encoder_cls: Callable[[IO[bytes]], cbor2.CBOREncoder]
         self.__decoder_cls: Callable[[IO[bytes]], cbor2.CBORDecoder]
 
         if encoder_config is None:
             encoder_config = CBOREncoderConfig()
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/serializers/json.py` & `easynetwork-1.0.0rc8/src/easynetwork/serializers/json.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -28,15 +28,15 @@
 from collections import Counter
 from collections.abc import Callable, Generator
 from dataclasses import asdict as dataclass_asdict, dataclass
 from typing import Any, final
 
 from ..exceptions import DeserializeError, IncrementalDeserializeError, LimitOverrunError
 from ..lowlevel._utils import iter_bytes
-from ..lowlevel.constants import _DEFAULT_LIMIT
+from ..lowlevel.constants import DEFAULT_SERIALIZER_LIMIT
 from .abc import AbstractIncrementalPacketSerializer
 from .tools import GeneratorStreamReader
 
 
 @dataclass(kw_only=True)
 class JSONEncoderConfig:
     """
@@ -87,15 +87,15 @@
     def __init__(
         self,
         encoder_config: JSONEncoderConfig | None = None,
         decoder_config: JSONDecoderConfig | None = None,
         *,
         encoding: str = "utf-8",
         unicode_errors: str = "strict",
-        limit: int = _DEFAULT_LIMIT,
+        limit: int = DEFAULT_SERIALIZER_LIMIT,
         use_lines: bool = True,
         debug: bool = False,
     ) -> None:
         """
         Parameters:
             encoder_config: Parameter object to configure the :class:`~json.JSONEncoder`.
             decoder_config: Parameter object to configure the :class:`~json.JSONDecoder`.
@@ -176,26 +176,17 @@
         Parameters:
             packet: The Python object to serialize.
 
         Yields:
             all the parts of the JSON :term:`packet`.
         """
         data = self.__encoder.encode(packet).encode(self.__encoding, self.__unicode_errors)
-        if not data.startswith((b"{", b"[", b'"')):
+        if self.__use_lines or not data.startswith((b"{", b"[", b'"')):
             data += b"\n"
-            yield data
-        elif not self.__use_lines:
-            yield data
-        elif len(data) + 1 <= self.__limit // 2:
-            data += b"\n"
-            yield data
-        else:
-            yield data
-            del data
-            yield b"\n"
+        yield data
 
     @final
     def deserialize(self, data: bytes) -> Any:
         """
         Creates a Python object representing the raw JSON :term:`packet` from `data`.
 
         Roughly equivalent to::
@@ -214,15 +205,15 @@
         Raises:
             DeserializeError: A :class:`UnicodeError` or :class:`~json.JSONDecodeError` have been raised.
 
         Returns:
             the deserialized Python object.
         """
         try:
-            document: str = data.decode(self.__encoding, self.__unicode_errors)
+            document: str = str(data, self.__encoding, self.__unicode_errors)
         except UnicodeError as exc:
             msg = f"Unicode decode error: {exc}"
             if self.debug:
                 raise DeserializeError(msg, error_info={"data": data}) from exc
             raise DeserializeError(msg) from exc
         finally:
             del data
@@ -276,15 +267,15 @@
             remaining_data = reader.read_all()
             del reader
         else:
             complete_document, remaining_data = yield from _JSONParser.raw_parse(limit=self.__limit)
 
         packet: Any
         try:
-            document: str = complete_document.decode(self.__encoding, self.__unicode_errors)
+            document: str = str(complete_document, self.__encoding, self.__unicode_errors)
         except UnicodeError as exc:
             msg = f"Unicode decode error: {exc}"
             if self.debug:
                 raise IncrementalDeserializeError(
                     msg,
                     remaining_data=remaining_data,
                     error_info={"data": complete_document},
@@ -314,14 +305,22 @@
     @final
     def debug(self) -> bool:
         """
         The debug mode flag. Read-only attribute.
         """
         return self.__debug
 
+    @property
+    @final
+    def buffer_limit(self) -> int:
+        """
+        Maximum buffer size. Read-only attribute.
+        """
+        return self.__limit
+
 
 class _JSONParser:
     _JSON_VALUE_BYTES: frozenset[int] = frozenset(bytes(string.digits + string.ascii_letters + string.punctuation, "ascii"))
     _ESCAPE_BYTE: int = ord(b"\\")
 
     _whitespaces_match: Callable[[bytes, int], re.Match[bytes]] = re.compile(rb"[ \t\n\r]*", re.MULTILINE | re.DOTALL).match  # type: ignore[assignment]
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/serializers/msgpack.py` & `easynetwork-1.0.0rc8/src/easynetwork/serializers/msgpack.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -24,23 +24,24 @@
 
 from collections.abc import Callable
 from dataclasses import asdict as dataclass_asdict, dataclass, field
 from functools import partial
 from typing import Any, final
 
 from ..exceptions import DeserializeError
+from ..lowlevel import _utils
 from .abc import AbstractPacketSerializer
 
 
 def _get_default_ext_hook() -> Callable[[int, bytes], Any]:
     try:
-        import msgpack
-    except ModuleNotFoundError as exc:  # pragma: no cover
-        raise ModuleNotFoundError("message-pack dependencies are missing. Consider adding 'msgpack' extra") from exc
-    return msgpack.ExtType
+        from msgpack import ExtType
+    except ModuleNotFoundError as exc:
+        raise _utils.missing_extra_deps("msgpack", feature_name="message-pack") from exc
+    return ExtType
 
 
 @dataclass(kw_only=True)
 class MessagePackerConfig:
     """
     A dataclass with the Packer options.
 
@@ -93,16 +94,16 @@
         Parameters:
             packer_config: Parameter object to configure the :class:`~msgpack.Packer`.
             unpacker_config: Parameter object to configure the :class:`~msgpack.Unpacker`.
             debug: If :data:`True`, add information to :exc:`.DeserializeError` via the ``error_info`` attribute.
         """
         try:
             import msgpack
-        except ModuleNotFoundError as exc:  # pragma: no cover
-            raise ModuleNotFoundError("message-pack dependencies are missing. Consider adding 'msgpack' extra") from exc
+        except ModuleNotFoundError as exc:
+            raise _utils.missing_extra_deps("msgpack", feature_name="message-pack") from exc
 
         super().__init__()
         self.__packb: Callable[[Any], bytes]
         self.__unpackb: Callable[[bytes], Any]
 
         if packer_config is None:
             packer_config = MessagePackerConfig()
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/serializers/pickle.py` & `easynetwork-1.0.0rc8/src/easynetwork/serializers/pickle.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/serializers/struct.py` & `easynetwork-1.0.0rc8/src/easynetwork/serializers/struct.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -12,67 +12,72 @@
 # limitations under the License.
 #
 #
 """struct.Struct-based network packet serializer module"""
 
 from __future__ import annotations
 
-__all__ = ["AbstractStructSerializer", "NamedTupleStructSerializer"]
+__all__ = ["AbstractStructSerializer", "NamedTupleStructSerializer", "StructSerializer"]
 
 from abc import abstractmethod
 from collections.abc import Iterable
 from typing import TYPE_CHECKING, Any, NamedTuple, TypeVar, final
 
-from .._typevars import _ReceivedDTOPacketT, _SentDTOPacketT
+from .._typevars import _T_ReceivedDTOPacket, _T_SentDTOPacket
 from ..exceptions import DeserializeError
+from ..lowlevel import _utils
 from .base_stream import FixedSizePacketSerializer
 
 if TYPE_CHECKING:
     import struct as _typing_struct
 
-    from _typeshed import SupportsKeysAndGetItem
+    from _typeshed import ReadableBuffer, SupportsKeysAndGetItem
 
 
 _ENDIANNESS_CHARACTERS: frozenset[str] = frozenset({"@", "=", "<", ">", "!"})
 
 
-class AbstractStructSerializer(FixedSizePacketSerializer[_SentDTOPacketT, _ReceivedDTOPacketT]):
+class AbstractStructSerializer(FixedSizePacketSerializer[_T_SentDTOPacket, _T_ReceivedDTOPacket]):
     r"""
     A base class for structured data.
 
     To use the serializer directly without additional layers, it is possible to create a subclass with the minimal requirements::
 
-        >>> class StructSerializer(AbstractStructSerializer):
+        >>> class MyStructSerializer(AbstractStructSerializer):
         ...     __slots__ = ()
         ...     def iter_values(self, packet):
         ...         return packet
         ...     def from_tuple(self, packet_tuple):
         ...         return packet_tuple
         ...
 
     And then::
 
-        >>> s = StructSerializer(">ii")
+        >>> s = MyStructSerializer(">ii")
         >>> data = s.serialize((10, 20))
         >>> data
         b'\x00\x00\x00\n\x00\x00\x00\x14'
         >>> s.deserialize(data)
         (10, 20)
 
     This is an abstract class in order to allow you to include fancy structures like :class:`ctypes.Structure` subclasses.
 
     Note:
         If the endianness is not specified, the network byte-order is used::
 
-            >>> s = StructSerializer("qq")
+            >>> s = MyStructSerializer("qq")
             >>> s.struct.format
             '!qq'
 
     See Also:
-        The :class:`.NamedTupleStructSerializer` class.
+        :class:`.StructSerializer`
+            Default implementation without additional layers.
+
+        :class:`.NamedTupleStructSerializer`
+            Specialization for named tuple instances.
     """
 
     __slots__ = ("__s", "__error_cls")
 
     def __init__(self, format: str, *, debug: bool = False) -> None:
         """
         Parameters:
@@ -85,45 +90,45 @@
             format = f"!{format}"  # network byte order
         struct = Struct(format)
         super().__init__(struct.size, debug=debug)
         self.__s: _typing_struct.Struct = struct
         self.__error_cls = error
 
     @abstractmethod
-    def iter_values(self, packet: _SentDTOPacketT, /) -> Iterable[Any]:
+    def iter_values(self, packet: _T_SentDTOPacket, /) -> Iterable[Any]:
         """
         Returns an object suitable for :meth:`struct.Struct.pack`.
 
         See :meth:`serialize` for details.
 
         Parameters:
             packet: The Python object to serialize.
 
         Returns:
             an iterable object yielding the structure values
         """
         raise NotImplementedError
 
     @abstractmethod
-    def from_tuple(self, packet_tuple: tuple[Any, ...], /) -> _ReceivedDTOPacketT:
+    def from_tuple(self, packet_tuple: tuple[Any, ...], /) -> _T_ReceivedDTOPacket:
         """
         Finishes the packet deserialization by parsing the tuple obtained by :meth:`struct.Struct.unpack`.
 
         See :meth:`deserialize` for details.
 
         Parameters:
             packet_tuple: A tuple of each elements extracted from the structure.
 
         Returns:
             the deserialized Python object.
         """
         raise NotImplementedError
 
     @final
-    def serialize(self, packet: _SentDTOPacketT) -> bytes:
+    def serialize(self, packet: _T_SentDTOPacket) -> bytes:
         """
         Returns the structured data representation of the Python object `packet`.
 
         Roughly equivalent to::
 
             def serialize(self, packet):
                 to_pack = self.iter_values(packet)
@@ -134,63 +139,97 @@
 
         Returns:
             a byte sequence.
         """
         return self.__s.pack(*self.iter_values(packet))
 
     @final
-    def deserialize(self, data: bytes | memoryview) -> _ReceivedDTOPacketT:
+    def deserialize(self, data: ReadableBuffer) -> _T_ReceivedDTOPacket:
         """
         Creates a Python object representing the structure from `data`.
 
         Roughly equivalent to::
 
             def deserialize(self, data):
                 unpacked_data = self.struct.unpack(data)
                 return self.from_tuple(unpacked_data)
 
         Parameters:
             data: The byte sequence to deserialize.
 
         Raises:
             DeserializeError: A :class:`struct.error` have been raised.
-            DeserializeError: :meth:`from_tuple` crashed.
 
         Returns:
             the deserialized Python object.
         """
         try:
             packet_tuple: tuple[Any, ...] = self.__s.unpack(data)
         except self.__error_cls as exc:
             msg = f"Invalid value: {exc}"
             if self.debug:
                 raise DeserializeError(msg, error_info={"data": data}) from exc
             raise DeserializeError(msg) from exc
-        try:
-            return self.from_tuple(packet_tuple)
-        except Exception as exc:
-            msg = f"Error when building packet from unpacked struct value: {exc}"
-            if self.debug:
-                raise DeserializeError(
-                    msg,
-                    error_info={"unpacked_struct": packet_tuple},
-                ) from exc
-            raise DeserializeError(msg) from exc
+        return self.from_tuple(packet_tuple)
+
+    @final
+    def deserialize_from_buffer(self, data: ReadableBuffer) -> _T_ReceivedDTOPacket:
+        """
+        Calls :meth:`deserialize`.
+        """
+        return self.deserialize(data)
 
     @property
     @final
     def struct(self) -> _typing_struct.Struct:
         """The underlying :class:`struct.Struct` instance. Read-only attribute."""
         return self.__s
 
 
-_NamedTupleVar = TypeVar("_NamedTupleVar", bound=NamedTuple)
+class StructSerializer(AbstractStructSerializer[tuple[Any, ...], tuple[Any, ...]]):
+    r"""
+    Generic class to handle a :class:`tuple` of data with a :class:`struct.Struct` object.
+
+    Example:
+
+        >>> s = StructSerializer(">ii")
+        >>> data = s.serialize((10, 20))
+        >>> data
+        b'\x00\x00\x00\n\x00\x00\x00\x14'
+        >>> s.deserialize(data)
+        (10, 20)
+
+    Note:
+        If the endianness is not specified, the network byte-order is used::
+
+            >>> s = StructSerializer("qq")
+            >>> s.struct.format
+            '!qq'
+    """
+
+    __slots__ = ()
+
+    @final
+    @_utils.inherit_doc(AbstractStructSerializer)
+    def iter_values(self, packet: tuple[Any, ...], /) -> tuple[Any, ...]:
+        if __debug__:
+            if not isinstance(packet, tuple):
+                raise TypeError(f"Expected a tuple instance, got {packet!r}")
+        return packet
+
+    @final
+    @_utils.inherit_doc(AbstractStructSerializer)
+    def from_tuple(self, packet_tuple: tuple[Any, ...], /) -> tuple[Any, ...]:
+        return packet_tuple
+
+
+_T_NamedTuple = TypeVar("_T_NamedTuple", bound=NamedTuple)
 
 
-class NamedTupleStructSerializer(AbstractStructSerializer[_NamedTupleVar, _NamedTupleVar]):
+class NamedTupleStructSerializer(AbstractStructSerializer[_T_NamedTuple, _T_NamedTuple]):
     r"""
     Generic class to handle a :term:`named tuple` with a :class:`struct.Struct` object.
 
     Accepts classes created directly from :func:`collections.namedtuple` factory::
 
         >>> import collections
         >>> Point = collections.namedtuple("Point", ("x", "y"))
@@ -209,21 +248,28 @@
         >>> s.struct.format
         '>ii'
         >>> data = s.serialize(Point(x=10, y=20))
         >>> data
         b'\x00\x00\x00\n\x00\x00\x00\x14'
         >>> s.deserialize(data)
         Point(x=10, y=20)
+
+    Note:
+        If the endianness is not specified, the network byte-order is used::
+
+            >>> s = NamedTupleStructSerializer(Point, {"x": "i", "y": "i"})
+            >>> s.struct.format
+            '!ii'
     """
 
     __slots__ = ("__namedtuple_cls", "__string_fields", "__encoding", "__unicode_errors", "__strip_trailing_nul")
 
     def __init__(
         self,
-        namedtuple_cls: type[_NamedTupleVar],
+        namedtuple_cls: type[_T_NamedTuple],
         field_formats: SupportsKeysAndGetItem[str, str],
         format_endianness: str = "",
         encoding: str | None = "utf-8",
         unicode_errors: str = "strict",
         strip_string_trailing_nul_bytes: bool = True,
         *,
         debug: bool = False,
@@ -254,22 +300,22 @@
             if field_fmt[-1:] == "s":
                 if len(field_fmt) > 1 and not field_fmt[:-1].isdecimal():
                     raise ValueError(f"{field!r}: Invalid field format")
                 string_fields.add(field)
             elif len(field_fmt) != 1 or not field_fmt.isalpha():
                 raise ValueError(f"{field!r}: Invalid field format")
         super().__init__(f"{format_endianness}{''.join(field_formats[field] for field in namedtuple_cls._fields)}", debug=debug)
-        self.__namedtuple_cls: type[_NamedTupleVar] = namedtuple_cls
+        self.__namedtuple_cls: type[_T_NamedTuple] = namedtuple_cls
         self.__string_fields: frozenset[str] = frozenset(string_fields)
         self.__encoding: str | None = encoding
         self.__unicode_errors: str = unicode_errors
         self.__strip_trailing_nul = bool(strip_string_trailing_nul_bytes)
 
     @final
-    def iter_values(self, packet: _NamedTupleVar) -> _NamedTupleVar:
+    def iter_values(self, packet: _T_NamedTuple) -> _T_NamedTuple:
         """
         Returns the named tuple to pack using :meth:`struct.Struct.pack`.
 
         In most case, this method will directly return `packet`.
 
         If there are string fields and `encoding` is not :data:`None`, this method will return a shallow copy of
         the named tuple with the encoded strings.
@@ -290,25 +336,26 @@
 
         Parameters:
             packet: The `namedtuple_cls` instance.
 
         Returns:
             a `namedtuple_cls` instance.
         """
-        if not isinstance(packet, self.__namedtuple_cls):
-            namedtuple_name = self.__namedtuple_cls.__name__
-            raise TypeError(f"Expected a {namedtuple_name} instance, got {packet!r}")
+        if __debug__:
+            if not isinstance(packet, self.__namedtuple_cls):
+                namedtuple_name = self.__namedtuple_cls.__name__
+                raise TypeError(f"Expected a {namedtuple_name} instance, got {packet!r}")
         if (encoding := self.__encoding) is not None and self.__string_fields:
             string_fields: dict[str, str] = {field: getattr(packet, field) for field in self.__string_fields}
             unicode_errors: str = self.__unicode_errors
             packet = packet._replace(**{field: value.encode(encoding, unicode_errors) for field, value in string_fields.items()})
         return packet
 
     @final
-    def from_tuple(self, packet_tuple: tuple[Any, ...], /) -> _NamedTupleVar:
+    def from_tuple(self, packet_tuple: tuple[Any, ...], /) -> _T_NamedTuple:
         r"""
         Constructs the named tuple from the given tuple.
 
         If there are string fields and `encoding` is not :data:`None`, their values will be decoded.
 
         If `strip_string_trailing_nul_bytes` was set to :data:`True`, the ``"\0"`` characters at the end of the string fields,
         added for padding, will be removed.
@@ -341,11 +388,20 @@
         if string_fields:
             to_replace: dict[str, Any] | None = None
             if self.__strip_trailing_nul:
                 string_fields = {field: value.rstrip(b"\0") for field, value in string_fields.items()}
                 to_replace = string_fields
             if (encoding := self.__encoding) is not None:
                 unicode_errors: str = self.__unicode_errors
-                to_replace = {field: value.decode(encoding, unicode_errors) for field, value in string_fields.items()}
+                try:
+                    to_replace = {field: str(value, encoding, unicode_errors) for field, value in string_fields.items()}
+                except UnicodeError as exc:
+                    msg = f"UnicodeError when building packet from unpacked struct value: {exc}"
+                    if self.debug:
+                        raise DeserializeError(
+                            msg,
+                            error_info={"unpacked_struct": packet_tuple},
+                        ) from exc
+                    raise DeserializeError(msg) from exc
             if to_replace is not None:
                 p = p._replace(**to_replace)
         return p
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/serializers/tools.py` & `easynetwork-1.0.0rc8/src/easynetwork/serializers/tools.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -12,29 +12,19 @@
 # limitations under the License.
 #
 #
 """Serializer implementation tools module"""
 
 from __future__ import annotations
 
-__all__ = [
-    "GeneratorStreamReader",
-    "_wrap_generic_buffered_incremental_deserialize",
-    "_wrap_generic_incremental_deserialize",
-]
-
-import contextlib
-from collections.abc import Callable, Generator
-from typing import TYPE_CHECKING
+__all__ = ["GeneratorStreamReader"]
 
-from .._typevars import _DTOPacketT
-from ..exceptions import LimitOverrunError
+from collections.abc import Generator
 
-if TYPE_CHECKING:
-    from _typeshed import ReadableBuffer
+from ..exceptions import LimitOverrunError
 
 
 class GeneratorStreamReader:
     """
     A binary stream-like object using an in-memory bytes buffer.
 
     The "blocking" operation is done with the generator's :keyword:`yield` statement. It is an helper for
@@ -82,19 +72,20 @@
         """
 
         if size < 0:
             raise ValueError("size must not be < 0")
         if size == 0:
             return b""
 
-        while not self.__buffer:
-            self.__buffer = bytes((yield))
+        buffer = self.__buffer
+        while not buffer:
+            self.__buffer = buffer = bytes((yield))
 
-        data = self.__buffer[:size]
-        self.__buffer = self.__buffer[size:]
+        data = buffer[:size]
+        self.__buffer = buffer[size:]
 
         return data
 
     def read_exactly(self, n: int) -> Generator[None, bytes, bytes]:
         """
         Read exactly `n` bytes.
 
@@ -116,21 +107,23 @@
         """
 
         if n < 0:
             raise ValueError("n must not be < 0")
         if n == 0:
             return b""
 
-        while not self.__buffer:
-            self.__buffer = bytes((yield))
-        while len(self.__buffer) < n:
-            self.__buffer += yield
+        buffer = self.__buffer
+        while not buffer:
+            self.__buffer = buffer = bytes((yield))
+        while len(buffer) < n:
+            buffer += yield
+            self.__buffer = buffer
 
-        data = self.__buffer[:n]
-        self.__buffer = self.__buffer[n:]
+        data = buffer[:n]
+        self.__buffer = buffer[n:]
 
         return data
 
     def read_until(self, separator: bytes, limit: int, *, keep_end: bool = True) -> Generator[None, bytes, bytes]:
         r"""
         Read data from the stream until `separator` is found.
 
@@ -166,63 +159,42 @@
 
         if limit <= 0:
             raise ValueError("limit must be a positive integer")
         seplen: int = len(separator)
         if seplen < 1:
             raise ValueError("Empty separator")
 
-        while not self.__buffer:
-            self.__buffer = bytes((yield))
+        buffer = self.__buffer
+        while not buffer:
+            self.__buffer = buffer = bytes((yield))
 
         offset: int = 0
         sepidx: int = -1
         while True:
-            buflen = len(self.__buffer)
+            buflen = len(buffer)
 
             if buflen - offset >= seplen:
-                sepidx = self.__buffer.find(separator, offset)
+                sepidx = buffer.find(separator, offset)
 
                 if sepidx != -1:
                     break
 
                 offset = buflen + 1 - seplen
                 if offset > limit:
                     msg = "Separator is not found, and chunk exceed the limit"
-                    raise LimitOverrunError(msg, self.__buffer, offset, separator)
+                    raise LimitOverrunError(msg, buffer, offset, separator)
 
-            self.__buffer += yield
+            buffer += yield
+            self.__buffer = buffer
 
         if sepidx > limit:
             msg = "Separator is found, but chunk is longer than limit"
-            raise LimitOverrunError(msg, self.__buffer, sepidx, separator)
+            raise LimitOverrunError(msg, buffer, sepidx, separator)
 
         offset = sepidx + seplen
         if keep_end:
-            data = self.__buffer[:offset]
+            data = buffer[:offset]
         else:
-            data = self.__buffer[:sepidx]
-        self.__buffer = self.__buffer[offset:]
+            data = buffer[:sepidx]
+        self.__buffer = buffer[offset:]
 
         return data
-
-
-def _wrap_generic_incremental_deserialize(
-    func: Callable[[], Generator[None, ReadableBuffer, tuple[_DTOPacketT, ReadableBuffer]]],
-) -> Generator[None, bytes, tuple[_DTOPacketT, bytes]]:
-    packet, remainder = yield from func()
-    # remainder is not copied if it is already a "bytes" object
-    remainder = bytes(remainder)
-    return packet, remainder
-
-
-def _wrap_generic_buffered_incremental_deserialize(
-    buffer: memoryview,
-    func: Callable[[], Generator[None, ReadableBuffer, tuple[_DTOPacketT, ReadableBuffer]]],
-) -> Generator[None, int, tuple[_DTOPacketT, ReadableBuffer]]:
-    next(gen := func())
-    with contextlib.closing(gen):
-        while True:
-            nbytes: int = yield
-            try:
-                gen.send(buffer[:nbytes])
-            except StopIteration as exc:
-                return exc.value
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/serializers/wrapper/base64.py` & `easynetwork-1.0.0rc8/src/easynetwork/serializers/wrapper/base64.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -18,38 +18,38 @@
 
 __all__ = [
     "Base64EncoderSerializer",
 ]
 
 import os
 from collections.abc import Callable
-from typing import Literal, assert_never, final
+from typing import Literal, final
 
-from ..._typevars import _ReceivedDTOPacketT, _SentDTOPacketT
+from ..._typevars import _T_ReceivedDTOPacket, _T_SentDTOPacket
 from ...exceptions import DeserializeError
-from ...lowlevel.constants import _DEFAULT_LIMIT
+from ...lowlevel.constants import DEFAULT_SERIALIZER_LIMIT
 from ..abc import AbstractPacketSerializer
 from ..base_stream import AutoSeparatedPacketSerializer
 
 
-class Base64EncoderSerializer(AutoSeparatedPacketSerializer[_SentDTOPacketT, _ReceivedDTOPacketT]):
+class Base64EncoderSerializer(AutoSeparatedPacketSerializer[_T_SentDTOPacket, _T_ReceivedDTOPacket]):
     """
     A :term:`serializer wrapper` to handle base64 encoded data, built on top of :mod:`base64` module.
     """
 
     __slots__ = ("__serializer", "__encode", "__decode", "__compare_digest", "__decode_error_cls", "__checksum")
 
     def __init__(
         self,
-        serializer: AbstractPacketSerializer[_SentDTOPacketT, _ReceivedDTOPacketT],
+        serializer: AbstractPacketSerializer[_T_SentDTOPacket, _T_ReceivedDTOPacket],
         *,
         alphabet: Literal["standard", "urlsafe"] = "urlsafe",
         checksum: bool | str | bytes = False,
         separator: bytes = b"\r\n",
-        limit: int = _DEFAULT_LIMIT,
+        limit: int = DEFAULT_SERIALIZER_LIMIT,
         debug: bool = False,
     ) -> None:
         """
         Parameters:
             serializer: The serializer to wrap.
             alphabet: The base64 alphabet to use. Possible values are:
 
@@ -72,15 +72,15 @@
             separator=separator,
             incremental_serialize_check_separator=not separator.isspace(),
             limit=limit,
             debug=debug,
         )
         if not isinstance(serializer, AbstractPacketSerializer):
             raise TypeError(f"Expected a serializer instance, got {serializer!r}")
-        self.__serializer: AbstractPacketSerializer[_SentDTOPacketT, _ReceivedDTOPacketT] = serializer
+        self.__serializer: AbstractPacketSerializer[_T_SentDTOPacket, _T_ReceivedDTOPacket] = serializer
         self.__checksum: Callable[[bytes], bytes] | None
         match checksum:
             case False:
                 self.__checksum = None
             case True:
                 from hashlib import sha256 as hashlib_sha256
 
@@ -91,26 +91,26 @@
                 try:
                     key: bytes = base64.urlsafe_b64decode(checksum)
                 except binascii.Error as exc:
                     raise ValueError("signing key must be 32 url-safe base64-encoded bytes.") from exc
                 if len(key) != 32:
                     raise ValueError("signing key must be 32 url-safe base64-encoded bytes.")
                 self.__checksum = lambda data: hmac_digest(key, data, "sha256")
-            case _:  # pragma: no cover
-                assert_never(checksum)
+            case _:
+                raise TypeError("Invalid checksum argument")
 
         match alphabet:
             case "standard":
                 self.__encode = base64.standard_b64encode
                 self.__decode = base64.standard_b64decode
             case "urlsafe":
                 self.__encode = base64.urlsafe_b64encode
                 self.__decode = base64.urlsafe_b64decode
-            case _:  # pragma: no cover
-                assert_never(alphabet)
+            case _:
+                raise TypeError("Invalid alphabet argument")
 
         self.__decode_error_cls = binascii.Error
         self.__compare_digest = compare_digest
 
     @classmethod
     def generate_key(cls) -> bytes:
         """
@@ -119,15 +119,15 @@
         Keep this some place safe!
         """
         import base64
 
         return base64.urlsafe_b64encode(os.urandom(32))
 
     @final
-    def serialize(self, packet: _SentDTOPacketT) -> bytes:
+    def serialize(self, packet: _T_SentDTOPacket) -> bytes:
         """
         Serializes `packet` and encodes the result in base64.
 
         Parameters:
             packet: The Python object to serialize.
 
         Returns:
@@ -135,15 +135,15 @@
         """
         data = self.__serializer.serialize(packet)
         if (checksum := self.__checksum) is not None:
             data += checksum(data)
         return self.__encode(data)
 
     @final
-    def deserialize(self, data: bytes) -> _ReceivedDTOPacketT:
+    def deserialize(self, data: bytes) -> _T_ReceivedDTOPacket:
         """
         Decodes base64 token `data` and deserializes the result.
 
         Parameters:
             data: The byte sequence to deserialize.
 
         Raises:
```

### Comparing `easynetwork-1.0.0rc7/src/easynetwork/serializers/wrapper/compressor.py` & `easynetwork-1.0.0rc8/src/easynetwork/serializers/wrapper/compressor.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -13,27 +13,30 @@
 #
 #
 """Data compressor serializer module"""
 
 from __future__ import annotations
 
 __all__ = [
+    "AbstractCompressorSerializer",
     "BZ2CompressorSerializer",
+    "CompressorInterface",
+    "DecompressorInterface",
     "ZlibCompressorSerializer",
 ]
 
 import abc
 from collections import deque
 from collections.abc import Generator
 from typing import TYPE_CHECKING, Protocol, final
 
-from ..._typevars import _ReceivedDTOPacketT, _SentDTOPacketT
+from ..._typevars import _T_ReceivedDTOPacket, _T_SentDTOPacket
 from ...exceptions import DeserializeError, IncrementalDeserializeError
 from ..abc import AbstractPacketSerializer, BufferedIncrementalPacketSerializer
-from ..tools import _wrap_generic_buffered_incremental_deserialize, _wrap_generic_incremental_deserialize
+from ..base_stream import _wrap_generic_buffered_incremental_deserialize, _wrap_generic_incremental_deserialize
 
 if TYPE_CHECKING:
     import bz2 as _typing_bz2
     import zlib as _typing_zlib
 
     from _typeshed import ReadableBuffer
 
@@ -60,24 +63,24 @@
 
     @property
     @abc.abstractmethod
     def unused_data(self) -> bytes:
         raise NotImplementedError
 
 
-class AbstractCompressorSerializer(BufferedIncrementalPacketSerializer[_SentDTOPacketT, _ReceivedDTOPacketT, memoryview]):
+class AbstractCompressorSerializer(BufferedIncrementalPacketSerializer[_T_SentDTOPacket, _T_ReceivedDTOPacket, memoryview]):
     """
     A :term:`serializer wrapper` base class for compressors.
     """
 
     __slots__ = ("__serializer", "__expected_error", "__debug")
 
     def __init__(
         self,
-        serializer: AbstractPacketSerializer[_SentDTOPacketT, _ReceivedDTOPacketT],
+        serializer: AbstractPacketSerializer[_T_SentDTOPacket, _T_ReceivedDTOPacket],
         expected_decompress_error: type[Exception] | tuple[type[Exception], ...],
         *,
         debug: bool = False,
     ) -> None:
         """
         Parameters:
             serializer: The serializer to wrap.
@@ -87,15 +90,15 @@
         """
         super().__init__()
         if not isinstance(serializer, AbstractPacketSerializer):
             raise TypeError(f"Expected a serializer instance, got {serializer!r}")
         if not isinstance(expected_decompress_error, tuple):
             expected_decompress_error = (expected_decompress_error,)
         assert all(issubclass(e, Exception) for e in expected_decompress_error)  # nosec assert_used
-        self.__serializer: AbstractPacketSerializer[_SentDTOPacketT, _ReceivedDTOPacketT] = serializer
+        self.__serializer: AbstractPacketSerializer[_T_SentDTOPacket, _T_ReceivedDTOPacket] = serializer
         self.__expected_error: tuple[type[Exception], ...] = expected_decompress_error
 
         self.__debug: bool = bool(debug)
 
     @abc.abstractmethod
     def new_compressor_stream(self) -> CompressorInterface:
         """
@@ -109,36 +112,36 @@
         """
         Returns:
             an object suitable for data decompression.
         """
         raise NotImplementedError
 
     @final
-    def serialize(self, packet: _SentDTOPacketT) -> bytes:
+    def serialize(self, packet: _T_SentDTOPacket) -> bytes:
         """
         Serializes `packet` and returns the compressed data parts.
 
         See :meth:`.AbstractPacketSerializer.serialize` documentation for details.
         """
         compressor: CompressorInterface = self.new_compressor_stream()
         return compressor.compress(self.__serializer.serialize(packet)) + compressor.flush()
 
     @final
-    def incremental_serialize(self, packet: _SentDTOPacketT) -> Generator[bytes, None, None]:
+    def incremental_serialize(self, packet: _T_SentDTOPacket) -> Generator[bytes, None, None]:
         """
         Serializes `packet` and yields the compressed data parts.
 
         See :meth:`.AbstractIncrementalPacketSerializer.incremental_serialize` documentation for details.
         """
         compressor: CompressorInterface = self.new_compressor_stream()
         yield compressor.compress(self.__serializer.serialize(packet))
         yield compressor.flush()
 
     @final
-    def deserialize(self, data: bytes) -> _ReceivedDTOPacketT:
+    def deserialize(self, data: bytes) -> _T_ReceivedDTOPacket:
         """
         Decompresses `data` and returns the deserialized packet.
 
         See :meth:`.AbstractPacketSerializer.deserialize` documentation for details.
 
         Raises:
             DeserializeError: :meth:`DecompressorInterface.decompress` does not read until EOF (unused trailing data).
@@ -163,15 +166,15 @@
             if self.debug:
                 raise DeserializeError(msg, error_info={"decompressed_data": data, "extra": decompressor.unused_data})
             raise DeserializeError(msg)
         del decompressor
         return self.__serializer.deserialize(data)
 
     @final
-    def incremental_deserialize(self) -> Generator[None, bytes, tuple[_ReceivedDTOPacketT, bytes]]:
+    def incremental_deserialize(self) -> Generator[None, bytes, tuple[_T_ReceivedDTOPacket, bytes]]:
         """
         Yields until data decompression is finished and deserializes the decompressed data using the underlying serializer.
 
         See :meth:`.AbstractIncrementalPacketSerializer.incremental_deserialize` documentation for details.
 
         Raises:
             IncrementalDeserializeError: :meth:`DecompressorInterface.decompress` raised an error
@@ -187,28 +190,28 @@
         """
         return memoryview(bytearray(sizehint))
 
     @final
     def buffered_incremental_deserialize(
         self,
         buffer: memoryview,
-    ) -> Generator[None, int, tuple[_ReceivedDTOPacketT, ReadableBuffer]]:
+    ) -> Generator[None, int, tuple[_T_ReceivedDTOPacket, ReadableBuffer]]:
         """
         Yields until data decompression is finished and deserializes the decompressed data using the underlying serializer.
 
         See :meth:`.BufferedIncrementalPacketSerializer.buffered_incremental_deserialize` documentation for details.
 
         Raises:
             IncrementalDeserializeError: :meth:`DecompressorInterface.decompress` raised an error
                                          that matches `expected_decompress_error`.
             Exception: Any other error raised by :meth:`DecompressorInterface.decompress` or the underlying serializer.
         """
         return (yield from _wrap_generic_buffered_incremental_deserialize(buffer, self.__generic_incremental_deserialize))
 
-    def __generic_incremental_deserialize(self) -> Generator[None, ReadableBuffer, tuple[_ReceivedDTOPacketT, ReadableBuffer]]:
+    def __generic_incremental_deserialize(self) -> Generator[None, ReadableBuffer, tuple[_T_ReceivedDTOPacket, ReadableBuffer]]:
         results: deque[bytes] = deque()
         decompressor: DecompressorInterface = self.new_decompressor_stream()
         while not decompressor.eof:
             chunk: ReadableBuffer = yield
             try:
                 chunk = decompressor.decompress(chunk)
             except self.__expected_error as exc:
@@ -231,15 +234,15 @@
             data = results[0]
         else:
             data = b"".join(results)
         unused_data: bytes = decompressor.unused_data
         del results, decompressor
 
         try:
-            packet: _ReceivedDTOPacketT = self.__serializer.deserialize(data)
+            packet: _T_ReceivedDTOPacket = self.__serializer.deserialize(data)
         except DeserializeError as exc:
             raise IncrementalDeserializeError(
                 f"Error while deserializing decompressed data: {exc}",
                 remaining_data=unused_data,
                 error_info=exc.error_info,
             ) from exc
 
@@ -250,24 +253,24 @@
     def debug(self) -> bool:
         """
         The debug mode flag. Read-only attribute.
         """
         return self.__debug
 
 
-class BZ2CompressorSerializer(AbstractCompressorSerializer[_SentDTOPacketT, _ReceivedDTOPacketT]):
+class BZ2CompressorSerializer(AbstractCompressorSerializer[_T_SentDTOPacket, _T_ReceivedDTOPacket]):
     """
     A :term:`serializer wrapper` to handle bzip2 compressed data, built on top of :mod:`bz2` module.
     """
 
     __slots__ = ("__compresslevel", "__compressor_factory", "__decompressor_factory")
 
     def __init__(
         self,
-        serializer: AbstractPacketSerializer[_SentDTOPacketT, _ReceivedDTOPacketT],
+        serializer: AbstractPacketSerializer[_T_SentDTOPacket, _T_ReceivedDTOPacket],
         *,
         compress_level: int | None = None,
         debug: bool = False,
     ) -> None:
         """
         Parameters:
             serializer: The serializer to wrap.
@@ -292,24 +295,24 @@
     def new_decompressor_stream(self) -> _typing_bz2.BZ2Decompressor:
         """
         See :meth:`.AbstractCompressorSerializer.new_decompressor_stream` documentation for details.
         """
         return self.__decompressor_factory()
 
 
-class ZlibCompressorSerializer(AbstractCompressorSerializer[_SentDTOPacketT, _ReceivedDTOPacketT]):
+class ZlibCompressorSerializer(AbstractCompressorSerializer[_T_SentDTOPacket, _T_ReceivedDTOPacket]):
     """
     A :term:`serializer wrapper` to handle zlib compressed data, built on top of :mod:`zlib` module.
     """
 
     __slots__ = ("__compresslevel", "__compressor_factory", "__decompressor_factory")
 
     def __init__(
         self,
-        serializer: AbstractPacketSerializer[_SentDTOPacketT, _ReceivedDTOPacketT],
+        serializer: AbstractPacketSerializer[_T_SentDTOPacket, _T_ReceivedDTOPacket],
         *,
         compress_level: int | None = None,
         debug: bool = False,
     ) -> None:
         """
         Parameters:
             serializer: The serializer to wrap.
```

### Comparing `easynetwork-1.0.0rc7/tests/conftest.py` & `easynetwork-1.0.0rc8/tests/conftest.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/tests/import_utils.py` & `easynetwork-1.0.0rc8/tests/import_utils.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/tests/tools.py` & `easynetwork-1.0.0rc8/tests/tools.py`

 * *Files 9% similar despite different names*

```diff
@@ -97,17 +97,17 @@
     nbytes = len(to_write)
     with memoryview(buffer) as buffer, buffer[start_pos or 0 :] as buffer:
         if len(buffer) >= nbytes:
             buffer[:nbytes] = to_write
         else:
             match too_short_buffer:
                 case "error":
-                    raise ValueError("Buffer is too short to contain the chunk to write.")
+                    raise ValueError(f"Buffer is too short to contain the chunk to write. ({len(buffer)} < {nbytes})")
                 case "xfail":
-                    pytest.xfail("Buffer is too short to contain the chunk to write.")
+                    pytest.xfail(f"Buffer is too short to contain the chunk to write. ({len(buffer)} < {nbytes})")
                 case "fill_at_most":
                     nbytes = len(buffer)
                     buffer[:] = memoryview(to_write)[:nbytes]
                 case _:
                     assert_never(too_short_buffer)
     return nbytes
 
@@ -140,13 +140,35 @@
         too_short_buffer=too_short_buffer_for_extra_data,
     )
     return complete_data_nbytes + extra_data_nbytes, extra_data[:extra_data_nbytes]
 
 
 @contextlib.contextmanager
 def temporary_backend(backend: AsyncBackend) -> Iterator[None]:
+    hook = AsyncBackendFactory.backend_factory_hook("asyncio", lambda: backend)
     with contextlib.ExitStack() as stack:
-        stack.callback(AsyncBackendFactory.invalidate_backends_cache)
-        stack.callback(AsyncBackendFactory.remove_installed_hooks)
-        AsyncBackendFactory.push_backend_factory("asyncio", lambda: backend)
+        stack.callback(AsyncBackendFactory.remove_factory_hook, hook)
+        AsyncBackendFactory.push_factory_hook(hook)
         assert AsyncBackendFactory.get_backend("asyncio") is backend
         yield
+
+
+@contextlib.contextmanager
+def temporary_exception_handler(
+    event_loop: asyncio.AbstractEventLoop,
+    handler: asyncio.events._ExceptionHandler | None,
+) -> Iterator[None]:
+    with contextlib.ExitStack() as stack:
+        stack.callback(event_loop.set_exception_handler, event_loop.get_exception_handler())
+        event_loop.set_exception_handler(handler)
+        yield
+
+
+@contextlib.contextmanager
+def temporary_task_factory(
+    event_loop: asyncio.AbstractEventLoop,
+    task_factory: asyncio.events._TaskFactory | None,
+) -> Iterator[None]:
+    with contextlib.ExitStack() as stack:
+        stack.callback(event_loop.set_task_factory, event_loop.get_task_factory())
+        event_loop.set_task_factory(task_factory)
+        yield
```

### Comparing `easynetwork-1.0.0rc7/tests/functional_test/test_async/test_backend/test_asyncio_backend.py` & `easynetwork-1.0.0rc8/tests/functional_test/test_async/test_backend/test_asyncio_backend.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,57 +1,81 @@
 from __future__ import annotations
 
 import asyncio
 import contextvars
+import sys
 import time
-from collections.abc import Awaitable, Callable
-from concurrent.futures import CancelledError as FutureCancelledError, Future, wait as wait_concurrent_futures
+from collections.abc import Awaitable, Callable, Iterator
+from concurrent.futures import CancelledError as FutureCancelledError, wait as wait_concurrent_futures
 from contextlib import ExitStack
-from typing import TYPE_CHECKING, Any, Literal
+from typing import TYPE_CHECKING, Any, Literal, Required, TypedDict
 
+from easynetwork.lowlevel.api_async.backend.abc import TaskInfo
 from easynetwork.lowlevel.api_async.backend.factory import AsyncBackendFactory
 from easynetwork.lowlevel.std_asyncio.backend import AsyncIOBackend
 
 import pytest
 
+from ....tools import temporary_exception_handler
+
 if TYPE_CHECKING:
     from unittest.mock import AsyncMock
 
     from pytest_mock import MockerFixture
 
-cvar_for_test: contextvars.ContextVar[str] = contextvars.ContextVar("cvar_for_test", default="")
+
+class ExceptionCaughtDict(TypedDict, total=False):
+    message: Required[str]
+    exception: Exception
+    future: asyncio.Future[Any]
+    task: asyncio.Task[Any]
+    handle: asyncio.Handle
+    protocol: asyncio.BaseProtocol
+    transport: asyncio.BaseTransport
 
 
 @pytest.mark.asyncio
+@pytest.mark.flaky(retries=3, delay=0)
 class TestAsyncioBackend:
     @pytest.fixture
     @staticmethod
+    def event_loop_exceptions_caught(
+        event_loop: asyncio.AbstractEventLoop,
+        mocker: MockerFixture,
+    ) -> Iterator[list[ExceptionCaughtDict]]:
+        event_loop_exceptions_caught: list[ExceptionCaughtDict] = []
+        handler_stub = mocker.MagicMock(
+            "ExceptionHandler",
+            side_effect=lambda loop, context: event_loop_exceptions_caught.append(context),
+        )
+
+        with temporary_exception_handler(event_loop, handler_stub):
+            yield event_loop_exceptions_caught
+
+    @pytest.fixture
+    @staticmethod
     def backend() -> AsyncIOBackend:
         backend = AsyncBackendFactory.get_backend("asyncio")
         assert isinstance(backend, AsyncIOBackend)
         return backend
 
-    async def test____use_asyncio_transport____True_by_default(self, backend: AsyncIOBackend) -> None:
-        assert backend.using_asyncio_transport()
-
     async def test____cancel_shielded_coro_yield____mute_cancellation(
         self,
         event_loop: asyncio.AbstractEventLoop,
         backend: AsyncIOBackend,
     ) -> None:
         task: asyncio.Task[None] = event_loop.create_task(backend.cancel_shielded_coro_yield())
 
         await asyncio.sleep(0)
 
         for _ in range(3):
             task.cancel()
 
         await task
         assert not task.cancelled()
-        assert task.cancelling() == 3
 
     @pytest.mark.parametrize("cancel_message", ["something", None], ids=lambda p: f"cancel_message=={p!r}")
     async def test____cancel_shielded_coro_yield____cancel_at_the_next_checkpoint(
         self,
         cancel_message: str | None,
         event_loop: asyncio.AbstractEventLoop,
         backend: AsyncIOBackend,
@@ -73,35 +97,55 @@
 
         for _ in range(3):
             task.cancel(msg=cancel_message)
 
         with pytest.raises(asyncio.CancelledError) as exc_info:
             await task
         assert task.cancelled()
-        assert task.cancelling() == 3
         assert test_list == ["a", "b", "c"]
         if cancel_message is None:
             assert exc_info.value.args == ()
         else:
             assert exc_info.value.args == (cancel_message,)
 
     async def test____ignore_cancellation____always_continue_on_cancellation(
         self,
         event_loop: asyncio.AbstractEventLoop,
         backend: AsyncIOBackend,
     ) -> None:
         task: asyncio.Task[int] = event_loop.create_task(backend.ignore_cancellation(asyncio.sleep(0.5, 42)))
 
-        for i in range(5):
+        for i in range(3):
             for _ in range(3):
                 event_loop.call_later(0.1 * i, task.cancel)
 
         assert await task == 42
-        assert not task.cancelled()
-        assert task.cancelling() == 15
+        assert task.cancelling() > 0
+
+    async def test____ignore_cancellation____exception_raised_in_task(
+        self,
+        event_loop: asyncio.AbstractEventLoop,
+        backend: AsyncIOBackend,
+    ) -> None:
+        exception = Exception("error")
+
+        async def coroutine() -> None:
+            raise exception
+
+        task: asyncio.Task[None] = event_loop.create_task(backend.ignore_cancellation(coroutine()))
+
+        for i in range(3):
+            for _ in range(3):
+                event_loop.call_later(0.1 * i, task.cancel)
+
+        with pytest.raises(Exception) as exc_info:
+            await task
+
+        assert task.exception() is exception
+        assert exc_info.value is exception
 
     async def test____ignore_cancellation____task_does_not_appear_in_registered_tasks(
         self,
         event_loop: asyncio.AbstractEventLoop,
         backend: AsyncIOBackend,
     ) -> None:
         async def coroutine() -> bool:
@@ -127,14 +171,53 @@
 
         task = event_loop.create_task(backend.ignore_cancellation(self_cancellation()))
 
         with pytest.raises(asyncio.CancelledError):
             await task
         assert task.cancelled()
 
+    @pytest.mark.parametrize(
+        "eager_task_factory",
+        [
+            pytest.param(
+                False,
+                marks=pytest.mark.xfail(
+                    sys.version_info < (3, 12),
+                    reason="asyncio.Task.get_context() does not exist before Python 3.12",
+                ),
+            ),
+            pytest.param(
+                True,
+                marks=pytest.mark.skipif(
+                    sys.version_info < (3, 12),
+                    reason="asyncio.eager_task_factory does not exist before Python 3.12",
+                ),
+            ),
+        ],
+    )
+    async def test____ignore_cancellation____share_same_context_with_host_task(
+        self,
+        event_loop: asyncio.AbstractEventLoop,
+        eager_task_factory: bool,
+        backend: AsyncIOBackend,
+    ) -> None:
+        if eager_task_factory:
+            event_loop.set_task_factory(getattr(asyncio, "eager_task_factory"))
+
+        cvar_for_test: contextvars.ContextVar[str] = contextvars.ContextVar("cvar_for_test", default="")
+
+        # coroutine() will be done right after create_task() with eager_task_factory==True
+        async def coroutine() -> None:
+            cvar_for_test.set("after_in_coroutine")
+
+        cvar_for_test.set("before_in_current_task")
+        await backend.ignore_cancellation(coroutine())
+
+        assert cvar_for_test.get() == "after_in_coroutine"
+
     async def test____timeout____respected(
         self,
         backend: AsyncIOBackend,
     ) -> None:
         with backend.timeout(1):
             assert await asyncio.sleep(0.5, 42) == 42
 
@@ -286,17 +369,15 @@
             scope.cancel()
             assert scope.cancel_called()
 
             assert current_task.cancelling() == 0
             await asyncio.sleep(0.1)
 
             with scope:
-                assert current_task.cancelling() == 1
                 scope.cancel()
-                assert current_task.cancelling() == 1
                 await backend.coro_yield()
 
             assert current_task.cancelling() == 0
             assert scope.cancelled_caught()
 
         await event_loop.create_task(coroutine())
 
@@ -313,15 +394,14 @@
             scope.reschedule(event_loop.time() + 1)
 
             assert current_task.cancelling() == 0
             await asyncio.sleep(0.5)
 
             with backend.timeout(0.6):
                 with scope:
-                    assert current_task.cancelling() == 0
                     await backend.sleep_forever()
 
             assert scope.cancelled_caught()
 
         await event_loop.create_task(coroutine())
 
     async def test____open_cancel_scope____overwrite_defined_deadline(
@@ -394,25 +474,63 @@
         with backend.open_cancel_scope():
             with pytest.raises(
                 RuntimeError, match=r"^Attempted to exit a cancel scope that isn't the current tasks's current cancel scope$"
             ):
                 stack.close()
             stack.pop_all()
 
-    async def test____create_task_group____task_pool(
+    async def test____create_task_group____start_soon(
+        self,
+        backend: AsyncIOBackend,
+    ) -> None:
+        tasks: list[TaskInfo] = []
+
+        async def coroutine(value: int) -> int:
+            tasks.append(backend.get_current_task())
+            return await asyncio.sleep(0.5, value)
+
+        async with backend.create_task_group() as tg:
+            tg.start_soon(coroutine, 42)
+            tg.start_soon(coroutine, 54)
+            await asyncio.sleep(0)
+            assert len(tasks) == 2
+
+    async def test____create_task_group____start_soon____set_name(
+        self,
+        backend: AsyncIOBackend,
+    ) -> None:
+        tasks: list[TaskInfo] = []
+
+        async def coroutine(value: int) -> int:
+            tasks.append(backend.get_current_task())
+            return await asyncio.sleep(0.5, value)
+
+        async with backend.create_task_group() as tg:
+            tg.start_soon(coroutine, 42, name="compute 42")
+            tg.start_soon(coroutine, 54, name="compute 54")
+            await asyncio.sleep(0)
+
+        assert sorted(t.name for t in tasks) == ["compute 42", "compute 54"]
+
+    async def test____create_task_group____start_and_wait(
         self,
         backend: AsyncIOBackend,
     ) -> None:
+        tasks: list[TaskInfo] = []
+
         async def coroutine(value: int) -> int:
+            tasks.append(backend.get_current_task())
             return await asyncio.sleep(0.5, value)
 
         async with backend.create_task_group() as tg:
-            task_42 = tg.start_soon(coroutine, 42)
-            task_54 = tg.start_soon(coroutine, 54)
+            task_42 = await tg.start(coroutine, 42)
+            task_54 = await tg.start(coroutine, 54)
 
+            assert len(tasks) == 2
+            assert tasks == [task_42.info, task_54.info]
             assert not task_42.done()
             assert not task_54.done()
 
         assert task_42.done()
         assert task_54.done()
         assert not task_42.cancelled()
         assert not task_54.cancelled()
@@ -425,24 +543,49 @@
 
         # Task already done cannot be cancelled
         assert not task_42.cancel()
         assert not task_54.cancel()
         assert await task_42.join() == 42
         assert await task_54.join() == 54
 
+    async def test____create_task_group____start_and_wait____set_name(
+        self,
+        backend: AsyncIOBackend,
+    ) -> None:
+        async def coroutine(value: int) -> int:
+            return await asyncio.sleep(0.5, value)
+
+        async with backend.create_task_group() as tg:
+            task_42 = await tg.start(coroutine, 42, name="compute 42")
+            task_54 = await tg.start(coroutine, 54, name="compute 54")
+
+            assert task_42.info.name == "compute 42"
+            assert task_54.info.name == "compute 54"
+
+    async def test____create_task_group____start_and_wait____waiter_cancelled(
+        self,
+        backend: AsyncIOBackend,
+    ) -> None:
+        async def coroutine(value: int) -> int:
+            return await asyncio.sleep(0.5, value)
+
+        with backend.move_on_after(0):
+            async with backend.create_task_group() as tg:
+                await tg.start(coroutine, 42, name="compute 42")
+
     async def test____create_task_group____task_cancellation(
         self,
         backend: AsyncIOBackend,
     ) -> None:
         async def coroutine(value: int) -> int:
             return await asyncio.sleep(0.5, value)
 
         async with backend.create_task_group() as tg:
-            task_42 = tg.start_soon(coroutine, 42)
-            task_54 = tg.start_soon(coroutine, 54)
+            task_42 = await tg.start(coroutine, 42)
+            task_54 = await tg.start(coroutine, 54)
 
             await asyncio.sleep(0)
             assert not task_42.done()
             assert not task_54.done()
 
             assert task_42.cancel()
 
@@ -453,169 +596,89 @@
         with pytest.raises(asyncio.CancelledError):
             await task_42.join()
         assert await task_54.join() == 54
 
         # Tasks cannot be cancelled twice
         assert not task_42.cancel()
 
-    @pytest.mark.parametrize("join_method", ["join", "join_or_cancel"])
+    @pytest.mark.parametrize("join_method", ["join", "join_or_cancel", "wait"])
     async def test____create_task_group____task_join_cancel_shielding(
         self,
-        join_method: Literal["join", "join_or_cancel"],
+        join_method: Literal["join", "join_or_cancel", "wait"],
         event_loop: asyncio.AbstractEventLoop,
         backend: AsyncIOBackend,
     ) -> None:
         async def coroutine(value: int) -> int:
             return await asyncio.sleep(0.5, value)
 
         async with backend.create_task_group() as task_group:
-            inner_task = task_group.start_soon(coroutine, 42)
+            inner_task = await task_group.start(coroutine, 42)
 
+            outer_task: asyncio.Task[Any]
             match join_method:
                 case "join":
                     outer_task = event_loop.create_task(inner_task.join())
                 case "join_or_cancel":
                     outer_task = event_loop.create_task(inner_task.join_or_cancel())
+                case "wait":
+                    outer_task = event_loop.create_task(inner_task.wait())
                 case _:
                     pytest.fail("invalid argument")
             event_loop.call_later(0.2, outer_task.cancel)
 
             with pytest.raises(asyncio.CancelledError):
                 await outer_task
 
             assert outer_task.cancelled()
             if join_method == "join_or_cancel":
                 assert inner_task.cancelled()
             else:
                 assert not inner_task.cancelled()
                 assert await inner_task.join() == 42
 
-    async def test____create_task_group____start_soon_with_context(
+    @pytest.mark.parametrize("task_state", ["result", "exception", "cancelled"])
+    async def test____create_task_group____task_wait(
         self,
-        backend: AsyncIOBackend,
-    ) -> None:
-        async def coroutine(value: str) -> None:
-            cvar_for_test.set(value)
-
-        async with backend.create_task_group() as task_group:
-            cvar_for_test.set("something")
-            ctx = contextvars.copy_context()
-            task = task_group.start_soon(coroutine, "other", context=ctx)
-            await task.wait()
-            assert cvar_for_test.get() == "something"
-            assert ctx.run(cvar_for_test.get) == "other"
-
-    async def test____wait_future____wait_until_done(
-        self,
-        event_loop: asyncio.AbstractEventLoop,
-        backend: AsyncIOBackend,
-    ) -> None:
-        future: Future[int] = Future()
-        event_loop.call_later(0.5, future.set_result, 42)
-
-        assert await backend.wait_future(future) == 42
-
-    @pytest.mark.parametrize("future_running", [None, "before", "after"], ids=lambda state: f"future_running=={state}")
-    async def test____wait_future____cancel_future_if_task_is_cancelled(
-        self,
-        future_running: str | None,
+        task_state: Literal["result", "exception", "cancelled"],
         event_loop: asyncio.AbstractEventLoop,
         backend: AsyncIOBackend,
     ) -> None:
-        future: Future[int] = Future()
-        if future_running == "before":
-            future.set_running_or_notify_cancel()
-            assert future.running()
-            event_loop.call_later(0.5, future.set_result, 42)
-
-        task = event_loop.create_task(backend.wait_future(future))
-        await asyncio.sleep(0.1)
+        future: asyncio.Future[None] = event_loop.create_future()
 
-        if future_running == "after":
-            future.set_running_or_notify_cancel()
-            assert future.running()
-            event_loop.call_later(0.5, future.set_result, 42)
-
-        for _ in range(3):
-            task.cancel()
-        await asyncio.wait([task])
-
-        if future_running is not None:
-            assert not future.cancelled()
-            assert not task.cancelled()
-            assert task.result() == 42
-        else:
-            assert future.cancelled()
-            assert task.cancelled()
-
-    async def test____wait_future____future_is_cancelled(
-        self,
-        event_loop: asyncio.AbstractEventLoop,
-        backend: AsyncIOBackend,
-    ) -> None:
-        future: Future[int] = Future()
-        task = event_loop.create_task(backend.wait_future(future))
+        class FutureException(Exception):
+            pass
 
-        event_loop.call_later(0.1, future.cancel)
-        await asyncio.wait([task])
-
-        assert not task.cancelled()
-        assert type(task.exception()) is FutureCancelledError
-
-    async def test____wait_future____already_done(
-        self,
-        backend: AsyncIOBackend,
-    ) -> None:
-        future: Future[int] = Future()
-        future.set_result(42)
-
-        assert await backend.wait_future(future) == 42
-
-    async def test____wait_future____already_cancelled(
-        self,
-        backend: AsyncIOBackend,
-    ) -> None:
-        future: Future[int] = Future()
-        future.cancel()
-
-        with pytest.raises(FutureCancelledError):
-            await backend.wait_future(future)
-
-    async def test____wait_future____already_cancelled____task_cancelled_too(
-        self,
-        event_loop: asyncio.AbstractEventLoop,
-        backend: AsyncIOBackend,
-    ) -> None:
-        future: Future[int] = Future()
-        future.cancel()
-
-        task = event_loop.create_task(backend.wait_future(future))
-        event_loop.call_soon(task.cancel)
-
-        with pytest.raises(asyncio.CancelledError):
-            await task
-
-    async def test____wait_future____task_cancellation_prevails_over_future_cancellation(
-        self,
-        event_loop: asyncio.AbstractEventLoop,
-        backend: AsyncIOBackend,
-    ) -> None:
-        future: Future[int] = Future()
+        def set_future_result() -> None:
+            match task_state:
+                case "result":
+                    future.set_result(None)
+                case "exception":
+                    future.set_exception(FutureException("Error"))
+                case "cancelled":
+                    future.cancel()
+                case _:
+                    pytest.fail("invalid argument")
 
-        task = event_loop.create_task(backend.wait_future(future))
+        async def coroutine() -> None:
+            return await future
 
-        event_loop.call_soon(future.cancel)
-        for _ in range(3):
-            await asyncio.sleep(0)
-        event_loop.call_soon(task.cancel)
+        event_loop.call_later(0.1, set_future_result)
 
-        with pytest.raises(asyncio.CancelledError):
-            await task
-
-        assert task.cancelled()
+        try:
+            async with backend.create_task_group() as task_group:
+                task = await task_group.start(coroutine)
+
+                await task.wait()
+                assert task.done()
+
+                # Must not yield if task is already done
+                async with asyncio.timeout(0):
+                    await task.wait()
+        except* FutureException:
+            pass
 
     async def test____run_in_thread____cannot_be_cancelled(
         self,
         event_loop: asyncio.AbstractEventLoop,
         backend: AsyncIOBackend,
     ) -> None:
         task = event_loop.create_task(backend.run_in_thread(time.sleep, 0.5))
@@ -686,31 +749,47 @@
                 return threads_portal.run_coroutine(coroutine, 42)
 
             return backend.bootstrap(main)
 
         async with backend.create_threads_portal() as threads_portal:
             assert await backend.run_in_thread(thread) == 42
 
+    @pytest.mark.parametrize("exception_cls", [Exception, BaseException])
     async def test____create_threads_portal____run_coroutine_from_thread____exception_raised(
         self,
         backend: AsyncIOBackend,
+        exception_cls: type[BaseException],
+        event_loop_exceptions_caught: list[ExceptionCaughtDict],
     ) -> None:
-        expected_exception = OSError("Why not?")
+        expected_exception = exception_cls("Why not?")
 
         async def coroutine(value: int) -> int:
             raise expected_exception
 
         def thread() -> int:
             return threads_portal.run_coroutine(coroutine, 42)
 
-        async with backend.create_threads_portal() as threads_portal:
-            with pytest.raises(OSError) as exc_info:
-                await backend.run_in_thread(thread)
+        threads_portal = backend.create_threads_portal()
+        if issubclass(exception_cls, Exception):
+            async with threads_portal:
+                with pytest.raises(BaseException) as exc_info:
+                    await backend.run_in_thread(thread)
+
+            assert exc_info.value is expected_exception
+            assert len(event_loop_exceptions_caught) == 0
+        else:
+            with pytest.raises(BaseExceptionGroup) as exc_group_info:
+                async with threads_portal:
+                    with pytest.raises(BaseException) as exc_info:
+                        await backend.run_in_thread(thread)
 
-        assert exc_info.value is expected_exception
+                assert exc_info.value is expected_exception
+
+            assert len(event_loop_exceptions_caught) == 0
+            assert exc_group_info.value.exceptions[0] is expected_exception
 
     async def test____create_threads_portal____run_coroutine_from_thread____coroutine_cancelled(
         self,
         backend: AsyncIOBackend,
     ) -> None:
         async def coroutine(value: int) -> int:
             task = asyncio.current_task()
@@ -806,31 +885,45 @@
                 return threads_portal.run_sync(not_threadsafe_func, 42)
 
             return backend.bootstrap(main)
 
         async with backend.create_threads_portal() as threads_portal:
             assert await backend.run_in_thread(thread) == 42
 
+    @pytest.mark.parametrize("exception_cls", [Exception, BaseException])
     async def test____create_threads_portal____run_sync_from_thread_in_event_loop____exception_raised(
         self,
         backend: AsyncIOBackend,
+        exception_cls: type[BaseException],
+        event_loop_exceptions_caught: list[ExceptionCaughtDict],
     ) -> None:
-        expected_exception = OSError("Why not?")
+        expected_exception = exception_cls("Why not?")
 
         def not_threadsafe_func(value: int) -> int:
             raise expected_exception
 
         def thread() -> int:
             return threads_portal.run_sync(not_threadsafe_func, 42)
 
-        async with backend.create_threads_portal() as threads_portal:
-            with pytest.raises(OSError) as exc_info:
-                await backend.run_in_thread(thread)
+        threads_portal = backend.create_threads_portal()
+        if issubclass(exception_cls, Exception):
+            async with threads_portal:
+                with pytest.raises(BaseException) as exc_info:
+                    await backend.run_in_thread(thread)
 
-        assert exc_info.value is expected_exception
+            assert exc_info.value is expected_exception
+            assert len(event_loop_exceptions_caught) == 0
+        else:
+            async with threads_portal:
+                with pytest.raises(BaseException) as exc_info:
+                    await backend.run_in_thread(thread)
+
+            assert exc_info.value is expected_exception
+            assert len(event_loop_exceptions_caught) == 1
+            assert event_loop_exceptions_caught[0]["exception"] is expected_exception
 
     async def test____create_threads_portal____run_sync_from_thread_in_event_loop____explicit_concurrent_future_Cancelled(
         self,
         backend: AsyncIOBackend,
     ) -> None:
         def not_threadsafe_func(value: int) -> int:
             raise FutureCancelledError()
@@ -924,14 +1017,15 @@
             await backend.run_in_thread(thread)
 
     @pytest.mark.parametrize("value", [42, ValueError("Not caught")], ids=repr)
     async def test____create_threads_portal____run_coroutine_soon____future_cancelled____cancellation_ignored(
         self,
         value: int | Exception,
         backend: AsyncIOBackend,
+        event_loop_exceptions_caught: list[ExceptionCaughtDict],
         mocker: MockerFixture,
     ) -> None:
         cancellation_ignored = mocker.stub()
 
         async def coroutine() -> int:
             try:
                 await asyncio.sleep(1)
@@ -953,27 +1047,37 @@
             wait_concurrent_futures({future}, timeout=0.2)  # Test if future.set_running_or_notify_cancel() have been called
             assert future.cancelled()
 
         async with backend.create_threads_portal() as threads_portal:
             await backend.run_in_thread(thread)
 
         cancellation_ignored.assert_called_once()
+        if isinstance(value, Exception):
+            assert len(event_loop_exceptions_caught) == 1
+            assert (
+                event_loop_exceptions_caught[0]["message"]
+                == "Task exception was not retrieved because future object is cancelled"
+            )
+            assert event_loop_exceptions_caught[0]["exception"] is value
+            assert isinstance(event_loop_exceptions_caught[0]["task"], asyncio.Task)
+        else:
+            assert len(event_loop_exceptions_caught) == 0
 
     async def test____create_threads_portal____run_coroutine_soon____future_cancelled_before_await(
         self,
         event_loop: asyncio.AbstractEventLoop,
         backend: AsyncIOBackend,
     ) -> None:
         checkpoints: list[str] = []
 
         async def coroutine() -> None:
             current_task = asyncio.current_task()
             assert current_task is not None
 
-            checkpoints.append(f"{current_task.cancelling()=}")
+            checkpoints.append("task started")
             await asyncio.sleep(0)
             checkpoints.append("does-not-raise-CancelledError")
 
         def thread() -> None:
             future = threads_portal.run_coroutine_soon(coroutine)
             future.cancel()
 
@@ -989,15 +1093,34 @@
             event_loop_slowdown_handle = event_loop.call_soon(event_loop_slowdown)
 
         event_loop_slowdown_handle = event_loop.call_soon(event_loop_slowdown)
         async with backend.create_threads_portal() as threads_portal:
             await backend.run_in_thread(thread)
 
         event_loop_slowdown_handle.cancel()
-        assert checkpoints == ["current_task.cancelling()=1"]
+        assert checkpoints == ["task started"]
+
+    @pytest.mark.skipif(not hasattr(asyncio, "eager_task_factory"), reason="asyncio.eager_task_factory not implemented")
+    async def test____create_threads_portal____run_coroutine_soon____eager_task(
+        self,
+        event_loop: asyncio.AbstractEventLoop,
+        backend: AsyncIOBackend,
+    ) -> None:
+        event_loop.set_task_factory(getattr(asyncio, "eager_task_factory"))
+
+        async def coroutine() -> int:
+            return 42
+
+        def thread() -> None:
+            future = threads_portal.run_coroutine_soon(coroutine)
+            assert future.done()
+            assert future.result() == 42
+
+        async with backend.create_threads_portal() as threads_portal:
+            await backend.run_in_thread(thread)
 
     async def test____create_threads_portal____context_exit____wait_scheduled_call_soon(
         self,
         event_loop: asyncio.AbstractEventLoop,
         backend: AsyncIOBackend,
         mocker: MockerFixture,
     ) -> None:
@@ -1051,37 +1174,45 @@
     @pytest.fixture
     @staticmethod
     def backend() -> AsyncIOBackend:
         backend = AsyncBackendFactory.get_backend("asyncio")
         assert isinstance(backend, AsyncIOBackend)
         return backend
 
-    @pytest.fixture(params=["cancel_shielded_coro_yield", "ignore_cancellation", "run_in_thread", "wait_future"])
+    @pytest.fixture(
+        params=[
+            "cancel_shielded_coro_yield",
+            "ignore_cancellation",
+            "run_in_thread",
+            "cancel_shielded_wait_asyncio_futures",
+        ]
+    )
     @staticmethod
     def cancel_shielded_coroutine(
         request: pytest.FixtureRequest,
         backend: AsyncIOBackend,
-        event_loop: asyncio.AbstractEventLoop,
     ) -> Callable[[], Awaitable[Any]]:
         match getattr(request, "param"):
             case "cancel_shielded_coro_yield":
                 return backend.cancel_shielded_coro_yield
             case "ignore_cancellation":
                 return lambda: backend.ignore_cancellation(backend.sleep(0.1))
             case "run_in_thread":
                 return lambda: backend.run_in_thread(time.sleep, 0.1)
-            case "wait_future":
+            case "cancel_shielded_wait_asyncio_futures":
+                from easynetwork.lowlevel.std_asyncio.tasks import TaskUtils
 
-                async def wait_future_coroutine() -> None:
-                    future: Future[None] = Future()
-                    future.set_running_or_notify_cancel()
-                    event_loop.call_later(0.1, future.set_result, None)
-                    return await backend.wait_future(future)
+                async def cancel_shielded_wait_asyncio_futures() -> None:
+                    loop = asyncio.get_running_loop()
+                    futures = [loop.create_future() for _ in range(3)]
+                    for i, f in enumerate(futures, start=1):
+                        loop.call_later(0.1 * i, f.set_result, None)
+                    await TaskUtils.cancel_shielded_wait_asyncio_futures(futures)
 
-                return wait_future_coroutine
+                return cancel_shielded_wait_asyncio_futures
             case _:
                 pytest.fail("Invalid parameter")
 
     async def test____cancel_shielded_coroutine____do_not_cancel_at_timeout_end(
         self,
         cancel_shielded_coroutine: Callable[[], Awaitable[Any]],
         event_loop: asyncio.AbstractEventLoop,
@@ -1118,27 +1249,22 @@
             assert current_task is not None
 
             with backend.move_on_after(0) as scope:
                 with backend.timeout(0):
                     with backend.timeout(0):
                         await cancel_shielded_coroutine()
                         checkpoints.append("inner_cancel_shielded_coroutine")
-                        assert current_task.cancelling() == 3
 
                     await cancel_shielded_coroutine()
-                    assert current_task.cancelling() == 2
                     checkpoints.append("cancel_shielded_coroutine")
 
-                assert current_task.cancelling() == 1
                 checkpoints.append("inner_coro_yield")
                 await backend.coro_yield()
                 checkpoints.append("should_not_be_here")
 
-            assert current_task.cancelling() == 0
-
             assert scope.cancel_called()
             assert scope.cancelled_caught()
             await backend.coro_yield()
             checkpoints.append("coro_yield")
             return value
 
         task = event_loop.create_task(coroutine(42))
@@ -1160,17 +1286,15 @@
 
             await cancel_shielded_coroutine()
             checkpoints.append("cancel_shielded_coroutine")
 
             with backend.timeout(0), backend.open_cancel_scope():
                 await cancel_shielded_coroutine()
                 checkpoints.append("inner_cancel_shielded_coroutine")
-                assert current_task.cancelling() == 2
 
-            assert current_task.cancelling() == 1
             await backend.coro_yield()
             checkpoints.append("should_not_be_here")
             return value
 
         task = event_loop.create_task(coroutine(42))
         event_loop.call_soon(task.cancel)
 
@@ -1190,17 +1314,15 @@
             current_task = asyncio.current_task()
             assert current_task is not None
 
             with backend.open_cancel_scope():
                 with backend.open_cancel_scope():
                     await cancel_shielded_coroutine()
                     checkpoints.append("cancel_shielded_coroutine")
-                    assert current_task.cancelling() == 1
 
-            assert current_task.cancelling() == 1
             await backend.coro_yield()
             checkpoints.append("should_not_be_here")
             return value
 
         task = event_loop.create_task(coroutine(42))
         event_loop.call_soon(task.cancel)
 
@@ -1223,16 +1345,16 @@
                     await backend.ignore_cancellation(backend.sleep(1))
                     inner_scope.cancel()
                     await backend.coro_yield()
 
             assert outer_scope.cancel_called()
             assert inner_scope.cancel_called()
 
-            assert not outer_scope.cancelled_caught()
-            assert inner_scope.cancelled_caught()
+            assert outer_scope.cancelled_caught()
+            assert not inner_scope.cancelled_caught()
 
         await event_loop.create_task(coroutine())
 
     async def test____cancel_shielded_coroutine____scope_cancellation_edge_case_2(
         self,
         event_loop: asyncio.AbstractEventLoop,
         backend: AsyncIOBackend,
@@ -1244,18 +1366,15 @@
             outer_scope = backend.move_on_after(1.5)
             inner_scope = backend.move_on_after(0.5)
             with outer_scope:
                 with backend.open_cancel_scope(), backend.open_cancel_scope():
                     with inner_scope:
                         await backend.ignore_cancellation(backend.sleep(1))
                 assert not inner_scope.cancelled_caught()
-                try:
-                    await backend.coro_yield()
-                except asyncio.CancelledError:
-                    pytest.fail("Cancelled")
+                await backend.coro_yield()
                 await backend.sleep(1)
 
             assert outer_scope.cancel_called()
             assert inner_scope.cancel_called()
 
             assert outer_scope.cancelled_caught()
             assert not inner_scope.cancelled_caught()
@@ -1272,21 +1391,20 @@
             assert current_task is not None
 
             with backend.move_on_after(0) as inner_scope:
                 pass
 
             await backend.coro_yield()
 
-            assert not inner_scope.cancel_called()
+            assert inner_scope.cancel_called()
 
             assert not inner_scope.cancelled_caught()
 
         await event_loop.create_task(coroutine())
 
-    @pytest.mark.xfail(raises=asyncio.CancelledError, reason="Task.cancel() cannot be erased", strict=True)
     async def test____cancel_shielded_coroutine____scope_cancellation_edge_case_4(
         self,
         event_loop: asyncio.AbstractEventLoop,
         backend: AsyncIOBackend,
     ) -> None:
         async def coroutine() -> None:
             current_task = asyncio.current_task()
@@ -1391,7 +1509,37 @@
         task = event_loop.create_task(coroutine())
         event_loop.call_soon(task.cancel)
 
         with pytest.raises(asyncio.CancelledError):
             await task
 
         assert checkpoints == ["inner_yield_in_cancel"]
+
+    async def test____ignore_cancellation____reschedule_erased_cancel_from_parent(
+        self,
+        event_loop: asyncio.AbstractEventLoop,
+        backend: AsyncIOBackend,
+    ) -> None:
+        checkpoints: list[str] = []
+
+        async def coroutine() -> None:
+            with backend.open_cancel_scope() as parent_scope:
+                parent_scope.cancel()
+
+                try:
+                    await backend.sleep_forever()
+                except asyncio.CancelledError:
+                    pass
+
+                checkpoints.append("cancel_erased")
+
+                await backend.ignore_cancellation(backend.sleep(0.1))
+                checkpoints.append("ignore_cancellation_called")
+
+                await backend.coro_yield()
+                checkpoints.append("should_not_be_here")
+
+            checkpoints.append("parent_scope_cancelled_caught")
+
+        await event_loop.create_task(coroutine())
+
+        assert checkpoints == ["cancel_erased", "ignore_cancellation_called", "parent_scope_cancelled_caught"]
```

### Comparing `easynetwork-1.0.0rc7/tests/functional_test/test_communication/conftest.py` & `easynetwork-1.0.0rc8/tests/functional_test/test_communication/conftest.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/tests/functional_test/test_communication/serializer.py` & `easynetwork-1.0.0rc8/tests/functional_test/test_communication/serializer.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/tests/functional_test/test_communication/test_async/test_client/test_tcp.py` & `easynetwork-1.0.0rc8/tests/functional_test/test_communication/test_async/test_client/test_tcp.py`

 * *Files 7% similar despite different names*

```diff
@@ -2,39 +2,38 @@
 
 import asyncio
 import contextlib
 import ssl
 from collections.abc import AsyncIterator
 from socket import AF_INET, IPPROTO_TCP, SHUT_WR, TCP_NODELAY, socket as Socket
 
-from easynetwork.api_async.client.tcp import AsyncTCPNetworkClient
+from easynetwork.clients.async_tcp import AsyncTCPNetworkClient
 from easynetwork.exceptions import ClientClosedError, StreamProtocolParseError
 from easynetwork.lowlevel.socket import IPv4SocketAddress, IPv6SocketAddress, SocketProxy
 from easynetwork.protocol import StreamProtocol
 
 import pytest
 import pytest_asyncio
 
-from .....tools import is_uvloop_event_loop
-
 
 async def readline(loop: asyncio.AbstractEventLoop, sock: Socket) -> bytes:
     buf: list[bytes] = []
     while True:
         chunk = await loop.sock_recv(sock, 1024)
         if not chunk:
             break
         buf.append(chunk)
         if b"\n" in chunk:
             break
     return b"".join(buf)
 
 
 @pytest.mark.asyncio
-@pytest.mark.usefixtures("simulate_no_ssl_module", "use_asyncio_transport")
+@pytest.mark.usefixtures("simulate_no_ssl_module")
+@pytest.mark.flaky(retries=3, delay=1)
 class TestAsyncTCPNetworkClient:
     @pytest.fixture
     @staticmethod
     def server(socket_pair: tuple[Socket, Socket]) -> Socket:
         server = socket_pair[0]
         server.setsockopt(IPPROTO_TCP, TCP_NODELAY, 1)
         server.setblocking(False)
@@ -71,70 +70,14 @@
         event_loop: asyncio.AbstractEventLoop,
         client: AsyncTCPNetworkClient[str, str],
         server: Socket,
     ) -> None:
         await client.send_packet("ABCDEF")
         assert await readline(event_loop, server) == b"ABCDEF\n"
 
-    async def test____send_packet____connection_error____fresh_connection_closed_by_server(
-        self,
-        use_asyncio_transport: bool,
-        event_loop: asyncio.AbstractEventLoop,
-        client: AsyncTCPNetworkClient[str, str],
-        server: Socket,
-    ) -> None:
-        if use_asyncio_transport:
-            pytest.skip("It is not mandatory for asyncio.Transport implementations to raise ConnectionAbortedError")
-        if is_uvloop_event_loop(event_loop):
-            pytest.skip("It is not mandatory for uvloop to raise ConnectionAbortedError")
-        server.close()
-        with pytest.raises(ConnectionAbortedError):
-            for _ in range(3):  # Windows and macOS catch the issue after several send()
-                await client.send_packet("ABCDEF")
-                await asyncio.sleep(0)
-
-    async def test____send_packet____connection_error____after_previous_successful_try(
-        self,
-        use_asyncio_transport: bool,
-        event_loop: asyncio.AbstractEventLoop,
-        client: AsyncTCPNetworkClient[str, str],
-        server: Socket,
-    ) -> None:
-        if use_asyncio_transport:
-            pytest.skip("It is not mandatory for asyncio.Transport implementations to raise ConnectionAbortedError")
-        if is_uvloop_event_loop(event_loop):
-            pytest.skip("It is not mandatory for uvloop to raise ConnectionAbortedError")
-
-        await client.send_packet("ABCDEF")
-        assert await readline(event_loop, server) == b"ABCDEF\n"
-        server.close()
-        with pytest.raises(ConnectionAbortedError):
-            for _ in range(3):  # Windows and macOS catch the issue after several send()
-                await client.send_packet("ABCDEF")
-                await asyncio.sleep(0)
-
-    async def test____send_packet____connection_error____partial_read_then_close(
-        self,
-        use_asyncio_transport: bool,
-        event_loop: asyncio.AbstractEventLoop,
-        client: AsyncTCPNetworkClient[str, str],
-        server: Socket,
-    ) -> None:
-        if use_asyncio_transport:
-            pytest.skip("It is not mandatory for asyncio.Transport implementations to raise ConnectionAbortedError")
-        if is_uvloop_event_loop(event_loop):
-            pytest.skip("It is not mandatory for uvloop to raise ConnectionAbortedError")
-        await client.send_packet("ABC")
-        assert await event_loop.sock_recv(server, 1) == b"A"
-        server.close()
-        with pytest.raises(ConnectionAbortedError):
-            for _ in range(3):  # Windows and macOS catch the issue after several send()
-                await client.send_packet("DEF")
-                await asyncio.sleep(0)
-
     async def test____send_packet____closed_client(self, client: AsyncTCPNetworkClient[str, str]) -> None:
         await client.aclose()
         with pytest.raises(ClientClosedError):
             await client.send_packet("ABCDEF")
 
     @pytest.mark.parametrize("incremental_serializer", [pytest.param("bad_serialize", id="serializer_crash")], indirect=True)
     async def test____send_packet____protocol_crashed(
@@ -258,22 +201,22 @@
             pytest.param("invalid_buffered", id="buffered_serializer_crash"),
         ],
         indirect=True,
     )
     async def test____recv_packet____protocol_crashed(
         self,
         event_loop: asyncio.AbstractEventLoop,
-        use_asyncio_transport: bool,
         is_buffered_protocol: bool,
         client: AsyncTCPNetworkClient[str, str],
         server: Socket,
     ) -> None:
         expected_pattern: str
-        if is_buffered_protocol and not use_asyncio_transport:
-            expected_pattern = r"^protocol\.build_packet_from_buffer\(\) crashed$"
+        if is_buffered_protocol:
+            pytest.xfail("asyncio implementation does not support buffered serializer yet.")
+            # expected_pattern = r"^protocol\.build_packet_from_buffer\(\) crashed$"
         else:
             expected_pattern = r"^protocol\.build_packet_from_chunks\(\) crashed$"
         await event_loop.sock_sendall(server, b"ABCDEF\n")
         with pytest.raises(RuntimeError, match=expected_pattern):
             await client.recv_packet()
 
     async def test____iter_received_packets____yields_available_packets_until_eof(
@@ -329,15 +272,15 @@
             assert isinstance(address, IPv4SocketAddress)
         else:
             assert isinstance(address, IPv6SocketAddress)
         assert address == client.socket.getpeername()
 
 
 @pytest.mark.asyncio
-@pytest.mark.usefixtures("simulate_no_ssl_module", "use_asyncio_transport")
+@pytest.mark.usefixtures("simulate_no_ssl_module")
 class TestAsyncTCPNetworkClientConnection:
     @pytest_asyncio.fixture(autouse=True)
     @staticmethod
     async def server(localhost_ip: str, socket_family: int) -> AsyncIterator[asyncio.Server]:
         async def client_connected_cb(reader: asyncio.StreamReader, writer: asyncio.StreamWriter) -> None:
             try:
                 data: bytes = await reader.readline()
```

### Comparing `easynetwork-1.0.0rc7/tests/functional_test/test_communication/test_async/test_client/test_udp.py` & `easynetwork-1.0.0rc8/tests/functional_test/test_communication/test_async/test_client/test_udp.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,26 +1,25 @@
 from __future__ import annotations
 
 import asyncio
 import contextlib
 from collections.abc import AsyncIterator, Awaitable, Callable
 from socket import AF_INET, socket as Socket
 
-from easynetwork.api_async.client.udp import AsyncUDPNetworkClient
+from easynetwork.clients.async_udp import AsyncUDPNetworkClient
 from easynetwork.exceptions import ClientClosedError, DatagramProtocolParseError
 from easynetwork.lowlevel.socket import IPv4SocketAddress, IPv6SocketAddress, SocketProxy
 from easynetwork.lowlevel.std_asyncio.datagram.endpoint import DatagramEndpoint, create_datagram_endpoint
 from easynetwork.protocol import DatagramProtocol
 
 import pytest
 import pytest_asyncio
 
 from .....tools import PlatformMarkers
 from .._utils import delay
-from ..conftest import use_asyncio_transport_xfail_uvloop
 
 
 @pytest.fixture
 def udp_socket_factory(
     request: pytest.FixtureRequest,
     localhost_ip: str,
 ) -> Callable[[], Socket]:
@@ -50,15 +49,15 @@
             stack.push_async_callback(lambda: asyncio.wait_for(endpoint.aclose(), 3))
             return endpoint
 
         yield factory
 
 
 @pytest.mark.asyncio
-@pytest.mark.usefixtures("use_asyncio_transport")
+@pytest.mark.flaky(retries=3, delay=1)
 class TestAsyncUDPNetworkClient:
     @pytest_asyncio.fixture
     @staticmethod
     async def server(datagram_endpoint_factory: Callable[[], Awaitable[DatagramEndpoint]]) -> DatagramEndpoint:
         return await datagram_endpoint_factory()
 
     @pytest.fixture
@@ -146,21 +145,19 @@
     async def test____send_packet____protocol_crashed(
         self,
         client: AsyncUDPNetworkClient[str, str],
     ) -> None:
         with pytest.raises(RuntimeError, match=r"^protocol\.make_datagram\(\) crashed$"):
             await client.send_packet("ABCDEF")
 
-    @use_asyncio_transport_xfail_uvloop
     async def test____recv_packet____default(self, client: AsyncUDPNetworkClient[str, str], server: DatagramEndpoint) -> None:
         await server.sendto(b"ABCDEF", client.get_local_address())
         async with asyncio.timeout(3):
             assert await client.recv_packet() == "ABCDEF"
 
-    @use_asyncio_transport_xfail_uvloop
     async def test____recv_packet____ignore_other_socket_packets(
         self,
         client: AsyncUDPNetworkClient[str, str],
         udp_socket_factory: Callable[[], Socket],
     ) -> None:
         other_client = udp_socket_factory()
         other_client.sendto(b"ABCDEF", client.get_local_address())
@@ -168,40 +165,37 @@
             await asyncio.wait_for(client.recv_packet(), timeout=0.1)
 
     async def test____recv_packet____closed_client(self, client: AsyncUDPNetworkClient[str, str]) -> None:
         await client.aclose()
         with pytest.raises(ClientClosedError):
             await client.recv_packet()
 
-    @use_asyncio_transport_xfail_uvloop
     async def test____recv_packet____invalid_data(
         self, client: AsyncUDPNetworkClient[str, str], server: DatagramEndpoint
     ) -> None:
         await server.sendto("\u00E9".encode("latin-1"), client.get_local_address())
         with pytest.raises(DatagramProtocolParseError):
             async with asyncio.timeout(3):
                 await client.recv_packet()
 
     @pytest.mark.parametrize("one_shot_serializer", [pytest.param("invalid", id="serializer_crash")], indirect=True)
-    @use_asyncio_transport_xfail_uvloop
     async def test____recv_packet____protocol_crashed(
         self,
         client: AsyncUDPNetworkClient[str, str],
         server: DatagramEndpoint,
     ) -> None:
         await server.sendto(b"ABCDEF", client.get_local_address())
         try:
             await client.recv_packet()
         except NotImplementedError:
             raise
         except Exception:
             with pytest.raises(RuntimeError, match=r"^protocol\.build_packet_from_datagram\(\) crashed$"):
                 raise
 
-    @use_asyncio_transport_xfail_uvloop
     async def test____iter_received_packets____yields_available_packets_until_close(
         self,
         client: AsyncUDPNetworkClient[str, str],
         server: DatagramEndpoint,
     ) -> None:
         for p in [b"A", b"B", b"C", b"D", b"E", b"F"]:
             await server.sendto(p, client.get_local_address())
@@ -211,15 +205,14 @@
         try:
             # NOTE: Comparison using set because equality check does not verify order
             assert {p async for p in client.iter_received_packets(timeout=None)} == {"A", "B", "C", "D", "E", "F"}
         finally:
             close_task.cancel()
             await asyncio.wait({close_task})
 
-    @use_asyncio_transport_xfail_uvloop
     async def test____iter_received_packets____yields_available_packets_within_given_timeout(
         self,
         client: AsyncUDPNetworkClient[str, str],
         server: DatagramEndpoint,
     ) -> None:
         async def send_coro() -> None:
             await server.sendto(b"A", client.get_local_address())
@@ -255,15 +248,14 @@
             assert isinstance(address, IPv4SocketAddress)
         else:
             assert isinstance(address, IPv6SocketAddress)
         assert address == client.socket.getpeername()
 
 
 @pytest.mark.asyncio
-@pytest.mark.usefixtures("use_asyncio_transport")
 class TestAsyncUDPNetworkClientConnection:
     class EchoProtocol(asyncio.DatagramProtocol):
         transport: asyncio.DatagramTransport | None = None
 
         def __init__(self, connection_lost_future: asyncio.Future[None]) -> None:
             super().__init__()
             self.connection_lost_future: asyncio.Future[None] = connection_lost_future
@@ -381,15 +373,14 @@
             with pytest.raises(OSError):
                 _ = client.get_remote_address()
 
             await client.wait_connected()
 
             assert client.get_remote_address()[:2] == remote_address
 
-    @use_asyncio_transport_xfail_uvloop
     async def test____send_packet____recv_packet____implicit_connection(
         self,
         remote_address: tuple[str, int],
         datagram_protocol: DatagramProtocol[str, str],
     ) -> None:
         async with contextlib.aclosing(AsyncUDPNetworkClient(remote_address, datagram_protocol)) as client:
             assert not client.is_connected()
```

### Comparing `easynetwork-1.0.0rc7/tests/functional_test/test_communication/test_async/test_server/base.py` & `easynetwork-1.0.0rc8/tests/functional_test/test_communication/test_async/test_server/base.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,47 +1,83 @@
 from __future__ import annotations
 
 import asyncio
+import collections
 import contextlib
+import itertools
+import logging
 from collections.abc import AsyncIterator
 from typing import NamedTuple
 
-from easynetwork.api_async.server.abc import AbstractAsyncNetworkServer
 from easynetwork.exceptions import ServerAlreadyRunning, ServerClosedError
+from easynetwork.servers.abc import AbstractAsyncNetworkServer
 
 import pytest
 import pytest_asyncio
 
 
 class _ServerBootstrapInfo(NamedTuple):
     task: asyncio.Task[None]
     is_up_event: asyncio.Event
 
 
 @pytest.mark.asyncio
+@pytest.mark.usefixtures("enable_eager_tasks")
 class BaseTestAsyncServer:
+    @pytest.fixture
+    @staticmethod
+    def logger_crash_threshold_level() -> dict[str, int]:
+        return {}
+
+    @pytest.fixture
+    @staticmethod
+    def logger_crash_maximum_nb_lines() -> dict[str, int]:
+        return {}
+
     @pytest.fixture(autouse=True)
     @staticmethod
     def disable_asyncio_logs(caplog: pytest.LogCaptureFixture) -> None:
         caplog.set_level("ERROR", "asyncio")
 
     @pytest_asyncio.fixture  # DO NOT SET autouse=True
     @staticmethod
-    async def _bootstrap_server(server: AbstractAsyncNetworkServer) -> AsyncIterator[_ServerBootstrapInfo]:
+    async def _bootstrap_server(
+        server: AbstractAsyncNetworkServer,
+        caplog: pytest.LogCaptureFixture,
+        logger_crash_threshold_level: dict[str, int],
+        logger_crash_maximum_nb_lines: dict[str, int],
+    ) -> AsyncIterator[_ServerBootstrapInfo]:
         async def serve_forever(server: AbstractAsyncNetworkServer, event: asyncio.Event) -> None:
             with contextlib.suppress(ServerClosedError):
                 await server.serve_forever(is_up_event=event)
 
         event = asyncio.Event()
         async with asyncio.TaskGroup() as tg:
             task = tg.create_task(serve_forever(server, event))
             await asyncio.sleep(0)
             yield _ServerBootstrapInfo(task, event)
             await server.shutdown()
 
+        log_line_counter: collections.Counter[str] = collections.Counter()
+        for record in itertools.chain(caplog.get_records("setup"), caplog.get_records("call")):
+            threshold_level = logger_crash_threshold_level.get(record.name, logging.ERROR)
+            if record.levelno < threshold_level:
+                continue
+            log_line_counter[record.name] += 1
+            maximum_nb_lines = max(logger_crash_maximum_nb_lines.get(record.name, 0), 0)
+            if log_line_counter[record.name] <= maximum_nb_lines:
+                continue
+            threshold_level_name = logging.getLevelName(threshold_level)
+            if not maximum_nb_lines:
+                pytest.fail(f"Logs with level equal to or greater than {threshold_level_name} caught in {record.name} logger")
+            else:
+                pytest.fail(
+                    f"More than {maximum_nb_lines} logs with level equal to or greater than {threshold_level_name} caught in {record.name} logger"
+                )
+
     @pytest_asyncio.fixture  # DO NOT SET autouse=True
     @staticmethod
     async def run_server(_bootstrap_server: _ServerBootstrapInfo) -> asyncio.Event:
         return _bootstrap_server.is_up_event
 
     @pytest_asyncio.fixture  # DO NOT SET autouse=True
     @staticmethod
@@ -70,38 +106,45 @@
         await server.server_close()
         with pytest.raises(ServerClosedError):
             await server.serve_forever()
 
     async def test____serve_forever____shutdown_during_setup(
         self,
         server: AbstractAsyncNetworkServer,
+        enable_eager_tasks: bool,
     ) -> None:
         event = asyncio.Event()
         async with asyncio.TaskGroup() as tg:
             _ = tg.create_task(server.serve_forever(is_up_event=event))
-            await asyncio.sleep(0)
+            if not enable_eager_tasks:
+                await asyncio.sleep(0)
             assert not event.is_set()
-            await server.shutdown()
+            async with asyncio.timeout(1):
+                await server.shutdown()
             assert not event.is_set()
 
     async def test____serve_forever____server_close_during_setup(
         self,
         server: AbstractAsyncNetworkServer,
+        enable_eager_tasks: bool,
     ) -> None:
         event = asyncio.Event()
-        server_task = None
-        with pytest.raises(ExceptionGroup):
-            async with asyncio.TaskGroup() as tg:
-                server_task = tg.create_task(server.serve_forever(is_up_event=event))
+
+        async def serve() -> None:
+            with pytest.raises(ServerClosedError):
+                await server.serve_forever(is_up_event=event)
+
+        async with asyncio.TaskGroup() as tg:
+            _ = tg.create_task(serve())
+            if not enable_eager_tasks:
                 await asyncio.sleep(0)
-                assert not event.is_set()
+            assert not event.is_set()
+            async with asyncio.timeout(1):
                 await server.server_close()
-                assert not event.is_set()
-        assert server_task is not None
-        assert isinstance(server_task.exception(), ServerClosedError)
+            assert not event.is_set()
 
     async def test____serve_forever____without_is_up_event(
         self,
         server: AbstractAsyncNetworkServer,
     ) -> None:
         async with asyncio.TaskGroup() as tg:
             _ = tg.create_task(server.serve_forever())
```

### Comparing `easynetwork-1.0.0rc7/tests/functional_test/test_communication/test_async/test_server/test_tcp.py` & `easynetwork-1.0.0rc8/tests/functional_test/test_communication/test_async/test_server/test_tcp.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,36 +1,34 @@
 from __future__ import annotations
 
 import asyncio
 import collections
 import contextlib
 import logging
 import ssl
-import weakref
 from collections.abc import AsyncGenerator, AsyncIterator, Awaitable, Callable, Sequence
 from socket import IPPROTO_TCP, TCP_NODELAY
-from typing import Any
+from typing import Any, Literal
 from weakref import WeakValueDictionary
 
-from easynetwork.api_async.server.handler import AsyncStreamClient, AsyncStreamRequestHandler, INETClientAttribute
-from easynetwork.api_async.server.tcp import AsyncTCPNetworkServer
 from easynetwork.exceptions import (
     BaseProtocolParseError,
     ClientClosedError,
     IncrementalDeserializeError,
     StreamProtocolParseError,
 )
 from easynetwork.lowlevel.api_async.backend.abc import AsyncBackend
 from easynetwork.lowlevel.api_async.backend.factory import current_async_backend
 from easynetwork.lowlevel.socket import SocketAddress, enable_socket_linger
 from easynetwork.lowlevel.std_asyncio._asyncio_utils import create_connection
 from easynetwork.lowlevel.std_asyncio.backend import AsyncIOBackend
 from easynetwork.lowlevel.std_asyncio.stream.listener import ListenerSocketAdapter
-from easynetwork.lowlevel.std_asyncio.stream.socket import AsyncioTransportStreamSocketAdapter, RawStreamSocketAdapter
 from easynetwork.protocol import StreamProtocol
+from easynetwork.servers.async_tcp import AsyncTCPNetworkServer
+from easynetwork.servers.handlers import AsyncStreamClient, AsyncStreamRequestHandler, INETClientAttribute
 
 import pytest
 import pytest_asyncio
 
 from .....tools import temporary_backend
 from .base import BaseTestAsyncServer
 
@@ -39,53 +37,44 @@
     async def create_tcp_listeners(
         self,
         host: str | Sequence[str] | None,
         port: int,
         backlog: int,
         *,
         reuse_port: bool = False,
-    ) -> Sequence[ListenerSocketAdapter[AsyncioTransportStreamSocketAdapter | RawStreamSocketAdapter]]:
-        return []
-
-    async def create_ssl_over_tcp_listeners(
-        self,
-        host: str | Sequence[str] | None,
-        port: int,
-        backlog: int,
-        ssl_context: ssl.SSLContext,
-        ssl_handshake_timeout: float,
-        ssl_shutdown_timeout: float,
-        *,
-        reuse_port: bool = False,
-    ) -> Sequence[ListenerSocketAdapter[AsyncioTransportStreamSocketAdapter]]:
+    ) -> Sequence[ListenerSocketAdapter[Any]]:
         return []
 
 
 def client_address(client: AsyncStreamClient[Any]) -> SocketAddress:
     return client.extra(INETClientAttribute.remote_address)
 
 
 class RandomError(Exception):
     pass
 
 
+LOGGER = logging.getLogger(__name__)
+
+
 class MyAsyncTCPRequestHandler(AsyncStreamRequestHandler[str, str]):
     connected_clients: WeakValueDictionary[SocketAddress, AsyncStreamClient[str]]
     request_received: collections.defaultdict[tuple[Any, ...], list[str]]
     request_count: collections.Counter[tuple[Any, ...]]
     bad_request_received: collections.defaultdict[tuple[Any, ...], list[BaseProtocolParseError]]
     milk_handshake: bool = True
     close_all_clients_on_connection: bool = False
     close_client_after_n_request: int = -1
     server: AsyncTCPNetworkServer[str, str]
     fail_on_disconnection: bool = False
 
     async def service_init(self, exit_stack: contextlib.AsyncExitStack, server: AsyncTCPNetworkServer[str, str]) -> None:
         await super().service_init(exit_stack, server)
         self.server = server
+        assert isinstance(self.server, AsyncTCPNetworkServer)
         self.connected_clients = WeakValueDictionary()
         self.request_received = collections.defaultdict(list)
         self.request_count = collections.Counter()
         self.bad_request_received = collections.defaultdict(list)
         exit_stack.push_async_callback(self.service_quit)
 
     async def service_quit(self) -> None:
@@ -121,21 +110,29 @@
             async with self.handle_bad_requests(client):
                 request = yield
                 break
         self.request_count[client_address(client)] += 1
         match request:
             case "__error__":
                 raise RandomError("Sorry man!")
+            case "__error_excgrp__":
+                raise ExceptionGroup("RandomError", [RandomError("Sorry man!")])
             case "__close__":
                 await client.aclose()
                 with pytest.raises(ClientClosedError):
                     await client.send_packet("something never sent")
             case "__closed_client_error__":
                 await client.aclose()
                 await client.send_packet("something never sent")
+            case "__closed_client_error_excgrp__":
+                await client.aclose()
+                try:
+                    await client.send_packet("something never sent")
+                except BaseException as exc:
+                    raise BaseExceptionGroup("ClosedClientError", [exc]) from None
             case "__connection_error__":
                 await client.aclose()  # Close before for graceful close
                 raise ConnectionResetError("Because why not?")
             case "__os_error__":
                 raise OSError("Server issue.")
             case "__stop_listening__":
                 self.server.stop_listening()
@@ -151,31 +148,54 @@
                 self.request_received[client_address(client)].append(request)
                 try:
                     await client.send_packet(request.upper())
                 except Exception as exc:
                     msg = f"{exc.__class__.__name__}: {exc}"
                     if exc.__cause__:
                         msg = f"{msg} (caused by {exc.__cause__.__class__.__name__}: {exc.__cause__})"
-                    self.server.logger.error(msg, exc_info=exc)
+                    LOGGER.error(msg, exc_info=exc)
                     await client.aclose()
 
     @contextlib.asynccontextmanager
     async def handle_bad_requests(self, client: AsyncStreamClient[str]) -> AsyncIterator[None]:
         try:
             yield
         except StreamProtocolParseError as exc:
             self.bad_request_received[client_address(client)].append(exc)
             await client.send_packet("wrong encoding man.")
 
 
-class TimeoutRequestHandler(AsyncStreamRequestHandler[str, str]):
+class TimeoutYieldedRequestHandler(AsyncStreamRequestHandler[str, str]):
     request_timeout: float = 1.0
     timeout_on_second_yield: bool = False
 
-    async def service_init(self, exit_stack: contextlib.AsyncExitStack, server: AsyncTCPNetworkServer[str, str]) -> None:
+    async def service_init(self, exit_stack: contextlib.AsyncExitStack, server: Any) -> None:
+        pass
+
+    async def on_connection(self, client: AsyncStreamClient[str]) -> None:
+        await client.send_packet("milk")
+
+    async def handle(self, client: AsyncStreamClient[str]) -> AsyncGenerator[float | None, str]:
+        if self.timeout_on_second_yield:
+            request = yield None
+            await client.send_packet(request)
+        try:
+            with pytest.raises(TimeoutError):
+                yield self.request_timeout
+            await client.send_packet("successfully timed out")
+        finally:
+            self.request_timeout = 1.0  # Force reset to 1 second in order not to overload the server
+
+
+class TimeoutContextRequestHandler(AsyncStreamRequestHandler[str, str]):
+    backend: AsyncBackend
+    request_timeout: float = 1.0
+    timeout_on_second_yield: bool = False
+
+    async def service_init(self, exit_stack: contextlib.AsyncExitStack, server: Any) -> None:
         self.backend = current_async_backend()
 
     async def on_connection(self, client: AsyncStreamClient[str]) -> None:
         await client.send_packet("milk")
 
     async def handle(self, client: AsyncStreamClient[str]) -> AsyncGenerator[None, str]:
         if self.timeout_on_second_yield:
@@ -196,67 +216,93 @@
 
     async def handle(self, client: AsyncStreamClient[str]) -> AsyncGenerator[None, str]:
         yield
         raise asyncio.CancelledError()
 
 
 class InitialHandshakeRequestHandler(AsyncStreamRequestHandler[str, str]):
-    backend: AsyncBackend
     bypass_handshake: bool = False
+    handshake_2fa: bool = False
 
-    async def service_init(self, exit_stack: contextlib.AsyncExitStack, server: AsyncTCPNetworkServer[str, str]) -> None:
-        self.backend = current_async_backend()
+    async def service_init(self, exit_stack: contextlib.AsyncExitStack, server: Any) -> None:
+        pass
 
-    async def on_connection(self, client: AsyncStreamClient[str]) -> AsyncGenerator[None, str]:
+    async def on_connection(self, client: AsyncStreamClient[str]) -> AsyncGenerator[float | None, str]:
         await client.send_packet("milk")
         if self.bypass_handshake:
             return
         try:
-            with self.backend.timeout(1):
-                password = yield
+            password = yield 1.0
+
+            if password != "chocolate":
+                await client.send_packet("wrong password")
+                await client.aclose()
+                return
+
+            if self.handshake_2fa:
+                await client.send_packet("2FA code needed")
+                code = yield 1.0
+
+                if code != "42":
+                    await client.send_packet("wrong code")
+                    await client.aclose()
+                    return
+
         except TimeoutError:
             await client.send_packet("timeout error")
             await client.aclose()
             return
-        if password != "chocolate":
-            await client.send_packet("wrong password")
-            await client.aclose()
-            return
+
         await client.send_packet("you can enter")
 
     async def handle(self, client: AsyncStreamClient[str]) -> AsyncGenerator[None, str]:
         request = yield
         await client.send_packet(request)
 
 
 class RequestRefusedHandler(AsyncStreamRequestHandler[str, str]):
     refuse_after: int = 2**64
 
-    async def service_init(self, exit_stack: contextlib.AsyncExitStack, server: AsyncTCPNetworkServer[str, str]) -> None:
+    async def service_init(self, exit_stack: contextlib.AsyncExitStack, server: Any) -> None:
         self.request_count: collections.Counter[AsyncStreamClient[str]] = collections.Counter()
 
     async def on_connection(self, client: AsyncStreamClient[str]) -> None:
         await client.send_packet("milk")
 
     async def on_disconnection(self, client: AsyncStreamClient[str]) -> None:
         self.request_count.pop(client, None)
 
     async def handle(self, client: AsyncStreamClient[str]) -> AsyncGenerator[None, str]:
         if self.request_count[client] >= self.refuse_after:
+            await asyncio.sleep(0.2)
             return
         request = yield
         self.request_count[client] += 1
         await client.send_packet(request)
 
 
 class ErrorInRequestHandler(AsyncStreamRequestHandler[str, str]):
     mute_thrown_exception: bool = False
+    read_on_connection: bool = False
 
-    async def on_connection(self, client: AsyncStreamClient[str]) -> None:
+    async def on_connection(self, client: AsyncStreamClient[str]) -> AsyncGenerator[None, str]:
         await client.send_packet("milk")
+        if not self.read_on_connection:
+            return
+        try:
+            request = yield
+        except Exception as exc:
+            msg = f"{exc.__class__.__name__}: {exc}"
+            if exc.__cause__:
+                msg = f"{msg} (caused by {exc.__cause__.__class__.__name__}: {exc.__cause__})"
+            await client.send_packet(msg)
+            if not self.mute_thrown_exception:
+                raise
+        else:
+            await client.send_packet(request)
 
     async def handle(self, client: AsyncStreamClient[str]) -> AsyncGenerator[None, str]:
         try:
             request = yield
         except Exception as exc:
             msg = f"{exc.__class__.__name__}: {exc}"
             if exc.__cause__:
@@ -269,76 +315,84 @@
 
 
 class ErrorBeforeYieldHandler(AsyncStreamRequestHandler[str, str]):
     async def on_connection(self, client: AsyncStreamClient[str]) -> None:
         await client.send_packet("milk")
 
     async def handle(self, client: AsyncStreamClient[str]) -> AsyncGenerator[None, str]:
+        await asyncio.sleep(0.2)
         raise RandomError("An error occurred")
         request = yield  # type: ignore[unreachable]
         await client.send_packet(request)
 
 
 class MyAsyncTCPServer(AsyncTCPNetworkServer[str, str]):
     __slots__ = ()
 
 
-@pytest.mark.asyncio
+@pytest.mark.flaky(retries=3, delay=1)
 class TestAsyncTCPNetworkServer(BaseTestAsyncServer):
     @pytest.fixture(params=["NO_SSL", "USE_SSL"])
     @staticmethod
     def use_ssl(request: Any) -> bool:
         match request.param:
             case "NO_SSL":
                 return False
             case "USE_SSL":
                 return True
             case _:
                 pytest.fail(f"Invalid parameter: {request.param}")
 
     @pytest.fixture
     @staticmethod
-    def use_asyncio_transport(use_asyncio_transport: bool, use_ssl: bool) -> bool:
-        if use_ssl and not use_asyncio_transport:
-            pytest.skip("SSL/TLS not supported with transport=False")
-        return use_asyncio_transport
-
-    @pytest.fixture
-    @staticmethod
     def request_handler(request: Any) -> AsyncStreamRequestHandler[str, str]:
         request_handler_cls: type[AsyncStreamRequestHandler[str, str]] = getattr(request, "param", MyAsyncTCPRequestHandler)
         return request_handler_cls()
 
     @pytest.fixture
     @staticmethod
     def ssl_handshake_timeout(request: Any) -> float | None:
         return getattr(request, "param", None)
 
+    @pytest.fixture
+    @staticmethod
+    def ssl_standard_compatible(request: Any) -> bool | None:
+        return getattr(request, "param", None)
+
     @pytest_asyncio.fixture
     @staticmethod
     async def server(
         request_handler: MyAsyncTCPRequestHandler,
         localhost_ip: str,
         stream_protocol: StreamProtocol[str, str],
         use_ssl: bool,
         server_ssl_context: ssl.SSLContext,
         ssl_handshake_timeout: float | None,
-        use_asyncio_transport: bool,  # Only here for dependency
+        ssl_standard_compatible: bool | None,
+        caplog: pytest.LogCaptureFixture,
+        logger_crash_threshold_level: dict[str, int],
     ) -> AsyncIterator[MyAsyncTCPServer]:
+        # Remove this option for non-regression
+        server_ssl_context.options &= ~ssl.OP_IGNORE_UNEXPECTED_EOF
+
         async with MyAsyncTCPServer(
             localhost_ip,
             0,
             stream_protocol,
             request_handler,
             backlog=1,
             ssl=server_ssl_context if use_ssl else None,
             ssl_handshake_timeout=ssl_handshake_timeout,
+            ssl_standard_compatible=ssl_standard_compatible,
+            logger=LOGGER,
         ) as server:
-            assert not server.sockets
+            assert not server.get_sockets()
             assert not server.get_addresses()
+            caplog.set_level(logging.INFO, LOGGER.name)
+            logger_crash_threshold_level[LOGGER.name] = logging.WARNING
             yield server
 
     @pytest_asyncio.fixture
     @staticmethod
     async def server_address(run_server: asyncio.Event, server: MyAsyncTCPServer) -> tuple[str, int]:
         async with asyncio.timeout(1):
             await run_server.wait()
@@ -387,49 +441,46 @@
             reader, writer = await client_factory_no_handshake()
             assert await reader.readline() == b"milk\n"
             return reader, writer
 
         return factory
 
     @staticmethod
-    async def _wait_client_disconnected(writer: asyncio.StreamWriter, request_handler: MyAsyncTCPRequestHandler) -> None:
+    async def _wait_client_disconnected(writer: asyncio.StreamWriter) -> None:
         writer.close()
         await writer.wait_closed()
-        async with asyncio.timeout(1):
-            while request_handler.connected_clients:
-                await asyncio.sleep(0.1)
+        await asyncio.sleep(0.1)
 
     @pytest.mark.parametrize("host", [None, ""], ids=repr)
     @pytest.mark.parametrize("log_client_connection", [True, False], ids=lambda p: f"log_client_connection=={p}")
-    @pytest.mark.parametrize("use_ssl", ["NO_SSL"], indirect=True)
-    @pytest.mark.usefixtures("use_asyncio_transport")
     async def test____dunder_init____bind_to_all_available_interfaces(
         self,
         host: str | None,
         log_client_connection: bool,
         request_handler: MyAsyncTCPRequestHandler,
         stream_protocol: StreamProtocol[str, str],
         caplog: pytest.LogCaptureFixture,
     ) -> None:
         async with MyAsyncTCPServer(
             host,
             0,
             stream_protocol,
             request_handler,
             log_client_connection=log_client_connection,
+            logger=LOGGER,
         ) as s:
-            caplog.set_level(logging.DEBUG, s.logger.name)
+            caplog.set_level(logging.DEBUG, LOGGER.name)
             is_up_event = asyncio.Event()
             _ = asyncio.create_task(s.serve_forever(is_up_event=is_up_event))
             async with asyncio.timeout(1):
                 await is_up_event.wait()
 
             try:
                 assert len(s.get_addresses()) > 0
-                assert len(s.sockets) > 0
+                assert len(s.get_sockets()) > 0
 
                 port = s.get_addresses()[0].port
 
                 reader, writer = await asyncio.open_connection("localhost", port)
 
                 assert await reader.readline() == b"milk\n"
 
@@ -448,15 +499,15 @@
 
             accept_record = next((record for record in caplog.records if record.message == expected_accept_message), None)
             disconnect_record = next((record for record in caplog.records if record.message == expected_disconnect_message), None)
 
             assert accept_record is not None and disconnect_record is not None
             assert accept_record.levelno == expected_log_level and disconnect_record.levelno == expected_log_level
 
-    @pytest.mark.parametrize("ssl_parameter", ["ssl_handshake_timeout", "ssl_shutdown_timeout"])
+    @pytest.mark.parametrize("ssl_parameter", ["ssl_handshake_timeout", "ssl_shutdown_timeout", "ssl_standard_compatible"])
     async def test____dunder_init____useless_parameter_if_no_ssl_context(
         self,
         ssl_parameter: str,
         request_handler: MyAsyncTCPRequestHandler,
         stream_protocol: StreamProtocol[str, str],
     ) -> None:
         kwargs: dict[str, Any] = {ssl_parameter: 30}
@@ -485,25 +536,24 @@
         stream_protocol: StreamProtocol[str, str],
     ) -> None:
         with temporary_backend(NoListenerErrorBackend()):
             async with MyAsyncTCPServer(None, 0, stream_protocol, request_handler) as s:
                 with pytest.raises(OSError, match=r"^empty listeners list$"):
                     await s.serve_forever()
 
-                assert not s.sockets
+                assert not s.get_sockets()
 
     @pytest.mark.usefixtures("run_server_and_wait")
     async def test____serve_forever____server_assignment(
         self,
         server: MyAsyncTCPServer,
         request_handler: MyAsyncTCPRequestHandler,
     ) -> None:
         assert request_handler.server == server
         assert isinstance(request_handler.server, AsyncTCPNetworkServer)
-        assert isinstance(request_handler.server, weakref.ProxyType)
 
     async def test____serve_forever____accept_client(
         self,
         client_factory: Callable[[], Awaitable[tuple[asyncio.StreamReader, asyncio.StreamWriter]]],
         request_handler: MyAsyncTCPRequestHandler,
     ) -> None:
         reader, writer = await client_factory()
@@ -512,29 +562,32 @@
         assert client_address in request_handler.connected_clients
 
         writer.write(b"hello, world.\n")
         assert await reader.readline() == b"HELLO, WORLD.\n"
 
         assert request_handler.request_received[client_address] == ["hello, world."]
 
-        await self._wait_client_disconnected(writer, request_handler)
+        await self._wait_client_disconnected(writer)
+        assert client_address not in request_handler.connected_clients
 
     # skip Windows for this test, the ECONNRESET will happen on socket.send() or socket.recv()
     @pytest.mark.xfail('sys.platform == "win32"', reason="socket.getpeername() works by some magic")
     @pytest.mark.parametrize("socket_family", ["AF_INET"], indirect=True)
     @pytest.mark.parametrize("use_ssl", ["NO_SSL"], indirect=True)
     async def test____serve_forever____accept_client____client_sent_RST_packet_right_after_accept(
         self,
         server_address: tuple[str, int],
         caplog: pytest.LogCaptureFixture,
-        server: MyAsyncTCPServer,
+        logger_crash_maximum_nb_lines: dict[str, int],
     ) -> None:
         from socket import socket as SocketType
 
-        caplog.set_level(logging.WARNING, server.logger.name)
+        caplog.set_level(logging.WARNING, LOGGER.name)
+        logger_crash_maximum_nb_lines[LOGGER.name] = 1
+
         socket = SocketType()
 
         # See this thread about SO_LINGER option with null timeout: https://stackoverflow.com/q/3757289
         enable_socket_linger(socket, timeout=0)
 
         socket.connect(server_address)
         socket.close()  # Sends RST packet instead of FIN because of null timeout linger
@@ -559,32 +612,34 @@
 
         tcp_nodelay_state: int = connected_client.extra(INETClientAttribute.socket).getsockopt(IPPROTO_TCP, TCP_NODELAY)
 
         # Do not test with '== 1', on MacOS it will return 4
         # (c.f. https://stackoverflow.com/a/31835137)
         assert tcp_nodelay_state != 0
 
-    async def test____serve_forever____close_during_loop____continue_until_all_clients_are_gone(
+    @pytest.mark.parametrize("action", ["shutdown", "server_close"])
+    async def test____serve_forever____close_during_loop____kill_client_tasks(
         self,
+        action: Literal["shutdown", "server_close"],
         server: MyAsyncTCPServer,
         client_factory: Callable[[], Awaitable[tuple[asyncio.StreamReader, asyncio.StreamWriter]]],
     ) -> None:
-        reader, writer = await client_factory()
+        reader, _ = await client_factory()
 
-        await server.server_close()
+        match action:
+            case "shutdown":
+                await server.shutdown()
+            case "server_close":
+                await server.server_close()
+            case _:
+                pytest.fail("Invalid argument")
         await asyncio.sleep(0.3)
 
-        writer.write(b"hello\n")
-        assert await reader.readline() == b"HELLO\n"
-        writer.write(b"world!\n")
-        assert await reader.readline() == b"WORLD!\n"
-
-        writer.close()
-        await writer.wait_closed()
-        await asyncio.sleep(0.3)
+        with contextlib.suppress(ConnectionError):
+            assert await reader.read() == b""
 
     async def test____serve_forever____partial_request(
         self,
         client_factory: Callable[[], Awaitable[tuple[asyncio.StreamReader, asyncio.StreamWriter]]],
         request_handler: MyAsyncTCPRequestHandler,
     ) -> None:
         reader, writer = await client_factory()
@@ -603,15 +658,14 @@
         client_factory: Callable[[], Awaitable[tuple[asyncio.StreamReader, asyncio.StreamWriter]]],
         request_handler: MyAsyncTCPRequestHandler,
     ) -> None:
         reader, writer = await client_factory()
         client_address: tuple[Any, ...] = writer.get_extra_info("sockname")
 
         writer.write(b"hello\nworld\n")
-        await asyncio.sleep(0.1)
 
         assert await reader.readline() == b"HELLO\n"
         assert await reader.readline() == b"WORLD\n"
         assert request_handler.request_received[client_address] == ["hello", "world"]
 
     async def test____serve_forever____several_requests_at_same_time____close_between(
         self,
@@ -619,237 +673,268 @@
         request_handler: MyAsyncTCPRequestHandler,
     ) -> None:
         reader, writer = await client_factory()
         client_address: tuple[Any, ...] = writer.get_extra_info("sockname")
         request_handler.close_client_after_n_request = 1
 
         writer.write(b"hello\nworld\n")
-        await asyncio.sleep(0.1)
 
         assert await reader.readline() == b"HELLO\n"
         assert await reader.read() == b""
         assert request_handler.request_received[client_address] == ["hello"]
 
     async def test____serve_forever____save_request_handler_context(
         self,
         client_factory: Callable[[], Awaitable[tuple[asyncio.StreamReader, asyncio.StreamWriter]]],
         request_handler: MyAsyncTCPRequestHandler,
     ) -> None:
         reader, writer = await client_factory()
         client_address: tuple[Any, ...] = writer.get_extra_info("sockname")
 
         writer.write(b"__wait__\nhello, world!\n")
-        await asyncio.sleep(0.1)
 
         assert await reader.readline() == b"After wait: hello, world!\n"
         assert request_handler.request_received[client_address] == ["hello, world!"]
 
     async def test____serve_forever____bad_request(
         self,
         client_factory: Callable[[], Awaitable[tuple[asyncio.StreamReader, asyncio.StreamWriter]]],
         request_handler: MyAsyncTCPRequestHandler,
     ) -> None:
         reader, writer = await client_factory()
         client_address: tuple[Any, ...] = writer.get_extra_info("sockname")
 
         writer.write("\u00E9\n".encode("latin-1"))  # StringSerializer does not accept unicode
-        await asyncio.sleep(0.1)
 
         assert await reader.readline() == b"wrong encoding man.\n"
         assert request_handler.request_received[client_address] == []
         assert isinstance(request_handler.bad_request_received[client_address][0], StreamProtocolParseError)
         assert isinstance(request_handler.bad_request_received[client_address][0].error, IncrementalDeserializeError)
 
     @pytest.mark.parametrize("socket_family", ["AF_INET"], indirect=True)
     @pytest.mark.parametrize("use_ssl", ["NO_SSL"], indirect=True)
+    @pytest.mark.parametrize(
+        "request_handler",
+        [
+            pytest.param(MyAsyncTCPRequestHandler, id="during_handle"),
+            pytest.param(InitialHandshakeRequestHandler, id="during_on_connection_hook"),
+        ],
+        indirect=True,
+    )
     async def test____serve_forever____connection_reset_error(
         self,
         client_factory: Callable[[], Awaitable[tuple[asyncio.StreamReader, asyncio.StreamWriter]]],
         caplog: pytest.LogCaptureFixture,
-        server: MyAsyncTCPServer,
-        request_handler: MyAsyncTCPRequestHandler,
     ) -> None:
-        caplog.set_level(logging.WARNING, server.logger.name)
+        caplog.set_level(logging.WARNING, LOGGER.name)
         _, writer = await client_factory()
 
         enable_socket_linger(writer.get_extra_info("socket"), timeout=0)
 
-        await self._wait_client_disconnected(writer, request_handler)
+        await self._wait_client_disconnected(writer)
 
         # ECONNRESET not logged
         assert len(caplog.records) == 0
 
-    @pytest.mark.parametrize("mute_thrown_exception", [False, True])
+    @pytest.mark.parametrize("mute_thrown_exception", [False, True], ids=lambda p: f"mute_thrown_exception=={p}")
+    @pytest.mark.parametrize("read_on_connection", [False, True], ids=lambda p: f"read_on_connection=={p}")
     @pytest.mark.parametrize("request_handler", [ErrorInRequestHandler], indirect=True)
     @pytest.mark.parametrize(
         "incremental_serializer",
         [
             pytest.param("invalid", id="serializer_crash"),
             pytest.param("invalid_buffered", id="buffered_serializer_crash"),
         ],
         indirect=True,
     )
     async def test____serve_forever____internal_error(
         self,
         mute_thrown_exception: bool,
+        read_on_connection: bool,
         request_handler: ErrorInRequestHandler,
         client_factory: Callable[[], Awaitable[tuple[asyncio.StreamReader, asyncio.StreamWriter]]],
         caplog: pytest.LogCaptureFixture,
-        server: MyAsyncTCPServer,
+        logger_crash_maximum_nb_lines: dict[str, int],
     ) -> None:
-        caplog.set_level(logging.ERROR, server.logger.name)
+        caplog.set_level(logging.ERROR, LOGGER.name)
+        if not mute_thrown_exception:
+            logger_crash_maximum_nb_lines[LOGGER.name] = 3
         request_handler.mute_thrown_exception = mute_thrown_exception
+        request_handler.read_on_connection = read_on_connection
         reader, writer = await client_factory()
 
         expected_messages = {
             b"RuntimeError: protocol.build_packet_from_buffer() crashed (caused by SystemError: CRASH)\n",
             b"RuntimeError: protocol.build_packet_from_chunks() crashed (caused by SystemError: CRASH)\n",
         }
 
         writer.write(b"something\n")
-        await asyncio.sleep(0.1)
 
         if mute_thrown_exception:
             assert await reader.readline() in expected_messages
             writer.write(b"something\n")
-            await asyncio.sleep(0.1)
             assert await reader.readline() in expected_messages
+            await asyncio.sleep(0.1)
             assert len(caplog.records) == 0  # After two attempts
         else:
-            with pytest.raises(ConnectionResetError):
+            with contextlib.suppress(ConnectionError):
                 assert await reader.readline() in expected_messages
                 assert await reader.read() == b""
-                raise ConnectionResetError
+            await asyncio.sleep(0.1)
             assert len(caplog.records) == 3
+            assert caplog.records[1].exc_info is not None
+            assert type(caplog.records[1].exc_info[1]) is RuntimeError
 
+    @pytest.mark.parametrize("excgrp", [False, True], ids=lambda p: f"exception_group_raised=={p}")
     async def test____serve_forever____unexpected_error_during_process(
         self,
+        excgrp: bool,
         client_factory: Callable[[], Awaitable[tuple[asyncio.StreamReader, asyncio.StreamWriter]]],
         caplog: pytest.LogCaptureFixture,
-        server: MyAsyncTCPServer,
+        logger_crash_maximum_nb_lines: dict[str, int],
     ) -> None:
-        caplog.set_level(logging.ERROR, server.logger.name)
+        caplog.set_level(logging.ERROR, LOGGER.name)
+        logger_crash_maximum_nb_lines[LOGGER.name] = 3
         reader, writer = await client_factory()
 
-        writer.write(b"__error__\n")
+        if excgrp:
+            writer.write(b"__error_excgrp__\n")
+        else:
+            writer.write(b"__error__\n")
+        with contextlib.suppress(ConnectionError):
+            assert await reader.read() == b""
         await asyncio.sleep(0.1)
 
-        with pytest.raises(ConnectionResetError):
-            assert await reader.read() == b""
-            raise ConnectionResetError
         assert len(caplog.records) == 3
+        assert caplog.records[1].exc_info is not None
+        if excgrp:
+            assert type(caplog.records[1].exc_info[1]) is ExceptionGroup
+            assert type(caplog.records[1].exc_info[1].exceptions[0]) is RandomError
+        else:
+            assert type(caplog.records[1].exc_info[1]) is RandomError
 
     @pytest.mark.parametrize("incremental_serializer", [pytest.param("bad_serialize", id="serializer_crash")], indirect=True)
     async def test____serve_forever____unexpected_error_during_response_serialization(
         self,
         client_factory_no_handshake: Callable[[], Awaitable[tuple[asyncio.StreamReader, asyncio.StreamWriter]]],
         caplog: pytest.LogCaptureFixture,
-        server: MyAsyncTCPServer,
+        logger_crash_maximum_nb_lines: dict[str, int],
         request_handler: MyAsyncTCPRequestHandler,
     ) -> None:
         request_handler.milk_handshake = False
-        caplog.set_level(logging.ERROR, server.logger.name)
+        caplog.set_level(logging.ERROR, LOGGER.name)
+        logger_crash_maximum_nb_lines[LOGGER.name] = 1
         reader, writer = await client_factory_no_handshake()
 
         while not request_handler.connected_clients:
             await asyncio.sleep(0.1)
 
         writer.write(b"request\n")
+        assert await reader.read() == b""
         await asyncio.sleep(0.1)
 
-        assert await reader.read() == b""
         assert len(caplog.records) == 1
         assert caplog.records[0].levelno == logging.ERROR
         assert caplog.records[0].message == "RuntimeError: protocol.generate_chunks() crashed (caused by SystemError: CRASH)"
 
     async def test____serve_forever____os_error(
         self,
+        caplog: pytest.LogCaptureFixture,
+        logger_crash_maximum_nb_lines: dict[str, int],
         client_factory: Callable[[], Awaitable[tuple[asyncio.StreamReader, asyncio.StreamWriter]]],
     ) -> None:
+        caplog.set_level(logging.ERROR, LOGGER.name)
+        logger_crash_maximum_nb_lines[LOGGER.name] = 3
         reader, writer = await client_factory()
 
         writer.write(b"__os_error__\n")
+        with contextlib.suppress(ConnectionError):
+            assert await reader.read() == b""
         await asyncio.sleep(0.1)
 
-        with pytest.raises(ConnectionResetError):
-            assert await reader.read() == b""
-            raise ConnectionResetError
+        assert len(caplog.records) == 3
+        assert caplog.records[1].exc_info is not None
+        assert type(caplog.records[1].exc_info[1]) is OSError
 
+    @pytest.mark.parametrize("excgrp", [False, True], ids=lambda p: f"exception_group_raised=={p}")
     async def test____serve_forever____use_of_a_closed_client_in_request_handler(
         self,
+        excgrp: bool,
         client_factory: Callable[[], Awaitable[tuple[asyncio.StreamReader, asyncio.StreamWriter]]],
         caplog: pytest.LogCaptureFixture,
-        server: MyAsyncTCPServer,
+        logger_crash_maximum_nb_lines: dict[str, int],
     ) -> None:
-        caplog.set_level(logging.WARNING, server.logger.name)
+        caplog.set_level(logging.WARNING, LOGGER.name)
+        logger_crash_maximum_nb_lines[LOGGER.name] = 1
         reader, writer = await client_factory()
         host, port = writer.get_extra_info("sockname")[:2]
 
-        writer.write(b"__closed_client_error__\n")
+        if excgrp:
+            writer.write(b"__closed_client_error_excgrp__\n")
+        else:
+            writer.write(b"__closed_client_error__\n")
+        assert await reader.read() == b""
         await asyncio.sleep(0.1)
 
-        assert await reader.read() == b""
         assert len(caplog.records) == 1
         assert caplog.records[0].levelno == logging.WARNING
         assert caplog.records[0].message == f"There have been attempts to do operation on closed client ({host!r}, {port})"
 
     async def test____serve_forever____connection_error_in_request_handler(
         self,
         client_factory: Callable[[], Awaitable[tuple[asyncio.StreamReader, asyncio.StreamWriter]]],
         caplog: pytest.LogCaptureFixture,
-        server: MyAsyncTCPServer,
     ) -> None:
-        caplog.set_level(logging.WARNING, server.logger.name)
+        caplog.set_level(logging.WARNING, LOGGER.name)
         reader, writer = await client_factory()
 
         writer.write(b"__connection_error__\n")
+        assert await reader.read() == b""
         await asyncio.sleep(0.1)
 
-        assert await reader.read() == b""
         assert len(caplog.records) == 0
 
     async def test____serve_forever____connection_error_in_disconnect_hook(
         self,
         client_factory: Callable[[], Awaitable[tuple[asyncio.StreamReader, asyncio.StreamWriter]]],
         request_handler: MyAsyncTCPRequestHandler,
         caplog: pytest.LogCaptureFixture,
-        server: MyAsyncTCPServer,
+        logger_crash_maximum_nb_lines: dict[str, int],
     ) -> None:
-        caplog.set_level(logging.WARNING, server.logger.name)
+        caplog.set_level(logging.WARNING, LOGGER.name)
+        logger_crash_maximum_nb_lines[LOGGER.name] = 1
         _, writer = await client_factory()
         request_handler.fail_on_disconnection = True
 
-        await self._wait_client_disconnected(writer, request_handler)
+        await self._wait_client_disconnected(writer)
 
         # ECONNRESET not logged
         assert len(caplog.records) == 1
         assert caplog.records[0].levelno == logging.WARNING
         assert caplog.records[0].message == "ConnectionError raised in request_handler.on_disconnection()"
 
     async def test____serve_forever____explicitly_closed_by_request_handler(
         self,
         client_factory: Callable[[], Awaitable[tuple[asyncio.StreamReader, asyncio.StreamWriter]]],
     ) -> None:
         reader, writer = await client_factory()
 
         writer.write(b"__close__\n")
-        await asyncio.sleep(0.1)
 
         assert await reader.read() == b""
 
     async def test____serve_forever____request_handler_ask_to_stop_accepting_new_connections(
         self,
         client_factory: Callable[[], Awaitable[tuple[asyncio.StreamReader, asyncio.StreamWriter]]],
         server_task: asyncio.Task[None],
         server: MyAsyncTCPServer,
     ) -> None:
         reader, writer = await client_factory()
 
         writer.write(b"__stop_listening__\n")
-        await asyncio.sleep(0.1)
 
         assert await reader.readline() == b"successfully stop listening\n"
 
         assert not server.is_serving()
         await asyncio.sleep(0.1)
 
         with pytest.raises(ExceptionGroup) as exc_info:
@@ -870,22 +955,22 @@
         request_handler: MyAsyncTCPRequestHandler,
     ) -> None:
         request_handler.close_all_clients_on_connection = True
         reader, _ = await client_factory()
 
         assert await reader.read() == b""
 
-    @pytest.mark.parametrize("request_handler", [TimeoutRequestHandler], indirect=True)
+    @pytest.mark.parametrize("request_handler", [TimeoutYieldedRequestHandler, TimeoutContextRequestHandler], indirect=True)
     @pytest.mark.parametrize("request_timeout", [0.0, 1.0], ids=lambda p: f"timeout=={p}")
     @pytest.mark.parametrize("timeout_on_second_yield", [False, True], ids=lambda p: f"timeout_on_second_yield=={p}")
     async def test____serve_forever____throw_cancelled_error(
         self,
         request_timeout: float,
         timeout_on_second_yield: bool,
-        request_handler: TimeoutRequestHandler,
+        request_handler: TimeoutYieldedRequestHandler | TimeoutContextRequestHandler,
         client_factory: Callable[[], Awaitable[tuple[asyncio.StreamReader, asyncio.StreamWriter]]],
     ) -> None:
         request_handler.request_timeout = request_timeout
         request_handler.timeout_on_second_yield = timeout_on_second_yield
         reader, writer = await client_factory()
 
         if timeout_on_second_yield:
@@ -898,91 +983,116 @@
     async def test____serve_forever____request_handler_is_cancelled(
         self,
         client_factory: Callable[[], Awaitable[tuple[asyncio.StreamReader, asyncio.StreamWriter]]],
     ) -> None:
         reader, writer = await client_factory()
 
         writer.write(b"something\n")
-        await asyncio.sleep(0.1)
-        with pytest.raises(ConnectionResetError):
+
+        with contextlib.suppress(ConnectionError):
             assert await reader.read() == b""
-            raise ConnectionResetError
 
     @pytest.mark.parametrize("request_handler", [ErrorBeforeYieldHandler], indirect=True)
     async def test____serve_forever____request_handler_crashed_before_yield(
         self,
         caplog: pytest.LogCaptureFixture,
+        logger_crash_maximum_nb_lines: dict[str, int],
         client_factory: Callable[[], Awaitable[tuple[asyncio.StreamReader, asyncio.StreamWriter]]],
-        server: MyAsyncTCPServer,
     ) -> None:
-        caplog.set_level(logging.ERROR, server.logger.name)
+        caplog.set_level(logging.ERROR, LOGGER.name)
+        logger_crash_maximum_nb_lines[LOGGER.name] = 3
 
-        with pytest.raises(ConnectionResetError):
+        with contextlib.suppress(ConnectionError):
             reader, _ = await client_factory()
             assert await reader.read() == b""
-            raise ConnectionResetError
         await asyncio.sleep(0.1)
         assert len(caplog.records) == 3
+        assert caplog.records[1].exc_info is not None
+        assert type(caplog.records[1].exc_info[1]) is RandomError
 
     @pytest.mark.parametrize("request_handler", [RequestRefusedHandler], indirect=True)
     @pytest.mark.parametrize("refuse_after", [0, 5], ids=lambda p: f"refuse_after=={p}")
     async def test____serve_forever____request_handler_did_not_yield(
         self,
         refuse_after: int,
         request_handler: RequestRefusedHandler,
         caplog: pytest.LogCaptureFixture,
         client_factory: Callable[[], Awaitable[tuple[asyncio.StreamReader, asyncio.StreamWriter]]],
-        server: MyAsyncTCPServer,
     ) -> None:
         request_handler.refuse_after = refuse_after
-        caplog.set_level(logging.ERROR, server.logger.name)
+        caplog.set_level(logging.ERROR, LOGGER.name)
 
-        with pytest.raises(ConnectionResetError):
+        with contextlib.suppress(ConnectionError):
             # If refuse after is equal to zero, client_factory() can raise ConnectionResetError
             reader, writer = await client_factory()
 
             for _ in range(refuse_after):
                 writer.write(b"something\n")
                 assert await reader.readline() == b"something\n"
 
             assert await reader.read() == b""
 
-            # Should not go here but just to be sure...
-            raise ConnectionResetError
+        await asyncio.sleep(0.1)
         assert len(caplog.records) == 0
 
     @pytest.mark.parametrize("request_handler", [InitialHandshakeRequestHandler], indirect=True)
+    @pytest.mark.parametrize("handshake_2fa", [True, False], ids=lambda p: f"handshake_2fa=={p}")
     async def test____serve_forever____request_handler_on_connection_is_async_gen(
         self,
         client_factory: Callable[[], Awaitable[tuple[asyncio.StreamReader, asyncio.StreamWriter]]],
+        handshake_2fa: bool,
+        request_handler: InitialHandshakeRequestHandler,
     ) -> None:
+        request_handler.handshake_2fa = handshake_2fa
         reader, writer = await client_factory()
 
         writer.write(b"chocolate\n")
+        if handshake_2fa:
+            assert await reader.readline() == b"2FA code needed\n"
+            writer.write(b"42\n")
+
         assert await reader.readline() == b"you can enter\n"
         writer.write(b"something\n")
         assert await reader.readline() == b"something\n"
 
     @pytest.mark.parametrize("request_handler", [InitialHandshakeRequestHandler], indirect=True)
+    @pytest.mark.parametrize("handshake_2fa", [True, False], ids=lambda p: f"handshake_2fa=={p}")
     async def test____serve_forever____request_handler_on_connection_is_async_gen____close_connection(
         self,
         client_factory: Callable[[], Awaitable[tuple[asyncio.StreamReader, asyncio.StreamWriter]]],
+        handshake_2fa: bool,
+        request_handler: InitialHandshakeRequestHandler,
     ) -> None:
+        request_handler.handshake_2fa = handshake_2fa
         reader, writer = await client_factory()
 
-        writer.write(b"something_else\n")
-        assert await reader.readline() == b"wrong password\n"
+        if handshake_2fa:
+            writer.write(b"chocolate\n")
+            assert await reader.readline() == b"2FA code needed\n"
+            writer.write(b"123\n")
+            assert await reader.readline() == b"wrong code\n"
+        else:
+            writer.write(b"something_else\n")
+            assert await reader.readline() == b"wrong password\n"
         assert await reader.read() == b""
 
     @pytest.mark.parametrize("request_handler", [InitialHandshakeRequestHandler], indirect=True)
+    @pytest.mark.parametrize("handshake_2fa", [True, False], ids=lambda p: f"handshake_2fa=={p}")
     async def test____serve_forever____request_handler_on_connection_is_async_gen____throw_cancel_error_within_generator(
         self,
         client_factory: Callable[[], Awaitable[tuple[asyncio.StreamReader, asyncio.StreamWriter]]],
+        handshake_2fa: bool,
+        request_handler: InitialHandshakeRequestHandler,
     ) -> None:
-        reader, _ = await client_factory()
+        request_handler.handshake_2fa = handshake_2fa
+        reader, writer = await client_factory()
+
+        if handshake_2fa:
+            writer.write(b"chocolate\n")
+            assert await reader.readline() == b"2FA code needed\n"
 
         assert await reader.readline() == b"timeout error\n"
 
     @pytest.mark.parametrize("request_handler", [InitialHandshakeRequestHandler], indirect=True)
     async def test____serve_forever____request_handler_on_connection_is_async_gen____exit_before_first_yield(
         self,
         request_handler: InitialHandshakeRequestHandler,
@@ -997,22 +1107,64 @@
     @pytest.mark.parametrize("use_ssl", ["USE_SSL"], indirect=True)
     @pytest.mark.parametrize("ssl_handshake_timeout", [pytest.param(1, id="timeout==1sec")], indirect=True)
     async def test____serve_forever____ssl_handshake_timeout_error(
         self,
         server_address: tuple[str, int],
         event_loop: asyncio.AbstractEventLoop,
         caplog: pytest.LogCaptureFixture,
-        server: MyAsyncTCPServer,
+        logger_crash_maximum_nb_lines: dict[str, int],
     ) -> None:
-        caplog.set_level(logging.ERROR, server.logger.name)
-        socket = await create_connection(*server_address, event_loop)
-        with pytest.raises(OSError):
+        caplog.set_level(logging.ERROR, LOGGER.name)
+        logger_crash_maximum_nb_lines[LOGGER.name] = 1
+        logger_crash_maximum_nb_lines["easynetwork.lowlevel.api_async.transports.tls"] = 1
+        with await create_connection(*server_address, event_loop) as socket, pytest.raises(OSError):
             # The SSL handshake expects the client to send the list of encryption algorithms.
             # But we won't, so the server will close the connection after 1 second
             # and raise a TimeoutError or ConnectionAbortedError.
             assert await event_loop.sock_recv(socket, 256 * 1024) == b""
             # If sock_recv() did not raise, manually trigger the error
             raise ConnectionAbortedError
 
         await asyncio.sleep(0.1)
         assert len(caplog.records) == 1
         assert caplog.records[0].message == "Error in client task (during TLS handshake)"
+        assert caplog.records[0].exc_info is not None
+        assert type(caplog.records[0].exc_info[1]) is TimeoutError
+
+    @pytest.mark.parametrize("use_ssl", ["USE_SSL"], indirect=True)
+    @pytest.mark.parametrize("ssl_handshake_timeout", [pytest.param(1, id="timeout==1sec")], indirect=True)
+    @pytest.mark.parametrize("ssl_standard_compatible", [False, True], indirect=True, ids=lambda p: f"standard_compatible=={p}")
+    @pytest.mark.parametrize(
+        "request_handler",
+        [
+            pytest.param(MyAsyncTCPRequestHandler, id="during_handle"),
+            pytest.param(InitialHandshakeRequestHandler, id="during_on_connection_hook"),
+        ],
+        indirect=True,
+    )
+    async def test____serve_forever____suppress_ssl_ragged_eof_errors(
+        self,
+        server_address: tuple[str, int],
+        server_ssl_context: ssl.SSLContext,
+        client_ssl_context: ssl.SSLContext,
+        caplog: pytest.LogCaptureFixture,
+    ) -> None:
+        caplog.set_level(logging.WARNING, LOGGER.name)
+
+        # This test must fail if this option was not unset when creating the server
+        assert (server_ssl_context.options & ssl.OP_IGNORE_UNEXPECTED_EOF) == 0
+
+        from easynetwork.lowlevel.api_async.transports.tls import AsyncTLSStreamTransport
+
+        transport = await current_async_backend().create_tcp_connection(*server_address)
+        transport = await AsyncTLSStreamTransport.wrap(
+            transport,
+            client_ssl_context,
+            standard_compatible=False,  # <- Will not do shutdown handshake on close.
+            server_hostname="test.example.com",
+            handshake_timeout=1,
+        )
+        await asyncio.sleep(0.1)
+        await transport.aclose()
+
+        await asyncio.sleep(0.1)
+        assert len(caplog.records) == 0
```

### Comparing `easynetwork-1.0.0rc7/tests/functional_test/test_communication/test_async/test_server/test_udp.py` & `easynetwork-1.0.0rc8/tests/functional_test/test_communication/test_async/test_server/test_udp.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,97 +1,104 @@
 from __future__ import annotations
 
 import asyncio
 import collections
 import contextlib
 import logging
-import weakref
 from collections.abc import AsyncGenerator, AsyncIterator, Awaitable, Callable, Sequence
 from typing import Any
 
-from easynetwork.api_async.server.handler import AsyncDatagramClient, AsyncDatagramRequestHandler, INETClientAttribute
-from easynetwork.api_async.server.udp import AsyncUDPNetworkServer
 from easynetwork.exceptions import BaseProtocolParseError, ClientClosedError, DatagramProtocolParseError, DeserializeError
 from easynetwork.lowlevel.socket import SocketAddress
 from easynetwork.lowlevel.std_asyncio.backend import AsyncIOBackend
 from easynetwork.lowlevel.std_asyncio.datagram.endpoint import DatagramEndpoint, create_datagram_endpoint
-from easynetwork.lowlevel.std_asyncio.datagram.listener import (
-    AsyncioTransportDatagramListenerSocketAdapter,
-    RawDatagramListenerSocketAdapter,
-)
+from easynetwork.lowlevel.std_asyncio.datagram.listener import DatagramListenerSocketAdapter
 from easynetwork.protocol import DatagramProtocol
+from easynetwork.servers.async_udp import AsyncUDPNetworkServer
+from easynetwork.servers.handlers import AsyncDatagramClient, AsyncDatagramRequestHandler, INETClientAttribute
 
 import pytest
 import pytest_asyncio
 
-from .....pytest_plugins.asyncio_event_loop import EventLoop
 from .....tools import temporary_backend
 from .base import BaseTestAsyncServer
 
 
 class NoListenerErrorBackend(AsyncIOBackend):
     async def create_udp_listeners(
         self,
         host: str | Sequence[str] | None,
         port: int,
         *,
         reuse_port: bool = False,
-    ) -> Sequence[RawDatagramListenerSocketAdapter] | Sequence[AsyncioTransportDatagramListenerSocketAdapter]:
+    ) -> Sequence[DatagramListenerSocketAdapter]:
         return []
 
 
 class RandomError(Exception):
     pass
 
 
 def client_address(client: AsyncDatagramClient[Any]) -> SocketAddress:
     return client.extra(INETClientAttribute.remote_address)
 
 
+LOGGER = logging.getLogger(__name__)
+
+
 class MyAsyncUDPRequestHandler(AsyncDatagramRequestHandler[str, str]):
+    request_count: collections.Counter[tuple[Any, ...]]
     request_received: collections.defaultdict[tuple[Any, ...], list[str]]
     bad_request_received: collections.defaultdict[tuple[Any, ...], list[BaseProtocolParseError]]
     created_clients: set[AsyncDatagramClient[str]]
     server: AsyncUDPNetworkServer[str, str]
 
     async def service_init(self, exit_stack: contextlib.AsyncExitStack, server: AsyncUDPNetworkServer[str, str]) -> None:
         await super().service_init(exit_stack, server)
         self.server = server
+        assert isinstance(self.server, AsyncUDPNetworkServer)
+        self.request_count = collections.Counter()
         self.request_received = collections.defaultdict(list)
         self.bad_request_received = collections.defaultdict(list)
         self.created_clients = set()
 
         exit_stack.push_async_callback(self.service_quit)
 
     async def service_quit(self) -> None:
         # At this point, ALL clients should be closed (since the UDP socket is closed)
         for client in self.created_clients:
             assert client.is_closing()
             with pytest.raises(ClientClosedError):
                 await client.send_packet("something")
 
         del (
+            self.request_count,
             self.request_received,
             self.bad_request_received,
             self.created_clients,
         )
 
     async def handle(self, client: AsyncDatagramClient[str]) -> AsyncGenerator[None, str]:
         self.created_clients.add(client)
         while True:
             async with self.handle_bad_requests(client):
                 request = yield
                 break
+        self.request_count[client_address(client)] += 1
         match request:
             case "__error__":
                 raise RandomError("Sorry man!")
+            case "__error_excgrp__":
+                raise ExceptionGroup("RandomError", [RandomError("Sorry man!")])
             case "__os_error__":
                 raise OSError("Server issue.")
             case "__closed_client_error__":
                 raise ClientClosedError
+            case "__closed_client_error_excgrp__":
+                raise ExceptionGroup("ClientClosedError", [ClientClosedError()])
             case "__eq__":
                 assert client in list(self.created_clients)
                 assert object() not in list(self.created_clients)
                 await client.send_packet("True")
             case "__wait__":
                 while True:
                     async with self.handle_bad_requests(client):
@@ -103,30 +110,54 @@
                 self.request_received[client_address(client)].append(request)
                 try:
                     await client.send_packet(request.upper())
                 except Exception as exc:
                     msg = f"{exc.__class__.__name__}: {exc}"
                     if exc.__cause__:
                         msg = f"{msg} (caused by {exc.__cause__.__class__.__name__}: {exc.__cause__})"
-                    self.server.logger.error(msg, exc_info=exc)
+                    LOGGER.error(msg, exc_info=exc)
 
     @contextlib.asynccontextmanager
     async def handle_bad_requests(self, client: AsyncDatagramClient[str]) -> AsyncIterator[None]:
         try:
             yield
         except DatagramProtocolParseError as exc:
             self.bad_request_received[client_address(client)].append(exc)
             await client.send_packet("wrong encoding man.")
 
 
-class TimeoutRequestHandler(AsyncDatagramRequestHandler[str, str]):
+class TimeoutYieldedRequestHandler(AsyncDatagramRequestHandler[str, str]):
+    request_timeout: float = 1.0
+    timeout_on_third_yield: bool = False
+
+    async def handle(self, client: AsyncDatagramClient[str]) -> AsyncGenerator[float | None, str]:
+        assert (yield None) == "something"
+        if self.timeout_on_third_yield:
+            request = yield None
+            await client.send_packet(request)
+        try:
+            with pytest.raises(TimeoutError):
+                yield self.request_timeout
+            await client.send_packet("successfully timed out")
+        except BaseException:
+            await client.send_packet("error occurred")
+            raise
+        finally:
+            self.request_timeout = 1.0  # Force reset to 1 second in order not to overload the server
+
+
+class TimeoutContextRequestHandler(AsyncDatagramRequestHandler[str, str]):
     request_timeout: float = 1.0
+    timeout_on_third_yield: bool = False
 
     async def handle(self, client: AsyncDatagramClient[str]) -> AsyncGenerator[None, str]:
         assert (yield) == "something"
+        if self.timeout_on_third_yield:
+            request = yield
+            await client.send_packet(request)
         try:
             with pytest.raises(TimeoutError):
                 async with asyncio.timeout(self.request_timeout):
                     yield
             await client.send_packet("successfully timed out")
         except BaseException:
             await client.send_packet("error occurred")
@@ -134,35 +165,39 @@
         finally:
             self.request_timeout = 1.0  # Force reset to 1 second in order not to overload the server
 
 
 class ConcurrencyTestRequestHandler(AsyncDatagramRequestHandler[str, str]):
     sleep_time_before_second_yield: float = 0.0
     sleep_time_before_response: float = 0.0
+    recreate_generator: bool = True
 
     async def handle(self, client: AsyncDatagramClient[str]) -> AsyncGenerator[None, str]:
-        assert (yield) == "something"
-        await asyncio.sleep(self.sleep_time_before_second_yield)
-        request = yield
-        await asyncio.sleep(self.sleep_time_before_response)
-        await client.send_packet(f"After wait: {request}")
+        while True:
+            assert (yield) == "something"
+            await asyncio.sleep(self.sleep_time_before_second_yield)
+            request = yield
+            await asyncio.sleep(self.sleep_time_before_response)
+            await client.send_packet(f"After wait: {request}")
+            if self.recreate_generator:
+                break
 
 
 class CancellationRequestHandler(AsyncDatagramRequestHandler[str, str]):
     async def handle(self, client: AsyncDatagramClient[str]) -> AsyncGenerator[None, str]:
         yield
         await client.send_packet("response")
         raise asyncio.CancelledError()
 
 
 class RequestRefusedHandler(AsyncDatagramRequestHandler[str, str]):
     refuse_after: int = 2**64
     bypass_refusal: bool = False
 
-    async def service_init(self, exit_stack: contextlib.AsyncExitStack, server: AsyncUDPNetworkServer[str, str]) -> None:
+    async def service_init(self, exit_stack: contextlib.AsyncExitStack, server: Any) -> None:
         self.request_count: collections.Counter[AsyncDatagramClient[str]] = collections.Counter()
 
     async def handle(self, client: AsyncDatagramClient[str]) -> AsyncGenerator[None, str]:
         if self.request_count[client] >= self.refuse_after and not self.bypass_refusal:
             return
         request = yield
         self.request_count[client] += 1
@@ -199,41 +234,36 @@
         pass
 
 
 class MyAsyncUDPServer(AsyncUDPNetworkServer[str, str]):
     __slots__ = ()
 
 
-@pytest.mark.asyncio
+@pytest.mark.flaky(retries=3, delay=1)
 class TestAsyncUDPNetworkServer(BaseTestAsyncServer):
     @pytest.fixture
     @staticmethod
-    def use_asyncio_transport(event_loop_name: EventLoop, use_asyncio_transport: bool) -> bool:
-        if not use_asyncio_transport:
-            if event_loop_name == EventLoop.UVLOOP:
-                pytest.xfail("uvloop runner does not implement the needed functions")
-        return use_asyncio_transport
-
-    @pytest.fixture
-    @staticmethod
     def request_handler(request: Any) -> AsyncDatagramRequestHandler[str, str]:
         request_handler_cls: type[AsyncDatagramRequestHandler[str, str]] = getattr(request, "param", MyAsyncUDPRequestHandler)
         return request_handler_cls()
 
     @pytest_asyncio.fixture
     @staticmethod
     async def server(
         request_handler: AsyncDatagramRequestHandler[str, str],
         localhost_ip: str,
         datagram_protocol: DatagramProtocol[str, str],
-        use_asyncio_transport: bool,  # Only here for dependency
+        caplog: pytest.LogCaptureFixture,
+        logger_crash_threshold_level: dict[str, int],
     ) -> AsyncIterator[MyAsyncUDPServer]:
-        async with MyAsyncUDPServer(localhost_ip, 0, datagram_protocol, request_handler) as server:
-            assert not server.sockets
+        async with MyAsyncUDPServer(localhost_ip, 0, datagram_protocol, request_handler, logger=LOGGER) as server:
+            assert not server.get_sockets()
             assert not server.get_addresses()
+            caplog.set_level(logging.INFO, LOGGER.name)
+            logger_crash_threshold_level[LOGGER.name] = logging.WARNING
             yield server
 
     @pytest_asyncio.fixture
     @staticmethod
     async def server_address(run_server: asyncio.Event, server: MyAsyncUDPServer) -> tuple[str, int]:
         async with asyncio.timeout(1):
             await run_server.wait()
@@ -273,25 +303,24 @@
         datagram_protocol: DatagramProtocol[str, str],
     ) -> None:
         with temporary_backend(NoListenerErrorBackend()):
             async with MyAsyncUDPServer(None, 0, datagram_protocol, request_handler) as s:
                 with pytest.raises(OSError, match=r"^empty listeners list$"):
                     await s.serve_forever()
 
-                assert not s.sockets
+                assert not s.get_sockets()
 
     @pytest.mark.usefixtures("run_server_and_wait")
     async def test____serve_forever____server_assignment(
         self,
         server: MyAsyncUDPServer,
         request_handler: MyAsyncUDPRequestHandler,
     ) -> None:
         assert request_handler.server == server
         assert isinstance(request_handler.server, AsyncUDPNetworkServer)
-        assert isinstance(request_handler.server, weakref.ProxyType)
 
     async def test____serve_forever____handle_request(
         self,
         client_factory: Callable[[], Awaitable[DatagramEndpoint]],
         request_handler: MyAsyncUDPRequestHandler,
     ) -> None:
         endpoint = await client_factory()
@@ -363,17 +392,19 @@
     @pytest.mark.parametrize("one_shot_serializer", [pytest.param("invalid", id="serializer_crash")], indirect=True)
     async def test____serve_forever____internal_error(
         self,
         mute_thrown_exception: bool,
         request_handler: ErrorInRequestHandler,
         client_factory: Callable[[], Awaitable[DatagramEndpoint]],
         caplog: pytest.LogCaptureFixture,
-        server: MyAsyncUDPServer,
+        logger_crash_maximum_nb_lines: dict[str, int],
     ) -> None:
-        caplog.set_level(logging.ERROR, server.logger.name)
+        caplog.set_level(logging.ERROR, LOGGER.name)
+        if not mute_thrown_exception:
+            logger_crash_maximum_nb_lines[LOGGER.name] = 3
         request_handler.mute_thrown_exception = mute_thrown_exception
         endpoint = await client_factory()
 
         expected_message = b"RuntimeError: protocol.build_packet_from_datagram() crashed (caused by SystemError: CRASH)"
 
         await endpoint.sendto(b"something", None)
         await asyncio.sleep(0.2)
@@ -382,90 +413,121 @@
         if mute_thrown_exception:
             await endpoint.sendto(b"something", None)
             await asyncio.sleep(0.2)
             assert (await endpoint.recvfrom())[0] == expected_message
             assert len(caplog.records) == 0  # After two attempts
         else:
             assert len(caplog.records) == 3
+            assert caplog.records[1].exc_info is not None
+            assert type(caplog.records[1].exc_info[1]) is RuntimeError
 
+    @pytest.mark.parametrize("excgrp", [False, True], ids=lambda p: f"exception_group_raised=={p}")
     async def test____serve_forever____unexpected_error_during_process(
         self,
+        excgrp: bool,
         client_factory: Callable[[], Awaitable[DatagramEndpoint]],
         caplog: pytest.LogCaptureFixture,
-        server: MyAsyncUDPServer,
+        logger_crash_maximum_nb_lines: dict[str, int],
     ) -> None:
-        caplog.set_level(logging.ERROR, server.logger.name)
+        caplog.set_level(logging.ERROR, LOGGER.name)
+        logger_crash_maximum_nb_lines[LOGGER.name] = 3
         endpoint = await client_factory()
 
-        await endpoint.sendto(b"__error__", None)
+        if excgrp:
+            await endpoint.sendto(b"__error_excgrp__", None)
+        else:
+            await endpoint.sendto(b"__error__", None)
         await asyncio.sleep(0.2)
 
         assert len(caplog.records) == 3
+        assert caplog.records[1].exc_info is not None
+        if excgrp:
+            assert type(caplog.records[1].exc_info[1]) is ExceptionGroup
+            assert type(caplog.records[1].exc_info[1].exceptions[0]) is RandomError
+        else:
+            assert type(caplog.records[1].exc_info[1]) is RandomError
 
     @pytest.mark.parametrize("one_shot_serializer", [pytest.param("bad_serialize", id="serializer_crash")], indirect=True)
     async def test____serve_forever____unexpected_error_during_response_serialization(
         self,
         client_factory: Callable[[], Awaitable[DatagramEndpoint]],
         caplog: pytest.LogCaptureFixture,
-        server: MyAsyncUDPServer,
+        logger_crash_maximum_nb_lines: dict[str, int],
     ) -> None:
-        caplog.set_level(logging.ERROR, server.logger.name)
+        caplog.set_level(logging.ERROR, LOGGER.name)
+        logger_crash_maximum_nb_lines[LOGGER.name] = 1
         endpoint = await client_factory()
 
         await endpoint.sendto(b"request", None)
         while not caplog.records:
             await asyncio.sleep(0.2)
 
         assert len(caplog.records) == 1
         assert caplog.records[0].levelno == logging.ERROR
         assert caplog.records[0].message == "RuntimeError: protocol.make_datagram() crashed (caused by SystemError: CRASH)"
 
     async def test____serve_forever____os_error(
         self,
         client_factory: Callable[[], Awaitable[DatagramEndpoint]],
         caplog: pytest.LogCaptureFixture,
-        server: MyAsyncUDPServer,
+        logger_crash_maximum_nb_lines: dict[str, int],
     ) -> None:
-        caplog.set_level(logging.ERROR, server.logger.name)
+        caplog.set_level(logging.ERROR, LOGGER.name)
+        logger_crash_maximum_nb_lines[LOGGER.name] = 3
         endpoint = await client_factory()
 
         await endpoint.sendto(b"__os_error__", None)
         await asyncio.sleep(0.2)
 
         assert len(caplog.records) == 3
+        assert caplog.records[1].exc_info is not None
+        assert type(caplog.records[1].exc_info[1]) is OSError
 
+    @pytest.mark.parametrize("excgrp", [False, True], ids=lambda p: f"exception_group_raised=={p}")
     async def test____serve_forever____use_of_a_closed_client_in_request_handler(  # In a world where this thing happen
         self,
+        excgrp: bool,
         client_factory: Callable[[], Awaitable[DatagramEndpoint]],
         caplog: pytest.LogCaptureFixture,
-        server: MyAsyncUDPServer,
+        logger_crash_maximum_nb_lines: dict[str, int],
     ) -> None:
-        caplog.set_level(logging.WARNING, server.logger.name)
+        caplog.set_level(logging.WARNING, LOGGER.name)
+        logger_crash_maximum_nb_lines[LOGGER.name] = 1
         endpoint = await client_factory()
         host, port = endpoint.get_extra_info("sockname")[:2]
 
-        await endpoint.sendto(b"__closed_client_error__", None)
+        if excgrp:
+            await endpoint.sendto(b"__closed_client_error_excgrp__", None)
+        else:
+            await endpoint.sendto(b"__closed_client_error__", None)
         await asyncio.sleep(0.2)
 
         assert len(caplog.records) == 1
         assert caplog.records[0].levelno == logging.WARNING
         assert caplog.records[0].message == f"There have been attempts to do operation on closed client ({host!r}, {port})"
 
-    @pytest.mark.parametrize("request_handler", [TimeoutRequestHandler], indirect=True)
+    @pytest.mark.parametrize("request_handler", [TimeoutYieldedRequestHandler, TimeoutContextRequestHandler], indirect=True)
     @pytest.mark.parametrize("request_timeout", [0.0, 1.0], ids=lambda p: f"timeout=={p}")
+    @pytest.mark.parametrize("timeout_on_third_yield", [False, True], ids=lambda p: f"timeout_on_third_yield=={p}")
     async def test____serve_forever____throw_cancelled_error(
         self,
         request_timeout: float,
-        request_handler: TimeoutRequestHandler,
+        timeout_on_third_yield: bool,
+        request_handler: TimeoutYieldedRequestHandler | TimeoutContextRequestHandler,
         client_factory: Callable[[], Awaitable[DatagramEndpoint]],
     ) -> None:
         request_handler.request_timeout = request_timeout
+        request_handler.timeout_on_third_yield = timeout_on_third_yield
         endpoint = await client_factory()
 
         await endpoint.sendto(b"something", None)
+        if timeout_on_third_yield:
+            await endpoint.sendto(b"something", None)
+            assert (await endpoint.recvfrom())[0] == b"something"
+
         async with asyncio.timeout(request_timeout + 1):
             assert (await endpoint.recvfrom())[0] == b"successfully timed out"
 
     @pytest.mark.parametrize("request_handler", [CancellationRequestHandler], indirect=True)
     async def test____serve_forever____request_handler_is_cancelled(
         self,
         client_factory: Callable[[], Awaitable[DatagramEndpoint]],
@@ -476,43 +538,45 @@
         assert (await endpoint.recvfrom())[0] == b"response"
 
     @pytest.mark.parametrize("request_handler", [ErrorBeforeYieldHandler], indirect=True)
     async def test____serve_forever____request_handler_crashed_before_yield(
         self,
         request_handler: ErrorBeforeYieldHandler,
         caplog: pytest.LogCaptureFixture,
+        logger_crash_maximum_nb_lines: dict[str, int],
         client_factory: Callable[[], Awaitable[DatagramEndpoint]],
-        server: MyAsyncUDPServer,
     ) -> None:
-        caplog.set_level(logging.ERROR, server.logger.name)
+        caplog.set_level(logging.ERROR, LOGGER.name)
+        logger_crash_maximum_nb_lines[LOGGER.name] = 3
         endpoint = await client_factory()
 
         request_handler.raise_error = True
         await endpoint.sendto(b"something", None)
         with pytest.raises(TimeoutError):
             async with asyncio.timeout(0.5):
                 await endpoint.recvfrom()
         assert len(caplog.records) == 3
+        assert caplog.records[1].exc_info is not None
+        assert type(caplog.records[1].exc_info[1]) is RandomError
         request_handler.raise_error = False
         await endpoint.sendto(b"hello world", None)
         assert (await endpoint.recvfrom())[0] == b"hello world"
 
     @pytest.mark.parametrize("request_handler", [RequestRefusedHandler], indirect=True)
     @pytest.mark.parametrize("refuse_after", [0, 5], ids=lambda p: f"refuse_after=={p}")
     async def test____serve_forever____request_handler_did_not_yield(
         self,
         refuse_after: int,
         request_handler: RequestRefusedHandler,
         caplog: pytest.LogCaptureFixture,
         client_factory: Callable[[], Awaitable[DatagramEndpoint]],
-        server: MyAsyncUDPServer,
     ) -> None:
         request_handler.bypass_refusal = False
         request_handler.refuse_after = refuse_after
-        caplog.set_level(logging.ERROR, server.logger.name)
+        caplog.set_level(logging.ERROR, LOGGER.name)
         endpoint = await client_factory()
 
         for _ in range(refuse_after):
             await endpoint.sendto(b"a", None)
             assert (await endpoint.recvfrom())[0] == b"a"
 
         await endpoint.sendto(b"something", None)
@@ -535,20 +599,23 @@
 
         await endpoint.sendto(b"something", None)
         await endpoint.sendto(b"hello, world.", None)
         async with asyncio.timeout(5):
             assert (await endpoint.recvfrom())[0] == b"After wait: hello, world."
 
     @pytest.mark.parametrize("request_handler", [ConcurrencyTestRequestHandler], indirect=True)
+    @pytest.mark.parametrize("recreate_generator", [False, True], ids=lambda p: f"recreate_generator=={p}")
     async def test____serve_forever____too_many_datagrams_while_request_handle_is_performed(
         self,
+        recreate_generator: bool,
         request_handler: ConcurrencyTestRequestHandler,
         client_factory: Callable[[], Awaitable[DatagramEndpoint]],
     ) -> None:
         request_handler.sleep_time_before_response = 0.5
+        request_handler.recreate_generator = recreate_generator
         endpoint = await client_factory()
 
         await endpoint.sendto(b"something", None)
         await asyncio.sleep(0.1)
         await endpoint.sendto(b"hello, world.", None)
         await endpoint.sendto(b"something", None)
         await asyncio.sleep(0.1)
```

### Comparing `easynetwork-1.0.0rc7/tests/functional_test/test_communication/test_sync/conftest.py` & `easynetwork-1.0.0rc8/tests/functional_test/test_communication/test_sync/conftest.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/tests/functional_test/test_communication/test_sync/test_client/test_tcp.py` & `easynetwork-1.0.0rc8/tests/functional_test/test_communication/test_sync/test_client/test_tcp.py`

 * *Files 0% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 import ssl
 import time
 from collections.abc import Callable, Iterator
 from concurrent.futures import Future
 from socket import AF_INET, IPPROTO_TCP, SHUT_WR, TCP_NODELAY, socket as Socket
 from typing import Any
 
-from easynetwork.api_sync.client.tcp import TCPNetworkClient
+from easynetwork.clients.tcp import TCPNetworkClient
 from easynetwork.exceptions import ClientClosedError, StreamProtocolParseError
 from easynetwork.lowlevel.socket import IPv4SocketAddress, IPv6SocketAddress
 from easynetwork.protocol import StreamProtocol
 
 import pytest
 
 from .....tools import TimeTest
@@ -27,14 +27,15 @@
         buf.append(chunk)
         if b"\n" in chunk:
             break
     return b"".join(buf)
 
 
 @pytest.mark.usefixtures("simulate_no_ssl_module")
+@pytest.mark.flaky(retries=3, delay=1)
 class TestTCPNetworkClient:
     @pytest.fixture
     @staticmethod
     def server(socket_pair: tuple[Socket, Socket]) -> Socket:
         server = socket_pair[0]
         server.setsockopt(IPPROTO_TCP, TCP_NODELAY, 1)
         return server
@@ -89,15 +90,17 @@
         client: TCPNetworkClient[str, str],
         server: Socket,
     ) -> None:
         client.send_packet("ABC")
         assert server.recv(1) == b"A"
         server.close()
         with pytest.raises(ConnectionAbortedError):
-            client.send_packet("DEF")
+            for _ in range(3):  # Windows and macOS catch the issue after several send()
+                client.send_packet("DEF")
+                time.sleep(0.01)
 
     def test____send_packet____closed_client(self, client: TCPNetworkClient[str, str]) -> None:
         client.close()
         with pytest.raises(ClientClosedError):
             client.send_packet("ABCDEF")
 
     def test____send_eof____close_write_stream(
```

### Comparing `easynetwork-1.0.0rc7/tests/functional_test/test_communication/test_sync/test_client/test_udp.py` & `easynetwork-1.0.0rc8/tests/functional_test/test_communication/test_sync/test_client/test_udp.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 from __future__ import annotations
 
 from collections.abc import Callable, Iterator
 from socket import AF_INET, socket as Socket
 from typing import Any
 
-from easynetwork.api_sync.client.udp import UDPNetworkClient
+from easynetwork.clients.udp import UDPNetworkClient
 from easynetwork.exceptions import ClientClosedError, DatagramProtocolParseError
 from easynetwork.lowlevel.socket import IPv4SocketAddress, IPv6SocketAddress
 from easynetwork.protocol import DatagramProtocol
 
 import pytest
 
 from .....tools import PlatformMarkers
@@ -22,14 +22,15 @@
         sock = udp_socket_factory()
         sock.bind((localhost_ip, 0))
         return sock
 
     return bound_udp_socket_factory
 
 
+@pytest.mark.flaky(retries=3, delay=1)
 class TestUDPNetworkClient:
     @pytest.fixture
     @staticmethod
     def server(udp_socket_factory: Callable[[], Socket]) -> Socket:
         return udp_socket_factory()
 
     @pytest.fixture
```

### Comparing `easynetwork-1.0.0rc7/tests/functional_test/test_communication/test_sync/test_server/test_standalone.py` & `easynetwork-1.0.0rc8/tests/functional_test/test_communication/test_sync/test_server/test_standalone.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,31 +1,26 @@
 from __future__ import annotations
 
-import contextlib
 import socket
 import threading
 import time
 from collections.abc import AsyncGenerator, Iterator
 
-from easynetwork.api_async.server.abc import AbstractAsyncNetworkServer
-from easynetwork.api_async.server.handler import AsyncBaseClientInterface, AsyncDatagramRequestHandler, AsyncStreamRequestHandler
-from easynetwork.api_sync.server.abc import AbstractNetworkServer
-from easynetwork.api_sync.server.tcp import StandaloneTCPNetworkServer
-from easynetwork.api_sync.server.thread import NetworkServerThread
-from easynetwork.api_sync.server.udp import StandaloneUDPNetworkServer
 from easynetwork.exceptions import ServerAlreadyRunning, ServerClosedError
 from easynetwork.protocol import DatagramProtocol, StreamProtocol
+from easynetwork.servers.abc import AbstractNetworkServer
+from easynetwork.servers.handlers import AsyncBaseClientInterface, AsyncDatagramRequestHandler, AsyncStreamRequestHandler
+from easynetwork.servers.standalone_tcp import StandaloneTCPNetworkServer
+from easynetwork.servers.standalone_udp import StandaloneUDPNetworkServer
+from easynetwork.servers.threads_helper import NetworkServerThread
 
 import pytest
 
 
 class EchoRequestHandler(AsyncStreamRequestHandler[str, str], AsyncDatagramRequestHandler[str, str]):
-    async def service_init(self, exit_stack: contextlib.AsyncExitStack, server: AbstractAsyncNetworkServer) -> None:
-        pass
-
     async def handle(self, client: AsyncBaseClientInterface[str]) -> AsyncGenerator[None, str]:
         request = yield
         await client.send_packet(request)
 
 
 class BaseTestStandaloneNetworkServer:
     @pytest.fixture
@@ -88,14 +83,29 @@
             if not server.is_serving():
                 pytest.fail("Timeout error")
 
             server.shutdown()
             assert not server.is_serving()
             t.join()
 
+    def test____serve_forever____serve_several_times(self, server: AbstractNetworkServer) -> None:
+        with server:
+            for _ in range(3):
+                assert not server.is_serving()
+                assert not server.get_addresses()
+
+                server_thread = NetworkServerThread(server, daemon=True)
+                server_thread.start()
+                try:
+                    assert server.is_serving()
+                    assert len(server.get_addresses()) > 0
+                    time.sleep(0.5)
+                finally:
+                    server_thread.join()
+
     def test____server_thread____several_join(
         self,
         start_server: NetworkServerThread,
     ) -> None:
         start_server.join()
         start_server.join()
 
@@ -109,70 +119,50 @@
     @pytest.fixture
     @staticmethod
     def client(server: StandaloneTCPNetworkServer[str, str], start_server: None) -> Iterator[socket.socket]:
         port = server.get_addresses()[0].port
         with socket.create_connection(("localhost", port)) as client:
             yield client
 
-    def test____serve_forever____serve_several_times(self, server: StandaloneTCPNetworkServer[str, str]) -> None:
-        with server:
-            for _ in range(3):
-                assert not server.is_serving()
-                assert not server.get_addresses()
-
-                server_thread = NetworkServerThread(server, daemon=True)
-                server_thread.start()
-                try:
-                    assert server.is_serving()
-                    assert len(server.get_addresses()) > 0
-                    time.sleep(0.5)
-                finally:
-                    server_thread.join()
-
     def test____stop_listening____default_to_noop(self, server: StandaloneTCPNetworkServer[str, str]) -> None:
         with server:
-            assert not server.sockets
+            assert not server.get_sockets()
             assert not server.get_addresses()
             server.stop_listening()
 
+    def test____socket_property____server_is_not_running(self, server: StandaloneTCPNetworkServer[str, str]) -> None:
+        with server:
+            assert len(server.get_sockets()) == 0
+            assert len(server.get_addresses()) == 0
+
+    @pytest.mark.usefixtures("start_server")
+    def test____socket_property____server_is_running(self, server: StandaloneTCPNetworkServer[str, str]) -> None:
+        assert len(server.get_sockets()) > 0
+        assert len(server.get_addresses()) > 0
+
     @pytest.mark.usefixtures("start_server", "client")
     def test____stop_listening____stop_accepting_new_connection(self, server: StandaloneTCPNetworkServer[str, str]) -> None:
         assert server.is_serving()
-        assert len(server.sockets) > 0
+        assert len(server.get_sockets()) > 0
         assert len(server.get_addresses()) > 0
 
         server.stop_listening()
         assert not server.is_serving()
-        assert len(server.sockets) > 0  # Sockets are closed, but always available until server_close() call
+        assert len(server.get_sockets()) > 0  # Sockets are closed, but always available until server_close() call
         assert len(server.get_addresses()) == 0
 
 
 class TestStandaloneUDPNetworkServer(BaseTestStandaloneNetworkServer):
     @pytest.fixture
     @staticmethod
     def server(datagram_protocol: DatagramProtocol[str, str]) -> StandaloneUDPNetworkServer[str, str]:
         return StandaloneUDPNetworkServer("localhost", 0, datagram_protocol, EchoRequestHandler())
 
-    def test____serve_forever____serve_several_times(self, server: StandaloneUDPNetworkServer[str, str]) -> None:
-        with server:
-            for _ in range(3):
-                assert not server.is_serving()
-                assert not server.get_addresses()
-
-                server_thread = NetworkServerThread(server, daemon=True)
-                server_thread.start()
-                try:
-                    assert server.is_serving()
-                    assert len(server.get_addresses()) > 0
-                    time.sleep(0.5)
-                finally:
-                    server_thread.join()
-
     def test____socket_property____server_is_not_running(self, server: StandaloneUDPNetworkServer[str, str]) -> None:
         with server:
-            assert len(server.sockets) == 0
+            assert len(server.get_sockets()) == 0
             assert len(server.get_addresses()) == 0
 
     @pytest.mark.usefixtures("start_server")
     def test____socket_property____server_is_running(self, server: StandaloneUDPNetworkServer[str, str]) -> None:
-        assert len(server.sockets) > 0
+        assert len(server.get_sockets()) > 0
         assert len(server.get_addresses()) > 0
```

### Comparing `easynetwork-1.0.0rc7/tests/functional_test/test_concurrency/conftest.py` & `easynetwork-1.0.0rc8/tests/functional_test/test_concurrency/conftest.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,30 +1,25 @@
 from __future__ import annotations
 
-import contextlib
 import threading
 from collections.abc import AsyncGenerator, Iterator
 from typing import Literal
 
-from easynetwork.api_async.server.abc import AbstractAsyncNetworkServer
-from easynetwork.api_async.server.handler import AsyncBaseClientInterface, AsyncDatagramRequestHandler, AsyncStreamRequestHandler
-from easynetwork.api_sync.server.abc import AbstractNetworkServer
-from easynetwork.api_sync.server.tcp import StandaloneTCPNetworkServer
-from easynetwork.api_sync.server.udp import StandaloneUDPNetworkServer
 from easynetwork.lowlevel.socket import IPv4SocketAddress
 from easynetwork.protocol import DatagramProtocol, StreamProtocol
 from easynetwork.serializers.line import StringLineSerializer
+from easynetwork.servers.abc import AbstractNetworkServer
+from easynetwork.servers.handlers import AsyncBaseClientInterface, AsyncDatagramRequestHandler, AsyncStreamRequestHandler
+from easynetwork.servers.standalone_tcp import StandaloneTCPNetworkServer
+from easynetwork.servers.standalone_udp import StandaloneUDPNetworkServer
 
 import pytest
 
 
 class EchoRequestHandler(AsyncStreamRequestHandler[str, str], AsyncDatagramRequestHandler[str, str]):
-    async def service_init(self, exit_stack: contextlib.AsyncExitStack, server: AbstractAsyncNetworkServer) -> None:
-        pass
-
     async def handle(self, client: AsyncBaseClientInterface[str]) -> AsyncGenerator[None, str]:
         request = yield
         await client.send_packet(request)
 
 
 @pytest.fixture(params=["TCP", "UDP"])
 def ipproto(request: pytest.FixtureRequest) -> Literal["TCP", "UDP"]:
```

### Comparing `easynetwork-1.0.0rc7/tests/functional_test/test_concurrency/test_sync/conftest.py` & `easynetwork-1.0.0rc8/tests/functional_test/test_concurrency/test_sync/conftest.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 from __future__ import annotations
 
 from collections.abc import Iterator
 from typing import Literal
 
-from easynetwork.api_sync.client import AbstractNetworkClient, TCPNetworkClient, UDPNetworkClient
+from easynetwork.clients import TCPNetworkClient, UDPNetworkClient
+from easynetwork.clients.abc import AbstractNetworkClient
 from easynetwork.protocol import DatagramProtocol, StreamProtocol
 from easynetwork.serializers.line import StringLineSerializer
 
 import pytest
 
 
 def _build_client(ipproto: Literal["TCP", "UDP"], server_address: tuple[str, int]) -> AbstractNetworkClient[str, str]:
```

### Comparing `easynetwork-1.0.0rc7/tests/functional_test/test_concurrency/test_sync/test_threaded_client.py` & `easynetwork-1.0.0rc8/tests/functional_test/test_concurrency/test_sync/test_threaded_client.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,24 +1,29 @@
 from __future__ import annotations
 
 import time
 from collections.abc import Iterator
 from concurrent.futures import ThreadPoolExecutor
 from typing import TypeAlias
 
-from easynetwork.api_sync.client.abc import AbstractNetworkClient
+from easynetwork.clients.abc import AbstractNetworkClient
 from easynetwork.exceptions import ClientClosedError
 
 import pytest
 
 from ....tools import TimeTest
 
 ClientType: TypeAlias = AbstractNetworkClient[str, str]
 
 
+pytestmark = [
+    pytest.mark.flaky(retries=3, delay=1),
+]
+
+
 @pytest.fixture
 def executor(client: ClientType) -> Iterator[ThreadPoolExecutor]:
     with ThreadPoolExecutor(max_workers=1) as executor:
         try:
             yield executor
         finally:
             client.close()
```

### Comparing `easynetwork-1.0.0rc7/tests/functional_test/test_serializers/base.py` & `easynetwork-1.0.0rc8/tests/functional_test/test_serializers/base.py`

 * *Files 2% similar despite different names*

```diff
@@ -100,14 +100,21 @@
     ) -> None:
         # Arrange
 
         # Act & Assert
         with pytest.raises(DeserializeError):
             _ = serializer_for_deserialization.deserialize(invalid_complete_data)
 
+
+class BaseTestSerializerExtraData(BaseTestSerializer):
+    @pytest.fixture(scope="class")
+    @staticmethod
+    def oneshot_extra_data() -> bytes:
+        return b"remaining_data"
+
     def test____deserialize____extra_data(
         self,
         serializer_for_deserialization: AbstractPacketSerializer[Any, Any],
         complete_data: bytes,
         oneshot_extra_data: bytes,
     ) -> None:
         # Arrange
@@ -358,15 +365,15 @@
         consumer = serializer_for_deserialization.buffered_incremental_deserialize(buffer)
         start_idx = next(consumer)
         nbytes, partial_remaining_data = write_data_and_extra_in_buffer(
             buffer,
             invalid_partial_data,
             sent_extra_data,
             start_pos=start_idx,
-            too_short_buffer_for_complete_data="xfail",
+            too_short_buffer_for_complete_data="fill_at_most",
         )
         expected_remainder = expected_remainder.replace(sent_extra_data, partial_remaining_data, 1)
         del partial_remaining_data
 
         # Act
         with pytest.raises(IncrementalDeserializeError) as exc_info:
             consumer.send(nbytes)
```

### Comparing `easynetwork-1.0.0rc7/tests/functional_test/test_serializers/test_base64.py` & `easynetwork-1.0.0rc8/tests/functional_test/test_serializers/test_base64.py`

 * *Files 6% similar despite different names*

```diff
@@ -9,29 +9,28 @@
 from typing import Any, Literal, final
 
 from easynetwork.exceptions import DeserializeError
 from easynetwork.serializers.wrapper.base64 import Base64EncoderSerializer
 
 import pytest
 
-from .base import BaseTestIncrementalSerializer, NoSerialization
+from .base import BaseTestBufferedIncrementalSerializer, NoSerialization
 
 
 def generate_key_from_string(s: str) -> bytes:
     return base64.urlsafe_b64encode(hashlib.sha256(s.encode("utf-8")).digest())
 
 
 SAMPLES = [
     (b"a", "one ascii byte"),
     (b"\xcc", "one unicode byte"),
-    (random.randbytes(255), "255 random generated bytes"),
 ]
 
 
-class BaseTestBase64EncoderSerializer(BaseTestIncrementalSerializer):
+class BaseTestBase64EncoderSerializer(BaseTestBufferedIncrementalSerializer):
     #### Serializers
 
     BUFFER_LIMIT = 1024
 
     @pytest.fixture(scope="class", params=["standard", "urlsafe"])
     @staticmethod
     def alphabet(request: pytest.FixtureRequest) -> Literal["standard", "urlsafe"]:
@@ -115,38 +114,32 @@
         return complete_data[:-1]  # Remove one byte at last will break the padding
 
     @pytest.fixture(scope="class", params=["missing_data", "limit_overrun_without_newline", "limit_overrun_with_newline"])
     @classmethod
     def invalid_partial_data(cls, request: pytest.FixtureRequest, alphabet: Literal["standard", "urlsafe"]) -> bytes:
         match request.param:
             case "missing_data":
+                token = b"abc" * 85
                 if alphabet == "standard":
-                    return base64.standard_b64encode(random.randbytes(255))[:-1] + b"\r\n"
-                return base64.urlsafe_b64encode(random.randbytes(255))[:-1] + b"\r\n"
+                    return base64.standard_b64encode(token)[:-1] + b"\r\n"
+                return base64.urlsafe_b64encode(token)[:-1] + b"\r\n"
             case "limit_overrun_without_newline":
                 return b"4" * (cls.BUFFER_LIMIT + 10)
             case "limit_overrun_with_newline":
                 return b"4" * (cls.BUFFER_LIMIT + 10) + b"\r\n"
             case _:
                 pytest.fail("Invalid fixture parameter")
 
     @pytest.fixture(scope="class")
     @classmethod
     def invalid_partial_data_extra_data(cls, invalid_partial_data: bytes) -> tuple[bytes, bytes]:
         if len(invalid_partial_data) > cls.BUFFER_LIMIT and not invalid_partial_data.endswith(b"\r\n"):
             return (b"remaining_data", b"")
         return (b"remaining_data", b"remaining_data")
 
-    #### Other
-
-    @pytest.fixture(scope="class")
-    @staticmethod
-    def oneshot_extra_data() -> bytes:
-        pytest.skip("Does not recognize extra data")
-
 
 @final
 class TestBase64EncoderSerializerChecksum(BaseTestBase64EncoderSerializer):
     @pytest.fixture(scope="class", params=[False, True], ids=lambda boolean: f"checksum=={boolean}")
     @staticmethod
     def checksum(request: pytest.FixtureRequest) -> bool:
         return getattr(request, "param")
```

### Comparing `easynetwork-1.0.0rc7/tests/functional_test/test_serializers/test_cbor.py` & `easynetwork-1.0.0rc8/tests/functional_test/test_serializers/test_cbor.py`

 * *Files 15% similar despite different names*

```diff
@@ -5,21 +5,21 @@
 import dataclasses
 from typing import Any, final
 
 from easynetwork.serializers.cbor import CBOREncoderConfig, CBORSerializer
 
 import pytest
 
-from .base import BaseTestBufferedIncrementalSerializer
+from .base import BaseTestBufferedIncrementalSerializer, BaseTestSerializerExtraData
 from .samples.json import SAMPLES
 
 
 @final
 @pytest.mark.feature_cbor
-class TestCBORSerializer(BaseTestBufferedIncrementalSerializer):
+class TestCBORSerializer(BaseTestBufferedIncrementalSerializer, BaseTestSerializerExtraData):
     #### Serializers
 
     ENCODER_CONFIG = CBOREncoderConfig()
 
     @pytest.fixture(scope="class")
     @classmethod
     def serializer_for_serialization(cls) -> CBORSerializer:
```

### Comparing `easynetwork-1.0.0rc7/tests/functional_test/test_serializers/test_compressors.py` & `easynetwork-1.0.0rc8/tests/functional_test/test_serializers/test_compressors.py`

 * *Files 6% similar despite different names*

```diff
@@ -5,30 +5,29 @@
 import random
 from typing import Any, final
 
 from easynetwork.serializers.wrapper.compressor import BZ2CompressorSerializer, ZlibCompressorSerializer
 
 import pytest
 
-from .base import BaseTestBufferedIncrementalSerializer, NoSerialization
+from .base import BaseTestBufferedIncrementalSerializer, BaseTestSerializerExtraData, NoSerialization
 
 SAMPLES = [
     (b"", "empty bytes"),
     (b"a", "one ascii byte"),
     (b"\xcc", "one unicode byte"),
     (b"z" * 255, "255 unique byte"),
-    (random.randbytes(255), "255 random generated bytes"),
 ]
 
 
 def _make_data_invalid(token: bytes) -> bytes:
     return token[:-2] + random.randbytes(5) + token[-2:]
 
 
-class BaseTestCompressorSerializer(BaseTestBufferedIncrementalSerializer):
+class BaseTestCompressorSerializer(BaseTestBufferedIncrementalSerializer, BaseTestSerializerExtraData):
     #### Serializers: To be defined in subclass
 
     #### Packets to test
 
     @pytest.fixture(scope="class", params=[pytest.param(p, id=f"packet: {id}") for p, id in SAMPLES])
     @staticmethod
     def packet_to_serialize(request: Any) -> Any:
```

### Comparing `easynetwork-1.0.0rc7/tests/functional_test/test_serializers/test_encryptor.py` & `easynetwork-1.0.0rc8/tests/functional_test/test_serializers/test_json.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,139 +1,130 @@
-# mypy: disable_error_code=override
+# mypy: disable-error-code=override
 
 from __future__ import annotations
 
-from collections.abc import Callable
+import dataclasses
+import json
 from typing import Any, final
 
-from easynetwork.serializers.wrapper.encryptor import EncryptorSerializer
+from easynetwork.serializers.json import JSONEncoderConfig, JSONSerializer
 
 import pytest
 
-from .base import BaseTestIncrementalSerializer, NoSerialization
-from .test_base64 import SAMPLES, generate_key_from_string
+from .base import BaseTestIncrementalSerializer, BaseTestSerializerExtraData
+from .samples.json import BIG_JSON, SAMPLES
+
+# 'BIG_JSON' serialized is approximatively 7.5KiB long.
+TOO_BIG_JSON = BIG_JSON * 2
+
+TOO_BIG_JSON_SERIALIZED = json.dumps(TOO_BIG_JSON, ensure_ascii=False).encode("utf-8")
 
 
 @final
-@pytest.mark.feature_encryption
-class TestEncryptorSerializer(BaseTestIncrementalSerializer):
+class TestJSONSerializer(BaseTestIncrementalSerializer, BaseTestSerializerExtraData):
     #### Serializers
 
-    KEY = generate_key_from_string("key")
+    ENCODER_CONFIG = JSONEncoderConfig(ensure_ascii=False)
 
-    BUFFER_LIMIT = 1024
+    BUFFER_LIMIT = 14 * 1024  # 14KiB
 
-    @pytest.fixture(scope="class")
-    @classmethod
-    def serializer(cls) -> EncryptorSerializer[bytes, bytes]:
-        return EncryptorSerializer(NoSerialization(), key=cls.KEY, limit=cls.BUFFER_LIMIT)
+    assert len(TOO_BIG_JSON_SERIALIZED) > BUFFER_LIMIT
 
-    @pytest.fixture(scope="class")
+    @pytest.fixture(scope="class", params=[False, True], ids=lambda p: f"use_lines=={p}")
     @staticmethod
-    def serializer_for_serialization(serializer: EncryptorSerializer[bytes, bytes]) -> EncryptorSerializer[bytes, bytes]:
-        return serializer
+    def use_lines(request: Any) -> bool:
+        return request.param
 
     @pytest.fixture(scope="class")
-    @staticmethod
-    def serializer_for_deserialization(serializer: EncryptorSerializer[bytes, bytes]) -> EncryptorSerializer[bytes, bytes]:
-        return serializer
+    @classmethod
+    def serializer_for_serialization(cls, use_lines: bool) -> JSONSerializer:
+        return JSONSerializer(encoder_config=cls.ENCODER_CONFIG, use_lines=use_lines, limit=cls.BUFFER_LIMIT)
+
+    @pytest.fixture(scope="class")
+    @classmethod
+    def serializer_for_deserialization(cls, use_lines: bool) -> JSONSerializer:
+        return JSONSerializer(use_lines=use_lines, limit=cls.BUFFER_LIMIT)
 
     #### Packets to test
 
     @pytest.fixture(scope="class", params=[pytest.param(p, id=f"packet: {id}") for p, id in SAMPLES])
     @staticmethod
     def packet_to_serialize(request: Any) -> Any:
         return request.param
 
     #### One-shot Serialize
 
     @pytest.fixture(scope="class")
     @classmethod
-    def expected_complete_data(cls, packet_to_serialize: bytes) -> Callable[[bytes], None]:
-        from cryptography.fernet import Fernet
-
-        fernet = Fernet(cls.KEY)
-
-        def assert_encrypted(data: bytes) -> None:
-            assert fernet.decrypt(data) == packet_to_serialize
-
-        return assert_encrypted
+    def expected_complete_data(cls, packet_to_serialize: Any) -> bytes:
+        return json.dumps(packet_to_serialize, **dataclasses.asdict(cls.ENCODER_CONFIG), separators=(",", ":")).encode("utf-8")
 
     #### Incremental Serialize
 
     @pytest.fixture(scope="class")
-    @classmethod
-    def expected_joined_data(cls, packet_to_serialize: bytes) -> Callable[[bytes], None]:
-        from cryptography.fernet import Fernet
-
-        fernet = Fernet(cls.KEY)
-
-        def assert_encrypted(data: bytes) -> None:
-            assert data.endswith(b"\r\n")
-            assert fernet.decrypt(data.removesuffix(b"\r\n")) == packet_to_serialize
-
-        return assert_encrypted
+    @staticmethod
+    def expected_joined_data(expected_complete_data: bytes, use_lines: bool) -> bytes:
+        if use_lines or not expected_complete_data.startswith((b"{", b"[", b'"')):
+            return expected_complete_data + b"\n"
+        return expected_complete_data
 
     #### One-shot Deserialize
 
     @pytest.fixture(scope="class")
-    @classmethod
-    def complete_data(cls, packet_to_serialize: bytes) -> bytes:
-        from cryptography.fernet import Fernet
+    @staticmethod
+    def complete_data(packet_to_serialize: Any, use_lines: bool) -> bytes:
+        indent: int | None = None
+        if not use_lines:
+            # Test with indentation to see whitespace handling
+            indent = 4
 
-        return Fernet(cls.KEY).encrypt_at_time(packet_to_serialize, 0)
+        return json.dumps(packet_to_serialize, ensure_ascii=False, indent=indent).encode("utf-8")
 
     #### Incremental Deserialize
 
     @pytest.fixture(scope="class")
     @staticmethod
     def complete_data_for_incremental_deserialize(complete_data: bytes) -> bytes:
-        return complete_data + b"\r\n"
+        return complete_data + b"\n"
 
     #### Invalid data
 
-    @pytest.fixture(scope="class")
+    @pytest.fixture(scope="class", params=[b"invalid", b"\0", '{"é": 123}'.encode("latin-1")])
     @staticmethod
-    def invalid_complete_data(complete_data: bytes) -> bytes:
-        if not complete_data:
-            pytest.skip("empty bytes")
-        return complete_data[:-1]  # Remove one byte at last will break the padding
-
-    @pytest.fixture(scope="class", params=["missing_data", "limit_overrun_without_newline", "limit_overrun_with_newline"])
-    @classmethod
-    def invalid_partial_data(cls, request: pytest.FixtureRequest) -> bytes:
-        match request.param:
-            case "missing_data":
-                from cryptography.fernet import Fernet
-
-                return Fernet(cls.KEY).encrypt_at_time(b"a", 0)[:-1] + b"\r\n"
-            case "limit_overrun_without_newline":
-                return b"4" * (cls.BUFFER_LIMIT + 10)
-            case "limit_overrun_with_newline":
-                return b"4" * (cls.BUFFER_LIMIT + 10) + b"\r\n"
-            case _:
-                pytest.fail("Invalid fixture parameter")
+    def invalid_complete_data(request: Any, use_lines: bool) -> bytes:
+        data: bytes = getattr(request, "param")
+        if use_lines:
+            data += b"\n"
+        return data
+
+    @pytest.fixture(
+        scope="class",
+        params=[
+            b"[ invalid ]",
+            b"\0",
+            '{"é": 123}'.encode("latin-1"),
+            pytest.param(TOO_BIG_JSON_SERIALIZED[:-20], id="too_big_json_partial"),
+            pytest.param(TOO_BIG_JSON_SERIALIZED + b"\n", id="too_big_json_with_newline"),
+            pytest.param(b"4" * (BUFFER_LIMIT + 1024), id="too_big_raw_value_no_newline"),
+            pytest.param(b"4" * (BUFFER_LIMIT + 1024) + b"\n", id="too_big_raw_value_with_newline"),
+        ],
+    )
+    @classmethod
+    def invalid_partial_data(cls, request: Any, use_lines: bool) -> bytes:
+        data: bytes = getattr(request, "param")
+        if len(data) <= cls.BUFFER_LIMIT and use_lines:
+            data += b"\n"
+        return data
 
     @pytest.fixture(scope="class")
     @classmethod
-    def invalid_partial_data_extra_data(cls, invalid_partial_data: bytes) -> tuple[bytes, bytes]:
-        if len(invalid_partial_data) > cls.BUFFER_LIMIT and not invalid_partial_data.endswith(b"\r\n"):
+    def invalid_partial_data_extra_data(cls, invalid_partial_data: bytes, use_lines: bool) -> tuple[bytes, bytes]:
+        if len(invalid_partial_data) > cls.BUFFER_LIMIT:
+            if invalid_partial_data.endswith(b"\n"):
+                if use_lines:
+                    return (b"remaining_data", b"remaining_data")
+                else:
+                    return (b"remaining_data", b"\nremaining_data")
             return (b"remaining_data", b"")
+        if invalid_partial_data.startswith(b"\0"):
+            return (b"", b"")
         return (b"remaining_data", b"remaining_data")
-
-    #### Other
-
-    @pytest.fixture(scope="class")
-    @staticmethod
-    def oneshot_extra_data() -> bytes:
-        pytest.skip("Does not recognize extra data")
-
-    def test____generate_key____create_url_safe_base64_encoded_bytes(self) -> None:
-        # Arrange
-        from base64 import urlsafe_b64decode
-
-        # Act
-        key = EncryptorSerializer.generate_key()
-
-        # Assert
-        assert isinstance(key, bytes)
-        assert len(urlsafe_b64decode(key)) == 32
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `easynetwork-1.0.0rc7/tests/functional_test/test_serializers/test_line.py` & `easynetwork-1.0.0rc8/tests/functional_test/test_serializers/test_line.py`

 * *Files 6% similar despite different names*

```diff
@@ -4,25 +4,25 @@
 
 from typing import Literal, final
 
 from easynetwork.serializers.line import StringLineSerializer
 
 import pytest
 
-from .base import BaseTestIncrementalSerializer
+from .base import BaseTestBufferedIncrementalSerializer
 
 _NEWLINES: dict[str, bytes] = {
     "LF": b"\n",
     "CR": b"\r",
     "CRLF": b"\r\n",
 }
 
 
 @final
-class TestStringLineSerializer(BaseTestIncrementalSerializer):
+class TestStringLineSerializer(BaseTestBufferedIncrementalSerializer):
     #### Serializers
 
     BUFFER_LIMIT = 1024
 
     @pytest.fixture(scope="class", params=list(_NEWLINES))
     @staticmethod
     def newline(request: pytest.FixtureRequest) -> Literal["LF", "CR", "CRLF"]:
@@ -115,14 +115,7 @@
         cls,
         invalid_partial_data: bytes,
         newline: Literal["CR", "LF", "CRLF"],
     ) -> tuple[bytes, bytes]:
         if len(invalid_partial_data) > cls.BUFFER_LIMIT and not invalid_partial_data.endswith(_NEWLINES[newline]):
             return (b"remaining_data", b"")
         return (b"remaining_data", b"remaining_data")
-
-    #### Other
-
-    @pytest.fixture(scope="class")
-    @staticmethod
-    def oneshot_extra_data(newline: Literal["CR", "LF", "CRLF"]) -> bytes:
-        return _NEWLINES[newline] + b"remaining_data"
```

### Comparing `easynetwork-1.0.0rc7/tests/functional_test/test_serializers/test_msgpack.py` & `easynetwork-1.0.0rc8/tests/functional_test/test_serializers/test_msgpack.py`

 * *Files 14% similar despite different names*

```diff
@@ -5,21 +5,21 @@
 import dataclasses
 from typing import Any, final
 
 from easynetwork.serializers.msgpack import MessagePackerConfig, MessagePackSerializer
 
 import pytest
 
-from .base import BaseTestSerializer
+from .base import BaseTestSerializerExtraData
 from .samples.json import SAMPLES
 
 
 @final
 @pytest.mark.feature_msgpack
-class TestMessagePackSerializer(BaseTestSerializer):
+class TestMessagePackSerializer(BaseTestSerializerExtraData):
     #### Serializers
 
     @pytest.fixture(scope="class")
     @staticmethod
     def packer_config() -> MessagePackerConfig:
         return MessagePackerConfig()
```

### Comparing `easynetwork-1.0.0rc7/tests/functional_test/test_serializers/test_namedtuple.py` & `easynetwork-1.0.0rc8/tests/functional_test/test_serializers/test_namedtuple.py`

 * *Files 2% similar despite different names*

```diff
@@ -5,15 +5,15 @@
 import struct
 from typing import Any, NamedTuple, final
 
 from easynetwork.serializers.struct import NamedTupleStructSerializer
 
 import pytest
 
-from .base import BaseTestBufferedIncrementalSerializer
+from .base import BaseTestBufferedIncrementalSerializer, BaseTestSerializerExtraData
 
 
 class Point(NamedTuple):
     name: str
     x: int
     y: bytes
 
@@ -29,15 +29,15 @@
 
 
 def pack_point(p: Point, *, encoding: str = "utf-8") -> bytes:
     return struct.pack(STRUCT_FORMAT, p.name.encode(encoding), p.x, p.y)
 
 
 @final
-class TestNamedTupleStructSerializer(BaseTestBufferedIncrementalSerializer):
+class TestNamedTupleStructSerializer(BaseTestBufferedIncrementalSerializer, BaseTestSerializerExtraData):
     #### Serializers
 
     @pytest.fixture(scope="class")
     @classmethod
     def serializer(cls) -> NamedTupleStructSerializer[Point]:
         return NamedTupleStructSerializer(Point, POINT_FIELD_FORMATS)
```

### Comparing `easynetwork-1.0.0rc7/tests/functional_test/test_serializers/test_pickle.py` & `easynetwork-1.0.0rc8/tests/functional_test/test_serializers/test_pickle.py`

 * *Files 7% similar despite different names*

```diff
@@ -5,22 +5,22 @@
 import pickletools
 from typing import Any, final
 
 from easynetwork.serializers.pickle import PicklerConfig, PickleSerializer, UnpicklerConfig
 
 import pytest
 
-from .base import BaseTestSerializer
+from .base import BaseTestSerializerExtraData
 from .samples.pickle import SAMPLES
 
 ALL_PROTOCOLS: tuple[int, ...] = tuple(range(0, pickle.HIGHEST_PROTOCOL + 1))
 
 
 @final
-class TestPickleSerializer(BaseTestSerializer):
+class TestPickleSerializer(BaseTestSerializerExtraData):
     @pytest.fixture(scope="class", params=ALL_PROTOCOLS, ids=lambda p: f"pickle_data_protocol=={p}")
     @staticmethod
     def pickle_data_protocol(request: Any) -> int:
         return request.param
 
     #### Serializers
```

### Comparing `easynetwork-1.0.0rc7/tests/functional_test/test_serializers/samples/json.py` & `easynetwork-1.0.0rc8/tests/functional_test/test_serializers/samples/json.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/tests/functional_test/test_serializers/samples/pickle.py` & `easynetwork-1.0.0rc8/tests/functional_test/test_serializers/samples/pickle.py`

 * *Files 14% similar despite different names*

```diff
@@ -10,36 +10,31 @@
     def __eq__(self, other: object, /) -> bool:
         if not isinstance(other, Dummy):
             return NotImplemented
         return self.attr == other.attr
 
 
 class BigDummy:
-    def __init__(self, level: int) -> None:
-        assert level > 0
-        level -= 1
-        if level == 0:
-            self.dummy = {}
-        else:
-            self.dummy = {
-                "dummy1": BigDummy(level),
-                "dummy2": {
-                    "subdummy1": BigDummy(level),
-                    "subdummy2": [BigDummy(level) for _ in range(10)],
-                },
-                "dummy3": [
-                    {
-                        "subdummy1": [BigDummy(level) for _ in range(5)],
-                        "subdummy2": {
-                            "sub-subdummy": BigDummy(level),
-                        },
-                    }
-                    for _ in range(20)
-                ],
-            }
+    def __init__(self) -> None:
+        self.dummy = {
+            "dummy1": Dummy(),
+            "dummy2": {
+                "subdummy1": Dummy(),
+                "subdummy2": [Dummy() for _ in range(10)],
+            },
+            "dummy3": [
+                {
+                    "subdummy1": [Dummy() for _ in range(5)],
+                    "subdummy2": {
+                        "sub-subdummy": Dummy(),
+                    },
+                }
+                for _ in range(20)
+            ],
+        }
 
     def __eq__(self, other: object, /) -> bool:
         if not isinstance(other, BigDummy):
             return NotImplemented
         return self.dummy == other.dummy
 
 
@@ -80,9 +75,9 @@
                 float("+inf"),
                 float("-inf"),
                 None,
             ],
         },
         "json-like object",
     ),
-    (BigDummy(level=1), "big object"),
+    (BigDummy(), "big object"),
 ]
```

### Comparing `easynetwork-1.0.0rc7/tests/other_test/test_import.py` & `easynetwork-1.0.0rc8/tests/other_test/test_import.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/tests/other_test/test_project_metadata.py` & `easynetwork-1.0.0rc8/tests/other_test/test_project_metadata.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/tests/pytest_plugins/asyncio_event_loop.py` & `easynetwork-1.0.0rc8/tests/pytest_plugins/asyncio_event_loop.py`

 * *Files 14% similar despite different names*

```diff
@@ -14,15 +14,16 @@
     UVLOOP = "uvloop"
 
 
 ASYNCIO_EVENT_LOOP_OPTION = "asyncio_event_loop"
 
 
 def pytest_addoption(parser: pytest.Parser) -> None:
-    parser.addoption(
+    group = parser.getgroup("asyncio")
+    group.addoption(
         "--asyncio-event-loop",
         dest=ASYNCIO_EVENT_LOOP_OPTION,
         type=EventLoop,
         default=EventLoop.ASYNCIO,
         choices=list(map(str, EventLoop)),
     )
 
@@ -64,15 +65,14 @@
         case _:
             assert_never(event_loop)
 
 
 def pytest_configure(config: pytest.Config) -> None:
     _set_event_loop_policy_according_to_configuration(config)
     config.addinivalue_line("markers", "skipif_uvloop: Skip asyncio test if uvloop is used")
-    config.addinivalue_line("markers", "xfail_uvloop: Expected asyncio test to fail if uvloop is used")
 
 
 @pytest.hookimpl(trylast=True)
 def pytest_report_header(config: pytest.Config) -> str:
     return f"asyncio event-loop: {config.getoption(ASYNCIO_EVENT_LOOP_OPTION)}"
 
 
@@ -82,35 +82,21 @@
             pytest.mark.skipif(
                 config.getoption(ASYNCIO_EVENT_LOOP_OPTION) == EventLoop.UVLOOP,
                 reason="Skipped because uvloop runner is used",
             )
         )
 
 
-def _xfail_test_if_uvloop_is_used(config: pytest.Config, item: pytest.Item) -> None:
-    if item.get_closest_marker("xfail_uvloop") is not None:
-        item.add_marker(
-            pytest.mark.xfail(
-                config.getoption(ASYNCIO_EVENT_LOOP_OPTION) == EventLoop.UVLOOP,
-                reason="uvloop runner does not implement the needed function",
-                strict=True,
-                raises=NotImplementedError,
-            )
-        )
-
-
 def pytest_collection_modifyitems(config: pytest.Config, items: list[pytest.Item]) -> None:
     for item in items:
         _skip_test_if_uvloop_is_used(config, item)
-        _xfail_test_if_uvloop_is_used(config, item)
 
 
 @pytest.fixture
 def event_loop_name(pytestconfig: pytest.Config) -> EventLoop:
     return pytestconfig.getoption(ASYNCIO_EVENT_LOOP_OPTION)
 
 
 if TYPE_CHECKING:
 
     @pytest.fixture
-    def event_loop(event_loop: asyncio.AbstractEventLoop) -> asyncio.AbstractEventLoop:
-        ...
+    def event_loop(event_loop: asyncio.AbstractEventLoop) -> asyncio.AbstractEventLoop: ...
```

### Comparing `easynetwork-1.0.0rc7/tests/pytest_plugins/auto_markers.py` & `easynetwork-1.0.0rc8/tests/pytest_plugins/auto_markers.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/tests/pytest_plugins/extra_features.py` & `easynetwork-1.0.0rc8/tests/pytest_plugins/extra_features.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/tests/pytest_plugins/ssl_module.py` & `easynetwork-1.0.0rc8/tests/pytest_plugins/ssl_module.py`

 * *Files 20% similar despite different names*

```diff
@@ -4,19 +4,19 @@
 import importlib
 import ssl
 from typing import Any
 
 import pytest
 
 ALL_MODULES_WHICH_USE_SSL_MODULE: tuple[tuple[str, str], ...] = (
-    ("easynetwork.api_async.client.tcp", "_ssl_module"),
-    ("easynetwork.api_sync.client.tcp", "_ssl_module"),
+    ("easynetwork.clients.async_tcp", "_ssl_module"),
+    ("easynetwork.clients.tcp", "_ssl_module"),
     ("easynetwork.lowlevel.api_sync.transports.socket", "_ssl_module"),
+    ("easynetwork.lowlevel.api_async.transports.tls", "_ssl_module"),
     ("easynetwork.lowlevel._utils", "ssl"),
-    ("easynetwork.lowlevel.std_asyncio.backend", "ssl"),
 )
 
 
 @functools.cache
 def _verify_ssl_alias(module_name: str, alias_name: str) -> None:
     module = importlib.import_module(module_name)
     retrieved_ssl_module: Any = functools.reduce(getattr, filter(None, alias_name.split(".")), module)
```

### Comparing `easynetwork-1.0.0rc7/tests/scripts/async_client_test.py` & `easynetwork-1.0.0rc8/tests/scripts/async_client_test.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,16 @@
 from __future__ import annotations
 
 import argparse
 import asyncio
 import logging
 from collections.abc import Callable
 
-from easynetwork.api_async.client import AbstractAsyncNetworkClient, AsyncTCPNetworkClient, AsyncUDPNetworkClient
+from easynetwork.clients import AsyncTCPNetworkClient, AsyncUDPNetworkClient
+from easynetwork.clients.abc import AbstractAsyncNetworkClient
 from easynetwork.protocol import DatagramProtocol, StreamProtocol
 from easynetwork.serializers.line import StringLineSerializer
 
 logger = logging.getLogger("app")
 
 
 def create_tcp_client(port: int) -> AsyncTCPNetworkClient[str, str]:
```

### Comparing `easynetwork-1.0.0rc7/tests/scripts/async_server_test.py` & `easynetwork-1.0.0rc8/tests/scripts/async_server_test.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,33 +1,32 @@
 from __future__ import annotations
 
 import argparse
 import contextlib
 import logging
 from collections.abc import AsyncGenerator, Callable
 
-from easynetwork.api_async.server.abc import AbstractAsyncNetworkServer
-from easynetwork.api_async.server.handler import AsyncBaseClientInterface, AsyncDatagramRequestHandler, AsyncStreamRequestHandler
-from easynetwork.api_sync.server.abc import AbstractNetworkServer
-from easynetwork.api_sync.server.tcp import StandaloneTCPNetworkServer
-from easynetwork.api_sync.server.udp import StandaloneUDPNetworkServer
 from easynetwork.protocol import DatagramProtocol, StreamProtocol
 from easynetwork.serializers.line import StringLineSerializer
+from easynetwork.servers.abc import AbstractAsyncNetworkServer, AbstractNetworkServer
+from easynetwork.servers.handlers import AsyncBaseClientInterface, AsyncDatagramRequestHandler, AsyncStreamRequestHandler
+from easynetwork.servers.standalone_tcp import StandaloneTCPNetworkServer
+from easynetwork.servers.standalone_udp import StandaloneUDPNetworkServer
 
 PORT = 9000
 
 logger = logging.getLogger("app")
 
 
 class MyAsyncRequestHandler(AsyncStreamRequestHandler[str, str], AsyncDatagramRequestHandler[str, str]):
     async def service_init(self, exit_stack: contextlib.AsyncExitStack, server: AbstractAsyncNetworkServer) -> None:
         self.server = server
 
     async def handle(self, client: AsyncBaseClientInterface[str]) -> AsyncGenerator[None, str]:
-        from easynetwork.api_async.server.tcp import AsyncTCPNetworkServer
+        from easynetwork.servers.async_tcp import AsyncTCPNetworkServer
 
         request: str = yield
         logger.debug(f"Received {request!r} from {client!r}")
         match request:
             case "wait:":
                 request = (yield) + " after wait"
             case "self_kill:" if isinstance(self.server, AsyncTCPNetworkServer):
```

### Comparing `easynetwork-1.0.0rc7/tests/scripts/threaded_client_test.py` & `easynetwork-1.0.0rc8/tests/scripts/threaded_client_test.py`

 * *Files 11% similar despite different names*

```diff
@@ -2,15 +2,16 @@
 
 import argparse
 import concurrent.futures
 import logging
 import time
 from collections.abc import Callable
 
-from easynetwork.api_sync.client import AbstractNetworkClient, TCPNetworkClient, UDPNetworkClient
+from easynetwork.clients import TCPNetworkClient, UDPNetworkClient
+from easynetwork.clients.abc import AbstractNetworkClient
 from easynetwork.protocol import DatagramProtocol, StreamProtocol
 from easynetwork.serializers.line import StringLineSerializer
 
 logger = logging.getLogger("app")
 
 
 def create_tcp_client(port: int) -> TCPNetworkClient[str, str]:
```

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/conftest.py` & `easynetwork-1.0.0rc8/tests/unit_test/conftest.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 from __future__ import annotations
 
 from collections.abc import Callable
 from socket import AF_INET, IPPROTO_TCP, IPPROTO_UDP, SOCK_DGRAM, SOCK_STREAM, socket as Socket
-from ssl import SSLContext, SSLSocket
+from ssl import SSLContext, SSLObject, SSLSocket
 from typing import TYPE_CHECKING, Any
 
 from easynetwork.converter import AbstractPacketConverterComposite
 from easynetwork.exceptions import UnsupportedOperation
 from easynetwork.protocol import BufferedStreamReceiver, DatagramProtocol, StreamProtocol
 from easynetwork.serializers.abc import (
     AbstractIncrementalPacketSerializer,
@@ -139,14 +139,30 @@
 
 @pytest.fixture
 def mock_ssl_context(mock_ssl_context_factory: Callable[[], MagicMock]) -> MagicMock:
     return mock_ssl_context_factory()
 
 
 @pytest.fixture
+def mock_ssl_object_factory(mocker: MockerFixture) -> Callable[[], MagicMock]:
+    def factory() -> MagicMock:
+        mock_ssl_object = mocker.NonCallableMagicMock(spec=SSLObject)
+        mock_ssl_object.do_handshake.return_value = None
+        mock_ssl_object.unwrap.return_value = None
+        return mock_ssl_object
+
+    return factory
+
+
+@pytest.fixture
+def mock_ssl_object(mock_ssl_object_factory: Callable[[], MagicMock]) -> MagicMock:
+    return mock_ssl_object_factory()
+
+
+@pytest.fixture
 def mock_serializer_factory(mocker: MockerFixture) -> Callable[[], Any]:
     return lambda: mocker.NonCallableMagicMock(spec=AbstractPacketSerializer)
 
 
 @pytest.fixture
 def mock_serializer(mock_serializer_factory: Callable[[], Any]) -> Any:
     return mock_serializer_factory()
```

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_converter.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_converter.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_protocol.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_protocol.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_async/conftest.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_async/conftest.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_async/test_api/test_client/test_abc.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_async/test_api/test_client/test_abc.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 from __future__ import annotations
 
 import math
 from collections.abc import Iterator
 from typing import TYPE_CHECKING, Any, final
 
-from easynetwork.api_async.client.abc import AbstractAsyncNetworkClient
+from easynetwork.clients.abc import AbstractAsyncNetworkClient
 from easynetwork.lowlevel.socket import SocketAddress
 
 import pytest
 
 from .....tools import temporary_backend
 
 if TYPE_CHECKING:
```

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_async/test_api/test_client/test_tcp.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_async/test_api/test_client/test_tcp.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,33 +1,30 @@
 from __future__ import annotations
 
 import contextlib
 import errno
 import os
+import ssl
 from collections.abc import Generator
 from socket import AF_INET6, IPPROTO_TCP, SO_ERROR, SO_KEEPALIVE, SOL_SOCKET, TCP_NODELAY
 from typing import TYPE_CHECKING, Any
 
-from easynetwork.api_async.client.tcp import AsyncTCPNetworkClient
+from easynetwork.clients.async_tcp import AsyncTCPNetworkClient
 from easynetwork.exceptions import ClientClosedError, IncrementalDeserializeError, StreamProtocolParseError
 from easynetwork.lowlevel._stream import StreamDataConsumer
-from easynetwork.lowlevel.constants import (
-    CLOSED_SOCKET_ERRNOS,
-    DEFAULT_STREAM_BUFSIZE,
-    SSL_HANDSHAKE_TIMEOUT,
-    SSL_SHUTDOWN_TIMEOUT,
-)
+from easynetwork.lowlevel.api_async.transports.tls import AsyncTLSStreamTransport
+from easynetwork.lowlevel.constants import CLOSED_SOCKET_ERRNOS, DEFAULT_STREAM_BUFSIZE
 from easynetwork.lowlevel.socket import IPv4SocketAddress, IPv6SocketAddress, SocketProxy, _get_socket_extra
 from easynetwork.lowlevel.typed_attr import TypedAttributeProvider
 
 import pytest
 import pytest_asyncio
 
 if TYPE_CHECKING:
-    from unittest.mock import MagicMock
+    from unittest.mock import AsyncMock, MagicMock
 
     from pytest_mock import MockerFixture
 
 from ...._utils import AsyncDummyLock
 from ....base import UNSUPPORTED_FAMILIES
 from .base import BaseTestClient
 
@@ -52,14 +49,19 @@
         return ("remote_address", 5000)
 
     @pytest.fixture(autouse=True)
     @staticmethod
     def mock_ssl_create_default_context(mock_ssl_context: MagicMock, mocker: MockerFixture) -> MagicMock:
         return mocker.patch("ssl.create_default_context", autospec=True, return_value=mock_ssl_context)
 
+    @pytest.fixture(autouse=True)
+    @staticmethod
+    def mock_tls_wrap_transport(mocker: MockerFixture) -> AsyncMock:
+        return mocker.patch.object(AsyncTLSStreamTransport, "wrap", autospec=True)
+
     @pytest.fixture
     @staticmethod
     def mock_stream_data_consumer(mocker: MockerFixture) -> MagicMock:
         return mocker.NonCallableMagicMock(spec=StreamDataConsumer)
 
     @pytest.fixture(autouse=True)
     @staticmethod
@@ -99,24 +101,24 @@
     @pytest.fixture(autouse=True)
     @staticmethod
     def set_default_socket_mock_configuration(
         mock_tcp_socket: MagicMock,
         mock_backend: MagicMock,
         socket_family: int,
         mock_stream_socket_adapter: MagicMock,
+        mock_tls_wrap_transport: AsyncMock,
     ) -> None:
         mock_tcp_socket.family = socket_family
         mock_tcp_socket.getsockopt.return_value = 0  # Needed for tests dealing with send_packet()
         del mock_tcp_socket.sendall
         del mock_tcp_socket.recv
 
         mock_backend.create_tcp_connection.return_value = mock_stream_socket_adapter
-        mock_backend.create_ssl_over_tcp_connection.return_value = mock_stream_socket_adapter
         mock_backend.wrap_stream_socket.return_value = mock_stream_socket_adapter
-        mock_backend.wrap_ssl_over_stream_socket_client_side.return_value = mock_stream_socket_adapter
+        mock_tls_wrap_transport.return_value = mock_stream_socket_adapter
 
         mock_stream_socket_adapter.extra_attributes = _get_socket_extra(mock_tcp_socket, wrap_in_proxy=False)
         mock_stream_socket_adapter.extra.side_effect = TypedAttributeProvider.extra.__get__(mock_stream_socket_adapter)
 
     @pytest.fixture  # DO NOT set autouse=True
     @staticmethod
     def setup_producer_mock(mock_stream_protocol: MagicMock) -> None:
@@ -128,30 +130,26 @@
     @pytest.fixture  # DO NOT set autouse=True
     @staticmethod
     def setup_consumer_mock(mock_stream_data_consumer: MagicMock, mocker: MockerFixture) -> None:
         bytes_buffer: bytes = b""
 
         sentinel = mocker.sentinel
 
-        def feed_side_effect(chunk: bytes) -> None:
-            nonlocal bytes_buffer
-            bytes_buffer += chunk
-
-        def next_side_effect() -> Any:
+        def next_side_effect(chunk: bytes | None) -> Any:
             nonlocal bytes_buffer
+            if chunk is not None:
+                bytes_buffer += chunk
             data, separator, bytes_buffer = bytes_buffer.partition(b"\n")
             if not separator:
                 assert not bytes_buffer
                 bytes_buffer = data
                 raise StopIteration
             return getattr(sentinel, data.decode("ascii"))
 
-        mock_stream_data_consumer.feed.side_effect = feed_side_effect
-        mock_stream_data_consumer.__iter__.side_effect = lambda: mock_stream_data_consumer
-        mock_stream_data_consumer.__next__.side_effect = next_side_effect
+        mock_stream_data_consumer.next.side_effect = next_side_effect
 
     @pytest_asyncio.fixture
     @staticmethod
     async def client_not_connected(
         remote_address: tuple[str, int],
         mock_stream_protocol: MagicMock,
     ) -> AsyncTCPNetworkClient[Any, Any]:
@@ -175,18 +173,18 @@
     async def test____dunder_init____connect_to_remote(
         self,
         remote_address: tuple[str, int],
         mock_tcp_socket: MagicMock,
         mock_backend: MagicMock,
         mock_stream_data_consumer_cls: MagicMock,
         mock_stream_protocol: MagicMock,
+        mock_tls_wrap_transport: AsyncMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
-        mock_backend.create_ssl_over_tcp_connection.side_effect = AssertionError
 
         # Act
         client: AsyncTCPNetworkClient[Any, Any] = AsyncTCPNetworkClient(
             remote_address,
             protocol=mock_stream_protocol,
             local_address=mocker.sentinel.local_address,
             happy_eyeballs_delay=mocker.sentinel.happy_eyeballs_delay,
@@ -196,37 +194,40 @@
         # Assert
         mock_stream_data_consumer_cls.assert_called_once_with(mock_stream_protocol)
         mock_backend.create_tcp_connection.assert_awaited_once_with(
             *remote_address,
             local_address=mocker.sentinel.local_address,
             happy_eyeballs_delay=mocker.sentinel.happy_eyeballs_delay,
         )
+        mock_tls_wrap_transport.assert_not_awaited()
         assert mock_tcp_socket.mock_calls == [
             mocker.call.setsockopt(IPPROTO_TCP, TCP_NODELAY, True),
             mocker.call.setsockopt(SOL_SOCKET, SO_KEEPALIVE, True),
         ]
         assert isinstance(client.socket, SocketProxy)
 
     async def test____dunder_init____use_given_socket(
         self,
         mock_tcp_socket: MagicMock,
         mock_backend: MagicMock,
         mock_stream_data_consumer_cls: MagicMock,
         mock_stream_protocol: MagicMock,
+        mock_tls_wrap_transport: AsyncMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
 
         # Act
         client: AsyncTCPNetworkClient[Any, Any] = AsyncTCPNetworkClient(mock_tcp_socket, protocol=mock_stream_protocol)
         await client.wait_connected()
 
         # Assert
         mock_stream_data_consumer_cls.assert_called_once_with(mock_stream_protocol)
         mock_backend.wrap_stream_socket.assert_awaited_once_with(mock_tcp_socket)
+        mock_tls_wrap_transport.assert_not_awaited()
         assert mock_tcp_socket.mock_calls == [
             mocker.call.getpeername(),
             mocker.call.setsockopt(IPPROTO_TCP, TCP_NODELAY, True),
             mocker.call.setsockopt(SOL_SOCKET, SO_KEEPALIVE, True),
         ]
         assert isinstance(client.socket, SocketProxy)
 
@@ -265,14 +266,45 @@
             _ = AsyncTCPNetworkClient(
                 mock_tcp_socket,
                 protocol=mock_stream_protocol,
                 ssl=ssl_context,
                 server_hostname=server_hostname,
             )
 
+    async def test____dunder_init____invalid_first_argument____invalid_object(
+        self,
+        mock_stream_protocol: MagicMock,
+        mocker: MockerFixture,
+    ) -> None:
+        # Arrange
+        invalid_object = mocker.NonCallableMagicMock(spec=object)
+
+        # Act & Assert
+        with pytest.raises(TypeError, match=r"^Invalid arguments$"):
+            _ = AsyncTCPNetworkClient(
+                invalid_object,
+                protocol=mock_stream_protocol,
+            )
+
+    async def test____dunder_init____invalid_first_argument____invalid_host_port_pair(
+        self,
+        mock_stream_protocol: MagicMock,
+        mocker: MockerFixture,
+    ) -> None:
+        # Arrange
+        invalid_host = mocker.NonCallableMagicMock(spec=object)
+        invalid_port = mocker.NonCallableMagicMock(spec=object)
+
+        # Act & Assert
+        with pytest.raises(TypeError, match=r"^Invalid arguments$"):
+            _ = AsyncTCPNetworkClient(
+                (invalid_host, invalid_port),
+                protocol=mock_stream_protocol,
+            )
+
     @pytest.mark.parametrize("use_socket", [False, True], ids=lambda p: f"use_socket=={p}")
     async def test____dunder_init____protocol____invalid_value(
         self,
         request: pytest.FixtureRequest,
         use_socket: bool,
         mock_datagram_protocol: MagicMock,
     ) -> None:
@@ -347,94 +379,167 @@
             else:
                 _ = AsyncTCPNetworkClient(
                     request.getfixturevalue("remote_address"),
                     protocol=mock_stream_protocol,
                     max_recv_size=max_recv_size,
                 )
 
+    @pytest.mark.parametrize("manual_buffer_allocation", ["unknown", ""], ids=lambda p: f"manual_buffer_allocation=={p!r}")
+    @pytest.mark.parametrize("use_socket", [False, True], ids=lambda p: f"use_socket=={p}")
+    async def test____dunder_init____manual_buffer_allocation____invalid_value(
+        self,
+        request: pytest.FixtureRequest,
+        manual_buffer_allocation: Any,
+        use_socket: bool,
+        mock_stream_protocol: MagicMock,
+    ) -> None:
+        # Arrange
+
+        # Act & Assert
+        with pytest.raises(ValueError, match=r'^"manual_buffer_allocation" must be "try", "no" or "force"$'):
+            if use_socket:
+                _ = AsyncTCPNetworkClient(
+                    request.getfixturevalue("mock_tcp_socket"),
+                    mock_stream_protocol,
+                    manual_buffer_allocation=manual_buffer_allocation,
+                )
+            else:
+                _ = AsyncTCPNetworkClient(
+                    request.getfixturevalue("remote_address"),
+                    protocol=mock_stream_protocol,
+                    manual_buffer_allocation=manual_buffer_allocation,
+                )
+
     @pytest.mark.parametrize("use_socket", [False, True], ids=lambda p: f"use_socket=={p}")
     async def test____dunder_init____ssl(
         self,
         use_socket: bool,
         remote_address: tuple[str, int],
         mock_tcp_socket: MagicMock,
         mock_backend: MagicMock,
         mock_stream_data_consumer_cls: MagicMock,
         mock_stream_protocol: MagicMock,
         mock_ssl_context: MagicMock,
         mock_ssl_create_default_context: MagicMock,
+        mock_tls_wrap_transport: AsyncMock,
+        mock_stream_socket_adapter: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
-        mock_backend.create_tcp_connection.side_effect = AssertionError
-        mock_backend.wrap_stream_socket.side_effect = AssertionError
 
         # Act
         client: AsyncTCPNetworkClient[Any, Any]
         if use_socket:
             client = AsyncTCPNetworkClient(
                 mock_tcp_socket,
                 protocol=mock_stream_protocol,
                 ssl=mock_ssl_context,
                 server_hostname="server_hostname",
                 ssl_handshake_timeout=mocker.sentinel.ssl_handshake_timeout,
                 ssl_shutdown_timeout=mocker.sentinel.ssl_shutdown_timeout,
+                ssl_standard_compatible=mocker.sentinel.ssl_standard_compatible,
             )
         else:
             client = AsyncTCPNetworkClient(
                 remote_address,
                 protocol=mock_stream_protocol,
                 ssl=mock_ssl_context,
                 server_hostname="server_hostname",
                 ssl_handshake_timeout=mocker.sentinel.ssl_handshake_timeout,
                 ssl_shutdown_timeout=mocker.sentinel.ssl_shutdown_timeout,
+                ssl_standard_compatible=mocker.sentinel.ssl_standard_compatible,
                 local_address=mocker.sentinel.local_address,
                 happy_eyeballs_delay=mocker.sentinel.happy_eyeballs_delay,
             )
         await client.wait_connected()
 
         # Assert
         mock_stream_data_consumer_cls.assert_called_once_with(mock_stream_protocol)
         mock_ssl_create_default_context.assert_not_called()
         if use_socket:
-            mock_backend.wrap_ssl_over_stream_socket_client_side.assert_awaited_once_with(
-                mock_tcp_socket,
-                ssl_context=mock_ssl_context,
-                server_hostname="server_hostname",
-                ssl_handshake_timeout=mocker.sentinel.ssl_handshake_timeout,
-                ssl_shutdown_timeout=mocker.sentinel.ssl_shutdown_timeout,
-            )
+            mock_backend.wrap_stream_socket.assert_awaited_once_with(mock_tcp_socket)
             assert mock_tcp_socket.mock_calls == [
                 mocker.call.getpeername(),
                 mocker.call.setsockopt(IPPROTO_TCP, TCP_NODELAY, True),
                 mocker.call.setsockopt(SOL_SOCKET, SO_KEEPALIVE, True),
             ]
         else:
-            mock_backend.create_ssl_over_tcp_connection.assert_awaited_once_with(
+            mock_backend.create_tcp_connection.assert_awaited_once_with(
                 *remote_address,
-                ssl_context=mock_ssl_context,
-                server_hostname="server_hostname",
-                ssl_handshake_timeout=mocker.sentinel.ssl_handshake_timeout,
-                ssl_shutdown_timeout=mocker.sentinel.ssl_shutdown_timeout,
                 local_address=mocker.sentinel.local_address,
                 happy_eyeballs_delay=mocker.sentinel.happy_eyeballs_delay,
             )
             assert mock_tcp_socket.mock_calls == [
                 mocker.call.setsockopt(IPPROTO_TCP, TCP_NODELAY, True),
                 mocker.call.setsockopt(SOL_SOCKET, SO_KEEPALIVE, True),
             ]
+        mock_tls_wrap_transport.assert_awaited_once_with(
+            mock_stream_socket_adapter,
+            mock_ssl_context,
+            server_side=False,
+            server_hostname="server_hostname",
+            handshake_timeout=mocker.sentinel.ssl_handshake_timeout,
+            shutdown_timeout=mocker.sentinel.ssl_shutdown_timeout,
+            standard_compatible=mocker.sentinel.ssl_standard_compatible,
+        )
         assert isinstance(client.socket, SocketProxy)
 
     @pytest.mark.parametrize("use_socket", [False, True], ids=lambda p: f"use_socket=={p}")
+    async def test____dunder_init____ssl____default_values(
+        self,
+        use_socket: bool,
+        remote_address: tuple[str, int],
+        mock_tcp_socket: MagicMock,
+        mock_stream_protocol: MagicMock,
+        mock_ssl_context: MagicMock,
+        mock_tls_wrap_transport: AsyncMock,
+        mock_stream_socket_adapter: MagicMock,
+        mocker: MockerFixture,
+    ) -> None:
+        # Arrange
+
+        # Act
+        client: AsyncTCPNetworkClient[Any, Any]
+        if use_socket:
+            client = AsyncTCPNetworkClient(
+                mock_tcp_socket,
+                protocol=mock_stream_protocol,
+                ssl=mock_ssl_context,
+                server_hostname="server_hostname",
+            )
+        else:
+            client = AsyncTCPNetworkClient(
+                remote_address,
+                protocol=mock_stream_protocol,
+                ssl=mock_ssl_context,
+                server_hostname="server_hostname",
+                local_address=mocker.sentinel.local_address,
+                happy_eyeballs_delay=mocker.sentinel.happy_eyeballs_delay,
+            )
+        await client.wait_connected()
+
+        # Assert
+        mock_tls_wrap_transport.assert_awaited_once_with(
+            mock_stream_socket_adapter,
+            mock_ssl_context,
+            server_side=False,
+            server_hostname="server_hostname",
+            handshake_timeout=None,
+            shutdown_timeout=None,
+            standard_compatible=True,
+        )
+
+    @pytest.mark.parametrize("use_socket", [False, True], ids=lambda p: f"use_socket=={p}")
     @pytest.mark.parametrize(
         "ssl_parameter",
         [
             "server_hostname",
             "ssl_handshake_timeout",
             "ssl_shutdown_timeout",
+            "ssl_standard_compatible",
             "ssl_shared_lock",
         ],
     )
     async def test____dunder_init____ssl____useless_parameter_if_no_context(
         self,
         ssl_parameter: str,
         use_socket: bool,
@@ -460,133 +565,65 @@
                     remote_address,
                     protocol=mock_stream_protocol,
                     ssl=None,
                     **kwargs,
                 )
 
     @pytest.mark.parametrize("use_socket", [False, True], ids=lambda p: f"use_socket=={p}")
-    async def test____dunder_init____ssl____default_timeouts(
-        self,
-        use_socket: bool,
-        remote_address: tuple[str, int],
-        mock_tcp_socket: MagicMock,
-        mock_backend: MagicMock,
-        mock_stream_protocol: MagicMock,
-        mock_ssl_context: MagicMock,
-        mocker: MockerFixture,
-    ) -> None:
-        # Arrange
-        mock_backend.create_tcp_connection.side_effect = AssertionError
-        mock_backend.wrap_stream_socket.side_effect = AssertionError
-
-        # Act
-        client: AsyncTCPNetworkClient[Any, Any]
-        if use_socket:
-            client = AsyncTCPNetworkClient(
-                mock_tcp_socket,
-                protocol=mock_stream_protocol,
-                ssl=mock_ssl_context,
-                server_hostname="server_hostname",
-                ssl_handshake_timeout=None,
-                ssl_shutdown_timeout=None,
-            )
-        else:
-            client = AsyncTCPNetworkClient(
-                remote_address,
-                protocol=mock_stream_protocol,
-                ssl=mock_ssl_context,
-                server_hostname="server_hostname",
-                ssl_handshake_timeout=None,
-                ssl_shutdown_timeout=None,
-                local_address=mocker.sentinel.local_address,
-                happy_eyeballs_delay=mocker.sentinel.happy_eyeballs_delay,
-            )
-        await client.wait_connected()
-
-        # Assert
-        if use_socket:
-            mock_backend.wrap_ssl_over_stream_socket_client_side.assert_awaited_once_with(
-                mock_tcp_socket,
-                ssl_context=mock_ssl_context,
-                server_hostname="server_hostname",
-                ssl_handshake_timeout=SSL_HANDSHAKE_TIMEOUT,
-                ssl_shutdown_timeout=SSL_SHUTDOWN_TIMEOUT,
-            )
-        else:
-            mock_backend.create_ssl_over_tcp_connection.assert_awaited_once_with(
-                *remote_address,
-                ssl_context=mock_ssl_context,
-                server_hostname="server_hostname",
-                ssl_handshake_timeout=SSL_HANDSHAKE_TIMEOUT,
-                ssl_shutdown_timeout=SSL_SHUTDOWN_TIMEOUT,
-                local_address=mocker.sentinel.local_address,
-                happy_eyeballs_delay=mocker.sentinel.happy_eyeballs_delay,
-            )
-
-    @pytest.mark.parametrize("use_socket", [False, True], ids=lambda p: f"use_socket=={p}")
     async def test____dunder_init____ssl____server_hostname____do_not_disable_hostname_check_for_external_context(
         self,
         use_socket: bool,
         remote_address: tuple[str, int],
         mock_tcp_socket: MagicMock,
-        mock_backend: MagicMock,
         mock_stream_protocol: MagicMock,
         mock_ssl_context: MagicMock,
+        mock_tls_wrap_transport: AsyncMock,
+        mock_stream_socket_adapter: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         check_hostname_by_default: bool = mock_ssl_context.check_hostname
         assert check_hostname_by_default
 
-        mock_backend.create_tcp_connection.side_effect = AssertionError
-        mock_backend.wrap_stream_socket.side_effect = AssertionError
-
         # Act
         client: AsyncTCPNetworkClient[Any, Any]
         if use_socket:
             client = AsyncTCPNetworkClient(
                 mock_tcp_socket,
                 protocol=mock_stream_protocol,
                 ssl=mock_ssl_context,
                 server_hostname="",
                 ssl_handshake_timeout=mocker.sentinel.ssl_handshake_timeout,
                 ssl_shutdown_timeout=mocker.sentinel.ssl_shutdown_timeout,
+                ssl_standard_compatible=mocker.sentinel.ssl_standard_compatible,
             )
         else:
             client = AsyncTCPNetworkClient(
                 remote_address,
                 protocol=mock_stream_protocol,
                 ssl=mock_ssl_context,
                 server_hostname="",
                 ssl_handshake_timeout=mocker.sentinel.ssl_handshake_timeout,
                 ssl_shutdown_timeout=mocker.sentinel.ssl_shutdown_timeout,
+                ssl_standard_compatible=mocker.sentinel.ssl_standard_compatible,
                 local_address=mocker.sentinel.local_address,
                 happy_eyeballs_delay=mocker.sentinel.happy_eyeballs_delay,
             )
         await client.wait_connected()
 
         # Assert
-        if use_socket:
-            mock_backend.wrap_ssl_over_stream_socket_client_side.assert_awaited_once_with(
-                mock_tcp_socket,
-                ssl_context=mock_ssl_context,
-                server_hostname="",
-                ssl_handshake_timeout=mocker.sentinel.ssl_handshake_timeout,
-                ssl_shutdown_timeout=mocker.sentinel.ssl_shutdown_timeout,
-            )
-        else:
-            mock_backend.create_ssl_over_tcp_connection.assert_awaited_once_with(
-                *remote_address,
-                ssl_context=mock_ssl_context,
-                server_hostname="",
-                ssl_handshake_timeout=mocker.sentinel.ssl_handshake_timeout,
-                ssl_shutdown_timeout=mocker.sentinel.ssl_shutdown_timeout,
-                local_address=mocker.sentinel.local_address,
-                happy_eyeballs_delay=mocker.sentinel.happy_eyeballs_delay,
-            )
+        mock_tls_wrap_transport.assert_awaited_once_with(
+            mock_stream_socket_adapter,
+            mock_ssl_context,
+            server_side=False,
+            server_hostname=None,
+            handshake_timeout=mocker.sentinel.ssl_handshake_timeout,
+            shutdown_timeout=mocker.sentinel.ssl_shutdown_timeout,
+            standard_compatible=mocker.sentinel.ssl_standard_compatible,
+        )
         assert mock_ssl_context.check_hostname is True
 
     async def test____dunder_init____ssl____server_hostname____required_if_socket_is_given(
         self,
         mock_tcp_socket: MagicMock,
         mock_backend: MagicMock,
         mock_stream_protocol: MagicMock,
@@ -602,140 +639,124 @@
             _ = AsyncTCPNetworkClient(
                 mock_tcp_socket,
                 protocol=mock_stream_protocol,
                 ssl=mock_ssl_context,
                 server_hostname=None,
                 ssl_handshake_timeout=mocker.sentinel.ssl_handshake_timeout,
                 ssl_shutdown_timeout=mocker.sentinel.ssl_shutdown_timeout,
+                ssl_standard_compatible=mocker.sentinel.ssl_standard_compatible,
             )
 
     @pytest.mark.parametrize("use_socket", [False, True], ids=lambda p: f"use_socket=={p}")
     async def test____dunder_init____ssl____create_default_context(
         self,
         use_socket: bool,
         remote_address: tuple[str, int],
         mock_tcp_socket: MagicMock,
-        mock_backend: MagicMock,
         mock_stream_protocol: MagicMock,
         mock_ssl_context: MagicMock,
         mock_ssl_create_default_context: MagicMock,
+        mock_tls_wrap_transport: AsyncMock,
+        mock_stream_socket_adapter: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
-        mock_backend.create_tcp_connection.side_effect = AssertionError
-        mock_backend.wrap_stream_socket.side_effect = AssertionError
 
         # Act
         client: AsyncTCPNetworkClient[Any, Any]
         if use_socket:
             client = AsyncTCPNetworkClient(
                 mock_tcp_socket,
                 protocol=mock_stream_protocol,
                 ssl=True,
                 server_hostname="server_hostname",
                 ssl_handshake_timeout=mocker.sentinel.ssl_handshake_timeout,
                 ssl_shutdown_timeout=mocker.sentinel.ssl_shutdown_timeout,
+                ssl_standard_compatible=mocker.sentinel.ssl_standard_compatible,
             )
         else:
             client = AsyncTCPNetworkClient(
                 remote_address,
                 protocol=mock_stream_protocol,
                 ssl=True,
                 server_hostname="server_hostname",
                 ssl_handshake_timeout=mocker.sentinel.ssl_handshake_timeout,
                 ssl_shutdown_timeout=mocker.sentinel.ssl_shutdown_timeout,
+                ssl_standard_compatible=mocker.sentinel.ssl_standard_compatible,
                 local_address=mocker.sentinel.local_address,
                 happy_eyeballs_delay=mocker.sentinel.happy_eyeballs_delay,
             )
         await client.wait_connected()
 
         # Assert
         mock_ssl_create_default_context.assert_called_once_with()
-        if use_socket:
-            mock_backend.wrap_ssl_over_stream_socket_client_side.assert_awaited_once_with(
-                mock_tcp_socket,
-                ssl_context=mock_ssl_context,
-                server_hostname="server_hostname",
-                ssl_handshake_timeout=mocker.sentinel.ssl_handshake_timeout,
-                ssl_shutdown_timeout=mocker.sentinel.ssl_shutdown_timeout,
-            )
-        else:
-            mock_backend.create_ssl_over_tcp_connection.assert_awaited_once_with(
-                *remote_address,
-                ssl_context=mock_ssl_context,
-                server_hostname="server_hostname",
-                ssl_handshake_timeout=mocker.sentinel.ssl_handshake_timeout,
-                ssl_shutdown_timeout=mocker.sentinel.ssl_shutdown_timeout,
-                local_address=mocker.sentinel.local_address,
-                happy_eyeballs_delay=mocker.sentinel.happy_eyeballs_delay,
-            )
+        mock_tls_wrap_transport.assert_awaited_once_with(
+            mock_stream_socket_adapter,
+            mock_ssl_context,
+            server_side=False,
+            server_hostname="server_hostname",
+            handshake_timeout=mocker.sentinel.ssl_handshake_timeout,
+            shutdown_timeout=mocker.sentinel.ssl_shutdown_timeout,
+            standard_compatible=mocker.sentinel.ssl_standard_compatible,
+        )
 
     @pytest.mark.parametrize("use_socket", [False, True], ids=lambda p: f"use_socket=={p}")
     async def test____dunder_init____ssl____create_default_context____disable_hostname_check(
         self,
         use_socket: bool,
         remote_address: tuple[str, int],
         mock_tcp_socket: MagicMock,
-        mock_backend: MagicMock,
         mock_stream_protocol: MagicMock,
         mock_ssl_context: MagicMock,
         mock_ssl_create_default_context: MagicMock,
+        mock_tls_wrap_transport: AsyncMock,
+        mock_stream_socket_adapter: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         check_hostname_by_default: bool = mock_ssl_context.check_hostname
         assert check_hostname_by_default
 
-        mock_backend.create_tcp_connection.side_effect = AssertionError
-        mock_backend.wrap_stream_socket.side_effect = AssertionError
-
         # Act
         client: AsyncTCPNetworkClient[Any, Any]
         if use_socket:
             client = AsyncTCPNetworkClient(
                 mock_tcp_socket,
                 protocol=mock_stream_protocol,
                 ssl=True,
                 server_hostname="",
                 ssl_handshake_timeout=mocker.sentinel.ssl_handshake_timeout,
                 ssl_shutdown_timeout=mocker.sentinel.ssl_shutdown_timeout,
+                ssl_standard_compatible=mocker.sentinel.ssl_standard_compatible,
             )
         else:
             client = AsyncTCPNetworkClient(
                 remote_address,
                 protocol=mock_stream_protocol,
                 ssl=True,
                 server_hostname="",
                 ssl_handshake_timeout=mocker.sentinel.ssl_handshake_timeout,
                 ssl_shutdown_timeout=mocker.sentinel.ssl_shutdown_timeout,
+                ssl_standard_compatible=mocker.sentinel.ssl_standard_compatible,
                 local_address=mocker.sentinel.local_address,
                 happy_eyeballs_delay=mocker.sentinel.happy_eyeballs_delay,
             )
         await client.wait_connected()
 
         # Assert
         mock_ssl_create_default_context.assert_called_once_with()
-        if use_socket:
-            mock_backend.wrap_ssl_over_stream_socket_client_side.assert_awaited_once_with(
-                mock_tcp_socket,
-                ssl_context=mock_ssl_context,
-                server_hostname="",
-                ssl_handshake_timeout=mocker.sentinel.ssl_handshake_timeout,
-                ssl_shutdown_timeout=mocker.sentinel.ssl_shutdown_timeout,
-            )
-        else:
-            mock_backend.create_ssl_over_tcp_connection.assert_awaited_once_with(
-                *remote_address,
-                ssl_context=mock_ssl_context,
-                server_hostname="",
-                ssl_handshake_timeout=mocker.sentinel.ssl_handshake_timeout,
-                ssl_shutdown_timeout=mocker.sentinel.ssl_shutdown_timeout,
-                local_address=mocker.sentinel.local_address,
-                happy_eyeballs_delay=mocker.sentinel.happy_eyeballs_delay,
-            )
+        mock_tls_wrap_transport.assert_awaited_once_with(
+            mock_stream_socket_adapter,
+            mock_ssl_context,
+            server_side=False,
+            server_hostname=None,
+            handshake_timeout=mocker.sentinel.ssl_handshake_timeout,
+            shutdown_timeout=mocker.sentinel.ssl_shutdown_timeout,
+            standard_compatible=mocker.sentinel.ssl_standard_compatible,
+        )
         assert mock_ssl_context.check_hostname is False
 
     async def test____is_closing____connection_not_performed_yet(
         self,
         client_not_connected: AsyncTCPNetworkClient[Any, Any],
     ) -> None:
         # Arrange
@@ -1018,14 +1039,39 @@
         # Assert
         mock_stream_protocol.generate_chunks.assert_called_once_with(mocker.sentinel.packet)
         mock_stream_socket_adapter.send_all_from_iterable.assert_called_once()
         mock_stream_socket_adapter.send_all.assert_awaited_once_with(b"packet\n")
         mock_tcp_socket.getsockopt.assert_not_called()
 
     @pytest.mark.usefixtures("setup_producer_mock")
+    async def test____send_packet____unrelated_ssl_errors(
+        self,
+        client_connected_or_not: AsyncTCPNetworkClient[Any, Any],
+        mock_tcp_socket: MagicMock,
+        mock_stream_socket_adapter: MagicMock,
+        mock_stream_protocol: MagicMock,
+        mocker: MockerFixture,
+    ) -> None:
+        # Arrange
+        mock_stream_socket_adapter.send_all.side_effect = ssl.SSLError(
+            ssl.SSLErrorNumber.SSL_ERROR_INVALID_ERROR_CODE,
+            "SOMETHING",
+        )
+
+        # Act
+        with pytest.raises(ssl.SSLError):
+            await client_connected_or_not.send_packet(mocker.sentinel.packet)
+
+        # Assert
+        mock_stream_protocol.generate_chunks.assert_called_once_with(mocker.sentinel.packet)
+        mock_stream_socket_adapter.send_all_from_iterable.assert_called_once()
+        mock_stream_socket_adapter.send_all.assert_awaited_once_with(b"packet\n")
+        mock_tcp_socket.getsockopt.assert_not_called()
+
+    @pytest.mark.usefixtures("setup_producer_mock")
     @pytest.mark.parametrize("closed_socket_errno", sorted(CLOSED_SOCKET_ERRNOS), ids=errno.errorcode.__getitem__)
     async def test____send_packet____convert_closed_socket_error(
         self,
         closed_socket_errno: int,
         client_connected_or_not: AsyncTCPNetworkClient[Any, Any],
         mock_tcp_socket: MagicMock,
         mock_stream_socket_adapter: MagicMock,
@@ -1127,15 +1173,15 @@
         mock_stream_socket_adapter.recv.side_effect = [b"packet\n"]
 
         # Act
         packet: Any = await client_connected_or_not.recv_packet()
 
         # Assert
         mock_stream_socket_adapter.recv.assert_awaited_once_with(DEFAULT_STREAM_BUFSIZE)
-        mock_stream_data_consumer.feed.assert_called_once_with(b"packet\n")
+        assert mock_stream_data_consumer.next.call_args_list == [mocker.call(None), mocker.call(b"packet\n")]
         assert packet is mocker.sentinel.packet
 
     @pytest.mark.usefixtures("setup_consumer_mock")
     async def test____recv_packet____partial_data(
         self,
         client_connected_or_not: AsyncTCPNetworkClient[Any, Any],
         mock_stream_socket_adapter: MagicMock,
@@ -1148,15 +1194,15 @@
 
         # Act
         packet: Any = await client_connected_or_not.recv_packet()
 
         # Assert
         mock_backend.coro_yield.assert_not_awaited()
         assert mock_stream_socket_adapter.recv.call_args_list == [mocker.call(DEFAULT_STREAM_BUFSIZE) for _ in range(2)]
-        assert mock_stream_data_consumer.feed.call_args_list == [mocker.call(b"pac"), mocker.call(b"ket\n")]
+        assert mock_stream_data_consumer.next.call_args_list == [mocker.call(None), mocker.call(b"pac"), mocker.call(b"ket\n")]
         assert packet is mocker.sentinel.packet
 
     @pytest.mark.usefixtures("setup_consumer_mock")
     async def test____recv_packet____extra_data(
         self,
         client_connected_or_not: AsyncTCPNetworkClient[Any, Any],
         mock_stream_socket_adapter: MagicMock,
@@ -1169,15 +1215,19 @@
 
         # Act
         packet_1: Any = await client_connected_or_not.recv_packet()
         packet_2: Any = await client_connected_or_not.recv_packet()
 
         # Assert
         mock_stream_socket_adapter.recv.assert_awaited_once()
-        mock_stream_data_consumer.feed.assert_called_once_with(b"packet_1\npacket_2\n")
+        assert mock_stream_data_consumer.next.call_args_list == [
+            mocker.call(None),
+            mocker.call(b"packet_1\npacket_2\n"),
+            mocker.call(None),
+        ]
         mock_backend.coro_yield.assert_not_awaited()
         assert packet_1 is mocker.sentinel.packet_1
         assert packet_2 is mocker.sentinel.packet_2
 
     @pytest.mark.usefixtures("setup_consumer_mock")
     async def test____recv_packet____eof_error____default(
         self,
@@ -1191,39 +1241,60 @@
 
         # Act
         with pytest.raises(ConnectionAbortedError):
             _ = await client_connected_or_not.recv_packet()
 
         # Assert
         mock_stream_socket_adapter.recv.assert_awaited_once_with(DEFAULT_STREAM_BUFSIZE)
-        mock_stream_data_consumer.feed.assert_not_called()
+        mock_stream_data_consumer.next.assert_called_once_with(None)
+        mock_backend.coro_yield.assert_not_awaited()
+
+    @pytest.mark.usefixtures("setup_consumer_mock")
+    async def test____recv_packet____eof_error____ssl(
+        self,
+        client_connected_or_not: AsyncTCPNetworkClient[Any, Any],
+        mock_stream_socket_adapter: MagicMock,
+        mock_stream_data_consumer: MagicMock,
+        mock_backend: MagicMock,
+    ) -> None:
+        # Arrange
+        mock_stream_socket_adapter.recv.side_effect = ssl.SSLEOFError()
+
+        # Act
+        with pytest.raises(ConnectionAbortedError):
+            _ = await client_connected_or_not.recv_packet()
+
+        # Assert
+        mock_stream_socket_adapter.recv.assert_awaited_once_with(DEFAULT_STREAM_BUFSIZE)
+        mock_stream_data_consumer.next.assert_called_once_with(None)
         mock_backend.coro_yield.assert_not_awaited()
 
     @pytest.mark.usefixtures("setup_consumer_mock")
     async def test____recv_packet____protocol_parse_error(
         self,
         client_connected_or_not: AsyncTCPNetworkClient[Any, Any],
         mock_stream_socket_adapter: MagicMock,
         mock_backend: MagicMock,
         mock_stream_data_consumer: MagicMock,
+        mocker: MockerFixture,
     ) -> None:
         # Arrange
 
         mock_stream_socket_adapter.recv.side_effect = [b"packet\n"]
         expected_error = StreamProtocolParseError(b"", IncrementalDeserializeError("Sorry", b""))
-        mock_stream_data_consumer.__next__.side_effect = [StopIteration, expected_error]
+        mock_stream_data_consumer.next.side_effect = [StopIteration, expected_error]
 
         # Act
         with pytest.raises(StreamProtocolParseError) as exc_info:
             _ = await client_connected_or_not.recv_packet()
         exception = exc_info.value
 
         # Assert
         mock_stream_socket_adapter.recv.assert_awaited_once_with(DEFAULT_STREAM_BUFSIZE)
-        mock_stream_data_consumer.feed.assert_called_once_with(b"packet\n")
+        assert mock_stream_data_consumer.next.call_args_list == [mocker.call(None), mocker.call(b"packet\n")]
         mock_backend.coro_yield.assert_not_awaited()
         assert exception is expected_error
 
     @pytest.mark.usefixtures("setup_consumer_mock")
     async def test____recv_packet____closed_client_error(
         self,
         client_connected_or_not: AsyncTCPNetworkClient[Any, Any],
@@ -1236,15 +1307,15 @@
         assert client_connected_or_not.is_closing()
 
         # Act
         with pytest.raises(ClientClosedError):
             _ = await client_connected_or_not.recv_packet()
 
         # Assert
-        mock_stream_data_consumer.feed.assert_not_called()
+        mock_stream_data_consumer.next.assert_not_called()
         mock_stream_socket_adapter.recv.assert_not_called()
         mock_backend.coro_yield.assert_not_awaited()
 
     @pytest.mark.usefixtures("setup_consumer_mock")
     async def test____recv_packet____unexpected_socket_close(
         self,
         client_connected_or_not: AsyncTCPNetworkClient[Any, Any],
@@ -1256,15 +1327,15 @@
         mock_stream_socket_adapter.is_closing.return_value = True
 
         # Act
         with pytest.raises(ClientClosedError):
             _ = await client_connected_or_not.recv_packet()
 
         # Assert
-        mock_stream_data_consumer.feed.assert_not_called()
+        mock_stream_data_consumer.next.assert_not_called()
         mock_stream_socket_adapter.recv.assert_not_called()
         mock_backend.coro_yield.assert_not_awaited()
 
     @pytest.mark.usefixtures("setup_consumer_mock")
     async def test____recv_packet____convert_connection_errors(
         self,
         client_connected_or_not: AsyncTCPNetworkClient[Any, Any],
@@ -1277,15 +1348,38 @@
 
         # Act
         with pytest.raises(ConnectionAbortedError):
             _ = await client_connected_or_not.recv_packet()
 
         # Assert
         mock_stream_socket_adapter.recv.assert_awaited_once_with(DEFAULT_STREAM_BUFSIZE)
-        mock_stream_data_consumer.feed.assert_not_called()
+        mock_stream_data_consumer.next.assert_called_once_with(None)
+        mock_backend.coro_yield.assert_not_awaited()
+
+    @pytest.mark.usefixtures("setup_consumer_mock")
+    async def test____recv_packet____unrelated_ssl_error(
+        self,
+        client_connected_or_not: AsyncTCPNetworkClient[Any, Any],
+        mock_stream_socket_adapter: MagicMock,
+        mock_stream_data_consumer: MagicMock,
+        mock_backend: MagicMock,
+    ) -> None:
+        # Arrange
+        mock_stream_socket_adapter.recv.side_effect = ssl.SSLError(
+            ssl.SSLErrorNumber.SSL_ERROR_INVALID_ERROR_CODE,
+            "SOMETHING",
+        )
+
+        # Act
+        with pytest.raises(ssl.SSLError):
+            _ = await client_connected_or_not.recv_packet()
+
+        # Assert
+        mock_stream_socket_adapter.recv.assert_awaited_once_with(DEFAULT_STREAM_BUFSIZE)
+        mock_stream_data_consumer.next.assert_called_once_with(None)
         mock_backend.coro_yield.assert_not_awaited()
 
     @pytest.mark.usefixtures("setup_consumer_mock")
     @pytest.mark.parametrize("closed_socket_errno", sorted(CLOSED_SOCKET_ERRNOS), ids=errno.errorcode.__getitem__)
     async def test____recv_packet____convert_closed_socket_errors(
         self,
         closed_socket_errno: int,
@@ -1299,15 +1393,15 @@
 
         # Act
         with pytest.raises(ClientClosedError):
             _ = await client_connected_or_not.recv_packet()
 
         # Assert
         mock_stream_socket_adapter.recv.assert_awaited_once_with(DEFAULT_STREAM_BUFSIZE)
-        mock_stream_data_consumer.feed.assert_not_called()
+        mock_stream_data_consumer.next.assert_called_once_with(None)
         mock_backend.coro_yield.assert_not_awaited()
 
     @pytest.mark.usefixtures("setup_producer_mock", "setup_consumer_mock")
     async def test____special_case____send_packet____eof_error____still_try_socket_send(
         self,
         client_connected_or_not: AsyncTCPNetworkClient[Any, Any],
         mock_stream_socket_adapter: MagicMock,
```

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_async/test_api/test_client/test_udp.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_async/test_api/test_client/test_udp.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 from __future__ import annotations
 
 import errno
 import os
 from socket import AF_INET6, AF_UNSPEC, SO_ERROR, SOL_SOCKET
 from typing import TYPE_CHECKING, Any
 
-from easynetwork.api_async.client.udp import AsyncUDPNetworkClient
+from easynetwork.clients.async_udp import AsyncUDPNetworkClient
 from easynetwork.exceptions import ClientClosedError, DatagramProtocolParseError, DeserializeError
 from easynetwork.lowlevel.constants import CLOSED_SOCKET_ERRNOS
 from easynetwork.lowlevel.socket import IPv4SocketAddress, IPv6SocketAddress, SocketProxy, _get_socket_extra
 from easynetwork.lowlevel.typed_attr import TypedAttributeProvider
 
 import pytest
 import pytest_asyncio
@@ -141,14 +141,15 @@
 
         # Assert
         mock_backend.create_udp_endpoint.assert_awaited_once_with(
             *remote_address,
             local_address=mocker.sentinel.local_address,
         )
         assert mock_udp_socket.mock_calls == [
+            mocker.call.fileno(),
             mocker.call.getsockname(),
         ]
         assert isinstance(client.socket, SocketProxy)
 
     async def test____dunder_init____with_remote_address____socket_family(
         self,
         remote_address: tuple[str, int],
@@ -259,14 +260,15 @@
 
         # Assert
         mock_udp_socket.bind.assert_not_called()
         mock_backend.wrap_connected_datagram_socket.assert_awaited_once_with(mock_udp_socket)
         assert mock_udp_socket.mock_calls == [
             mocker.call.getpeername(),
             mocker.call.getsockname(),
+            mocker.call.fileno(),
             mocker.call.getsockname(),
         ]
         assert isinstance(client.socket, SocketProxy)
 
     async def test____dunder_init____use_given_socket____error_no_remote_address(
         self,
         mock_udp_socket: MagicMock,
@@ -310,14 +312,45 @@
         # Act & Assert
         with pytest.raises(ValueError, match=r"^Only these families are supported: .+$"):
             _ = AsyncUDPNetworkClient(
                 mock_udp_socket,
                 mock_datagram_protocol,
             )
 
+    async def test____dunder_init____invalid_first_argument____invalid_object(
+        self,
+        mock_datagram_protocol: MagicMock,
+        mocker: MockerFixture,
+    ) -> None:
+        # Arrange
+        invalid_object = mocker.NonCallableMagicMock(spec=object)
+
+        # Act & Assert
+        with pytest.raises(TypeError, match=r"^Invalid arguments$"):
+            _ = AsyncUDPNetworkClient(
+                invalid_object,
+                protocol=mock_datagram_protocol,
+            )
+
+    async def test____dunder_init____invalid_first_argument____invalid_host_port_pair(
+        self,
+        mock_datagram_protocol: MagicMock,
+        mocker: MockerFixture,
+    ) -> None:
+        # Arrange
+        invalid_host = mocker.NonCallableMagicMock(spec=object)
+        invalid_port = mocker.NonCallableMagicMock(spec=object)
+
+        # Act & Assert
+        with pytest.raises(TypeError, match=r"^Invalid arguments$"):
+            _ = AsyncUDPNetworkClient(
+                (invalid_host, invalid_port),
+                protocol=mock_datagram_protocol,
+            )
+
     @pytest.mark.parametrize("use_socket", [False, True], ids=lambda p: f"use_socket=={p}")
     async def test____dunder_init____protocol____invalid_value(
         self,
         use_socket: bool,
         remote_address: tuple[str, int],
         mock_udp_socket: MagicMock,
         mock_stream_protocol: MagicMock,
```

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_async/test_api/test_server/conftest.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_async/test_api/test_server/conftest.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 from __future__ import annotations
 
 from typing import TYPE_CHECKING
 
-from easynetwork.api_async.server.handler import (
+from easynetwork.servers.handlers import (
     AsyncDatagramClient,
     AsyncDatagramRequestHandler,
     AsyncStreamClient,
     AsyncStreamRequestHandler,
 )
 
 import pytest
```

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_async/test_api/test_server/test_handler.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_async/test_api/test_server/test_handler.py`

 * *Files 9% similar despite different names*

```diff
@@ -3,17 +3,17 @@
 from __future__ import annotations
 
 import contextlib
 import inspect
 from collections.abc import AsyncGenerator
 from typing import TYPE_CHECKING, Any
 
-from easynetwork.api_async.server.handler import AsyncBaseClientInterface, AsyncDatagramRequestHandler, AsyncStreamRequestHandler
-from easynetwork.api_async.server.tcp import AsyncTCPNetworkServer
-from easynetwork.api_async.server.udp import AsyncUDPNetworkServer
+from easynetwork.servers.async_tcp import AsyncTCPNetworkServer
+from easynetwork.servers.async_udp import AsyncUDPNetworkServer
+from easynetwork.servers.handlers import AsyncBaseClientInterface, AsyncDatagramRequestHandler, AsyncStreamRequestHandler
 
 import pytest
 
 if TYPE_CHECKING:
     from unittest.mock import MagicMock
 
     from pytest_mock import MockerFixture
```

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_async/test_api/test_server/test_tcp.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_async/test_api/test_server/test_udp.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,35 +1,35 @@
 from __future__ import annotations
 
 from typing import TYPE_CHECKING
 
-from easynetwork.api_async.server.tcp import AsyncTCPNetworkServer
+from easynetwork.servers.async_udp import AsyncUDPNetworkServer
 
 import pytest
 
 if TYPE_CHECKING:
     from unittest.mock import MagicMock
 
 
 @pytest.mark.asyncio
-class TestAsyncTCPNetworkServer:
+class TestAsyncUDPNetworkServer:
     async def test____dunder_init____protocol____invalid_value(
         self,
-        mock_datagram_protocol: MagicMock,
-        mock_stream_request_handler: MagicMock,
+        mock_stream_protocol: MagicMock,
+        mock_datagram_request_handler: MagicMock,
     ) -> None:
         # Arrange
 
         # Act & Assert
-        with pytest.raises(TypeError, match=r"^Expected a StreamProtocol object, got .*$"):
-            _ = AsyncTCPNetworkServer(None, 0, mock_datagram_protocol, mock_stream_request_handler)
+        with pytest.raises(TypeError, match=r"^Expected a DatagramProtocol object, got .*$"):
+            _ = AsyncUDPNetworkServer("localhost", 0, mock_stream_protocol, mock_datagram_request_handler)
 
     async def test____dunder_init____request_handler____invalid_value(
         self,
-        mock_stream_protocol: MagicMock,
-        mock_datagram_request_handler: MagicMock,
+        mock_datagram_protocol: MagicMock,
+        mock_stream_request_handler: MagicMock,
     ) -> None:
         # Arrange
 
         # Act & Assert
-        with pytest.raises(TypeError, match=r"^Expected an AsyncStreamRequestHandler object, got .*$"):
-            _ = AsyncTCPNetworkServer(None, 0, mock_stream_protocol, mock_datagram_request_handler)
+        with pytest.raises(TypeError, match=r"^Expected an AsyncDatagramRequestHandler object, got .*$"):
+            _ = AsyncUDPNetworkServer("localhost", 0, mock_datagram_protocol, mock_stream_request_handler)
```

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_async/test_asyncio_backend/conftest.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_async/test_asyncio_backend/conftest.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_async/test_asyncio_backend/test_datagram.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_async/test_asyncio_backend/test_datagram.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,48 +1,48 @@
 from __future__ import annotations
 
 import asyncio
+import logging
 from collections.abc import Callable
 from errno import ECONNABORTED
 from socket import AI_PASSIVE
 from typing import TYPE_CHECKING, Any, Literal, cast
 
-from easynetwork.lowlevel.api_async.transports.abc import AsyncBaseTransport
-from easynetwork.lowlevel.constants import MAX_DATAGRAM_BUFSIZE
+from easynetwork.lowlevel.api_async.backend.abc import TaskGroup
+from easynetwork.lowlevel.api_async.backend.factory import current_async_backend
 from easynetwork.lowlevel.socket import SocketAttribute
 from easynetwork.lowlevel.std_asyncio.datagram.endpoint import (
     DatagramEndpoint,
     DatagramEndpointProtocol,
     create_datagram_endpoint,
 )
-from easynetwork.lowlevel.std_asyncio.datagram.listener import (
-    AsyncioTransportDatagramListenerSocketAdapter,
-    RawDatagramListenerSocketAdapter,
-)
-from easynetwork.lowlevel.std_asyncio.datagram.socket import AsyncioTransportDatagramSocketAdapter, RawDatagramSocketAdapter
+from easynetwork.lowlevel.std_asyncio.datagram.listener import DatagramListenerProtocol, DatagramListenerSocketAdapter
+from easynetwork.lowlevel.std_asyncio.datagram.socket import AsyncioTransportDatagramSocketAdapter
 
 import pytest
 
 if TYPE_CHECKING:
     from unittest.mock import AsyncMock, MagicMock
 
     from pytest_mock import MockerFixture
 
+from ..._utils import partial_eq
 from ...base import BaseTestSocket
 
 
 class CustomException(Exception):
     """Helper to test exception_queue usage"""
 
 
 @pytest.mark.asyncio
 @pytest.mark.parametrize("local_address", ["local_address", None], ids=lambda p: f"local_address=={p}")
 @pytest.mark.parametrize("remote_address", ["remote_address", None], ids=lambda p: f"remote_address=={p}")
 @pytest.mark.parametrize("reuse_port", [False, True], ids=lambda p: f"reuse_port=={p}")
 async def test____create_datagram_endpoint____return_DatagramEndpoint_instance(
+    event_loop: asyncio.AbstractEventLoop,
     local_address: Any | None,
     remote_address: Any | None,
     reuse_port: bool,
     mocker: MockerFixture,
 ) -> None:
     # Arrange
     if local_address is not None:
@@ -52,15 +52,15 @@
     mock_DatagramEndpoint = mocker.patch(
         f"{create_datagram_endpoint.__module__}.DatagramEndpoint",
         return_value=mocker.sentinel.endpoint,
     )
     mock_loop_create_datagram_endpoint: AsyncMock = cast(
         "AsyncMock",
         mocker.patch.object(
-            asyncio.get_running_loop(),
+            event_loop,
             "create_datagram_endpoint",
             new_callable=mocker.AsyncMock,
             return_value=(
                 mocker.sentinel.transport,
                 mocker.sentinel.protocol,
             ),
         ),
@@ -76,15 +76,15 @@
         remote_addr=remote_address,
         reuse_port=reuse_port,
         sock=mocker.sentinel.stdlib_socket,
     )
 
     # Assert
     mock_loop_create_datagram_endpoint.assert_awaited_once_with(
-        mocker.ANY,  # protocol_factory
+        partial_eq(DatagramEndpointProtocol, loop=event_loop, recv_queue=mocker.ANY, exception_queue=mocker.ANY),
         family=mocker.sentinel.socket_family,
         local_addr=local_address,
         remote_addr=remote_address,
         reuse_port=reuse_port,
         sock=mocker.sentinel.stdlib_socket,
         flags=expected_flags,
     )
@@ -140,24 +140,14 @@
         return DatagramEndpoint(
             mock_asyncio_transport,
             mock_asyncio_protocol,
             recv_queue=mock_asyncio_recv_queue,
             exception_queue=mock_asyncio_exception_queue,
         )
 
-    async def test____transport____property(
-        self,
-        endpoint: DatagramEndpoint,
-        mock_asyncio_transport: MagicMock,
-    ) -> None:
-        # Arrange
-
-        # Act & Assert
-        assert endpoint.transport is mock_asyncio_transport
-
     @pytest.mark.parametrize("transport_is_closing", [False, True], ids=lambda p: f"transport_is_closing=={p}")
     async def test____aclose____close_transport_and_wait(
         self,
         transport_is_closing: bool,
         endpoint: DatagramEndpoint,
         mock_asyncio_transport: MagicMock,
         mock_asyncio_protocol: MagicMock,
@@ -249,47 +239,56 @@
         mock_asyncio_transport.is_closing.return_value = True
         mock_asyncio_recv_queue.get_nowait.return_value = (b"some data", ("an_address", 12345))
 
         # Act
         data, address = await endpoint.recvfrom()
 
         # Assert
-        mock_asyncio_exception_queue.get_nowait.assert_called_once()
+        mock_asyncio_exception_queue.get_nowait.assert_not_called()
         mock_asyncio_recv_queue.get.assert_not_awaited()
         mock_asyncio_recv_queue.get_nowait.assert_called_once()
         assert data == b"some data"
         assert address == ("an_address", 12345)
 
     @pytest.mark.parametrize("condition", ["empty_queue", "None_pushed"])
+    @pytest.mark.parametrize("exception", [CustomException, None])
     async def test____recvfrom____connection_lost____transport_already_closed____no_more_data(
         self,
         endpoint: DatagramEndpoint,
+        exception: type[CustomException] | None,
         condition: Literal["empty_queue", "None_pushed"],
         mock_asyncio_transport: MagicMock,
         mock_asyncio_recv_queue: MagicMock,
         mock_asyncio_exception_queue: MagicMock,
     ) -> None:
         # Arrange
-        mock_asyncio_exception_queue.get_nowait.side_effect = asyncio.QueueEmpty
+        if exception is None:
+            mock_asyncio_exception_queue.get_nowait.side_effect = asyncio.QueueEmpty
+        else:
+            mock_asyncio_exception_queue.get_nowait.return_value = exception()
         mock_asyncio_transport.is_closing.return_value = True
 
         match condition:
             case "empty_queue":
                 mock_asyncio_recv_queue.get_nowait.side_effect = asyncio.QueueEmpty
             case "None_pushed":
                 mock_asyncio_recv_queue.get_nowait.side_effect = [None]
             case _:
                 pytest.fail("Invalid condition")
 
-        # Act
-        with pytest.raises(OSError) as exc_info:
-            await endpoint.recvfrom()
+        # Act & Assert
+        if exception is None:
+            with pytest.raises(OSError) as exc_info:
+                await endpoint.recvfrom()
+            assert exc_info.value.errno == ECONNABORTED
+        else:
+            with pytest.raises(exception):
+                await endpoint.recvfrom()
 
         # Assert
-        assert exc_info.value.errno == ECONNABORTED
         mock_asyncio_exception_queue.get_nowait.assert_called_once()
         mock_asyncio_recv_queue.get.assert_not_awaited()
         mock_asyncio_recv_queue.get_nowait.assert_called_once()
 
     async def test____recvfrom____connection_lost____transport_closed_by_protocol_while_waiting(
         self,
         endpoint: DatagramEndpoint,
@@ -306,112 +305,61 @@
 
         # Act
         with pytest.raises(OSError) as exc_info:
             await endpoint.recvfrom()
 
         # Assert
         assert exc_info.value.errno == ECONNABORTED
-        assert len(mock_asyncio_exception_queue.get_nowait.call_args_list) == 2
-        mock_asyncio_recv_queue.get.assert_awaited_once_with()
-
-    async def test____recvfrom____raise_exception____protocol_already_sent_exception_in_queue(
-        self,
-        endpoint: DatagramEndpoint,
-        mock_asyncio_recv_queue: MagicMock,
-        mock_asyncio_exception_queue: MagicMock,
-    ) -> None:
-        # Arrange
-        mock_asyncio_exception_queue.get_nowait.return_value = CustomException()
-
-        # Act
-        with pytest.raises(CustomException):
-            await endpoint.recvfrom()
-
-        # Assert
         mock_asyncio_exception_queue.get_nowait.assert_called_once_with()
-        mock_asyncio_recv_queue.get.assert_not_awaited()
+        mock_asyncio_recv_queue.get.assert_awaited_once_with()
 
     async def test____recvfrom____raise_exception____transport_closed_by_protocol_with_exception_while_waiting(
         self,
         endpoint: DatagramEndpoint,
         mock_asyncio_transport: MagicMock,
         mock_asyncio_recv_queue: MagicMock,
         mock_asyncio_exception_queue: MagicMock,
     ) -> None:
         # Arrange
-        from itertools import count
-
         mock_asyncio_recv_queue.get.return_value = None  # None is sent to queue when readers must wake up
-
-        _count = count()
-
-        # Cannot use list for side_effect because returned value is an exception instance...
-        def get_nowait_side_effect() -> Exception:
-            if next(_count) == 0:
-                raise asyncio.QueueEmpty
-            return CustomException()
-
-        mock_asyncio_exception_queue.get_nowait.side_effect = get_nowait_side_effect
-
+        mock_asyncio_exception_queue.get_nowait.return_value = CustomException()
         mock_asyncio_transport.is_closing.side_effect = [False, True]  # 1st call OK, 2nd not so much
 
         # Act
         with pytest.raises(CustomException):
             await endpoint.recvfrom()
 
         # Assert
-        assert len(mock_asyncio_exception_queue.get_nowait.call_args_list) == 2
+        mock_asyncio_exception_queue.get_nowait.assert_called_once_with()
         mock_asyncio_recv_queue.get.assert_awaited_once_with()
 
     @pytest.mark.parametrize("address", [("127.0.0.1", 12345), None], ids=repr)
+    @pytest.mark.parametrize("transport_is_closing", [False, True], ids=lambda p: f"transport_is_closing=={p}")
     async def test____sendto____send_and_await_drain(
         self,
+        transport_is_closing: bool,
         endpoint: DatagramEndpoint,
         address: tuple[str, int] | None,
         mock_asyncio_transport: MagicMock,
         mock_asyncio_protocol: MagicMock,
         mock_asyncio_exception_queue: MagicMock,
     ) -> None:
         # Arrange
+        mock_asyncio_transport.is_closing.side_effect = [transport_is_closing]
         mock_asyncio_exception_queue.get_nowait.side_effect = asyncio.QueueEmpty
 
         # Act
         await endpoint.sendto(b"some data", address)
 
         # Assert
-        mock_asyncio_exception_queue.get_nowait.assert_called_once_with()
+        mock_asyncio_exception_queue.get_nowait.assert_not_called()
         mock_asyncio_transport.sendto.assert_called_once_with(b"some data", address)
         mock_asyncio_protocol._drain_helper.assert_awaited_once_with()
 
-    @pytest.mark.parametrize("address", [("127.0.0.1", 12345), None], ids=repr)
-    async def test____sendto____transport_already_closed(
-        self,
-        endpoint: DatagramEndpoint,
-        address: tuple[str, int] | None,
-        mock_asyncio_transport: MagicMock,
-        mock_asyncio_protocol: MagicMock,
-        mock_asyncio_exception_queue: MagicMock,
-    ) -> None:
-        # Arrange
-        from errno import ECONNABORTED
-
-        mock_asyncio_transport.is_closing.return_value = True
-        mock_asyncio_exception_queue.get_nowait.side_effect = asyncio.QueueEmpty
-
-        # Act
-        with pytest.raises(OSError) as exc_info:
-            await endpoint.sendto(b"some data", address)
-
-        # Assert
-        assert exc_info.value.errno == ECONNABORTED
-        mock_asyncio_exception_queue.get_nowait.assert_called_once()
-        mock_asyncio_transport.sendto.assert_not_called()
-        mock_asyncio_protocol._drain_helper.assert_not_awaited()
-
-    async def test____get_extra_info____get_transport_extra_info(
+    async def test____extra_attributes____get_transport_extra_info(
         self,
         endpoint: DatagramEndpoint,
         mock_asyncio_transport: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         mock_asyncio_transport.get_extra_info.return_value = mocker.sentinel.extra_info
@@ -477,14 +425,25 @@
 
         # Act
         protocol = DatagramEndpointProtocol(recv_queue=mock_asyncio_recv_queue, exception_queue=mock_asyncio_exception_queue)
 
         # Assert
         assert protocol._get_loop() is event_loop
 
+    def test____dunder_init____use_running_loop____not_in_asyncio_loop(
+        self,
+        mock_asyncio_recv_queue: MagicMock,
+        mock_asyncio_exception_queue: MagicMock,
+    ) -> None:
+        # Arrange
+
+        # Act & Assert
+        with pytest.raises(RuntimeError):
+            _ = DatagramEndpointProtocol(recv_queue=mock_asyncio_recv_queue, exception_queue=mock_asyncio_exception_queue)
+
     def test____connection_lost____by_closed_transport(
         self,
         protocol: DatagramEndpointProtocol,
         mock_asyncio_transport: MagicMock,
         mock_asyncio_recv_queue: MagicMock,
         mock_asyncio_exception_queue: MagicMock,
     ) -> None:
@@ -496,15 +455,15 @@
         protocol.connection_lost(None)
         protocol.connection_lost(None)  # Double call must not change anything
 
         # Assert
         assert close_waiter.done() and close_waiter.exception() is None and close_waiter.result() is None
         mock_asyncio_recv_queue.put_nowait.assert_called_once_with(None)
         mock_asyncio_exception_queue.put_nowait.assert_not_called()
-        mock_asyncio_transport.close.assert_called_once_with()  # just to be sure :)
+        mock_asyncio_transport.close.assert_not_called()  # just to be sure :)
 
     def test____connection_lost____by_unrelated_error(
         self,
         protocol: DatagramEndpointProtocol,
         mock_asyncio_transport: MagicMock,
         mock_asyncio_recv_queue: MagicMock,
         mock_asyncio_exception_queue: MagicMock,
@@ -519,30 +478,30 @@
         protocol.connection_lost(exception)
         protocol.connection_lost(exception)  # Double call must not change anything
 
         # Assert
         assert close_waiter.done() and close_waiter.exception() is None
         mock_asyncio_recv_queue.put_nowait.assert_called_once_with(None)
         mock_asyncio_exception_queue.put_nowait.assert_called_once_with(exception)
-        mock_asyncio_transport.close.assert_called_once_with()  # just to be sure :)
+        mock_asyncio_transport.close.assert_not_called()  # just to be sure :)
 
     def test____datagram_received____push_to_queue(
         self,
         protocol: DatagramEndpointProtocol,
         mock_asyncio_recv_queue: MagicMock,
     ) -> None:
         # Arrange
 
         # Act
         protocol.datagram_received(b"datagram", ("an_address", 12345))
 
         # Assert
         mock_asyncio_recv_queue.put_nowait.assert_called_once_with((b"datagram", ("an_address", 12345)))
 
-    def test____datagram_received____do_not_push_to_queue_ater_connection_lost(
+    def test____datagram_received____do_not_push_to_queue_after_connection_lost(
         self,
         protocol: DatagramEndpointProtocol,
         mock_asyncio_recv_queue: MagicMock,
     ) -> None:
         # Arrange
         protocol.connection_lost(None)
         mock_asyncio_recv_queue.put_nowait.reset_mock()  # Needed to use assert_not_called()
@@ -565,15 +524,15 @@
         # Act
         protocol.error_received(exception)
 
         # Assert
         mock_asyncio_exception_queue.put_nowait.assert_called_once_with(exception)
         mock_asyncio_recv_queue.put_nowait.assert_called_once_with(None)
 
-    def test____error_received____do_not_push_to_queue_ater_connection_lost(
+    def test____error_received____do_not_push_to_queue_after_connection_lost(
         self,
         protocol: DatagramEndpointProtocol,
         mock_asyncio_recv_queue: MagicMock,
         mock_asyncio_exception_queue: MagicMock,
     ) -> None:
         # Arrange
         exception = OSError("Something bad happen")
@@ -680,58 +639,94 @@
         protocol.connection_lost(exception)
         await asyncio.sleep(0)  # Suspend to let the event loop run all tasks
 
         # Assert
         if cancel_tasks:
             assert all(t.done() and t.cancelled() for t in tasks)
         elif exception is None:
-            assert all(t.done() and t.exception() is None and t.result() is None for t in tasks), tasks
+            assert all(t.done() and isinstance(t.exception(), ConnectionAbortedError) for t in tasks), tasks
         else:
             assert all(t.done() and t.exception() is exception for t in tasks)
 
 
 @pytest.mark.asyncio
-class BaseTestAsyncioTransportDatagramSocket(BaseTestSocket):
+class BaseTestAsyncioDatagramTransport(BaseTestSocket):
+    @pytest.fixture
+    @classmethod
+    def mock_udp_socket(cls, mock_udp_socket: MagicMock, remote_address: tuple[Any, ...] | None) -> MagicMock:
+        cls.set_local_address_to_socket_mock(mock_udp_socket, mock_udp_socket.family, ("127.0.0.1", 11111))
+        if remote_address is None:
+            cls.configure_socket_mock_to_raise_ENOTCONN(mock_udp_socket)
+        else:
+            cls.set_remote_address_to_socket_mock(mock_udp_socket, mock_udp_socket.family, remote_address)
+        return mock_udp_socket
+
+    @pytest.fixture
+    @staticmethod
+    def asyncio_transport_extra_info(mock_udp_socket: MagicMock, remote_address: tuple[Any, ...] | None) -> dict[str, Any]:
+        return {
+            "socket": mock_udp_socket,
+            "sockname": mock_udp_socket.getsockname.return_value,
+            "peername": remote_address,
+        }
+
+    @pytest.fixture
+    @staticmethod
+    def mock_asyncio_transport(asyncio_transport_extra_info: dict[str, Any], mocker: MockerFixture) -> MagicMock:
+        mock = mocker.NonCallableMagicMock(spec=asyncio.DatagramTransport)
+        mock.get_extra_info.side_effect = asyncio_transport_extra_info.get
+        mock.is_closing.return_value = False
+        return mock
+
+    @pytest.fixture
+    @staticmethod
+    def endpoint_extra_info(asyncio_transport_extra_info: dict[str, Any]) -> dict[str, Any]:
+        return asyncio_transport_extra_info
+
     @pytest.fixture
     @staticmethod
     def mock_endpoint(
         endpoint_extra_info: dict[str, Any],
         mock_datagram_endpoint_factory: Callable[[], MagicMock],
     ) -> MagicMock:
         mock = mock_datagram_endpoint_factory()
         mock.get_extra_info.side_effect = endpoint_extra_info.get
         return mock
 
+
+@pytest.mark.asyncio
+class TestAsyncioTransportDatagramSocketAdapter(BaseTestAsyncioDatagramTransport):
     @pytest.fixture
     @classmethod
-    def socket(cls, mock_endpoint: MagicMock) -> AsyncBaseTransport:
-        return cls.socket_factory(mock_endpoint)
+    def remote_address(cls) -> tuple[Any, ...] | None:
+        return ("127.0.0.1", 12345)
 
+    @pytest.fixture
     @classmethod
-    def socket_factory(cls, mock_endpoint: MagicMock) -> AsyncBaseTransport:
-        raise NotImplementedError
+    def socket(cls, mock_endpoint: MagicMock) -> AsyncioTransportDatagramSocketAdapter:
+        return AsyncioTransportDatagramSocketAdapter(mock_endpoint)
 
     async def test____aclose____close_transport_and_wait(
         self,
-        socket: AsyncBaseTransport,
+        socket: AsyncioTransportDatagramSocketAdapter,
         mock_endpoint: MagicMock,
     ) -> None:
         # Arrange
 
         # Act
         await socket.aclose()
 
         # Assert
         mock_endpoint.aclose.assert_awaited_once_with()
 
     @pytest.mark.parametrize("transport_closed", [False, True], ids=lambda p: f"transport_closed=={p}")
     async def test____is_closing____return_internal_flag(
         self,
         transport_closed: bool,
-        socket: AsyncBaseTransport,
+        socket: AsyncioTransportDatagramSocketAdapter,
         mock_endpoint: MagicMock,
     ) -> None:
         # Arrange
         if transport_closed:
             await socket.aclose()
             mock_endpoint.reset_mock()
         mock_endpoint.is_closing.side_effect = AssertionError
@@ -739,37 +734,14 @@
         # Act
         state = socket.is_closing()
 
         # Assert
         mock_endpoint.is_closing.assert_not_called()
         assert state is transport_closed
 
-
-@pytest.mark.asyncio
-class TestAsyncioTransportDatagramSocketAdapter(BaseTestAsyncioTransportDatagramSocket):
-    @pytest.fixture
-    @classmethod
-    def mock_udp_socket(cls, mock_udp_socket: MagicMock) -> MagicMock:
-        cls.set_local_address_to_socket_mock(mock_udp_socket, mock_udp_socket.family, ("127.0.0.1", 11111))
-        cls.set_remote_address_to_socket_mock(mock_udp_socket, mock_udp_socket.family, ("127.0.0.1", 12345))
-        return mock_udp_socket
-
-    @pytest.fixture
-    @staticmethod
-    def endpoint_extra_info(mock_udp_socket: MagicMock) -> dict[str, Any]:
-        return {
-            "socket": mock_udp_socket,
-            "sockname": mock_udp_socket.getsockname.return_value,
-            "peername": mock_udp_socket.getpeername.return_value,
-        }
-
-    @classmethod
-    def socket_factory(cls, mock_endpoint: MagicMock) -> AsyncioTransportDatagramSocketAdapter:
-        return AsyncioTransportDatagramSocketAdapter(mock_endpoint)
-
     async def test____recv____read_from_reader(
         self,
         socket: AsyncioTransportDatagramSocketAdapter,
         mock_endpoint: MagicMock,
     ) -> None:
         # Arrange
         received_data = b"data"
@@ -791,277 +763,494 @@
 
         # Act
         await socket.send(b"data to send")
 
         # Assert
         mock_endpoint.sendto.assert_awaited_once_with(b"data to send", None)
 
-    async def test____get_extra_info____returns_socket_info(
+    async def test____extra_attributes____returns_socket_info(
         self,
         socket: AsyncioTransportDatagramSocketAdapter,
         mock_udp_socket: MagicMock,
     ) -> None:
         # Arrange
 
         # Act & Assert
         assert socket.extra(SocketAttribute.socket) is mock_udp_socket
         assert socket.extra(SocketAttribute.family) == mock_udp_socket.family
         assert socket.extra(SocketAttribute.sockname) == ("127.0.0.1", 11111)
         assert socket.extra(SocketAttribute.peername) == ("127.0.0.1", 12345)
 
 
 @pytest.mark.asyncio
-class TestAsyncioTransportDatagramListenerSocketAdapter(BaseTestAsyncioTransportDatagramSocket):
+class TestDatagramListenerSocketAdapter(BaseTestAsyncioDatagramTransport):
+    @pytest.fixture
+    @staticmethod
+    def mock_endpoint() -> Any:  # type: ignore[override]
+        raise ValueError("Do not use this fixture here")
+
     @pytest.fixture
     @classmethod
-    def mock_udp_socket(cls, mock_udp_socket: MagicMock) -> MagicMock:
-        cls.set_local_address_to_socket_mock(mock_udp_socket, mock_udp_socket.family, ("127.0.0.1", 11111))
-        cls.configure_socket_mock_to_raise_ENOTCONN(mock_udp_socket)
-        return mock_udp_socket
+    def remote_address(cls) -> tuple[Any, ...] | None:
+        return None
 
     @pytest.fixture
     @staticmethod
-    def endpoint_extra_info(mock_udp_socket: MagicMock) -> dict[str, Any]:
-        return {
-            "socket": mock_udp_socket,
-            "sockname": mock_udp_socket.getsockname.return_value,
-            "peername": None,
-        }
+    def mock_asyncio_protocol(mocker: MockerFixture, event_loop: asyncio.AbstractEventLoop) -> MagicMock:
+        mock = mocker.NonCallableMagicMock(spec=DatagramListenerProtocol)
+        # Currently, _get_close_waiter() is a synchronous function returning a Future, but it will be awaited so this works
+        mock._get_close_waiter = mocker.AsyncMock()
+        mock._get_loop.return_value = event_loop
+        return mock
 
-    @classmethod
-    def socket_factory(cls, mock_endpoint: MagicMock) -> AsyncioTransportDatagramListenerSocketAdapter:
-        return AsyncioTransportDatagramListenerSocketAdapter(mock_endpoint)
+    @pytest.fixture
+    @staticmethod
+    def socket(
+        mock_asyncio_transport: MagicMock,
+        mock_asyncio_protocol: MagicMock,
+    ) -> DatagramListenerSocketAdapter:
+        return DatagramListenerSocketAdapter(mock_asyncio_transport, mock_asyncio_protocol)
 
-    async def test____recv_from____read_from_reader(
+    @pytest.mark.parametrize("transport_is_closing", [False, True], ids=lambda p: f"transport_is_closing=={p}")
+    async def test____aclose____close_transport_and_wait(
         self,
-        socket: AsyncioTransportDatagramListenerSocketAdapter,
-        mock_endpoint: MagicMock,
+        transport_is_closing: bool,
+        socket: DatagramListenerSocketAdapter,
+        mock_asyncio_transport: MagicMock,
+        mock_asyncio_protocol: MagicMock,
     ) -> None:
         # Arrange
-        received_data = b"data"
-        mock_endpoint.recvfrom.return_value = (received_data, ("127.0.0.1", 12345))
+        mock_asyncio_transport.is_closing.return_value = transport_is_closing
 
         # Act
-        data, address = await socket.recv_from()
+        await socket.aclose()
 
         # Assert
-        mock_endpoint.recvfrom.assert_awaited_once_with()
-        assert data is received_data  # Should not be copied
-        assert address == ("127.0.0.1", 12345)
+        if transport_is_closing:
+            mock_asyncio_transport.close.assert_not_called()
+            mock_asyncio_protocol._get_close_waiter.assert_awaited_once_with()
+        else:
+            mock_asyncio_transport.close.assert_called_once_with()
+            mock_asyncio_protocol._get_close_waiter.assert_awaited_once_with()
+        mock_asyncio_transport.abort.assert_not_called()
+
+    @pytest.mark.parametrize("transport_is_closing", [False, True], ids=lambda p: f"transport_is_closing=={p}")
+    async def test____aclose____abort_transport_if_cancelled(
+        self,
+        transport_is_closing: bool,
+        socket: DatagramListenerSocketAdapter,
+        mock_asyncio_transport: MagicMock,
+        mock_asyncio_protocol: MagicMock,
+    ) -> None:
+        # Arrange
+        mock_asyncio_transport.is_closing.return_value = transport_is_closing
+        mock_asyncio_protocol._get_close_waiter.side_effect = asyncio.CancelledError
+
+        # Act
+        with pytest.raises(asyncio.CancelledError):
+            await socket.aclose()
+
+        # Assert
+        if transport_is_closing:
+            mock_asyncio_transport.close.assert_not_called()
+            mock_asyncio_protocol._get_close_waiter.assert_awaited_once_with()
+        else:
+            mock_asyncio_transport.close.assert_called_once_with()
+            mock_asyncio_protocol._get_close_waiter.assert_awaited_once_with()
+        mock_asyncio_transport.abort.assert_not_called()
+
+    @pytest.mark.parametrize("transport_closed", [False, True], ids=lambda p: f"transport_closed=={p}")
+    async def test____is_closing____return_internal_flag(
+        self,
+        transport_closed: bool,
+        socket: DatagramListenerSocketAdapter,
+        mock_asyncio_transport: MagicMock,
+    ) -> None:
+        # Arrange
+        if transport_closed:
+            await socket.aclose()
+            mock_asyncio_transport.reset_mock()
+        mock_asyncio_transport.is_closing.side_effect = AssertionError
+
+        # Act
+        state = socket.is_closing()
+
+        # Assert
+        mock_asyncio_transport.is_closing.assert_not_called()
+        assert state is transport_closed
+
+    @pytest.mark.parametrize("external_group", [True, False], ids=lambda p: f"external_group=={p}")
+    async def test____serve____task_group(
+        self,
+        external_group: bool,
+        socket: DatagramListenerSocketAdapter,
+        mock_asyncio_protocol: MagicMock,
+        mocker: MockerFixture,
+    ) -> None:
+        # Arrange
+        mock_task_group = mocker.NonCallableMagicMock(spec=TaskGroup)
+        mock_task_group.__aenter__.return_value = mock_task_group
+        mock_task_group.start_soon.return_value = None
+        mock_AsyncIOTaskGroup = mocker.patch(
+            f"{DatagramListenerSocketAdapter.__module__}.AsyncIOTaskGroup",
+            side_effect=[mock_task_group],
+        )
+        datagram_received_cb = mocker.async_stub()
+        mock_asyncio_protocol.serve.side_effect = asyncio.CancelledError
 
+        # Act
+        with pytest.raises(asyncio.CancelledError):
+            if external_group:
+                await socket.serve(datagram_received_cb, mock_task_group)
+            else:
+                await socket.serve(datagram_received_cb)
+
+        # Assert
+        if external_group:
+            mock_AsyncIOTaskGroup.assert_not_called()
+            mock_task_group.__aenter__.assert_not_awaited()
+        else:
+            mock_AsyncIOTaskGroup.assert_called_once_with()
+            mock_task_group.__aenter__.assert_awaited_once()
+
+    @pytest.mark.parametrize("transport_is_closing", [False, True], ids=lambda p: f"transport_is_closing=={p}")
     async def test____send_to____write_and_drain(
         self,
-        socket: AsyncioTransportDatagramListenerSocketAdapter,
-        mock_endpoint: MagicMock,
+        transport_is_closing: bool,
+        socket: DatagramListenerSocketAdapter,
+        mock_asyncio_transport: MagicMock,
+        mock_asyncio_protocol: MagicMock,
     ) -> None:
         # Arrange
+        address: tuple[str, int] = ("127.0.0.1", 12345)
+        mock_asyncio_transport.is_closing.side_effect = [transport_is_closing]
 
         # Act
-        await socket.send_to(b"data to send", ("127.0.0.1", 12345))
+        await socket.send_to(b"data to send", address)
 
         # Assert
-        mock_endpoint.sendto.assert_awaited_once_with(b"data to send", ("127.0.0.1", 12345))
+        mock_asyncio_transport.sendto.assert_called_once_with(b"data to send", address)
+        mock_asyncio_protocol.writer_drain.assert_awaited_once_with()
 
-    async def test____get_extra_info____returns_socket_info(
+    async def test____extra_attributes____returns_socket_info(
         self,
-        socket: AsyncioTransportDatagramListenerSocketAdapter,
+        socket: DatagramListenerSocketAdapter,
         mock_udp_socket: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
 
         # Act & Assert
         assert socket.extra(SocketAttribute.socket) is mock_udp_socket
         assert socket.extra(SocketAttribute.family) == mock_udp_socket.family
         assert socket.extra(SocketAttribute.sockname) == ("127.0.0.1", 11111)
         assert socket.extra(SocketAttribute.peername, mocker.sentinel.no_value) is mocker.sentinel.no_value
 
 
-@pytest.mark.asyncio
-class BaseTestRawDatagramSocket(BaseTestSocket):
+class TestDatagramListenerProtocol:
     @pytest.fixture
     @staticmethod
-    def mock_async_socket(
-        mock_async_socket: MagicMock,
-        mock_udp_socket: MagicMock,
-    ) -> MagicMock:
-        mock_async_socket.socket = mock_udp_socket
-        return mock_async_socket
+    def mock_asyncio_transport(mocker: MockerFixture) -> MagicMock:
+        mock = mocker.NonCallableMagicMock(spec=asyncio.DatagramTransport)
+        mock.is_closing.return_value = False
 
-    @pytest.fixture
-    @classmethod
-    def socket(cls, mock_udp_socket: MagicMock, event_loop: asyncio.AbstractEventLoop) -> AsyncBaseTransport:
-        return cls.socket_factory(mock_udp_socket, event_loop)
+        # Tell connection_made() not to try to monkeypatch this mock object
+        del mock._address
 
-    @classmethod
-    def socket_factory(cls, mock_udp_socket: MagicMock, event_loop: asyncio.AbstractEventLoop) -> AsyncBaseTransport:
-        raise NotImplementedError
+        return mock
 
-    async def test____dunder_init____invalid_socket_type(
+    @pytest.fixture
+    @staticmethod
+    def protocol(
+        event_loop: asyncio.AbstractEventLoop,
+        mock_asyncio_transport: MagicMock,
+    ) -> DatagramListenerProtocol:
+        protocol = DatagramListenerProtocol(loop=event_loop)
+        protocol.connection_made(mock_asyncio_transport)
+        return protocol
+
+    @pytest.mark.asyncio
+    async def test____dunder_init____use_running_loop(
         self,
         event_loop: asyncio.AbstractEventLoop,
-        mock_tcp_socket: MagicMock,
     ) -> None:
         # Arrange
 
+        # Act
+        protocol = DatagramListenerProtocol()
+
+        # Assert
+        assert protocol._get_loop() is event_loop
+
+    def test____dunder_init____use_running_loop____not_in_asyncio_loop(self) -> None:
+        # Arrange
+
         # Act & Assert
-        with pytest.raises(ValueError, match=r"^A 'SOCK_DGRAM' socket is expected$"):
-            _ = self.socket_factory(mock_tcp_socket, event_loop)
+        with pytest.raises(RuntimeError):
+            _ = DatagramListenerProtocol()
 
-    async def test____is_closing____default(
+    def test____connection_lost____by_closed_transport(
         self,
-        socket: AsyncBaseTransport,
-        mock_async_socket: MagicMock,
-        mocker: MockerFixture,
+        protocol: DatagramListenerProtocol,
+        mock_asyncio_transport: MagicMock,
     ) -> None:
         # Arrange
-        mock_async_socket.is_closing.return_value = mocker.sentinel.is_closing
+        close_waiter = protocol._get_close_waiter()
+        assert not close_waiter.done()
 
         # Act
-        state = socket.is_closing()
+        protocol.connection_lost(None)
+        protocol.connection_lost(None)  # Double call must not change anything
 
         # Assert
-        assert state is mocker.sentinel.is_closing
-        mock_async_socket.is_closing.assert_called_once_with()
+        assert close_waiter.done() and close_waiter.exception() is None and close_waiter.result() is None
+        mock_asyncio_transport.close.assert_not_called()  # just to be sure :)
 
-    async def test____aclose____close_socket(
+    def test____connection_lost____by_unrelated_error(
         self,
-        socket: AsyncBaseTransport,
-        mock_async_socket: MagicMock,
+        protocol: DatagramListenerProtocol,
+        mock_asyncio_transport: MagicMock,
     ) -> None:
         # Arrange
+        exception = OSError("Something bad happen")
+
+        close_waiter = protocol._get_close_waiter()
+        assert not close_waiter.done()
 
         # Act
-        await socket.aclose()
+        protocol.connection_lost(exception)
+        protocol.connection_lost(exception)  # Double call must not change anything
 
         # Assert
-        mock_async_socket.aclose.assert_awaited_once_with()
-
-
-@pytest.mark.asyncio
-class TestRawDatagramSocketAdapter(BaseTestRawDatagramSocket):
-    @pytest.fixture(autouse=True)
-    @staticmethod
-    def mock_async_socket_cls(mock_async_socket: MagicMock, mocker: MockerFixture) -> MagicMock:
-        return mocker.patch(f"{RawDatagramSocketAdapter.__module__}.AsyncSocket", return_value=mock_async_socket)
+        assert close_waiter.done() and close_waiter.exception() is None
+        mock_asyncio_transport.close.assert_not_called()  # just to be sure :)
 
-    @pytest.fixture
-    @classmethod
-    def mock_udp_socket(cls, mock_udp_socket: MagicMock) -> MagicMock:
-        cls.set_local_address_to_socket_mock(mock_udp_socket, mock_udp_socket.family, ("127.0.0.1", 11111))
-        cls.set_remote_address_to_socket_mock(mock_udp_socket, mock_udp_socket.family, ("127.0.0.1", 12345))
-        return mock_udp_socket
+    @pytest.mark.asyncio
+    async def test____serve____called_twice(
+        self,
+        protocol: DatagramListenerProtocol,
+        mocker: MockerFixture,
+    ) -> None:
+        # Arrange
+        backend = current_async_backend()
+        datagram_received_stub = mocker.async_stub()
 
-    @classmethod
-    def socket_factory(cls, mock_udp_socket: MagicMock, event_loop: asyncio.AbstractEventLoop) -> RawDatagramSocketAdapter:
-        return RawDatagramSocketAdapter(mock_udp_socket, event_loop)
+        # Act & Assert
+        async with backend.create_task_group() as tg:
+            with pytest.raises(RuntimeError, match=r"^DatagramListenerProtocol.serve\(\) awaited twice"):
+                task = await tg.start(protocol.serve, datagram_received_stub, tg)
+                try:
+                    with backend.timeout(10):
+                        await protocol.serve(datagram_received_stub, tg)
+                finally:
+                    task.cancel()
 
-    async def test____recv____returns_data_from_async_socket(
+    @pytest.mark.asyncio
+    async def test____serve____datagram_received____start_task(
         self,
-        socket: RawDatagramSocketAdapter,
-        mock_async_socket: MagicMock,
+        event_loop: asyncio.AbstractEventLoop,
+        protocol: DatagramListenerProtocol,
+        mocker: MockerFixture,
     ) -> None:
         # Arrange
-        mock_async_socket.recvfrom.return_value = (b"data", ("127.0.0.1", 12345))
+        backend = current_async_backend()
+        serve_scope = backend.move_on_after(10)
+        datagram_received_stub = mocker.async_stub()
+        datagram_received_stub.side_effect = lambda *args: serve_scope.cancel()
 
         # Act
-        data = await socket.recv()
+        async with backend.create_task_group() as tg:
+            with serve_scope:
+                event_loop.call_later(0.1, protocol.datagram_received, b"datagram", ("an_address", 12345))
+                await protocol.serve(datagram_received_stub, tg)
 
         # Assert
-        assert data == b"data"
-        mock_async_socket.recvfrom.assert_awaited_once_with(MAX_DATAGRAM_BUFSIZE)
+        datagram_received_stub.assert_awaited_once_with(b"datagram", ("an_address", 12345))
 
-    async def test____send____sends_data_to_async_socket(
+    @pytest.mark.asyncio
+    async def test____serve____datagram_received____start_task_for_datagrams_received_before_serving(
         self,
-        socket: RawDatagramSocketAdapter,
-        mock_async_socket: MagicMock,
+        protocol: DatagramListenerProtocol,
+        mocker: MockerFixture,
     ) -> None:
         # Arrange
-        mock_async_socket.sendto.return_value = None
+        backend = current_async_backend()
+        serve_scope = backend.move_on_after(10)
+        datagram_received_stub = mocker.async_stub()
+        datagram_received_stub.side_effect = lambda *args: serve_scope.cancel()
+        protocol.datagram_received(b"datagram", ("an_address", 12345))
+        protocol.datagram_received(b"datagram_2", ("other_address", 54321))
 
         # Act
-        await socket.send(b"data")
+        async with backend.create_task_group() as tg:
+            with serve_scope:
+                await protocol.serve(datagram_received_stub, tg)
 
         # Assert
-        mock_async_socket.sendall.assert_awaited_once_with(b"data")
-        mock_async_socket.sendto.assert_not_called()
+        assert datagram_received_stub.await_args_list == [
+            mocker.call(b"datagram", ("an_address", 12345)),
+            mocker.call(b"datagram_2", ("other_address", 54321)),
+        ]
 
-    async def test____get_extra_info____returns_socket_info(
+    @pytest.mark.parametrize("event_loop_debug", [False, True], ids=lambda p: f"event_loop_debug=={p}")
+    def test____error_received____log_error(
         self,
-        socket: RawDatagramSocketAdapter,
-        mock_udp_socket: MagicMock,
+        event_loop_debug: bool,
+        event_loop: asyncio.AbstractEventLoop,
+        protocol: DatagramListenerProtocol,
+        caplog: pytest.LogCaptureFixture,
     ) -> None:
         # Arrange
+        event_loop.set_debug(event_loop_debug)
+        assert event_loop.get_debug() is event_loop_debug
+        caplog.set_level(logging.INFO, DatagramListenerProtocol.__module__)
+        exception = OSError("Something bad happen")
 
-        # Act & Assert
-        assert socket.extra(SocketAttribute.socket) is mock_udp_socket
-        assert socket.extra(SocketAttribute.family) == mock_udp_socket.family
-        assert socket.extra(SocketAttribute.sockname) == ("127.0.0.1", 11111)
-        assert socket.extra(SocketAttribute.peername) == ("127.0.0.1", 12345)
+        # Act
+        protocol.error_received(exception)
 
+        # Assert
+        assert len(caplog.records) == 1
+        assert caplog.records[0].levelno == logging.WARNING
+        assert caplog.records[0].getMessage() == "Unrelated error occurred on datagram reception: OSError: Something bad happen"
+        if event_loop_debug:
+            assert caplog.records[0].exc_info == (type(exception), exception, exception.__traceback__)
+        else:
+            assert caplog.records[0].exc_info is None
 
-@pytest.mark.asyncio
-class TestRawDatagramListenerSocketAdapter(BaseTestRawDatagramSocket):
-    @pytest.fixture(autouse=True)
-    @staticmethod
-    def mock_async_socket_cls(mock_async_socket: MagicMock, mocker: MockerFixture) -> MagicMock:
-        return mocker.patch(f"{RawDatagramListenerSocketAdapter.__module__}.AsyncSocket", return_value=mock_async_socket)
+    @pytest.mark.parametrize("event_loop_debug", [False, True], ids=lambda p: f"event_loop_debug=={p}")
+    def test____error_received____do_not_log_after_connection_lost(
+        self,
+        event_loop_debug: bool,
+        event_loop: asyncio.AbstractEventLoop,
+        protocol: DatagramListenerProtocol,
+        caplog: pytest.LogCaptureFixture,
+    ) -> None:
+        # Arrange
+        event_loop.set_debug(event_loop_debug)
+        assert event_loop.get_debug() is event_loop_debug
+        caplog.set_level(logging.INFO, DatagramListenerProtocol.__module__)
+        exception = OSError("Something bad happen")
+        protocol.connection_lost(None)
 
-    @pytest.fixture
-    @classmethod
-    def mock_udp_socket(cls, mock_udp_socket: MagicMock) -> MagicMock:
-        cls.set_local_address_to_socket_mock(mock_udp_socket, mock_udp_socket.family, ("127.0.0.1", 11111))
-        cls.configure_socket_mock_to_raise_ENOTCONN(mock_udp_socket)
-        return mock_udp_socket
+        # Act
+        protocol.error_received(exception)
 
-    @classmethod
-    def socket_factory(
-        cls,
-        mock_udp_socket: MagicMock,
-        event_loop: asyncio.AbstractEventLoop,
-    ) -> RawDatagramListenerSocketAdapter:
-        return RawDatagramListenerSocketAdapter(mock_udp_socket, event_loop)
+        # Assert
+        assert len(caplog.records) == 0
 
-    async def test____recv_from____returns_data_from_async_socket(
+    @pytest.mark.asyncio
+    async def test____drain_helper____quick_exit_if_not_paused(
         self,
-        socket: RawDatagramListenerSocketAdapter,
-        mock_async_socket: MagicMock,
+        event_loop: asyncio.AbstractEventLoop,
+        protocol: DatagramListenerProtocol,
+        mocker: MockerFixture,
     ) -> None:
         # Arrange
-        mock_async_socket.recvfrom.return_value = (b"data", ("127.0.0.1", 12345))
+        assert not protocol._writing_paused()
+        mock_create_future: MagicMock = mocker.patch.object(event_loop, "create_future")
 
         # Act
-        data, address = await socket.recv_from()
+        await protocol.writer_drain()
 
         # Assert
-        assert data == b"data"
-        assert address == ("127.0.0.1", 12345)
-        mock_async_socket.recvfrom.assert_awaited_once_with(MAX_DATAGRAM_BUFSIZE)
+        mock_create_future.assert_not_called()
 
-    async def test____send_to____sends_data_to_async_socket(
+    @pytest.mark.asyncio
+    async def test____drain_helper____raise_connection_aborted_if_connection_is_lost(
         self,
-        socket: RawDatagramListenerSocketAdapter,
-        mock_async_socket: MagicMock,
+        protocol: DatagramListenerProtocol,
     ) -> None:
         # Arrange
-        mock_async_socket.sendto.return_value = None
+        assert not protocol._writing_paused()
+
+        from errno import ECONNABORTED
+
+        protocol.connection_lost(None)
+
+        # Act & Assert
+        with pytest.raises(OSError) as exc_info:
+            await protocol.writer_drain()
+
+        assert exc_info.value.errno == ECONNABORTED
+
+    @pytest.mark.asyncio
+    async def test____drain_helper____raise_given_exception_if_connection_is_lost(
+        self,
+        protocol: DatagramListenerProtocol,
+    ) -> None:
+        # Arrange
+        error = OSError("socket error")
+        assert not protocol._writing_paused()
+
+        protocol.connection_lost(error)
+
+        # Act & Assert
+        with pytest.raises(OSError) as exc_info:
+            await protocol.writer_drain()
+
+        assert exc_info.value is error
+
+    @pytest.mark.asyncio
+    @pytest.mark.parametrize("cancel_tasks", [False, True], ids=lambda p: f"cancel_tasks_before=={p}")
+    async def test____drain_helper____wait_during_writing_pause(
+        self,
+        cancel_tasks: bool,
+        protocol: DatagramListenerProtocol,
+    ) -> None:
+        # Arrange
+        import inspect
 
         # Act
-        await socket.send_to(b"data", ("127.0.0.1", 12345))
+        protocol.pause_writing()
+        assert protocol._writing_paused()
+        tasks: set[asyncio.Task[None]] = set()
+        for _ in range(10):
+            tasks.add(asyncio.create_task(protocol.writer_drain()))
+        await asyncio.sleep(0)  # Suspend to let the event loop start all tasks
+        assert all(inspect.getcoroutinestate(t.get_coro()) == "CORO_SUSPENDED" for t in tasks)  # type: ignore[arg-type]
+        if cancel_tasks:
+            for t in tasks:
+                t.cancel()
+        protocol.resume_writing()
+        await asyncio.sleep(0)  # Suspend to let the event loop run all tasks
 
         # Assert
-        mock_async_socket.sendto.assert_awaited_once_with(b"data", ("127.0.0.1", 12345))
-        mock_async_socket.sendall.assert_not_called()
+        if cancel_tasks:
+            assert all(t.done() and t.cancelled() for t in tasks)
+        else:
+            assert all(t.done() and t.exception() is None and t.result() is None for t in tasks)
 
-    async def test____get_extra_info____returns_socket_info(
+    @pytest.mark.asyncio
+    @pytest.mark.parametrize("exception", [None, OSError("Something bad happen")])
+    @pytest.mark.parametrize("cancel_tasks", [False, True], ids=lambda p: f"cancel_tasks_before=={p}")
+    async def test____drain_helper____wait_during_writing_pause____connection_lost_while_waiting(
         self,
-        socket: RawDatagramSocketAdapter,
-        mock_udp_socket: MagicMock,
-        mocker: MockerFixture,
+        cancel_tasks: bool,
+        exception: Exception | None,
+        protocol: DatagramListenerProtocol,
     ) -> None:
         # Arrange
+        import inspect
 
-        # Act & Assert
-        assert socket.extra(SocketAttribute.socket) is mock_udp_socket
-        assert socket.extra(SocketAttribute.family) == mock_udp_socket.family
-        assert socket.extra(SocketAttribute.sockname) == ("127.0.0.1", 11111)
-        assert socket.extra(SocketAttribute.peername, mocker.sentinel.no_value) is mocker.sentinel.no_value
+        protocol.pause_writing()
+        assert protocol._writing_paused()
+        tasks: set[asyncio.Task[None]] = set()
+        for _ in range(10):
+            tasks.add(asyncio.create_task(protocol.writer_drain()))
+        await asyncio.sleep(0)  # Suspend to let the event loop start all tasks
+        assert all(inspect.getcoroutinestate(t.get_coro()) == "CORO_SUSPENDED" for t in tasks)  # type: ignore[arg-type]
+        if cancel_tasks:
+            for t in tasks:
+                t.cancel()
+
+        # Act
+        protocol.connection_lost(exception)
+        await asyncio.sleep(0)  # Suspend to let the event loop run all tasks
+
+        # Assert
+        if cancel_tasks:
+            assert all(t.done() and t.cancelled() for t in tasks)
+        elif exception is None:
+            assert all(t.done() and isinstance(t.exception(), ConnectionAbortedError) for t in tasks), tasks
+        else:
+            assert all(t.done() and t.exception() is exception for t in tasks)
```

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_async/test_asyncio_backend/test_socket.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_async/test_asyncio_backend/test_socket.py`

 * *Files 1% similar despite different names*

```diff
@@ -95,15 +95,17 @@
 
     @staticmethod
     async def _busy_socket_task(
         coroutine: Coroutine[Any, Any, Any],
         event_loop: asyncio.AbstractEventLoop,
         mock_socket_method: MagicMock,
     ) -> asyncio.Task[Any]:
+        mock_socket_method.reset_mock()
         accept_task = event_loop.create_task(coroutine)
+        await asyncio.sleep(0)
         async with asyncio.timeout(5):
             while len(mock_socket_method.call_args_list) == 0:
                 await asyncio.sleep(0)
         mock_socket_method.reset_mock()
         return accept_task
 
 
@@ -216,14 +218,25 @@
     @staticmethod
     def socket(
         event_loop: asyncio.AbstractEventLoop,
         mock_stdlib_socket: MagicMock,
     ) -> AsyncSocket:
         return AsyncSocket(mock_stdlib_socket, event_loop)
 
+    async def test____dunder_init____forbids_ssl_sockets(
+        self,
+        event_loop: asyncio.AbstractEventLoop,
+        mock_ssl_socket: MagicMock,
+    ) -> None:
+        # Arrange
+
+        # Act & Assert
+        with pytest.raises(TypeError, match=r"^ssl\.SSLSocket instances are forbidden$"):
+            _ = AsyncSocket(mock_ssl_socket, event_loop)
+
     @pytest.mark.usefixtures("socket")
     async def test____dunder_init____ensure_non_blocking_socket(
         self,
         mock_stdlib_socket: MagicMock,
     ) -> None:
         # Arrange
```

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_async/test_asyncio_backend/test_stream.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_async/test_asyncio_backend/test_stream.py`

 * *Files 22% similar despite different names*

```diff
@@ -4,351 +4,120 @@
 
 import asyncio
 import asyncio.trsock
 import contextlib
 import logging
 import os
 import ssl
-from collections.abc import Callable
+from collections.abc import Callable, Coroutine
 from errno import errorcode as errno_errorcode
-from socket import SHUT_RDWR, SHUT_WR
-from typing import TYPE_CHECKING, Any
+from typing import TYPE_CHECKING, Any, Literal, TypeAlias
 
 from easynetwork.exceptions import UnsupportedOperation
 from easynetwork.lowlevel.constants import ACCEPT_CAPACITY_ERRNOS, NOT_CONNECTED_SOCKET_ERRNOS
-from easynetwork.lowlevel.socket import SocketAttribute, TLSAttribute
+from easynetwork.lowlevel.socket import SocketAttribute
 from easynetwork.lowlevel.std_asyncio.stream.listener import (
     AbstractAcceptedSocketFactory,
     AcceptedSocketFactory,
-    AcceptedSSLSocketFactory,
     ListenerSocketAdapter,
 )
-from easynetwork.lowlevel.std_asyncio.stream.socket import AsyncioTransportStreamSocketAdapter, RawStreamSocketAdapter
+from easynetwork.lowlevel.std_asyncio.stream.socket import AsyncioTransportStreamSocketAdapter, StreamReaderBufferedProtocol
 from easynetwork.lowlevel.std_asyncio.tasks import CancelScope, TaskGroup as AsyncIOTaskGroup
 
 import pytest
 
 if TYPE_CHECKING:
     from unittest.mock import AsyncMock, MagicMock
 
     from pytest_mock import MockerFixture
 
 from ....tools import PlatformMarkers
+from ..._utils import partial_eq
 from ...base import BaseTestSocket
 
 
-class BaseTestTransportStreamSocket:
+class BaseTestTransportStreamSocket(BaseTestSocket):
     @pytest.fixture
     @staticmethod
     def mock_asyncio_reader(mock_asyncio_stream_reader_factory: Callable[[], MagicMock]) -> MagicMock:
         return mock_asyncio_stream_reader_factory()
 
     @pytest.fixture
-    @staticmethod
-    def mock_tcp_socket(mock_tcp_socket: MagicMock) -> MagicMock:
-        mock_tcp_socket.getsockname.return_value = ("127.0.0.1", 11111)
-        mock_tcp_socket.getpeername.return_value = ("127.0.0.1", 12345)
+    @classmethod
+    def mock_tcp_socket(cls, mock_tcp_socket: MagicMock) -> MagicMock:
+        cls.set_local_address_to_socket_mock(mock_tcp_socket, mock_tcp_socket.family, ("127.0.0.1", 11111))
+        cls.set_remote_address_to_socket_mock(mock_tcp_socket, mock_tcp_socket.family, ("127.0.0.1", 12345))
         return mock_tcp_socket
 
     @pytest.fixture
     @staticmethod
-    def asyncio_writer_extra_info(mock_tcp_socket: MagicMock) -> dict[str, Any]:
+    def asyncio_transport_extra_info(mock_tcp_socket: MagicMock) -> dict[str, Any]:
         return {
             "socket": mock_tcp_socket,
             "sockname": mock_tcp_socket.getsockname.return_value,
             "peername": mock_tcp_socket.getpeername.return_value,
         }
 
     @pytest.fixture
     @staticmethod
+    def mock_asyncio_transport(asyncio_transport_extra_info: dict[str, Any], mocker: MockerFixture) -> MagicMock:
+        mock = mocker.NonCallableMagicMock(spec=asyncio.Transport)
+        mock.get_extra_info.side_effect = asyncio_transport_extra_info.get
+        mock.is_closing.return_value = False
+        return mock
+
+    @pytest.fixture
+    @staticmethod
+    def asyncio_writer_extra_info(asyncio_transport_extra_info: dict[str, Any]) -> dict[str, Any]:
+        return asyncio_transport_extra_info
+
+    @pytest.fixture
+    @staticmethod
     def mock_asyncio_writer(
         asyncio_writer_extra_info: dict[str, Any],
         mock_asyncio_stream_writer_factory: Callable[[], MagicMock],
     ) -> MagicMock:
         mock = mock_asyncio_stream_writer_factory()
         mock.get_extra_info.side_effect = asyncio_writer_extra_info.get
         return mock
 
 
-@pytest.mark.asyncio
-class TestAsyncioTransportStreamSocketAdapter(BaseTestTransportStreamSocket):
+class BaseTestTransportWithSSL(BaseTestTransportStreamSocket):
     @pytest.fixture
     @staticmethod
     def mock_ssl_object(mock_ssl_context: MagicMock, mocker: MockerFixture) -> MagicMock:
         mock_ssl_object = mocker.NonCallableMagicMock(spec=ssl.SSLObject)
         mock_ssl_object.context = mock_ssl_context
         mock_ssl_object.getpeercert.return_value = mocker.sentinel.peercert
         mock_ssl_object.cipher.return_value = mocker.sentinel.cipher
         mock_ssl_object.compression.return_value = mocker.sentinel.compression
         mock_ssl_object.version.return_value = mocker.sentinel.tls_version
         return mock_ssl_object
 
     @pytest.fixture
     @staticmethod
     def add_ssl_extra_to_transport(
-        asyncio_writer_extra_info: dict[str, Any],
+        asyncio_transport_extra_info: dict[str, Any],
         mock_ssl_context: MagicMock,
         mock_ssl_object: MagicMock,
         mocker: MockerFixture,
     ) -> None:
-        asyncio_writer_extra_info.update(
+        asyncio_transport_extra_info.update(
             {
                 "sslcontext": mock_ssl_context,
                 "ssl_object": mock_ssl_object,
                 "peercert": mocker.sentinel.peercert,
                 "cipher": mocker.sentinel.cipher,
                 "compression": mocker.sentinel.compression,
             }
         )
 
-    @pytest.fixture
-    @staticmethod
-    def socket(mock_asyncio_reader: MagicMock, mock_asyncio_writer: MagicMock) -> AsyncioTransportStreamSocketAdapter:
-        mock_asyncio_writer.can_write_eof.return_value = True
-        return AsyncioTransportStreamSocketAdapter(mock_asyncio_reader, mock_asyncio_writer)
-
-    @pytest.mark.parametrize("transport_is_closing", [False, True], ids=lambda p: f"transport_is_closing=={p}")
-    @pytest.mark.parametrize("can_write_eof", [False, True], ids=lambda p: f"can_write_eof=={p}")
-    @pytest.mark.parametrize("wait_close_raise_error", [False, True], ids=lambda p: f"wait_close_raise_error=={p}")
-    @pytest.mark.parametrize("write_eof_raise_error", [False, True], ids=lambda p: f"write_eof_raise_error=={p}")
-    async def test____aclose____close_transport_and_wait(
-        self,
-        transport_is_closing: bool,
-        can_write_eof: bool,
-        wait_close_raise_error: bool,
-        write_eof_raise_error: bool,
-        socket: AsyncioTransportStreamSocketAdapter,
-        mock_asyncio_writer: MagicMock,
-    ) -> None:
-        # Arrange
-        mock_asyncio_writer.is_closing.return_value = transport_is_closing
-        mock_asyncio_writer.can_write_eof.return_value = can_write_eof
-        if wait_close_raise_error:
-            mock_asyncio_writer.wait_closed.side_effect = OSError
-        if write_eof_raise_error:
-            mock_asyncio_writer.write_eof.side_effect = OSError
-
-        # Act
-        await socket.aclose()
-
-        # Assert
-        if transport_is_closing:
-            mock_asyncio_writer.close.assert_not_called()
-            mock_asyncio_writer.wait_closed.assert_awaited_once_with()
-            mock_asyncio_writer.transport.abort.assert_not_called()
-        else:
-            if can_write_eof:
-                mock_asyncio_writer.write_eof.assert_called_once_with()
-            else:
-                mock_asyncio_writer.write_eof.assert_not_called()
-            mock_asyncio_writer.close.assert_called_once_with()
-            mock_asyncio_writer.wait_closed.assert_awaited_once_with()
-            mock_asyncio_writer.transport.abort.assert_not_called()
-
-    @pytest.mark.parametrize("transport_is_closing", [False, True], ids=lambda p: f"transport_is_closing=={p}")
-    async def test____aclose____abort_transport_if_cancelled(
-        self,
-        transport_is_closing: bool,
-        socket: AsyncioTransportStreamSocketAdapter,
-        mock_asyncio_writer: MagicMock,
-    ) -> None:
-        # Arrange
-        mock_asyncio_writer.is_closing.return_value = transport_is_closing
-        mock_asyncio_writer.wait_closed.side_effect = asyncio.CancelledError
-
-        # Act
-        with pytest.raises(asyncio.CancelledError):
-            await socket.aclose()
-
-        # Assert
-        if transport_is_closing:
-            mock_asyncio_writer.close.assert_not_called()
-            mock_asyncio_writer.wait_closed.assert_awaited_once_with()
-        else:
-            mock_asyncio_writer.close.assert_called_once_with()
-            mock_asyncio_writer.wait_closed.assert_awaited_once_with()
-        mock_asyncio_writer.transport.abort.assert_not_called()
-
-    @pytest.mark.usefixtures("add_ssl_extra_to_transport")
-    @pytest.mark.parametrize("transport_is_closing", [False, True], ids=lambda p: f"transport_is_closing=={p}")
-    async def test____aclose____abort_transport_if_cancelled____ssl(
-        self,
-        transport_is_closing: bool,
-        socket: AsyncioTransportStreamSocketAdapter,
-        mock_asyncio_writer: MagicMock,
-    ) -> None:
-        # Arrange
-        mock_asyncio_writer.is_closing.return_value = transport_is_closing
-        mock_asyncio_writer.wait_closed.side_effect = asyncio.CancelledError
-
-        # Act
-        with pytest.raises(asyncio.CancelledError):
-            await socket.aclose()
-
-        # Assert
-        if transport_is_closing:
-            mock_asyncio_writer.close.assert_not_called()
-            mock_asyncio_writer.wait_closed.assert_awaited_once_with()
-            mock_asyncio_writer.transport.abort.assert_not_called()
-        else:
-            mock_asyncio_writer.close.assert_called_once_with()
-            mock_asyncio_writer.wait_closed.assert_awaited_once_with()
-            mock_asyncio_writer.transport.abort.assert_called_once_with()
-
-    @pytest.mark.parametrize("transport_closed", [False, True], ids=lambda p: f"transport_closed=={p}")
-    async def test____is_closing____return_internal_flag(
-        self,
-        transport_closed: bool,
-        socket: AsyncioTransportStreamSocketAdapter,
-        mock_asyncio_writer: MagicMock,
-    ) -> None:
-        # Arrange
-        if transport_closed:
-            await socket.aclose()
-            mock_asyncio_writer.reset_mock()
-        mock_asyncio_writer.is_closing.side_effect = AssertionError
-
-        # Act
-        state = socket.is_closing()
-
-        # Assert
-        mock_asyncio_writer.is_closing.assert_not_called()
-        assert state is transport_closed
-
-    async def test____recv____read_from_reader(
-        self,
-        socket: AsyncioTransportStreamSocketAdapter,
-        mock_asyncio_reader: MagicMock,
-    ) -> None:
-        # Arrange
-        mock_asyncio_reader.read.return_value = b"data"
-
-        # Act
-        data: bytes = await socket.recv(1024)
-
-        # Assert
-        mock_asyncio_reader.read.assert_awaited_once_with(1024)
-        assert data == b"data"
-
-    async def test____recv____null_bufsize(
-        self,
-        socket: AsyncioTransportStreamSocketAdapter,
-        mock_asyncio_reader: MagicMock,
-    ) -> None:
-        # Arrange
-        mock_asyncio_reader.read.return_value = b""
-
-        # Act
-        data: bytes = await socket.recv(0)
-
-        # Assert
-        mock_asyncio_reader.read.assert_awaited_once_with(0)
-        assert data == b""
-
-    async def test____recv____negative_bufsize_error(
-        self,
-        socket: AsyncioTransportStreamSocketAdapter,
-        mock_asyncio_reader: MagicMock,
-    ) -> None:
-        # Arrange
-
-        # Act
-        with pytest.raises(ValueError):
-            await socket.recv(-1)
-
-        # Assert
-        mock_asyncio_reader.read.assert_not_awaited()
-
-    async def test____send_all____write_and_drain(
-        self,
-        socket: AsyncioTransportStreamSocketAdapter,
-        mock_asyncio_writer: MagicMock,
-    ) -> None:
-        # Arrange
-
-        # Act
-        await socket.send_all(b"data to send")
-
-        # Assert
-        mock_asyncio_writer.write.assert_called_once_with(b"data to send")
-        mock_asyncio_writer.writelines.assert_not_called()
-        mock_asyncio_writer.drain.assert_awaited_once_with()
-
-    async def test____send_all_from_iterable____writelines_and_drain(
-        self,
-        socket: AsyncioTransportStreamSocketAdapter,
-        mock_asyncio_writer: MagicMock,
-    ) -> None:
-        # Arrange
-        written_chunks: list[bytes] = []
-
-        mock_asyncio_writer.writelines.side_effect = written_chunks.extend
-
-        # Act
-        await socket.send_all_from_iterable([b"data", b"to", b"send"])
-
-        # Assert
-        mock_asyncio_writer.write.assert_not_called()
-        mock_asyncio_writer.writelines.assert_called_once()
-        mock_asyncio_writer.drain.assert_awaited_once_with()
-        assert written_chunks == [b"data", b"to", b"send"]
-
-    @pytest.mark.parametrize("can_write_eof", [False, True], ids=lambda p: f"can_write_eof=={p}")
-    async def test____send_eof____write_eof(
-        self,
-        can_write_eof: bool,
-        socket: AsyncioTransportStreamSocketAdapter,
-        mock_asyncio_writer: MagicMock,
-    ) -> None:
-        # Arrange
-        mock_asyncio_writer.can_write_eof.return_value = can_write_eof
-        mock_asyncio_writer.write_eof.return_value = None
-
-        # Act & Assert
-        if can_write_eof:
-            await socket.send_eof()
-            mock_asyncio_writer.write_eof.assert_called_once_with()
-        else:
-            with pytest.raises(UnsupportedOperation):
-                await socket.send_eof()
-            mock_asyncio_writer.write_eof.assert_not_called()
-
-    async def test____get_extra_info____returns_socket_info(
-        self,
-        socket: AsyncioTransportStreamSocketAdapter,
-        mock_tcp_socket: MagicMock,
-    ) -> None:
-        # Arrange
-
-        # Act & Assert
-        assert socket.extra(SocketAttribute.socket) is mock_tcp_socket
-        assert socket.extra(SocketAttribute.family) == mock_tcp_socket.family
-        assert socket.extra(SocketAttribute.sockname) == ("127.0.0.1", 11111)
-        assert socket.extra(SocketAttribute.peername) == ("127.0.0.1", 12345)
-
-    @pytest.mark.usefixtures("add_ssl_extra_to_transport")
-    async def test____get_extra_info____returns_ssl_info(
-        self,
-        socket: AsyncioTransportStreamSocketAdapter,
-        mock_ssl_context: MagicMock,
-        mocker: MockerFixture,
-    ) -> None:
-        # Arrange
-
-        # Act & Assert
-        assert socket.extra(TLSAttribute.sslcontext) is mock_ssl_context
-        assert socket.extra(TLSAttribute.peercert) is mocker.sentinel.peercert
-        assert socket.extra(TLSAttribute.cipher) is mocker.sentinel.cipher
-        assert socket.extra(TLSAttribute.compression) is mocker.sentinel.compression
-        assert socket.extra(TLSAttribute.tls_version) is mocker.sentinel.tls_version
-        assert socket.extra(TLSAttribute.standard_compatible) is True
-
 
 @pytest.mark.asyncio
-class TestListenerSocketAdapter(BaseTestTransportStreamSocket, BaseTestSocket):
+class TestListenerSocketAdapter(BaseTestTransportStreamSocket):
     @pytest.fixture(autouse=True)
     @staticmethod
     def mock_async_socket_cls(mock_async_socket: MagicMock, mocker: MockerFixture) -> MagicMock:
         return mocker.patch(f"{ListenerSocketAdapter.__module__}.AsyncSocket", return_value=mock_async_socket)
 
     @pytest.fixture
     @classmethod
@@ -389,14 +158,28 @@
     def listener(
         event_loop: asyncio.AbstractEventLoop,
         mock_tcp_listener_socket: MagicMock,
         accepted_socket_factory: MagicMock,
     ) -> ListenerSocketAdapter[Any]:
         return ListenerSocketAdapter(mock_tcp_listener_socket, event_loop, accepted_socket_factory)
 
+    @staticmethod
+    def _make_accept_side_effect(
+        side_effect: Any,
+        mocker: MockerFixture,
+        sleep_time: float = 0,
+    ) -> Callable[[], Coroutine[Any, Any, MagicMock]]:
+        accept_cb = mocker.MagicMock(side_effect=side_effect)
+
+        async def accept_side_effect() -> MagicMock:
+            await asyncio.sleep(sleep_time)
+            return accept_cb()
+
+        return accept_side_effect
+
     async def test____dunder_init____invalid_socket_type(
         self,
         event_loop: asyncio.AbstractEventLoop,
         mock_udp_socket: MagicMock,
         accepted_socket_factory: MagicMock,
     ) -> None:
         # Arrange
@@ -430,34 +213,41 @@
 
         # Act
         await listener.aclose()
 
         # Assert
         mock_async_socket.aclose.assert_awaited_once_with()
 
+    @pytest.mark.parametrize("external_group", [True, False], ids=lambda p: f"external_group=={p}")
     async def test____serve____default(
         self,
         event_loop: asyncio.AbstractEventLoop,
         listener: ListenerSocketAdapter[Any],
+        external_group: bool,
         mock_async_socket: MagicMock,
         accepted_socket_factory: MagicMock,
         handler: AsyncMock,
         mock_tcp_socket_factory: Callable[[], MagicMock],
         mock_stream_socket_adapter_factory: Callable[[], MagicMock],
-        fake_cancellation_cls: type[BaseException],
+        mocker: MockerFixture,
     ) -> None:
         # Arrange
         stream = mock_stream_socket_adapter_factory()
         client_socket = mock_tcp_socket_factory()
         accepted_socket_factory.connect.return_value = stream
-        mock_async_socket.accept.side_effect = [client_socket, fake_cancellation_cls]
+        mock_async_socket.accept.side_effect = self._make_accept_side_effect(
+            [client_socket, asyncio.CancelledError],
+            mocker,
+            sleep_time=0.1,
+        )
 
         # Act
-        async with AsyncIOTaskGroup() as task_group:
-            with pytest.raises(fake_cancellation_cls):
+        task_group: AsyncIOTaskGroup | None
+        async with AsyncIOTaskGroup() if external_group else contextlib.nullcontext() as task_group:  # type: ignore[attr-defined]
+            with pytest.raises(asyncio.CancelledError):
                 await listener.serve(handler, task_group)
 
         # Assert
         accepted_socket_factory.connect.assert_awaited_once_with(client_socket, event_loop)
         handler.assert_awaited_once_with(stream)
 
     @pytest.mark.parametrize(
@@ -475,28 +265,27 @@
         exc: BaseException,
         event_loop: asyncio.AbstractEventLoop,
         listener: ListenerSocketAdapter[Any],
         mock_async_socket: MagicMock,
         accepted_socket_factory: MagicMock,
         handler: AsyncMock,
         mock_tcp_socket_factory: Callable[[], MagicMock],
-        fake_cancellation_cls: type[BaseException],
         caplog: pytest.LogCaptureFixture,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
-        caplog.set_level(logging.DEBUG)
+        caplog.set_level(logging.INFO)
         client_socket = mock_tcp_socket_factory()
         accepted_socket_factory.connect.side_effect = exc
-        mock_async_socket.accept.side_effect = [client_socket, fake_cancellation_cls]
+        mock_async_socket.accept.side_effect = self._make_accept_side_effect([client_socket, asyncio.CancelledError], mocker)
 
         # Act
         with pytest.raises(BaseExceptionGroup) if type(exc) is BaseException else contextlib.nullcontext():
             async with AsyncIOTaskGroup() as task_group:
-                with pytest.raises(fake_cancellation_cls):
+                with pytest.raises(asyncio.CancelledError):
                     await listener.serve(handler, task_group)
 
         # Assert
         accepted_socket_factory.connect.assert_awaited_once_with(client_socket, event_loop)
         handler.assert_not_awaited()
         client_socket.close.assert_called_once_with()
 
@@ -505,23 +294,25 @@
                 assert len(caplog.records) == 0
                 accepted_socket_factory.log_connection_error.assert_not_called()
             case OSError():
                 # ENOTCONN error should not create a big Traceback error but only a warning (at least)
                 assert len(caplog.records) == 1
                 assert caplog.records[0].levelno == logging.WARNING
                 assert caplog.records[0].message == "A client connection was interrupted just after listener.accept()"
+                assert caplog.records[0].exc_info is None
             case _:
                 assert len(caplog.records) == 0
                 accepted_socket_factory.log_connection_error.assert_called_once_with(
                     mocker.ANY,  # logger
                     exc,
                 )
 
     @PlatformMarkers.skipif_platform_win32
     @pytest.mark.parametrize("errno_value", sorted(ACCEPT_CAPACITY_ERRNOS), ids=errno_errorcode.__getitem__)
+    @pytest.mark.flaky(retries=3, delay=0.1)
     async def test____serve____accept_capacity_error(
         self,
         errno_value: int,
         listener: ListenerSocketAdapter[Any],
         mock_async_socket: MagicMock,
         accepted_socket_factory: MagicMock,
         handler: AsyncMock,
@@ -529,19 +320,18 @@
     ) -> None:
         # Arrange
         caplog.set_level(logging.ERROR)
         accepted_socket_factory.connect.side_effect = AssertionError
         mock_async_socket.accept.side_effect = OSError(errno_value, os.strerror(errno_value))
 
         # Act
-        async with AsyncIOTaskGroup() as task_group:
-            # It retries every 100 ms, so in 975 ms it will retry at 0, 100, ..., 900
-            # = 10 times total
-            with CancelScope(deadline=asyncio.get_running_loop().time() + 0.975):
-                await listener.serve(handler, task_group)
+        # It retries every 100 ms, so in 975 ms it will retry at 0, 100, ..., 900
+        # = 10 times total
+        with CancelScope(deadline=asyncio.get_running_loop().time() + 0.975):
+            await listener.serve(handler)
 
         # Assert
         accepted_socket_factory.connect.assert_not_awaited()
         handler.assert_not_awaited()
         assert len(caplog.records) == 10
         for record in caplog.records:
             assert "retrying" in record.message
@@ -572,15 +362,15 @@
 
         # Assert
         accepted_socket_factory.connect.assert_not_awaited()
         handler.assert_not_awaited()
         assert len(caplog.records) == 0
         assert exc_info.value is exc
 
-    async def test____get_extra_info____returns_socket_info(
+    async def test____extra_attributes____returns_socket_info(
         self,
         listener: ListenerSocketAdapter[Any],
         mock_tcp_listener_socket: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
 
@@ -588,86 +378,36 @@
         assert listener.extra(SocketAttribute.socket) is mock_tcp_listener_socket
         assert listener.extra(SocketAttribute.family) == mock_tcp_listener_socket.family
         assert listener.extra(SocketAttribute.sockname) == ("127.0.0.1", 11111)
         assert listener.extra(SocketAttribute.peername, mocker.sentinel.no_value) is mocker.sentinel.no_value
 
 
 @pytest.mark.asyncio
-class TestAcceptedSocketFactory(BaseTestTransportStreamSocket, BaseTestSocket):
-    @pytest.fixture(autouse=True)
-    @staticmethod
-    def mock_async_socket_cls(mock_async_socket: MagicMock, mocker: MockerFixture) -> MagicMock:
-        return mocker.patch(f"{ListenerSocketAdapter.__module__}.AsyncSocket", return_value=mock_async_socket)
-
-    @pytest.fixture(params=[False, True], ids=lambda boolean: f"use_asyncio_transport=={boolean}")
-    @staticmethod
-    def use_asyncio_transport(request: Any) -> bool:
-        return request.param
-
-    @pytest.fixture
-    @staticmethod
-    def mock_transport_stream_socket_adapter_cls(mock_stream_socket_adapter: MagicMock, mocker: MockerFixture) -> MagicMock:
-        return mocker.patch(
-            f"{ListenerSocketAdapter.__module__}.AsyncioTransportStreamSocketAdapter",
-            return_value=mock_stream_socket_adapter,
-        )
-
-    @pytest.fixture
-    @staticmethod
-    def mock_raw_stream_socket_adapter_cls(mock_stream_socket_adapter: MagicMock, mocker: MockerFixture) -> MagicMock:
-        return mocker.patch(
-            f"{ListenerSocketAdapter.__module__}.RawStreamSocketAdapter",
-            return_value=mock_stream_socket_adapter,
-        )
-
-    @pytest.fixture
-    @staticmethod
-    def mock_asyncio_reader_cls(mock_asyncio_reader: MagicMock, mocker: MockerFixture) -> MagicMock:
-        return mocker.patch("asyncio.streams.StreamReader", return_value=mock_asyncio_reader)
-
-    @pytest.fixture
-    @staticmethod
-    def mock_asyncio_writer_cls(mock_asyncio_writer: MagicMock, mocker: MockerFixture) -> MagicMock:
-        return mocker.patch("asyncio.streams.StreamWriter", return_value=mock_asyncio_writer)
-
-    @pytest.fixture
-    @staticmethod
-    def mock_asyncio_reader_protocol(mocker: MockerFixture) -> MagicMock:
-        return mocker.NonCallableMagicMock(spec=asyncio.streams.StreamReaderProtocol)
-
-    @pytest.fixture
-    @staticmethod
-    def mock_asyncio_transport(mocker: MockerFixture) -> MagicMock:
-        return mocker.NonCallableMagicMock(spec=asyncio.Transport)
-
-    @pytest.fixture
-    @staticmethod
-    def mock_asyncio_reader_protocol_cls(mock_asyncio_reader_protocol: MagicMock, mocker: MockerFixture) -> MagicMock:
-        return mocker.patch("asyncio.streams.StreamReaderProtocol", return_value=mock_asyncio_reader_protocol)
-
+class TestAcceptedSocketFactory(BaseTestTransportStreamSocket):
     @pytest.fixture
     @staticmethod
     def mock_event_loop_connect_accepted_socket(
         event_loop: asyncio.AbstractEventLoop,
         mocker: MockerFixture,
         mock_asyncio_transport: MagicMock,
     ) -> AsyncMock:
+        async def side_effect(protocol_factory: Callable[[], asyncio.Protocol], sock: Any, *args: Any, **kwargs: Any) -> Any:
+            return mock_asyncio_transport, protocol_factory()
+
         return mocker.patch.object(
             event_loop,
             "connect_accepted_socket",
             new_callable=mocker.AsyncMock,
-            return_value=(mock_asyncio_transport, mocker.sentinel.final_protocol),
+            side_effect=side_effect,
         )
 
     @pytest.fixture
     @staticmethod
-    def accepted_socket(
-        use_asyncio_transport: bool,
-    ) -> AcceptedSocketFactory:
-        return AcceptedSocketFactory(use_asyncio_transport=use_asyncio_transport)
+    def accepted_socket() -> AcceptedSocketFactory:
+        return AcceptedSocketFactory()
 
     async def test____log_connection_error____error_log(
         self,
         caplog: pytest.LogCaptureFixture,
         accepted_socket: AcceptedSocketFactory,
     ) -> None:
         # Arrange
@@ -683,385 +423,859 @@
         assert caplog.records[0].levelno == logging.ERROR
         assert caplog.records[0].message == "Error in client task"
         assert caplog.records[0].exc_info is not None and caplog.records[0].exc_info[1] is exc
 
     async def test____connect____creates_new_stream_socket(
         self,
         accepted_socket: AcceptedSocketFactory,
-        use_asyncio_transport: bool,
         event_loop: asyncio.AbstractEventLoop,
         mock_event_loop_connect_accepted_socket: AsyncMock,
-        mock_transport_stream_socket_adapter_cls: MagicMock,
-        mock_raw_stream_socket_adapter_cls: MagicMock,
-        mock_asyncio_reader_protocol: MagicMock,
-        mock_asyncio_reader_protocol_cls: MagicMock,
-        mock_asyncio_transport: MagicMock,
-        mock_asyncio_reader_cls: MagicMock,
-        mock_asyncio_reader: MagicMock,
-        mock_asyncio_writer_cls: MagicMock,
-        mock_asyncio_writer: MagicMock,
         mock_tcp_socket: MagicMock,
-        mock_stream_socket_adapter: MagicMock,
-        mocker: MockerFixture,
     ) -> None:
         # Arrange
 
         # Act
         socket = await accepted_socket.connect(mock_tcp_socket, event_loop)
 
         # Assert
-        assert socket is mock_stream_socket_adapter
-        if use_asyncio_transport:
-            mock_raw_stream_socket_adapter_cls.assert_not_called()
-            mock_asyncio_reader_cls.assert_called_once_with(limit=mocker.ANY, loop=event_loop)
-            mock_asyncio_reader_protocol_cls.assert_called_once_with(mock_asyncio_reader, loop=event_loop)
-            mock_event_loop_connect_accepted_socket.assert_awaited_once_with(
-                mocker.ANY,  # protocol_factory
-                mock_tcp_socket,
-                ssl=None,
-                ssl_handshake_timeout=None,
-                ssl_shutdown_timeout=None,
-            )
-            mock_asyncio_writer_cls.assert_called_once_with(
-                mock_asyncio_transport,
-                mock_asyncio_reader_protocol,
-                mock_asyncio_reader,
-                event_loop,
-            )
-            mock_transport_stream_socket_adapter_cls.assert_called_once_with(
-                mock_asyncio_reader,
-                mock_asyncio_writer,
-            )
-        else:
-            mock_raw_stream_socket_adapter_cls.assert_called_once_with(
-                mock_tcp_socket,
-                event_loop,
-            )
-            mock_asyncio_reader_cls.assert_not_called()
-            mock_asyncio_reader_protocol_cls.assert_not_called()
-            mock_asyncio_writer_cls.assert_not_called()
-            mock_event_loop_connect_accepted_socket.assert_not_called()
-            mock_transport_stream_socket_adapter_cls.assert_not_called()
+        assert isinstance(socket, AsyncioTransportStreamSocketAdapter)
+        mock_event_loop_connect_accepted_socket.assert_awaited_once_with(
+            partial_eq(StreamReaderBufferedProtocol, loop=event_loop),
+            mock_tcp_socket,
+        )
 
 
 @pytest.mark.asyncio
-class TestAcceptedSSLSocketFactory(BaseTestTransportStreamSocket):
-    @pytest.fixture(scope="class")
-    @staticmethod
-    def ssl_handshake_timeout() -> float:
-        return 123456.789
-
-    @pytest.fixture(scope="class")
-    @staticmethod
-    def ssl_shutdown_timeout() -> float:
-        return 9876543.21
-
+class TestAsyncioTransportStreamSocketAdapter(BaseTestTransportWithSSL):
     @pytest.fixture
     @staticmethod
-    def mock_transport_stream_socket_adapter_cls(mock_stream_socket_adapter: MagicMock, mocker: MockerFixture) -> MagicMock:
-        return mocker.patch(
-            f"{ListenerSocketAdapter.__module__}.AsyncioTransportStreamSocketAdapter",
-            return_value=mock_stream_socket_adapter,
-        )
+    def mock_asyncio_protocol(mocker: MockerFixture, event_loop: asyncio.AbstractEventLoop) -> MagicMock:
+        mock = mocker.NonCallableMagicMock(spec=StreamReaderBufferedProtocol)
+        # Currently, _get_close_waiter() is a synchronous function returning a Future, but it will be awaited so this works
+        mock._get_close_waiter = mocker.AsyncMock()
+        mock._get_loop.return_value = event_loop
+        return mock
 
     @pytest.fixture
     @staticmethod
-    def mock_asyncio_reader_cls(mock_asyncio_reader: MagicMock, mocker: MockerFixture) -> MagicMock:
-        return mocker.patch("asyncio.streams.StreamReader", return_value=mock_asyncio_reader)
+    def socket(
+        mock_asyncio_transport: MagicMock,
+        mock_asyncio_protocol: MagicMock,
+    ) -> AsyncioTransportStreamSocketAdapter:
+        mock_asyncio_transport.can_write_eof.return_value = True
+        return AsyncioTransportStreamSocketAdapter(mock_asyncio_transport, mock_asyncio_protocol)
 
-    @pytest.fixture
-    @staticmethod
-    def mock_asyncio_writer_cls(mock_asyncio_writer: MagicMock, mocker: MockerFixture) -> MagicMock:
-        return mocker.patch("asyncio.streams.StreamWriter", return_value=mock_asyncio_writer)
+    @pytest.mark.usefixtures("add_ssl_extra_to_transport")
+    async def test____dunder_init____refuse_transports_over_ssl(
+        self,
+        mock_asyncio_transport: MagicMock,
+        mock_asyncio_protocol: MagicMock,
+    ) -> None:
+        # Arrange
+        mock_asyncio_transport.can_write_eof.return_value = True
 
-    @pytest.fixture
-    @staticmethod
-    def mock_asyncio_reader_protocol(mocker: MockerFixture) -> MagicMock:
-        return mocker.NonCallableMagicMock(spec=asyncio.streams.StreamReaderProtocol)
+        # Act & Assert
+        with pytest.raises(NotImplementedError):
+            _ = AsyncioTransportStreamSocketAdapter(mock_asyncio_transport, mock_asyncio_protocol)
 
-    @pytest.fixture
-    @staticmethod
-    def mock_asyncio_transport(mocker: MockerFixture) -> MagicMock:
-        return mocker.NonCallableMagicMock(spec=asyncio.Transport)
+    @pytest.mark.parametrize("transport_is_closing", [False, True], ids=lambda p: f"transport_is_closing=={p}")
+    @pytest.mark.parametrize("can_write_eof", [False, True], ids=lambda p: f"can_write_eof=={p}")
+    @pytest.mark.parametrize("wait_close_raise_error", [False, True], ids=lambda p: f"wait_close_raise_error=={p}")
+    @pytest.mark.parametrize("write_eof_raise_error", [False, True], ids=lambda p: f"write_eof_raise_error=={p}")
+    async def test____aclose____close_transport_and_wait(
+        self,
+        transport_is_closing: bool,
+        can_write_eof: bool,
+        wait_close_raise_error: bool,
+        write_eof_raise_error: bool,
+        socket: AsyncioTransportStreamSocketAdapter,
+        mock_asyncio_transport: MagicMock,
+        mock_asyncio_protocol: MagicMock,
+    ) -> None:
+        # Arrange
+        mock_asyncio_transport.is_closing.return_value = transport_is_closing
+        mock_asyncio_transport.can_write_eof.return_value = can_write_eof
+        if wait_close_raise_error:
+            mock_asyncio_protocol._get_close_waiter.side_effect = OSError
+        if write_eof_raise_error:
+            mock_asyncio_transport.write_eof.side_effect = OSError
 
-    @pytest.fixture
-    @staticmethod
-    def mock_asyncio_reader_protocol_cls(mock_asyncio_reader_protocol: MagicMock, mocker: MockerFixture) -> MagicMock:
-        return mocker.patch("asyncio.streams.StreamReaderProtocol", return_value=mock_asyncio_reader_protocol)
+        # Act
+        await socket.aclose()
 
-    @pytest.fixture
-    @staticmethod
-    def mock_event_loop_connect_accepted_socket(
-        event_loop: asyncio.AbstractEventLoop,
-        mocker: MockerFixture,
+        # Assert
+        if transport_is_closing:
+            mock_asyncio_transport.close.assert_not_called()
+            mock_asyncio_protocol._get_close_waiter.assert_awaited_once_with()
+            mock_asyncio_transport.abort.assert_not_called()
+            mock_asyncio_transport.write_eof.assert_not_called()
+        else:
+            if can_write_eof:
+                mock_asyncio_transport.write_eof.assert_called_once_with()
+            else:
+                mock_asyncio_transport.write_eof.assert_not_called()
+            mock_asyncio_transport.close.assert_called_once_with()
+            mock_asyncio_protocol._get_close_waiter.assert_awaited_once_with()
+            mock_asyncio_transport.abort.assert_not_called()
+
+    @pytest.mark.parametrize("transport_is_closing", [False, True], ids=lambda p: f"transport_is_closing=={p}")
+    async def test____aclose____abort_transport_if_cancelled(
+        self,
+        transport_is_closing: bool,
+        socket: AsyncioTransportStreamSocketAdapter,
         mock_asyncio_transport: MagicMock,
-    ) -> AsyncMock:
-        return mocker.patch.object(
-            event_loop,
-            "connect_accepted_socket",
-            new_callable=mocker.AsyncMock,
-            return_value=(mock_asyncio_transport, mocker.sentinel.final_protocol),
-        )
+        mock_asyncio_protocol: MagicMock,
+    ) -> None:
+        # Arrange
+        mock_asyncio_transport.is_closing.return_value = transport_is_closing
+        mock_asyncio_protocol._get_close_waiter.side_effect = asyncio.CancelledError
 
-    @pytest.fixture
-    @staticmethod
-    def accepted_socket(
-        mock_ssl_context: MagicMock,
-        ssl_handshake_timeout: float,
-        ssl_shutdown_timeout: float,
-    ) -> AcceptedSSLSocketFactory:
-        return AcceptedSSLSocketFactory(
-            ssl_context=mock_ssl_context,
-            ssl_handshake_timeout=ssl_handshake_timeout,
-            ssl_shutdown_timeout=ssl_shutdown_timeout,
-        )
+        # Act
+        with pytest.raises(asyncio.CancelledError):
+            await socket.aclose()
 
-    async def test____log_connection_error____error_log(
+        # Assert
+        if transport_is_closing:
+            mock_asyncio_transport.close.assert_not_called()
+            mock_asyncio_protocol._get_close_waiter.assert_awaited_once_with()
+        else:
+            mock_asyncio_transport.close.assert_called_once_with()
+            mock_asyncio_protocol._get_close_waiter.assert_awaited_once_with()
+        mock_asyncio_transport.abort.assert_not_called()
+
+    @pytest.mark.parametrize("transport_closed", [False, True], ids=lambda p: f"transport_closed=={p}")
+    async def test____is_closing____return_internal_flag(
         self,
-        caplog: pytest.LogCaptureFixture,
-        accepted_socket: AcceptedSocketFactory,
+        transport_closed: bool,
+        socket: AsyncioTransportStreamSocketAdapter,
+        mock_asyncio_transport: MagicMock,
     ) -> None:
         # Arrange
-        logger = logging.getLogger(__name__)
-        caplog.set_level(logging.ERROR, logger.name)
-        exc = BaseException()
+        if transport_closed:
+            await socket.aclose()
+            mock_asyncio_transport.reset_mock()
+        mock_asyncio_transport.is_closing.side_effect = AssertionError
 
         # Act
-        accepted_socket.log_connection_error(logger, exc)
+        state = socket.is_closing()
 
         # Assert
-        assert len(caplog.records) == 1
-        assert caplog.records[0].levelno == logging.ERROR
-        assert caplog.records[0].message == "Error in client task (during TLS handshake)"
-        assert caplog.records[0].exc_info is not None and caplog.records[0].exc_info[1] is exc
+        mock_asyncio_transport.is_closing.assert_not_called()
+        assert state is transport_closed
 
-    async def test____connect____creates_new_stream_socket(
+    async def test____recv____read_from_reader(
         self,
-        accepted_socket: AcceptedSSLSocketFactory,
-        mock_ssl_context: MagicMock,
-        ssl_handshake_timeout: float,
-        ssl_shutdown_timeout: float,
-        event_loop: asyncio.AbstractEventLoop,
-        mock_event_loop_connect_accepted_socket: AsyncMock,
-        mock_transport_stream_socket_adapter_cls: MagicMock,
-        mock_asyncio_reader_protocol: MagicMock,
-        mock_asyncio_reader_protocol_cls: MagicMock,
-        mock_asyncio_transport: MagicMock,
-        mock_asyncio_reader_cls: MagicMock,
-        mock_asyncio_reader: MagicMock,
-        mock_asyncio_writer_cls: MagicMock,
-        mock_asyncio_writer: MagicMock,
-        mock_tcp_socket: MagicMock,
-        mock_stream_socket_adapter: MagicMock,
-        mocker: MockerFixture,
+        socket: AsyncioTransportStreamSocketAdapter,
+        mock_asyncio_protocol: MagicMock,
     ) -> None:
         # Arrange
+        mock_asyncio_protocol.receive_data.return_value = b"data"
 
         # Act
-        socket = await accepted_socket.connect(mock_tcp_socket, event_loop)
+        data: bytes = await socket.recv(1024)
 
         # Assert
-        assert socket is mock_stream_socket_adapter
+        mock_asyncio_protocol.receive_data.assert_awaited_once_with(1024)
+        assert data == b"data"
 
-        mock_asyncio_reader_cls.assert_called_once_with(limit=mocker.ANY, loop=event_loop)
-        mock_asyncio_reader_protocol_cls.assert_called_once_with(mock_asyncio_reader, loop=event_loop)
-        mock_event_loop_connect_accepted_socket.assert_awaited_once_with(
-            mocker.ANY,  # protocol_factory
-            mock_tcp_socket,
-            ssl=mock_ssl_context,
-            ssl_handshake_timeout=ssl_handshake_timeout,
-            ssl_shutdown_timeout=ssl_shutdown_timeout,
-        )
-        mock_asyncio_writer_cls.assert_called_once_with(
-            mock_asyncio_transport,
-            mock_asyncio_reader_protocol,
-            mock_asyncio_reader,
-            event_loop,
-        )
-        mock_transport_stream_socket_adapter_cls.assert_called_once_with(
-            mock_asyncio_reader,
-            mock_asyncio_writer,
-        )
+    async def test____recv____null_bufsize(
+        self,
+        socket: AsyncioTransportStreamSocketAdapter,
+        mock_asyncio_protocol: MagicMock,
+    ) -> None:
+        # Arrange
+        mock_asyncio_protocol.receive_data.return_value = b""
+
+        # Act
+        data: bytes = await socket.recv(0)
 
+        # Assert
+        mock_asyncio_protocol.receive_data.assert_awaited_once_with(0)
+        assert data == b""
 
-@pytest.mark.asyncio
-class TestRawStreamSocketAdapter(BaseTestSocket):
+    async def test____recv_into____read_from_reader(
+        self,
+        socket: AsyncioTransportStreamSocketAdapter,
+        mock_asyncio_protocol: MagicMock,
+    ) -> None:
+        # Arrange
+        mock_asyncio_protocol.receive_data_into.return_value = 4
+        buffer = bytearray(4)
+
+        # Act
+        nbytes = await socket.recv_into(buffer)
+
+        # Assert
+        mock_asyncio_protocol.receive_data_into.assert_awaited_once_with(buffer)
+        assert nbytes == 4
+
+    async def test____recv_into____null_buffer(
+        self,
+        socket: AsyncioTransportStreamSocketAdapter,
+        mock_asyncio_protocol: MagicMock,
+    ) -> None:
+        # Arrange
+        mock_asyncio_protocol.receive_data_into.return_value = 0
+        buffer = bytearray()
+
+        # Act
+        nbytes = await socket.recv_into(buffer)
+
+        # Assert
+        mock_asyncio_protocol.receive_data_into.assert_awaited_once_with(buffer)
+        assert nbytes == 0
+
+    @pytest.mark.parametrize("transport_is_closing", [False, True], ids=lambda p: f"transport_is_closing=={p}")
+    async def test____send_all____write_and_drain(
+        self,
+        transport_is_closing: bool,
+        socket: AsyncioTransportStreamSocketAdapter,
+        mock_asyncio_transport: MagicMock,
+        mock_asyncio_protocol: MagicMock,
+    ) -> None:
+        # Arrange
+        mock_asyncio_transport.is_closing.side_effect = [transport_is_closing]
+
+        # Act
+        await socket.send_all(b"data to send")
+
+        # Assert
+        mock_asyncio_transport.write.assert_called_once_with(b"data to send")
+        mock_asyncio_transport.writelines.assert_not_called()
+        mock_asyncio_protocol.writer_drain.assert_awaited_once_with()
+
+    @pytest.mark.parametrize("transport_is_closing", [False, True], ids=lambda p: f"transport_is_closing=={p}")
+    async def test____send_all_from_iterable____writelines_and_drain(
+        self,
+        transport_is_closing: bool,
+        socket: AsyncioTransportStreamSocketAdapter,
+        mock_asyncio_transport: MagicMock,
+        mock_asyncio_protocol: MagicMock,
+    ) -> None:
+        # Arrange
+        written_chunks: list[bytes] = []
+        mock_asyncio_transport.is_closing.side_effect = [transport_is_closing]
+        mock_asyncio_transport.writelines.side_effect = written_chunks.extend
+
+        # Act
+        await socket.send_all_from_iterable([b"data", b"to", b"send"])
+
+        # Assert
+        mock_asyncio_transport.write.assert_not_called()
+        mock_asyncio_transport.writelines.assert_called_once()
+        mock_asyncio_protocol.writer_drain.assert_awaited_once_with()
+        assert written_chunks == [b"data", b"to", b"send"]
+
+    @pytest.mark.parametrize("can_write_eof", [False, True], ids=lambda p: f"can_write_eof=={p}")
+    async def test____send_eof____write_eof(
+        self,
+        can_write_eof: bool,
+        socket: AsyncioTransportStreamSocketAdapter,
+        mock_asyncio_transport: MagicMock,
+    ) -> None:
+        # Arrange
+        mock_asyncio_transport.can_write_eof.return_value = can_write_eof
+        mock_asyncio_transport.write_eof.return_value = None
+
+        # Act & Assert
+        if can_write_eof:
+            await socket.send_eof()
+            mock_asyncio_transport.write_eof.assert_called_once_with()
+        else:
+            with pytest.raises(UnsupportedOperation):
+                await socket.send_eof()
+            mock_asyncio_transport.write_eof.assert_not_called()
+
+    async def test____extra_attributes____returns_socket_info(
+        self,
+        socket: AsyncioTransportStreamSocketAdapter,
+        mock_tcp_socket: MagicMock,
+    ) -> None:
+        # Arrange
+
+        # Act & Assert
+        assert socket.extra(SocketAttribute.socket) is mock_tcp_socket
+        assert socket.extra(SocketAttribute.family) == mock_tcp_socket.family
+        assert socket.extra(SocketAttribute.sockname) == ("127.0.0.1", 11111)
+        assert socket.extra(SocketAttribute.peername) == ("127.0.0.1", 12345)
+
+
+_ProtocolDataReceiver: TypeAlias = Callable[[StreamReaderBufferedProtocol, int], Coroutine[Any, Any, bytes]]
+
+
+class TestStreamReaderBufferedProtocol(BaseTestTransportWithSSL):
     @pytest.fixture(autouse=True)
     @staticmethod
-    def mock_async_socket_cls(mock_async_socket: MagicMock, mocker: MockerFixture) -> MagicMock:
-        return mocker.patch(f"{RawStreamSocketAdapter.__module__}.AsyncSocket", return_value=mock_async_socket)
+    def reduce_protocol_buffer_size(monkeypatch: pytest.MonkeyPatch) -> None:
+        monkeypatch.setattr(StreamReaderBufferedProtocol, "max_size", 16 * 1024)
 
     @pytest.fixture
-    @classmethod
-    def mock_tcp_socket(cls, mock_tcp_socket: MagicMock) -> MagicMock:
-        cls.set_local_address_to_socket_mock(mock_tcp_socket, mock_tcp_socket.family, ("127.0.0.1", 11111))
-        cls.set_remote_address_to_socket_mock(mock_tcp_socket, mock_tcp_socket.family, ("127.0.0.1", 12345))
-        return mock_tcp_socket
+    @staticmethod
+    def protocol(
+        event_loop: asyncio.AbstractEventLoop,
+        mock_asyncio_transport: MagicMock,
+    ) -> StreamReaderBufferedProtocol:
+        protocol = StreamReaderBufferedProtocol(loop=event_loop)
+        protocol.connection_made(mock_asyncio_transport)
+        return protocol
+
+    @pytest.fixture(params=["data", "buffer"])
+    @staticmethod
+    def data_receiver(request: pytest.FixtureRequest) -> _ProtocolDataReceiver:
+        match request.param:
+            case "data":
+
+                async def data_receiver(protocol: StreamReaderBufferedProtocol, bufsize: int, /) -> bytes:
+                    return await protocol.receive_data(bufsize)
+
+            case "buffer":
+
+                async def data_receiver(protocol: StreamReaderBufferedProtocol, bufsize: int, /) -> bytes:
+                    assert bufsize >= 0
+                    with memoryview(bytearray(bufsize)) as buffer:
+                        nbytes = await protocol.receive_data_into(buffer)
+                        assert nbytes >= 0
+                        return bytes(buffer[:nbytes])
+
+            case _:
+                pytest.fail("Invalid fixture param")
+
+        return data_receiver
 
-    @pytest.fixture
     @staticmethod
-    def mock_async_socket(
-        mock_async_socket: MagicMock,
-        mock_tcp_socket: MagicMock,
-    ) -> MagicMock:
-        mock_async_socket.socket = mock_tcp_socket
-        return mock_async_socket
+    def write_in_protocol_buffer(protocol: asyncio.BufferedProtocol, data: bytes) -> None:
+        with memoryview(protocol.get_buffer(-1)).cast("B") as buffer:
+            if not data:
+                written = 0
+            elif buffer.nbytes < len(data):
+                written = buffer.nbytes
+                buffer[:] = data[:written]
+            else:
+                written = len(data)
+                buffer[:written] = data
+        protocol.buffer_updated(written)
 
-    @pytest.fixture
     @staticmethod
-    def socket(event_loop: asyncio.AbstractEventLoop, mock_tcp_socket: MagicMock) -> RawStreamSocketAdapter:
-        return RawStreamSocketAdapter(mock_tcp_socket, event_loop)
+    def write_eof_in_protocol_buffer(protocol: asyncio.BufferedProtocol) -> bool | None:
+        with memoryview(protocol.get_buffer(-1)):
+            pass
+        return protocol.eof_received()
 
-    async def test____dunder_init____invalid_socket_type(
+    @pytest.mark.asyncio
+    async def test____dunder_init____use_running_loop(
         self,
         event_loop: asyncio.AbstractEventLoop,
-        mock_udp_socket: MagicMock,
     ) -> None:
         # Arrange
 
+        # Act
+        protocol = StreamReaderBufferedProtocol()
+
+        # Assert
+        assert protocol._get_loop() is event_loop
+
+    def test____dunder_init____use_running_loop____not_in_asyncio_loop(self) -> None:
+        # Arrange
+
         # Act & Assert
-        with pytest.raises(ValueError, match=r"^A 'SOCK_STREAM' socket is expected$"):
-            _ = RawStreamSocketAdapter(mock_udp_socket, event_loop)
+        with pytest.raises(RuntimeError):
+            _ = StreamReaderBufferedProtocol()
 
-    async def test____is_closing____default(
+    def test____connection_lost____by_closed_transport(
         self,
-        socket: RawStreamSocketAdapter,
-        mock_async_socket: MagicMock,
-        mocker: MockerFixture,
+        protocol: StreamReaderBufferedProtocol,
+        mock_asyncio_transport: MagicMock,
     ) -> None:
         # Arrange
-        mock_async_socket.is_closing.return_value = mocker.sentinel.is_closing
+        close_waiter = protocol._get_close_waiter()
+        assert not close_waiter.done()
 
         # Act
-        state = socket.is_closing()
+        protocol.connection_lost(None)
+        protocol.connection_lost(None)  # Double call must not change anything
 
         # Assert
-        assert state is mocker.sentinel.is_closing
-        mock_async_socket.is_closing.assert_called_once_with()
+        assert close_waiter.done() and close_waiter.exception() is None and close_waiter.result() is None
+        mock_asyncio_transport.close.assert_not_called()
 
-    @pytest.mark.parametrize("shutdown_raise_error", [OSError, None])
-    async def test____aclose____close_socket(
+        with pytest.raises(BufferError):
+            protocol.get_buffer(-1)
+
+    def test____connection_lost____close_waiter_done(
         self,
-        shutdown_raise_error: type[BaseException] | None,
-        socket: RawStreamSocketAdapter,
-        mock_async_socket: MagicMock,
+        protocol: StreamReaderBufferedProtocol,
+        mock_asyncio_transport: MagicMock,
     ) -> None:
         # Arrange
-        if shutdown_raise_error is not None:
-            mock_async_socket.shutdown.side_effect = shutdown_raise_error
+        close_waiter = protocol._get_close_waiter()
+        close_waiter.cancel()
+        assert close_waiter.done()
 
         # Act
-        await socket.aclose()
+        protocol.connection_lost(None)
 
         # Assert
-        mock_async_socket.shutdown.assert_awaited_once_with(SHUT_RDWR)
-        mock_async_socket.aclose.assert_awaited_once_with()
+        mock_asyncio_transport.close.assert_not_called()
+
+        with pytest.raises(BufferError):
+            protocol.get_buffer(-1)
 
-    async def test____recv____returns_data_from_async_socket(
+    def test____connection_lost____by_unrelated_error(
         self,
-        socket: RawStreamSocketAdapter,
-        mock_async_socket: MagicMock,
+        protocol: StreamReaderBufferedProtocol,
+        mock_asyncio_transport: MagicMock,
     ) -> None:
         # Arrange
-        mock_async_socket.recv.return_value = b"data"
+        exception = OSError("Something bad happen")
+
+        close_waiter = protocol._get_close_waiter()
+        assert not close_waiter.done()
 
         # Act
-        data: bytes = await socket.recv(123456789)
+        protocol.connection_lost(exception)
+        protocol.connection_lost(exception)  # Double call must not change anything
 
         # Assert
-        assert data == b"data"
-        mock_async_socket.recv.assert_awaited_once_with(123456789)
+        assert close_waiter.done() and close_waiter.exception() is None
+        mock_asyncio_transport.close.assert_not_called()
 
-    async def test____recv_into____returns_data_from_async_socket(
+    def test____eof_received____returns_True(
         self,
-        socket: RawStreamSocketAdapter,
-        mock_async_socket: MagicMock,
+        protocol: StreamReaderBufferedProtocol,
     ) -> None:
         # Arrange
-        def recv_into_side_effect(buf: bytearray | memoryview) -> int:
-            with memoryview(buf) as buf:
-                buf[:4] = b"data"
-                return 4
 
-        mock_async_socket.recv_into.side_effect = recv_into_side_effect
-        buffer = bytearray(1024)
+        # Act & Assert
+        assert protocol.eof_received() is True
+
+    @pytest.mark.usefixtures("add_ssl_extra_to_transport")
+    def test____eof_received____returns_False_for_ssl_transport(
+        self,
+        protocol: StreamReaderBufferedProtocol,
+    ) -> None:
+        # Arrange
+
+        # Act & Assert
+        assert protocol.eof_received() is False
+
+    @pytest.mark.asyncio
+    @pytest.mark.parametrize("blocking", [False, True], ids=lambda p: f"blocking=={p}")
+    async def test____receive_data____default(
+        self,
+        blocking: bool,
+        event_loop: asyncio.AbstractEventLoop,
+        protocol: StreamReaderBufferedProtocol,
+        mock_asyncio_transport: MagicMock,
+        data_receiver: _ProtocolDataReceiver,
+    ) -> None:
+        # Arrange
+        if blocking:
+            event_loop.call_later(0.5, self.write_in_protocol_buffer, protocol, b"abcdef")
+        else:
+            self.write_in_protocol_buffer(protocol, b"abcdef")
 
         # Act
-        nbytes = await socket.recv_into(buffer)
+        data = await data_receiver(protocol, 1024)
 
         # Assert
-        assert nbytes == 4
-        assert buffer[:nbytes] == b"data"
-        mock_async_socket.recv_into.assert_awaited_once_with(buffer)
+        assert data == b"abcdef"
+        mock_asyncio_transport.resume_reading.assert_not_called()
 
-    async def test____send_all____sends_data_to_async_socket(
+    @pytest.mark.asyncio
+    async def test____receive_data____partial_read(
         self,
-        socket: RawStreamSocketAdapter,
-        mock_async_socket: MagicMock,
+        protocol: StreamReaderBufferedProtocol,
+        mock_asyncio_transport: MagicMock,
+        data_receiver: _ProtocolDataReceiver,
     ) -> None:
         # Arrange
-        mock_async_socket.sendall.return_value = None
+        self.write_in_protocol_buffer(protocol, b"abcdef")
 
         # Act
-        await socket.send_all(b"data")
+        first = await data_receiver(protocol, 3)
+        second = await data_receiver(protocol, 3)
 
         # Assert
-        mock_async_socket.sendall.assert_awaited_once_with(b"data")
+        assert first == b"abc"
+        assert second == b"def"
+        mock_asyncio_transport.resume_reading.assert_not_called()
 
-    async def test____send_all_from_iterable____use_async_socket_sendmsg(
+    @pytest.mark.asyncio
+    async def test____receive_data____buffer_updated_several_times(
         self,
-        socket: RawStreamSocketAdapter,
-        mock_async_socket: MagicMock,
+        event_loop: asyncio.AbstractEventLoop,
+        protocol: StreamReaderBufferedProtocol,
+        mock_asyncio_transport: MagicMock,
+        data_receiver: _ProtocolDataReceiver,
     ) -> None:
         # Arrange
-        mock_async_socket.sendmsg.return_value = None
+        event_loop.call_soon(self.write_in_protocol_buffer, protocol, b"abc")
+        event_loop.call_soon(self.write_in_protocol_buffer, protocol, b"def")
 
         # Act
-        await socket.send_all_from_iterable([b"data", b"to", b"send"])
+        data = await data_receiver(protocol, 1024)
 
         # Assert
-        mock_async_socket.sendmsg.assert_awaited_once_with([b"data", b"to", b"send"])
-        mock_async_socket.sendall.assert_not_called()
+        assert data == b"abcdef"
+        mock_asyncio_transport.resume_reading.assert_not_called()
 
-    async def test____send_all_from_iterable____fallback_to_sendall(
+    @pytest.mark.asyncio
+    async def test____receive_data____null_bufsize(
         self,
-        socket: RawStreamSocketAdapter,
-        mock_async_socket: MagicMock,
+        protocol: StreamReaderBufferedProtocol,
+        mock_asyncio_transport: MagicMock,
+        data_receiver: _ProtocolDataReceiver,
+    ) -> None:
+        # Arrange
+
+        # Act
+        data = await data_receiver(protocol, 0)
+
+        # Assert
+        assert data == b""
+        mock_asyncio_transport.resume_reading.assert_not_called()
+
+    @pytest.mark.asyncio
+    @pytest.mark.parametrize("eof_reason", ["eof_received", "connection_lost"])
+    @pytest.mark.parametrize("blocking", [False, True], ids=lambda p: f"blocking=={p}")
+    async def test____receive_data____eof(
+        self,
+        blocking: bool,
+        eof_reason: Literal["eof_received", "connection_lost"],
+        event_loop: asyncio.AbstractEventLoop,
+        protocol: StreamReaderBufferedProtocol,
+        mock_asyncio_transport: MagicMock,
+        data_receiver: _ProtocolDataReceiver,
+    ) -> None:
+        # Arrange
+        def protocol_eof_handler() -> None:
+            match eof_reason:
+                case "eof_received":
+                    keep_open = self.write_eof_in_protocol_buffer(protocol)
+                    assert keep_open is True
+                case "connection_lost":
+                    protocol.connection_lost(None)
+                case _:
+                    pytest.fail("Invalid argument")
+
+        if blocking:
+            event_loop.call_later(0.5, protocol_eof_handler)
+        else:
+            protocol_eof_handler()
+
+        # Act
+        data = await data_receiver(protocol, 1024)
+
+        # Assert
+        assert data == b""
+        mock_asyncio_transport.resume_reading.assert_not_called()
+
+    @pytest.mark.asyncio
+    @pytest.mark.parametrize("blocking", [False, True], ids=lambda p: f"blocking=={p}")
+    async def test____receive_data____connection_lost_by_unrelated_error(
+        self,
+        blocking: bool,
+        event_loop: asyncio.AbstractEventLoop,
+        protocol: StreamReaderBufferedProtocol,
+        data_receiver: _ProtocolDataReceiver,
+    ) -> None:
+        # Arrange
+        exception = OSError("Something bad happen")
+        if blocking:
+            event_loop.call_later(0.5, protocol.connection_lost, exception)
+        else:
+            protocol.connection_lost(exception)
+
+        # Act & Assert
+        with pytest.raises(OSError) as exc_info:
+            _ = await data_receiver(protocol, 1024)
+
+        assert exc_info.value is exception
+
+    @pytest.mark.asyncio
+    async def test____receive_data____connection_reset(
+        self,
+        event_loop: asyncio.AbstractEventLoop,
+        protocol: StreamReaderBufferedProtocol,
+        data_receiver: _ProtocolDataReceiver,
+    ) -> None:
+        # Arrange
+        event_loop.call_soon(self.write_in_protocol_buffer, protocol, b"abc")
+        event_loop.call_soon(protocol.connection_lost, None)
+
+        # Act & Assert
+        for _ in range(3):
+            with pytest.raises(ConnectionResetError):
+                _ = await data_receiver(protocol, 1024)
+
+    @pytest.mark.asyncio
+    async def test____receive_data____invalid_bufsize(
+        self,
+        protocol: StreamReaderBufferedProtocol,
+    ) -> None:
+        # Arrange
+
+        # Act & Assert
+        with pytest.raises(ValueError, match=r"^'bufsize' must be a positive or null integer$"):
+            _ = await protocol.receive_data(-1)
+
+    @pytest.mark.asyncio
+    @pytest.mark.parametrize("read_event", ["buffer_updated", "eof_received", "connection_lost", "connection_lost_with_error"])
+    async def test____receive_data____read_event_and_waiter_is_cancelled(
+        self,
+        read_event: Literal["buffer_updated", "eof_received", "connection_lost", "connection_lost_with_error"],
+        protocol: StreamReaderBufferedProtocol,
+        mock_asyncio_transport: MagicMock,
+        data_receiver: _ProtocolDataReceiver,
+    ) -> None:
+        # Arrange
+        read_task = asyncio.create_task(data_receiver(protocol, 1024))
+        await asyncio.sleep(0)
+        read_task.cancel()
+
+        # Act
+        match read_event:
+            case "buffer_updated":
+                self.write_in_protocol_buffer(protocol, b"data")
+            case "eof_received":
+                self.write_eof_in_protocol_buffer(protocol)
+            case "connection_lost":
+                protocol.connection_lost(None)
+            case "connection_lost_with_error":
+                protocol.connection_lost(OSError("Something bad happen"))
+            case _:
+                pytest.fail("Invalid argument")
+        await asyncio.wait({read_task})
+
+        # Assert
+        assert read_task.cancelled()
+        mock_asyncio_transport.resume_reading.assert_not_called()
+
+    @pytest.mark.asyncio
+    async def test____receive_data____read_flow_control(
+        self,
+        protocol: StreamReaderBufferedProtocol,
+        mock_asyncio_transport: MagicMock,
+        data_receiver: _ProtocolDataReceiver,
+    ) -> None:
+        # Arrange
+        low_water, high_water = protocol._get_read_buffer_limits()
+
+        # Act & Assert
+        while protocol._get_read_buffer_size() < high_water:
+            assert not protocol._reading_paused()
+            mock_asyncio_transport.pause_reading.assert_not_called()
+            self.write_in_protocol_buffer(protocol, b"X")
+
+        mock_asyncio_transport.pause_reading.assert_called_once()
+        assert protocol._reading_paused()
+        mock_asyncio_transport.resume_reading.assert_not_called()
+
+        while protocol._get_read_buffer_size() > low_water:
+            assert protocol._reading_paused()
+            mock_asyncio_transport.resume_reading.assert_not_called()
+            data = await data_receiver(protocol, 1)
+            assert data == b"X"
+
+        assert not protocol._reading_paused()
+        mock_asyncio_transport.resume_reading.assert_called_once()
+
+    @pytest.mark.asyncio
+    @pytest.mark.parametrize("exception", [None, OSError("Something bad happen")])
+    async def test____receive_data____buffer_release(
+        self,
+        exception: OSError | None,
+        protocol: StreamReaderBufferedProtocol,
+        data_receiver: _ProtocolDataReceiver,
+    ) -> None:
+        # Arrange
+        self.write_in_protocol_buffer(protocol, b"abcdef")
+
+        # Act & Assert
+        protocol.connection_lost(exception)
+        assert protocol._get_read_buffer_size() == 0
+        with pytest.raises(BufferError):
+            protocol.get_buffer(-1)
+
+        with pytest.raises(OSError):
+            _ = await data_receiver(protocol, 3)
+
+    @pytest.mark.asyncio
+    @pytest.mark.parametrize("actual_reader", ["receive_data", "receive_data_into"])
+    @pytest.mark.parametrize("new_reader", ["receive_data", "receive_data_into"])
+    async def test____receive_data____concurrent_read_error(
+        self,
+        actual_reader: Literal["receive_data", "receive_data_into"],
+        new_reader: Literal["receive_data", "receive_data_into"],
+        protocol: StreamReaderBufferedProtocol,
+        request: pytest.FixtureRequest,
+    ) -> None:
+        # Arrange
+        read_task: asyncio.Task[Any]
+        match actual_reader:
+            case "receive_data":
+                read_task = asyncio.create_task(protocol.receive_data(1024))
+            case "receive_data_into":
+                read_task = asyncio.create_task(protocol.receive_data_into(bytearray(1024)))
+            case _:
+                pytest.fail("Invalid param")
+        await asyncio.sleep(0)
+        request.addfinalizer(read_task.cancel)
+
+        # Act & Assert
+        with pytest.raises(RuntimeError, match=r"^\w+\(\) called while another coroutine is already waiting for incoming data$"):
+            match new_reader:
+                case "receive_data":
+                    await protocol.receive_data(1024)
+                case "receive_data_into":
+                    await protocol.receive_data_into(bytearray(1024))
+                case _:
+                    pytest.fail("Invalid param")
+
+    @pytest.mark.asyncio
+    @pytest.mark.parametrize("transport_is_closing", [False, True], ids=lambda p: f"transport_is_closing=={p}")
+    async def test____drain_helper____quick_exit_if_not_paused(
+        self,
+        transport_is_closing: bool,
+        event_loop: asyncio.AbstractEventLoop,
+        protocol: StreamReaderBufferedProtocol,
+        mock_asyncio_transport: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
-        mock_async_socket.sendmsg.side_effect = UnsupportedOperation
+        assert not protocol._writing_paused()
+        mock_create_future: MagicMock = mocker.patch.object(event_loop, "create_future")
+        mock_asyncio_transport.is_closing.side_effect = [transport_is_closing]
 
         # Act
-        await socket.send_all_from_iterable([b"data", b"to", b"send"])
+        await protocol.writer_drain()
 
         # Assert
-        mock_async_socket.sendmsg.assert_awaited_once()
-        assert mock_async_socket.sendall.await_args_list == list(map(mocker.call, [b"data", b"to", b"send"]))
+        mock_create_future.assert_not_called()
 
-    async def test____send_eof____shutdown_socket(
+    @pytest.mark.asyncio
+    async def test____drain_helper____raise_connection_reset_if_connection_is_lost(
         self,
-        socket: RawStreamSocketAdapter,
-        mock_async_socket: MagicMock,
+        protocol: StreamReaderBufferedProtocol,
+        mock_asyncio_transport: MagicMock,
+    ) -> None:
+        # Arrange
+        assert not protocol._writing_paused()
+
+        from errno import ECONNRESET
+
+        protocol.connection_lost(None)
+        mock_asyncio_transport.is_closing.return_value = True
+
+        # Act & Assert
+        with pytest.raises(OSError) as exc_info:
+            await protocol.writer_drain()
+
+        assert exc_info.value.errno == ECONNRESET
+
+    @pytest.mark.asyncio
+    async def test____drain_helper____connection_lost_by_unrelated_error(
+        self,
+        protocol: StreamReaderBufferedProtocol,
+        mock_asyncio_transport: MagicMock,
+    ) -> None:
+        # Arrange
+        exception = OSError("Something bad happen")
+        protocol.connection_lost(exception)
+        mock_asyncio_transport.is_closing.return_value = True
+
+        # Act & Assert
+        with pytest.raises(OSError) as exc_info:
+            await protocol.writer_drain()
+
+        assert exc_info.value is exception
+
+    @pytest.mark.asyncio
+    @pytest.mark.parametrize("cancel_tasks", [False, True], ids=lambda p: f"cancel_tasks_before=={p}")
+    async def test____drain_helper____wait_during_writing_pause(
+        self,
+        cancel_tasks: bool,
+        protocol: StreamReaderBufferedProtocol,
+    ) -> None:
+        # Arrange
+        import inspect
+
+        # Act
+        protocol.pause_writing()
+        assert protocol._writing_paused()
+        tasks: set[asyncio.Task[None]] = set()
+        for _ in range(10):
+            tasks.add(asyncio.create_task(protocol.writer_drain()))
+        await asyncio.sleep(0)  # Suspend to let the event loop start all tasks
+        assert all(inspect.getcoroutinestate(t.get_coro()) == "CORO_SUSPENDED" for t in tasks)  # type: ignore[arg-type]
+        if cancel_tasks:
+            for t in tasks:
+                t.cancel()
+        protocol.resume_writing()
+        await asyncio.sleep(0)  # Suspend to let the event loop run all tasks
+
+        # Assert
+        if cancel_tasks:
+            assert all(t.done() and t.cancelled() for t in tasks)
+        else:
+            assert all(t.done() and t.exception() is None and t.result() is None for t in tasks)
+
+    @pytest.mark.asyncio
+    @pytest.mark.parametrize("exception", [None, OSError("Something bad happen")])
+    @pytest.mark.parametrize("cancel_tasks", [False, True], ids=lambda p: f"cancel_tasks_before=={p}")
+    async def test____drain_helper____wait_during_writing_pause____connection_lost_while_waiting(
+        self,
+        cancel_tasks: bool,
+        exception: Exception | None,
+        protocol: StreamReaderBufferedProtocol,
     ) -> None:
         # Arrange
-        mock_async_socket.shutdown.return_value = None
-        mock_async_socket.did_shutdown_SHUT_WR = False
+        import inspect
+
+        protocol.pause_writing()
+        assert protocol._writing_paused()
+        tasks: set[asyncio.Task[None]] = set()
+        for _ in range(10):
+            tasks.add(asyncio.create_task(protocol.writer_drain()))
+        await asyncio.sleep(0)  # Suspend to let the event loop start all tasks
+        assert all(inspect.getcoroutinestate(t.get_coro()) == "CORO_SUSPENDED" for t in tasks)  # type: ignore[arg-type]
+        if cancel_tasks:
+            for t in tasks:
+                t.cancel()
 
         # Act
-        await socket.send_eof()
+        protocol.connection_lost(exception)
+        await asyncio.sleep(0)  # Suspend to let the event loop run all tasks
 
         # Assert
-        mock_async_socket.shutdown.assert_awaited_once_with(SHUT_WR)
+        if cancel_tasks:
+            assert all(t.done() and t.cancelled() for t in tasks)
+        elif exception is None:
+            assert all(t.done() and isinstance(t.exception(), ConnectionResetError) for t in tasks), tasks
+        else:
+            assert all(t.done() and t.exception() is exception for t in tasks)
 
-    async def test____get_extra_info____returns_socket_info(
+    @pytest.mark.asyncio
+    async def test____special_case____transport_pause_reading_not_supported(
         self,
-        socket: RawStreamSocketAdapter,
-        mock_tcp_socket: MagicMock,
+        protocol: StreamReaderBufferedProtocol,
+        mock_asyncio_transport: MagicMock,
+        data_receiver: _ProtocolDataReceiver,
     ) -> None:
         # Arrange
+        mock_asyncio_transport.pause_reading.side_effect = NotImplementedError
+        _, high_water = protocol._get_read_buffer_limits()
 
         # Act & Assert
-        assert socket.extra(SocketAttribute.socket) is mock_tcp_socket
-        assert socket.extra(SocketAttribute.family) == mock_tcp_socket.family
-        assert socket.extra(SocketAttribute.sockname) == ("127.0.0.1", 11111)
-        assert socket.extra(SocketAttribute.peername) == ("127.0.0.1", 12345)
+        self.write_in_protocol_buffer(protocol, b"X" * (high_water + 1))
+        mock_asyncio_transport.pause_reading.assert_called_once()
+        assert not protocol._reading_paused()
+        mock_asyncio_transport.resume_reading.assert_not_called()
+
+        await data_receiver(protocol, protocol._get_read_buffer_size())
+        assert protocol._get_read_buffer_size() == 0
+
+        with pytest.raises(ConnectionAbortedError):
+            _ = await data_receiver(protocol, 1024)
+
+        assert not protocol._reading_paused()
+        mock_asyncio_transport.resume_reading.assert_not_called()
```

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_async/test_asyncio_backend/test_tasks.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_async/test_asyncio_backend/test_tasks.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 from __future__ import annotations
 
 import asyncio
+from collections.abc import Coroutine
 from typing import TYPE_CHECKING, Any
 
+from easynetwork.lowlevel.api_async.backend.abc import TaskInfo
 from easynetwork.lowlevel.std_asyncio.tasks import Task, TaskUtils
 
 import pytest
 
 if TYPE_CHECKING:
     from unittest.mock import AsyncMock
 
@@ -14,24 +16,44 @@
 
 
 class TestTask:
     @pytest.fixture
     @staticmethod
     def mock_asyncio_task(mocker: MockerFixture) -> AsyncMock:
         mock = mocker.NonCallableMagicMock(spec=asyncio.Task)
+        mock.get_name.return_value = "mock_asyncio_task"
+        mock.get_coro.return_value = mocker.NonCallableMagicMock(spec=Coroutine)
         mock.done.return_value = False
         mock.cancelled.return_value = False
         mock.cancel.return_value = True
         return mock
 
     @pytest.fixture
     @staticmethod
     def task(mock_asyncio_task: AsyncMock) -> Task[Any]:
         return Task(mock_asyncio_task)
 
+    def test____info_property____asyncio_task_introspection(
+        self,
+        mock_asyncio_task: AsyncMock,
+    ) -> None:
+        # Arrange
+        task: Task[Any] = Task(mock_asyncio_task)
+        mock_asyncio_task.get_name.assert_not_called()
+        mock_asyncio_task.get_coro.assert_not_called()
+
+        # Act
+        task_info = task.info
+
+        # Assert
+        assert isinstance(task_info, TaskInfo)
+        assert task_info.name == "mock_asyncio_task"
+        assert task_info.id == id(mock_asyncio_task)
+        assert task_info.coro is mock_asyncio_task.get_coro.return_value
+
     def test____equality____between_two_tasks_referencing_same_asyncio_task(self, mock_asyncio_task: AsyncMock) -> None:
         # Arrange
         task1: Task[Any] = Task(mock_asyncio_task)
         task2: Task[Any] = Task(mock_asyncio_task)
 
         # Act & Assert
         assert task1 is not task2
@@ -86,48 +108,14 @@
         task_cancelled = task.cancel()
 
         # Assert
         mock_asyncio_task.cancel.assert_called_once_with()
         assert task_cancelled is mocker.sentinel.task_cancelled
 
     @pytest.mark.asyncio
-    async def test____wait____await_task(
-        self,
-        task: Task[Any],
-        mock_asyncio_task: AsyncMock,
-        mocker: MockerFixture,
-    ) -> None:
-        # Arrange
-        mock_asyncio_task.done.return_value = False
-        mock_asyncio_wait: AsyncMock = mocker.patch("asyncio.wait", autospec=True)
-
-        # Act
-        await task.wait()
-
-        # Assert
-        mock_asyncio_wait.assert_awaited_once_with({mock_asyncio_task})
-
-    @pytest.mark.asyncio
-    async def test____wait____task_already_done(
-        self,
-        task: Task[Any],
-        mock_asyncio_task: AsyncMock,
-        mocker: MockerFixture,
-    ) -> None:
-        # Arrange
-        mock_asyncio_task.done.return_value = True
-        mock_asyncio_wait: AsyncMock = mocker.patch("asyncio.wait", autospec=True)
-
-        # Act
-        await task.wait()
-
-        # Assert
-        mock_asyncio_wait.assert_awaited_once_with({mock_asyncio_task})
-
-    @pytest.mark.asyncio
     async def test____join____await_task(
         self,
         task: Task[Any],
         mock_asyncio_task: AsyncMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
```

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_async/test_asyncio_backend/test_utils.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_async/test_asyncio_backend/test_utils.py`

 * *Files 18% similar despite different names*

```diff
@@ -31,14 +31,15 @@
     create_connection,
     create_datagram_connection,
     ensure_resolved,
     open_listener_sockets_from_getaddrinfo_result,
     wait_until_readable,
     wait_until_writable,
 )
+from easynetwork.lowlevel.std_asyncio.tasks import TaskUtils
 
 import pytest
 
 from ....tools import is_proactor_event_loop
 from ..._utils import datagram_addrinfo_list, stream_addrinfo_list
 
 if TYPE_CHECKING:
@@ -70,15 +71,28 @@
 @pytest.fixture
 def mock_socket_ipv6(mock_socket_factory: Callable[[], MagicMock]) -> MagicMock:
     return mock_socket_factory()
 
 
 @pytest.fixture(autouse=True)
 def mock_socket_cls(mock_socket_ipv4: MagicMock, mock_socket_ipv6: MagicMock, mocker: MockerFixture) -> MagicMock:
-    return mocker.patch("socket.socket", side_effect=[mock_socket_ipv4, mock_socket_ipv6])
+    def side_effect(family: int, type: int, proto: int) -> MagicMock:
+        if family == AF_INET6:
+            used_socket = mock_socket_ipv6
+        elif family == AF_INET:
+            used_socket = mock_socket_ipv4
+        else:
+            raise error_from_errno(errno.EAFNOSUPPORT)
+
+        used_socket.family = family
+        used_socket.type = type
+        used_socket.proto = proto
+        return used_socket
+
+    return mocker.patch("socket.socket", side_effect=side_effect)
 
 
 @pytest.mark.asyncio
 async def test____ensure_resolved____try_numeric_first(
     event_loop: asyncio.AbstractEventLoop,
     mock_getaddrinfo: AsyncMock,
     mock_stdlib_socket_getaddrinfo: MagicMock,
@@ -207,25 +221,23 @@
 class _CreateConnectionCallable(TypingProtocol):
     async def __call__(
         self,
         host: str,
         port: int,
         loop: asyncio.AbstractEventLoop,
         local_address: tuple[str, int] | None = None,
-    ) -> SocketType:
-        ...
+    ) -> SocketType: ...
 
 
 class _AddrInfoListFactory(TypingProtocol):
     def __call__(
         self,
         port: int,
         families: Sequence[int] = ...,
-    ) -> Sequence[tuple[int, int, int, str, tuple[Any, ...]]]:
-        ...
+    ) -> Sequence[tuple[int, int, int, str, tuple[Any, ...]]]: ...
 
 
 @pytest.fixture(params=[SOCK_STREAM, SOCK_DGRAM], ids=lambda sock_type: f"sock_type=={sock_type!r}")
 def connection_socktype(request: pytest.FixtureRequest) -> int:
     return request.param
 
 
@@ -282,27 +294,27 @@
         ]
     else:
         assert mock_getaddrinfo.await_args_list == [
             mocker.call(remote_host, remote_port, family=AF_UNSPEC, type=connection_socktype, proto=0, flags=0),
             mocker.call(*local_address, family=AF_UNSPEC, type=connection_socktype, proto=0, flags=0),
         ]
 
-    mock_socket_cls.assert_called_once_with(AF_INET, connection_socktype, expected_proto)
-    assert socket is mock_socket_ipv4
+    mock_socket_cls.assert_called_once_with(AF_INET6, connection_socktype, expected_proto)
+    assert socket is mock_socket_ipv6
 
-    mock_socket_ipv4.setblocking.assert_called_once_with(False)
+    mock_socket_ipv6.setblocking.assert_called_once_with(False)
     if local_address is None:
-        mock_socket_ipv4.bind.assert_not_called()
+        mock_socket_ipv6.bind.assert_not_called()
     else:
-        mock_socket_ipv4.bind.assert_called_once_with(("127.0.0.1", 11111))
-    mock_sock_connect.assert_awaited_once_with(mock_socket_ipv4, ("127.0.0.1", 12345))
-    mock_socket_ipv4.close.assert_not_called()
+        mock_socket_ipv6.bind.assert_called_once_with(("::1", 11111, 0, 0))
+    mock_sock_connect.assert_awaited_once_with(mock_socket_ipv6, ("::1", 12345, 0, 0))
+    mock_socket_ipv6.close.assert_not_called()
 
-    mock_socket_ipv6.setblocking.assert_not_called()
-    mock_socket_ipv6.bind.assert_not_called()
+    mock_socket_ipv4.setblocking.assert_not_called()
+    mock_socket_ipv4.bind.assert_not_called()
 
 
 @pytest.mark.asyncio
 @pytest.mark.parametrize("fail_on", ["socket", "bind", "connect"], ids=lambda fail_on: f"fail_on=={fail_on}")
 async def test____create_connection____first_failed(
     event_loop: asyncio.AbstractEventLoop,
     fail_on: Literal["socket", "bind", "connect"],
@@ -323,60 +335,60 @@
     if local_address is None:
         mock_getaddrinfo.side_effect = [addrinfo_list_factory(remote_port)]
     else:
         mock_getaddrinfo.side_effect = [addrinfo_list_factory(remote_port), addrinfo_list_factory(local_address[1])]
 
     match fail_on:
         case "socket":
-            mock_socket_cls.side_effect = [error_from_errno(errno.EAFNOSUPPORT), mock_socket_ipv6]
+            mock_socket_cls.side_effect = [error_from_errno(errno.EAFNOSUPPORT), mock_socket_ipv4]
         case "bind":
-            mock_socket_ipv4.bind.side_effect = error_from_errno(errno.EADDRINUSE)
+            mock_socket_ipv6.bind.side_effect = error_from_errno(errno.EADDRINUSE)
         case "connect":
             mock_sock_connect.side_effect = [error_from_errno(errno.ECONNREFUSED), None]
         case _:
             assert_never(fail_on)
 
     # Act
     socket = await create_connection_of_socktype(remote_host, remote_port, event_loop, local_address=local_address)
 
     # Assert
     if connection_socktype == SOCK_STREAM:
         assert mock_socket_cls.call_args_list == [
-            mocker.call(AF_INET, SOCK_STREAM, IPPROTO_TCP),
             mocker.call(AF_INET6, SOCK_STREAM, IPPROTO_TCP),
+            mocker.call(AF_INET, SOCK_STREAM, IPPROTO_TCP),
         ]
     else:
         assert mock_socket_cls.call_args_list == [
-            mocker.call(AF_INET, SOCK_DGRAM, IPPROTO_UDP),
             mocker.call(AF_INET6, SOCK_DGRAM, IPPROTO_UDP),
+            mocker.call(AF_INET, SOCK_DGRAM, IPPROTO_UDP),
         ]
-    assert socket is mock_socket_ipv6
+    assert socket is mock_socket_ipv4
 
     if fail_on != "socket":
-        mock_socket_ipv4.setblocking.assert_called_once_with(False)
+        mock_socket_ipv6.setblocking.assert_called_once_with(False)
         if local_address is None:
-            mock_socket_ipv4.bind.assert_not_called()
+            mock_socket_ipv6.bind.assert_not_called()
         else:
-            mock_socket_ipv4.bind.assert_called_once_with(("127.0.0.1", 11111))
+            mock_socket_ipv6.bind.assert_called_once_with(("::1", 11111, 0, 0))
         match fail_on:
             case "bind":
-                assert mocker.call(mock_socket_ipv4, ("127.0.0.1", 12345)) not in mock_sock_connect.await_args_list
+                assert mocker.call(mock_socket_ipv6, ("::1", 12345, 0, 0)) not in mock_sock_connect.await_args_list
             case "connect":
-                mock_sock_connect.assert_any_await(mock_socket_ipv4, ("127.0.0.1", 12345))
+                mock_sock_connect.assert_any_await(mock_socket_ipv6, ("::1", 12345, 0, 0))
             case _:
                 assert_never(fail_on)
-        mock_socket_ipv4.close.assert_called_once_with()
+        mock_socket_ipv6.close.assert_called_once_with()
 
-    mock_socket_ipv6.setblocking.assert_called_once_with(False)
+    mock_socket_ipv4.setblocking.assert_called_once_with(False)
     if local_address is None:
-        mock_socket_ipv6.bind.assert_not_called()
+        mock_socket_ipv4.bind.assert_not_called()
     else:
-        mock_socket_ipv6.bind.assert_called_once_with(("::1", 11111, 0, 0))
-    mock_sock_connect.assert_awaited_with(mock_socket_ipv6, ("::1", 12345, 0, 0))
-    mock_socket_ipv6.close.assert_not_called()
+        mock_socket_ipv4.bind.assert_called_once_with(("127.0.0.1", 11111))
+    mock_sock_connect.assert_awaited_with(mock_socket_ipv4, ("127.0.0.1", 12345))
+    mock_socket_ipv4.close.assert_not_called()
 
 
 @pytest.mark.asyncio
 @pytest.mark.parametrize("fail_on", ["socket", "bind", "connect"], ids=lambda fail_on: f"fail_on=={fail_on}")
 async def test____create_connection____all_failed(
     event_loop: asyncio.AbstractEventLoop,
     fail_on: Literal["socket", "bind", "connect"],
@@ -420,21 +432,21 @@
     assert os_errors is not None
     assert len(os_errors.exceptions) == 2
     assert all(isinstance(exc, OSError) for exc in os_errors.exceptions)
     del os_errors
 
     if connection_socktype == SOCK_STREAM:
         assert mock_socket_cls.call_args_list == [
-            mocker.call(AF_INET, SOCK_STREAM, IPPROTO_TCP),
             mocker.call(AF_INET6, SOCK_STREAM, IPPROTO_TCP),
+            mocker.call(AF_INET, SOCK_STREAM, IPPROTO_TCP),
         ]
     else:
         assert mock_socket_cls.call_args_list == [
-            mocker.call(AF_INET, SOCK_DGRAM, IPPROTO_UDP),
             mocker.call(AF_INET6, SOCK_DGRAM, IPPROTO_UDP),
+            mocker.call(AF_INET, SOCK_DGRAM, IPPROTO_UDP),
         ]
 
     if fail_on != "socket":
         mock_socket_ipv4.setblocking.assert_called_once_with(False)
         mock_socket_ipv6.setblocking.assert_called_once_with(False)
         if local_address is None:
             mock_socket_ipv4.bind.assert_not_called()
@@ -456,18 +468,19 @@
 
 
 @pytest.mark.asyncio
 @pytest.mark.parametrize("fail_on", ["socket", "connect"], ids=lambda fail_on: f"fail_on=={fail_on}")
 async def test____create_connection____unrelated_exception(
     event_loop: asyncio.AbstractEventLoop,
     fail_on: Literal["socket", "connect"],
+    connection_socktype: int,
     create_connection_of_socktype: _CreateConnectionCallable,
     addrinfo_list_factory: _AddrInfoListFactory,
     mock_socket_cls: MagicMock,
-    mock_socket_ipv4: MagicMock,
+    mock_socket_ipv6: MagicMock,
     mock_getaddrinfo: AsyncMock,
     mock_sock_connect: AsyncMock,
 ) -> None:
     # Arrange
     remote_host, remote_port = "localhost", 12345
 
     mock_getaddrinfo.side_effect = [addrinfo_list_factory(remote_port)]
@@ -482,17 +495,22 @@
             assert_never(fail_on)
 
     # Act
     with pytest.raises(BaseException) as exc_info:
         await create_connection_of_socktype(remote_host, remote_port, event_loop)
 
     # Assert
-    assert exc_info.value is expected_failure_exception
+    if connection_socktype == SOCK_STREAM:
+        assert isinstance(exc_info.value, BaseExceptionGroup)
+        assert len(exc_info.value.exceptions) == 1
+        assert exc_info.value.exceptions[0] is expected_failure_exception
+    else:
+        assert exc_info.value is expected_failure_exception
     if fail_on != "socket":
-        mock_socket_ipv4.close.assert_called_once_with()
+        mock_socket_ipv6.close.assert_called_once_with()
 
 
 @pytest.mark.asyncio
 @pytest.mark.parametrize("fail_on", ["remote_address", "local_address"], ids=lambda fail_on: f"fail_on=={fail_on}")
 async def test____create_connection____getaddrinfo_returned_empty_list(
     event_loop: asyncio.AbstractEventLoop,
     fail_on: Literal["remote_address", "local_address"],
@@ -559,14 +577,205 @@
     del os_errors
 
     mock_socket_ipv4.bind.assert_not_called()
     mock_socket_ipv6.bind.assert_not_called()
     mock_sock_connect.assert_not_called()
 
 
+@pytest.mark.asyncio
+@pytest.mark.parametrize("connection_socktype", [SOCK_STREAM], indirect=True, ids=repr)
+@pytest.mark.flaky(retries=3)
+async def test____create_connection____happy_eyeballs_delay____connect_cancellation(
+    event_loop: asyncio.AbstractEventLoop,
+    addrinfo_list_factory: _AddrInfoListFactory,
+    mock_socket_cls: MagicMock,
+    mock_socket_ipv4: MagicMock,
+    mock_socket_ipv6: MagicMock,
+    mock_getaddrinfo: AsyncMock,
+    mock_sock_connect: AsyncMock,
+    mocker: MockerFixture,
+) -> None:
+    # Arrange
+    timestamps: list[float] = []
+    remote_host, remote_port = "localhost", 12345
+    mock_getaddrinfo.side_effect = [addrinfo_list_factory(remote_port, families=[AF_INET6, AF_INET])]
+
+    async def connect_side_effect(sock: SocketType, address: tuple[Any, ...]) -> None:
+        timestamps.append(event_loop.time())
+        if sock.family == AF_INET6:
+            await asyncio.sleep(1)
+        else:
+            await asyncio.sleep(0.01)
+
+    mock_sock_connect.side_effect = connect_side_effect
+
+    # Act
+    socket = await create_connection(remote_host, remote_port, event_loop, happy_eyeballs_delay=0.5)
+
+    # Assert
+    assert socket is mock_socket_ipv4
+    assert mock_socket_cls.call_args_list == [
+        mocker.call(AF_INET6, SOCK_STREAM, IPPROTO_TCP),
+        mocker.call(AF_INET, SOCK_STREAM, IPPROTO_TCP),
+    ]
+
+    mock_socket_ipv6.close.assert_called_once_with()
+    mock_socket_ipv4.close.assert_not_called()
+
+    ipv6_start_time, ipv4_start_time = timestamps
+    assert ipv4_start_time - ipv6_start_time == pytest.approx(0.5, rel=1e-1)
+
+
+@pytest.mark.asyncio
+@pytest.mark.parametrize("connection_socktype", [SOCK_STREAM], indirect=True, ids=repr)
+async def test____create_connection____happy_eyeballs_delay____connect_too_late(
+    event_loop: asyncio.AbstractEventLoop,
+    addrinfo_list_factory: _AddrInfoListFactory,
+    mock_socket_cls: MagicMock,
+    mock_socket_ipv4: MagicMock,
+    mock_socket_ipv6: MagicMock,
+    mock_getaddrinfo: AsyncMock,
+    mock_sock_connect: AsyncMock,
+    mocker: MockerFixture,
+) -> None:
+    # Arrange
+    remote_host, remote_port = "localhost", 12345
+    mock_getaddrinfo.side_effect = [addrinfo_list_factory(remote_port, families=[AF_INET6, AF_INET])]
+
+    async def connect_side_effect(sock: SocketType, address: tuple[Any, ...]) -> None:
+        try:
+            await asyncio.sleep(1)
+        except asyncio.CancelledError:
+            TaskUtils.current_asyncio_task().uncancel()
+            await asyncio.sleep(0)
+
+    mock_sock_connect.side_effect = connect_side_effect
+
+    # Act
+    socket = await create_connection(remote_host, remote_port, event_loop, happy_eyeballs_delay=0.25)
+
+    # Assert
+    assert socket is mock_socket_ipv6
+    assert mock_socket_cls.call_args_list == [
+        mocker.call(AF_INET6, SOCK_STREAM, IPPROTO_TCP),
+        mocker.call(AF_INET, SOCK_STREAM, IPPROTO_TCP),
+    ]
+
+    mock_socket_ipv4.close.assert_called_once_with()
+    mock_socket_ipv6.close.assert_not_called()
+
+
+@pytest.mark.asyncio
+@pytest.mark.parametrize("connection_socktype", [SOCK_STREAM], indirect=True, ids=repr)
+async def test____create_connection____happy_eyeballs_delay____winner_closed_because_of_exception_in_another_task(
+    event_loop: asyncio.AbstractEventLoop,
+    addrinfo_list_factory: _AddrInfoListFactory,
+    mock_socket_cls: MagicMock,
+    mock_socket_ipv4: MagicMock,
+    mock_socket_ipv6: MagicMock,
+    mock_getaddrinfo: AsyncMock,
+    mock_sock_connect: AsyncMock,
+    mocker: MockerFixture,
+) -> None:
+    # Arrange
+    expected_failure_exception = BaseException("error")
+    remote_host, remote_port = "localhost", 12345
+    mock_getaddrinfo.side_effect = [addrinfo_list_factory(remote_port, families=[AF_INET6, AF_INET])]
+
+    async def connect_side_effect(sock: SocketType, address: tuple[Any, ...]) -> None:
+        try:
+            await asyncio.sleep(0.5)
+        except asyncio.CancelledError:
+            raise expected_failure_exception from None
+
+    mock_sock_connect.side_effect = connect_side_effect
+
+    # Act
+    with pytest.raises(BaseExceptionGroup) as exc_info:
+        await create_connection(remote_host, remote_port, event_loop, happy_eyeballs_delay=0.25)
+
+    # Assert
+    assert mock_socket_cls.call_args_list == [
+        mocker.call(AF_INET6, SOCK_STREAM, IPPROTO_TCP),
+        mocker.call(AF_INET, SOCK_STREAM, IPPROTO_TCP),
+    ]
+    assert list(exc_info.value.exceptions) == [expected_failure_exception]
+
+    mock_socket_ipv4.close.assert_called_once_with()
+    mock_socket_ipv6.close.assert_called_once_with()
+
+
+@pytest.mark.asyncio
+@pytest.mark.parametrize("connection_socktype", [SOCK_STREAM], indirect=True, ids=repr)
+async def test____create_connection____happy_eyeballs_delay____addrinfo_reordering____prioritize_ipv6_over_ipv4(
+    event_loop: asyncio.AbstractEventLoop,
+    addrinfo_list_factory: _AddrInfoListFactory,
+    mock_socket_cls: MagicMock,
+    mock_socket_ipv6: MagicMock,
+    mock_getaddrinfo: AsyncMock,
+    mock_sock_connect: AsyncMock,
+    mocker: MockerFixture,
+) -> None:
+    # Arrange
+    remote_host, remote_port = "localhost", 12345
+    mock_getaddrinfo.side_effect = [addrinfo_list_factory(remote_port, families=[AF_INET, AF_INET6])]
+
+    async def connect_side_effect(sock: SocketType, address: tuple[Any, ...]) -> None:
+        await asyncio.sleep(0.5)
+
+    mock_sock_connect.side_effect = connect_side_effect
+
+    # Act
+    socket = await create_connection(remote_host, remote_port, event_loop, happy_eyeballs_delay=0.25)
+
+    # Assert
+    assert socket is mock_socket_ipv6
+    assert mock_socket_cls.call_args_list == [
+        mocker.call(AF_INET6, SOCK_STREAM, IPPROTO_TCP),
+        mocker.call(AF_INET, SOCK_STREAM, IPPROTO_TCP),
+    ]
+
+
+@pytest.mark.asyncio
+@pytest.mark.parametrize("connection_socktype", [SOCK_STREAM], indirect=True, ids=repr)
+async def test____create_connection____happy_eyeballs_delay____addrinfo_reordering____interleave_families(
+    event_loop: asyncio.AbstractEventLoop,
+    addrinfo_list_factory: _AddrInfoListFactory,
+    mock_socket_cls: MagicMock,
+    mock_socket_ipv6: MagicMock,
+    mock_getaddrinfo: AsyncMock,
+    mock_sock_connect: AsyncMock,
+    mocker: MockerFixture,
+) -> None:
+    # Arrange
+    remote_host, remote_port = "localhost", 12345
+    mock_getaddrinfo.side_effect = [
+        addrinfo_list_factory(remote_port, families=[AF_INET6, AF_INET6, AF_INET6, AF_INET, AF_INET, AF_INET]),
+    ]
+
+    async def connect_side_effect(sock: SocketType, address: tuple[Any, ...]) -> None:
+        await asyncio.sleep(1)
+
+    mock_sock_connect.side_effect = connect_side_effect
+
+    # Act
+    socket = await create_connection(remote_host, remote_port, event_loop, happy_eyeballs_delay=0.1)
+
+    # Assert
+    assert socket is mock_socket_ipv6
+    assert mock_socket_cls.call_args_list == [
+        mocker.call(AF_INET6, SOCK_STREAM, IPPROTO_TCP),
+        mocker.call(AF_INET, SOCK_STREAM, IPPROTO_TCP),
+        mocker.call(AF_INET6, SOCK_STREAM, IPPROTO_TCP),
+        mocker.call(AF_INET, SOCK_STREAM, IPPROTO_TCP),
+        mocker.call(AF_INET6, SOCK_STREAM, IPPROTO_TCP),
+        mocker.call(AF_INET, SOCK_STREAM, IPPROTO_TCP),
+    ]
+
+
 @pytest.fixture
 def addrinfo_list() -> Sequence[tuple[int, int, int, str, tuple[Any, ...]]]:
     return (
         (AF_INET, SOCK_STREAM, IPPROTO_TCP, "", ("0.0.0.0", 65432)),
         (AF_INET6, SOCK_STREAM, IPPROTO_TCP, "", ("::", 65432, 0, 0)),
     )
```

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_async/test_lowlevel_api/test_backend/_fake_backends.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_async/test_lowlevel_api/test_backend/_fake_backends.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,15 +1,16 @@
 from __future__ import annotations
 
-from typing import Any, NoReturn, final
+from typing import Any, NoReturn, final, no_type_check
 
 from easynetwork.lowlevel.api_async.backend.abc import AsyncBackend
 
 
 class BaseFakeBackend(AsyncBackend):
+    @no_type_check
     def bootstrap(self, *args: Any, **kwargs: Any) -> Any:
         raise NotImplementedError
 
     async def sleep(self, delay: float) -> None:
         raise NotImplementedError
 
     async def sleep_forever(self) -> NoReturn:
@@ -44,32 +45,26 @@
 
     def get_cancelled_exc_class(self) -> type[BaseException]:
         raise NotImplementedError
 
     def create_task_group(self) -> Any:
         raise NotImplementedError
 
-    async def create_tcp_connection(self, *args: Any, **kwargs: Any) -> Any:
+    def get_current_task(self) -> Any:
         raise NotImplementedError
 
-    async def create_ssl_over_tcp_connection(self, *args: Any, **kwargs: Any) -> Any:
+    async def create_tcp_connection(self, *args: Any, **kwargs: Any) -> Any:
         raise NotImplementedError
 
     async def wrap_stream_socket(self, *args: Any, **kwargs: Any) -> Any:
         raise NotImplementedError
 
-    async def wrap_ssl_over_stream_socket_client_side(self, *args: Any, **kwargs: Any) -> Any:
-        raise NotImplementedError
-
     async def create_tcp_listeners(self, *args: Any, **kwargs: Any) -> Any:
         raise NotImplementedError
 
-    async def create_ssl_over_tcp_listeners(self, *args: Any, **kwargs: Any) -> Any:
-        raise NotImplementedError
-
     async def create_udp_endpoint(self, *args: Any, **kwargs: Any) -> Any:
         raise NotImplementedError
 
     async def wrap_connected_datagram_socket(self, *args: Any, **kwargs: Any) -> Any:
         raise NotImplementedError
 
     async def create_udp_listeners(self, *args: Any, **kwargs: Any) -> Any:
```

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_async/test_lowlevel_api/test_backend/test_futures.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_async/test_lowlevel_api/test_futures.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,20 +1,20 @@
 from __future__ import annotations
 
 import concurrent.futures
 import contextvars
 from collections.abc import Iterator
 from typing import TYPE_CHECKING
 
-from easynetwork.lowlevel.api_async.backend.futures import AsyncExecutor
+from easynetwork.lowlevel.futures import AsyncExecutor
 
 import pytest
 
-from .....tools import temporary_backend
-from ...._utils import partial_eq
+from ....tools import temporary_backend
+from ..._utils import partial_eq
 
 if TYPE_CHECKING:
     from unittest.mock import MagicMock
 
     from pytest_mock import MockerFixture
 
 
@@ -34,53 +34,68 @@
 
     @pytest.fixture
     @staticmethod
     def executor(
         mock_backend: MagicMock,
         mock_stdlib_executor: MagicMock,
         executor_handle_contexts: bool,
-    ) -> Iterator[AsyncExecutor]:
+    ) -> Iterator[AsyncExecutor[concurrent.futures.Executor]]:
         with temporary_backend(mock_backend):
             yield AsyncExecutor(mock_stdlib_executor, handle_contexts=executor_handle_contexts)
 
     @pytest.fixture(autouse=True)
     @staticmethod
     def mock_contextvars_copy_context(mocker: MockerFixture) -> MagicMock:
         return mocker.patch("contextvars.copy_context", autospec=True)
 
+    @pytest.fixture(autouse=True)
+    @staticmethod
+    def mock_unwrap_future(mocker: MockerFixture) -> MagicMock:
+        return mocker.patch(f"{AsyncExecutor.__module__}.unwrap_future", autospec=True)
+
     async def test____dunder_init____invalid_executor(
         self,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         invalid_executor = mocker.NonCallableMagicMock(spec=object)
 
         # Act & Assert
         with pytest.raises(TypeError):
             _ = AsyncExecutor(invalid_executor)
 
+    async def test____wrapped_property____returned_wrapped_executor_instance(
+        self,
+        executor: AsyncExecutor[concurrent.futures.Executor],
+        mock_stdlib_executor: MagicMock,
+    ) -> None:
+        # Arrange
+
+        # Act & Assert
+        assert executor.wrapped is mock_stdlib_executor
+
     async def test____run____submit_to_executor_and_wait(
         self,
-        executor: AsyncExecutor,
+        executor: AsyncExecutor[concurrent.futures.Executor],
         executor_handle_contexts: bool,
-        mock_backend: MagicMock,
         mock_stdlib_executor: MagicMock,
         mock_contextvars_copy_context: MagicMock,
+        mock_unwrap_future: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         mock_context: MagicMock = mocker.NonCallableMagicMock(spec=contextvars.Context)
         mock_contextvars_copy_context.return_value = mock_context
         func = mocker.stub()
         mock_future = mocker.NonCallableMagicMock(
             spec=concurrent.futures.Future,
             **{"cancel.return_value": False},
         )
         mock_stdlib_executor.submit.return_value = mock_future
-        mock_backend.wait_future.return_value = mocker.sentinel.result
+        mock_unwrap_future.return_value = mocker.sentinel.result
 
         # Act
         result = await executor.run(
             func,
             mocker.sentinel.arg1,
             mocker.sentinel.arg2,
             kw1=mocker.sentinel.kw1,
@@ -103,27 +118,27 @@
                 func,
                 mocker.sentinel.arg1,
                 mocker.sentinel.arg2,
                 kw1=mocker.sentinel.kw1,
                 kw2=mocker.sentinel.kw2,
             )
         func.assert_not_called()
-        mock_backend.wait_future.assert_awaited_once_with(mock_future)
+        mock_unwrap_future.assert_awaited_once_with(mock_future)
         mock_future.cancel.assert_called_once_with()
         assert result is mocker.sentinel.result
 
     @pytest.mark.parametrize("future_exception", [Exception, None])
     async def test____map____submit_to_executor_and_wait(
         self,
-        executor: AsyncExecutor,
+        executor: AsyncExecutor[concurrent.futures.Executor],
         executor_handle_contexts: bool,
         future_exception: type[BaseException] | None,
-        mock_backend: MagicMock,
         mock_stdlib_executor: MagicMock,
         mock_contextvars_copy_context: MagicMock,
+        mock_unwrap_future: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         mock_contexts: list[MagicMock] = [mocker.NonCallableMagicMock(spec=contextvars.Context) for _ in range(3)]
         mock_futures: list[MagicMock] = [
             mocker.NonCallableMagicMock(
                 spec=concurrent.futures.Future,
@@ -131,80 +146,80 @@
             )
             for i in range(3)
         ]
         mock_contextvars_copy_context.side_effect = mock_contexts
         func = mocker.stub()
         mock_stdlib_executor.submit.side_effect = mock_futures
         if future_exception is None:
-            mock_backend.wait_future.side_effect = [mocker.sentinel.result_1, mocker.sentinel.result_2, mocker.sentinel.result_3]
+            mock_unwrap_future.side_effect = [mocker.sentinel.result_1, mocker.sentinel.result_2, mocker.sentinel.result_3]
         else:
-            mock_backend.wait_future.side_effect = future_exception()
+            mock_unwrap_future.side_effect = future_exception()
         func_args = (mocker.sentinel.arg1, mocker.sentinel.arg2, mocker.sentinel.args3)
 
         # Act
         if future_exception is None:
             results = [result async for result in executor.map(func, func_args)]
         else:
             results = []
             with pytest.raises(Exception) as exc_info:
                 results = [result async for result in executor.map(func, func_args)]
-            assert exc_info.value is mock_backend.wait_future.side_effect
+            assert exc_info.value is mock_unwrap_future.side_effect
 
         # Assert
         if executor_handle_contexts:
             assert mock_contextvars_copy_context.call_args_list == [mocker.call() for _ in range(len(mock_contexts))]
             assert mock_stdlib_executor.submit.call_args_list == [
                 mocker.call(partial_eq(mock_context.run, func), arg) for mock_context, arg in zip(mock_contexts, func_args)
             ]
         else:
             mock_contextvars_copy_context.assert_not_called()
             for mock_context in mock_contexts:
                 mock_context.run.assert_not_called()
             assert mock_stdlib_executor.submit.call_args_list == [mocker.call(func, arg) for arg in func_args]
         func.assert_not_called()
         if future_exception is None:
-            mock_backend.wait_future.await_args_list == [mocker.call(mock_fut) for mock_fut in mock_futures]
+            mock_unwrap_future.await_args_list == [mocker.call(mock_fut) for mock_fut in mock_futures]
             assert results == [mocker.sentinel.result_1, mocker.sentinel.result_2, mocker.sentinel.result_3]
         else:
-            mock_backend.wait_future.await_args_list == [mocker.call(mock_futures[0])]
+            mock_unwrap_future.await_args_list == [mocker.call(mock_futures[0])]
             assert results == []
         for mock_fut in mock_futures:
             mock_fut.cancel.assert_called_once_with()
 
     async def test____shutdown_nowait____shutdown_executor(
         self,
-        executor: AsyncExecutor,
+        executor: AsyncExecutor[concurrent.futures.Executor],
         mock_stdlib_executor: MagicMock,
     ) -> None:
         # Arrange
 
         # Act
         executor.shutdown_nowait()
 
         # Assert
         mock_stdlib_executor.shutdown.assert_called_once_with(wait=False, cancel_futures=False)
 
     @pytest.mark.parametrize("cancel_futures", [False, True])
     async def test____shutdown_nowait____shutdown_executor____cancel_futures(
         self,
         cancel_futures: bool,
-        executor: AsyncExecutor,
+        executor: AsyncExecutor[concurrent.futures.Executor],
         mock_stdlib_executor: MagicMock,
     ) -> None:
         # Arrange
 
         # Act
         executor.shutdown_nowait(cancel_futures=cancel_futures)
 
         # Assert
         mock_stdlib_executor.shutdown.assert_called_once_with(wait=False, cancel_futures=cancel_futures)
 
     async def test____shutdown____shutdown_executor(
         self,
-        executor: AsyncExecutor,
+        executor: AsyncExecutor[concurrent.futures.Executor],
         mock_backend: MagicMock,
         mock_stdlib_executor: MagicMock,
     ) -> None:
         # Arrange
         mock_backend.run_in_thread.return_value = None
 
         # Act
@@ -214,15 +229,15 @@
         mock_stdlib_executor.shutdown.assert_not_called()
         mock_backend.run_in_thread.assert_awaited_once_with(mock_stdlib_executor.shutdown, wait=True, cancel_futures=False)
 
     @pytest.mark.parametrize("cancel_futures", [False, True])
     async def test____shutdown____shutdown_executor____cancel_futures(
         self,
         cancel_futures: bool,
-        executor: AsyncExecutor,
+        executor: AsyncExecutor[concurrent.futures.Executor],
         mock_backend: MagicMock,
         mock_stdlib_executor: MagicMock,
     ) -> None:
         # Arrange
         mock_backend.run_in_thread.return_value = None
 
         # Act
@@ -234,15 +249,15 @@
             mock_stdlib_executor.shutdown,
             wait=True,
             cancel_futures=cancel_futures,
         )
 
     async def test____context_manager____shutdown_executor_at_end(
         self,
-        executor: AsyncExecutor,
+        executor: AsyncExecutor[concurrent.futures.Executor],
         mock_backend: MagicMock,
         mock_stdlib_executor: MagicMock,
     ) -> None:
         # Arrange
         mock_backend.run_in_thread.return_value = None
 
         # Act
```

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_async/test_lowlevel_api/test_backend/test_sniffio_helpers.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_async/test_lowlevel_api/test_backend/test_sniffio_helpers.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_async/test_lowlevel_api/test_endpoints/test_datagram.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_async/test_lowlevel_api/test_endpoints/test_datagram.py`

 * *Files 11% similar despite different names*

```diff
@@ -9,49 +9,38 @@
     AsyncDatagramReadTransport,
     AsyncDatagramTransport,
     AsyncDatagramWriteTransport,
 )
 
 import pytest
 
+from ....base import BaseTestWithDatagramProtocol
+
 if TYPE_CHECKING:
     from unittest.mock import MagicMock
 
     from pytest_mock import MockerFixture
 
 
 @pytest.mark.asyncio
-class TestAsyncDatagramEndpoint:
+class TestAsyncDatagramEndpoint(BaseTestWithDatagramProtocol):
     @pytest.fixture(params=[AsyncDatagramReadTransport, AsyncDatagramWriteTransport, AsyncDatagramTransport])
     @staticmethod
     def mock_datagram_transport(request: pytest.FixtureRequest, mocker: MockerFixture) -> MagicMock:
         mock_datagram_transport = mocker.NonCallableMagicMock(spec=request.param)
         mock_datagram_transport.is_closing.return_value = False
 
         def close_side_effect() -> None:
             mock_datagram_transport.is_closing.return_value = True
 
         mock_datagram_transport.aclose.side_effect = close_side_effect
         return mock_datagram_transport
 
     @pytest.fixture
     @staticmethod
-    def mock_datagram_protocol(mock_datagram_protocol: MagicMock, mocker: MockerFixture) -> MagicMock:
-        def make_datagram_side_effect(packet: Any) -> bytes:
-            return str(packet).encode("ascii").removeprefix(b"sentinel.")
-
-        def build_packet_from_datagram_side_effect(data: bytes) -> Any:
-            return getattr(mocker.sentinel, data.decode("ascii"))
-
-        mock_datagram_protocol.make_datagram.side_effect = make_datagram_side_effect
-        mock_datagram_protocol.build_packet_from_datagram.side_effect = build_packet_from_datagram_side_effect
-        return mock_datagram_protocol
-
-    @pytest.fixture
-    @staticmethod
     def endpoint(mock_datagram_transport: MagicMock, mock_datagram_protocol: MagicMock) -> AsyncDatagramEndpoint[Any, Any]:
         return AsyncDatagramEndpoint(mock_datagram_transport, mock_datagram_protocol)
 
     async def test____dunder_init____invalid_transport(
         self,
         mock_datagram_protocol: MagicMock,
         mocker: MockerFixture,
@@ -103,15 +92,15 @@
 
         # Act
         await endpoint.aclose()
 
         # Assert
         mock_datagram_transport.aclose.assert_awaited_once_with()
 
-    async def test____get_extra_info____default(
+    async def test____extra_attributes____default(
         self,
         endpoint: AsyncDatagramEndpoint[Any, Any],
         mock_datagram_transport: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         mock_datagram_transport.extra_attributes = {mocker.sentinel.name: lambda: mocker.sentinel.extra_info}
```

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_async/test_lowlevel_api/test_endpoints/test_stream.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_sync/test_lowlevel_api/test_endpoints/test_stream.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,547 +1,582 @@
 from __future__ import annotations
 
 import contextlib
-from collections.abc import Awaitable, Callable, Generator
+import math
+import warnings
+from collections.abc import Generator
 from typing import TYPE_CHECKING, Any, Literal
 
 from easynetwork.exceptions import IncrementalDeserializeError, StreamProtocolParseError, UnsupportedOperation
-from easynetwork.lowlevel._stream import BufferedStreamDataConsumer, StreamDataConsumer
-from easynetwork.lowlevel.api_async.endpoints.stream import AsyncStreamEndpoint
-from easynetwork.lowlevel.api_async.transports.abc import (
-    AsyncBufferedStreamReadTransport,
-    AsyncStreamReadTransport,
-    AsyncStreamTransport,
-    AsyncStreamWriteTransport,
+from easynetwork.lowlevel.api_sync.endpoints.stream import StreamEndpoint
+from easynetwork.lowlevel.api_sync.transports.abc import (
+    BufferedStreamReadTransport,
+    StreamReadTransport,
+    StreamTransport,
+    StreamWriteTransport,
 )
+from easynetwork.warnings import ManualBufferAllocationWarning
 
 import pytest
 
+from ...._utils import make_recv_into_side_effect
+from ....base import BaseTestWithStreamProtocol
+
 if TYPE_CHECKING:
     from unittest.mock import MagicMock
 
     from pytest_mock import MockerFixture
 
 
-def make_recv_into_side_effect(to_write: bytes | list[bytes]) -> Callable[[memoryview], Awaitable[int]]:
-    def write_in_buffer(buffer: memoryview, to_write: bytes) -> int:
-        nbytes = len(to_write)
-        buffer[:nbytes] = to_write
-        return nbytes
-
-    match to_write:
-        case bytes():
-
-            async def recv_into_side_effect(buffer: bytearray | memoryview) -> int:
-                return write_in_buffer(memoryview(buffer), to_write)
-
-        case list() if all(isinstance(b, bytes) for b in to_write):
-            iterator = iter(to_write)
-
-            async def recv_into_side_effect(buffer: bytearray | memoryview) -> int:
-                try:
-                    to_write = next(iterator)
-                except StopIteration:
-                    raise StopAsyncIteration from None
-                return write_in_buffer(memoryview(buffer), to_write)
-
-        case _:
-            pytest.fail("Invalid setup")
-
-    return recv_into_side_effect
-
+pytest_mark_ignore_manual_buffer_allocation_warning = pytest.mark.filterwarnings(
+    f"ignore::{ManualBufferAllocationWarning.__module__}.{ManualBufferAllocationWarning.__qualname__}",
+)
 
-@pytest.mark.asyncio
-class TestAsyncStreamEndpoint:
-    @pytest.fixture
-    @staticmethod
-    def consumer_feed(mocker: MockerFixture) -> MagicMock:
-        return mocker.patch.object(StreamDataConsumer, "feed", autospec=True, side_effect=StreamDataConsumer.feed)
 
-    @pytest.fixture
+class TestStreamEndpoint(BaseTestWithStreamProtocol):
+    @pytest.fixture(autouse=True)
     @staticmethod
-    def consumer_buffer_updated(mocker: MockerFixture) -> MagicMock:
-        return mocker.patch.object(
-            BufferedStreamDataConsumer,
-            "buffer_updated",
-            autospec=True,
-            side_effect=BufferedStreamDataConsumer.buffer_updated,
-        )
+    def mock_time_perfcounter(mocker: MockerFixture) -> MagicMock:
+        return mocker.patch("time.perf_counter", autospec=True, return_value=12345)
 
-    @pytest.fixture(
-        params=[AsyncStreamReadTransport, AsyncBufferedStreamReadTransport, AsyncStreamWriteTransport, AsyncStreamTransport]
-    )
+    @pytest.fixture(params=[StreamReadTransport, BufferedStreamReadTransport, StreamWriteTransport, StreamTransport])
     @staticmethod
     def mock_stream_transport(request: pytest.FixtureRequest, mocker: MockerFixture) -> MagicMock:
         mock_stream_transport = mocker.NonCallableMagicMock(spec=request.param)
-        mock_stream_transport.is_closing.return_value = False
+        mock_stream_transport.is_closed.return_value = False
 
         def close_side_effect() -> None:
-            mock_stream_transport.is_closing.return_value = True
+            mock_stream_transport.is_closed.return_value = True
 
-        mock_stream_transport.aclose.side_effect = close_side_effect
+        mock_stream_transport.close.side_effect = close_side_effect
         return mock_stream_transport
 
     @pytest.fixture
     @staticmethod
-    def mock_buffered_stream_receiver(
-        mock_buffered_stream_receiver: MagicMock,
-        mocker: MockerFixture,
-    ) -> MagicMock:
-        def build_packet_from_buffer_side_effect(buffer: memoryview) -> Generator[None, int, tuple[Any, bytes]]:
-            chunk: bytes = b""
-            while True:
-                nbytes = yield
-                chunk += buffer[:nbytes]
-                if b"\n" not in chunk:
-                    continue
-                del buffer
-                data, chunk = chunk.split(b"\n", 1)
-                return getattr(mocker.sentinel, data.decode(encoding="ascii")), chunk
+    def max_recv_size(request: Any) -> int:
+        return getattr(request, "param", 256 * 1024)
 
-        mock_buffered_stream_receiver.build_packet_from_buffer.side_effect = build_packet_from_buffer_side_effect
-        return mock_buffered_stream_receiver
+    @pytest.fixture
+    @staticmethod
+    def endpoint(
+        mock_stream_transport: MagicMock,
+        mock_stream_protocol: MagicMock,
+        max_recv_size: int,
+    ) -> StreamEndpoint[Any, Any]:
+        with warnings.catch_warnings():
+            warnings.simplefilter("ignore", ManualBufferAllocationWarning)
+            return StreamEndpoint(mock_stream_transport, mock_stream_protocol, max_recv_size)
 
-    @pytest.fixture(params=["data", "buffer"])
+    @pytest.fixture(
+        params=[
+            pytest.param(None, id="blocking (None)"),
+            pytest.param(math.inf, id="blocking (+inf)"),
+            pytest.param(0, id="non_blocking"),
+            pytest.param(123456789, id="with_timeout"),
+        ]
+    )
     @staticmethod
-    def stream_protocol_mode(request: pytest.FixtureRequest) -> str:
-        assert request.param in ("data", "buffer")
+    def recv_timeout(request: Any) -> Any:
         return request.param
 
     @pytest.fixture
     @staticmethod
-    def mock_stream_protocol(
-        stream_protocol_mode: str,
-        mock_stream_protocol: MagicMock,
-        mock_buffered_stream_receiver: MagicMock,
-        mocker: MockerFixture,
-    ) -> MagicMock:
-        def generate_chunks_side_effect(packet: Any) -> Generator[bytes, None, None]:
-            yield str(packet).removeprefix("sentinel.").encode("ascii") + b"\n"
-
-        def build_packet_from_chunks_side_effect() -> Generator[None, bytes, tuple[Any, bytes]]:
-            buffer = b""
-            while True:
-                buffer += yield
-                if b"\n" not in buffer:
-                    continue
-                data, buffer = buffer.split(b"\n", 1)
-                return getattr(mocker.sentinel, data.decode(encoding="ascii")), buffer
-
-        mock_stream_protocol.generate_chunks.side_effect = generate_chunks_side_effect
-        mock_stream_protocol.build_packet_from_chunks.side_effect = build_packet_from_chunks_side_effect
-
-        match stream_protocol_mode:
-            case "data":
-                mock_stream_protocol.buffered_receiver.side_effect = UnsupportedOperation
-            case "buffer":
-                mock_stream_protocol.buffered_receiver.side_effect = None
-                mock_stream_protocol.buffered_receiver.return_value = mock_buffered_stream_receiver
-            case _:
-                pytest.fail("Invalid parameter")
-
-        return mock_stream_protocol
+    def expected_recv_timeout(recv_timeout: float | None) -> float:
+        if recv_timeout is None:
+            return math.inf
+        return recv_timeout
 
-    @pytest.fixture
+    @pytest.fixture(
+        params=[
+            pytest.param(None, id="blocking (None)"),
+            pytest.param(math.inf, id="blocking (+inf)"),
+            pytest.param(0, id="non_blocking"),
+            pytest.param(123456789, id="with_timeout"),
+        ]
+    )
     @staticmethod
-    def max_recv_size(request: Any) -> int:
-        return getattr(request, "param", 256 * 1024)
+    def send_timeout(request: Any) -> Any:
+        return request.param
 
     @pytest.fixture
     @staticmethod
-    def endpoint(
-        mock_stream_transport: MagicMock,
-        mock_stream_protocol: MagicMock,
-        max_recv_size: int,
-    ) -> AsyncStreamEndpoint[Any, Any]:
-        return AsyncStreamEndpoint(mock_stream_transport, mock_stream_protocol, max_recv_size)
+    def expected_send_timeout(send_timeout: float | None) -> float:
+        if send_timeout is None:
+            return math.inf
+        return send_timeout
 
-    async def test____dunder_init____invalid_transport(
+    def test____dunder_init____invalid_transport(
         self,
         mock_stream_protocol: MagicMock,
         max_recv_size: int,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         mock_invalid_transport = mocker.NonCallableMagicMock(spec=object)
 
         # Act & Assert
-        with pytest.raises(TypeError, match=r"^Expected an AsyncStreamTransport object, got .*$"):
-            _ = AsyncStreamEndpoint(mock_invalid_transport, mock_stream_protocol, max_recv_size)
+        with pytest.raises(TypeError, match=r"^Expected a StreamTransport object, got .*$"):
+            _ = StreamEndpoint(mock_invalid_transport, mock_stream_protocol, max_recv_size)
 
-    async def test____dunder_init____invalid_protocol(
+    @pytest_mark_ignore_manual_buffer_allocation_warning
+    def test____dunder_init____invalid_protocol(
         self,
         mock_stream_transport: MagicMock,
         max_recv_size: int,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         mock_invalid_protocol = mocker.NonCallableMagicMock(spec=object)
 
         # Act & Assert
         with pytest.raises(TypeError, match=r"^Expected a StreamProtocol object, got .*$"):
-            _ = AsyncStreamEndpoint(mock_stream_transport, mock_invalid_protocol, max_recv_size)
+            _ = StreamEndpoint(mock_stream_transport, mock_invalid_protocol, max_recv_size)
 
+    @pytest_mark_ignore_manual_buffer_allocation_warning
     @pytest.mark.parametrize("max_recv_size", [1, 2**16], ids=lambda p: f"max_recv_size=={p}")
-    async def test____dunder_init____max_recv_size____valid_value(
+    def test____dunder_init____max_recv_size____valid_value(
         self,
         mock_stream_transport: MagicMock,
         mock_stream_protocol: MagicMock,
         max_recv_size: int,
     ) -> None:
         # Arrange
 
         # Act
-        endpoint: AsyncStreamEndpoint[Any, Any] = AsyncStreamEndpoint(mock_stream_transport, mock_stream_protocol, max_recv_size)
+        transport: StreamEndpoint[Any, Any] = StreamEndpoint(mock_stream_transport, mock_stream_protocol, max_recv_size)
 
         # Assert
-        if isinstance(mock_stream_transport, AsyncStreamReadTransport):
-            assert endpoint.max_recv_size == max_recv_size
+        if isinstance(mock_stream_transport, StreamReadTransport):
+            assert transport.max_recv_size == max_recv_size
         else:
-            assert endpoint.max_recv_size == 0
+            assert transport.max_recv_size == 0
 
+    @pytest_mark_ignore_manual_buffer_allocation_warning
     @pytest.mark.parametrize("max_recv_size", [0, -1, 10.4], ids=lambda p: f"max_recv_size=={p}")
-    async def test____dunder_init____max_recv_size____invalid_value(
+    def test____dunder_init____max_recv_size____invalid_value(
         self,
         mock_stream_transport: MagicMock,
         mock_stream_protocol: MagicMock,
         max_recv_size: Any,
     ) -> None:
         # Arrange
 
         # Act & Assert
         with pytest.raises(ValueError, match=r"^'max_recv_size' must be a strictly positive integer$"):
-            _ = AsyncStreamEndpoint(mock_stream_transport, mock_stream_protocol, max_recv_size)
+            _ = StreamEndpoint(mock_stream_transport, mock_stream_protocol, max_recv_size)
+
+    @pytest.mark.parametrize("manual_buffer_allocation", ["unknown", ""], ids=lambda p: f"manual_buffer_allocation=={p!r}")
+    def test____dunder_init____manual_buffer_allocation____invalid_value(
+        self,
+        mock_stream_transport: MagicMock,
+        mock_stream_protocol: MagicMock,
+        max_recv_size: int,
+        manual_buffer_allocation: Any,
+    ) -> None:
+        # Arrange
+
+        # Act & Assert
+        with pytest.raises(ValueError, match=r'^"manual_buffer_allocation" must be "try", "no" or "force"$'):
+            _ = StreamEndpoint(
+                mock_stream_transport,
+                mock_stream_protocol,
+                max_recv_size,
+                manual_buffer_allocation=manual_buffer_allocation,
+            )
 
     @pytest.mark.parametrize("transport_closed", [False, True])
-    async def test____is_closing____default(
+    def test____is_closed____default(
         self,
-        endpoint: AsyncStreamEndpoint[Any, Any],
+        endpoint: StreamEndpoint[Any, Any],
         mock_stream_transport: MagicMock,
         transport_closed: bool,
     ) -> None:
         # Arrange
-        mock_stream_transport.is_closing.assert_not_called()
-        mock_stream_transport.is_closing.return_value = transport_closed
+        mock_stream_transport.is_closed.assert_not_called()
+        mock_stream_transport.is_closed.return_value = transport_closed
 
         # Act
-        state = endpoint.is_closing()
+        state = endpoint.is_closed()
 
         # Assert
-        mock_stream_transport.is_closing.assert_called_once_with()
+        mock_stream_transport.is_closed.assert_called_once_with()
         assert state is transport_closed
 
-    async def test____aclose____default(self, endpoint: AsyncStreamEndpoint[Any, Any], mock_stream_transport: MagicMock) -> None:
+    def test____close____default(self, endpoint: StreamEndpoint[Any, Any], mock_stream_transport: MagicMock) -> None:
         # Arrange
-        mock_stream_transport.aclose.assert_not_called()
+        mock_stream_transport.close.assert_not_called()
 
         # Act
-        await endpoint.aclose()
+        endpoint.close()
 
         # Assert
-        mock_stream_transport.aclose.assert_awaited_once_with()
+        mock_stream_transport.close.assert_called_once_with()
 
-    async def test____get_extra_info____default(
+    def test____extra_attributes____default(
         self,
-        endpoint: AsyncStreamEndpoint[Any, Any],
+        endpoint: StreamEndpoint[Any, Any],
         mock_stream_transport: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         mock_stream_transport.extra_attributes = {mocker.sentinel.name: lambda: mocker.sentinel.extra_info}
 
         # Act
         value = endpoint.extra(mocker.sentinel.name)
 
         # Assert
         assert value is mocker.sentinel.extra_info
 
-    async def test____send_packet____send_bytes_to_transport(
+    def test____send_packet____send_bytes_to_transport(
         self,
-        endpoint: AsyncStreamEndpoint[Any, Any],
+        send_timeout: float | None,
+        expected_send_timeout: float,
+        endpoint: StreamEndpoint[Any, Any],
         mock_stream_transport: MagicMock,
         mock_stream_protocol: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         chunks: list[bytes] = []
-        with contextlib.suppress(AttributeError):
-            mock_stream_transport.send_all_from_iterable.side_effect = lambda it: chunks.extend(it)
+        if hasattr(mock_stream_transport, "send_all_from_iterable"):
+            mock_stream_transport.send_all_from_iterable.side_effect = lambda it, timeout: chunks.extend(it)
 
         # Act
         with (
             pytest.raises(UnsupportedOperation, match=r"^transport does not support sending data$")
-            if mock_stream_transport.__class__ not in (AsyncStreamWriteTransport, AsyncStreamTransport)
+            if mock_stream_transport.__class__ not in (StreamWriteTransport, StreamTransport)
             else contextlib.nullcontext()
         ):
-            await endpoint.send_packet(mocker.sentinel.packet)
+            endpoint.send_packet(mocker.sentinel.packet, timeout=send_timeout)
 
         # Assert
-        if mock_stream_transport.__class__ in (AsyncStreamWriteTransport, AsyncStreamTransport):
+        if mock_stream_transport.__class__ in (StreamWriteTransport, StreamTransport):
             mock_stream_protocol.generate_chunks.assert_called_once_with(mocker.sentinel.packet)
-            mock_stream_transport.send_all_from_iterable.assert_awaited_once_with(mocker.ANY)
+            mock_stream_transport.send_all_from_iterable.assert_called_once_with(mocker.ANY, expected_send_timeout)
             mock_stream_transport.send_all.assert_not_called()
+            mock_stream_transport.send.assert_not_called()
             assert chunks == [b"packet\n"]
         else:
             mock_stream_protocol.generate_chunks.assert_not_called()
             assert chunks == []
 
-    @pytest.mark.parametrize("mock_stream_transport", [AsyncStreamWriteTransport], indirect=True)
-    async def test____send_packet____protocol_crashed(
+    @pytest.mark.parametrize("mock_stream_transport", [StreamWriteTransport], indirect=True)
+    def test____send_packet____protocol_crashed(
         self,
-        endpoint: AsyncStreamEndpoint[Any, Any],
+        endpoint: StreamEndpoint[Any, Any],
+        send_timeout: float | None,
         mock_stream_transport: MagicMock,
         mock_stream_protocol: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         chunks: list[bytes] = []
-        mock_stream_transport.send_all_from_iterable.side_effect = lambda it: chunks.extend(it)
+        mock_stream_transport.send_all_from_iterable.side_effect = lambda it, timeout: chunks.extend(it)
         expected_error = Exception("Error")
 
         def side_effect(packet: Any) -> Generator[bytes, None, None]:
             raise expected_error
             yield  # type: ignore[unreachable]
 
         mock_stream_protocol.generate_chunks.side_effect = side_effect
 
         # Act
         with pytest.raises(RuntimeError, match=r"^protocol\.generate_chunks\(\) crashed$") as exc_info:
-            await endpoint.send_packet(mocker.sentinel.packet)
+            endpoint.send_packet(mocker.sentinel.packet, timeout=send_timeout)
 
         # Assert
         assert exc_info.value.__cause__ is expected_error
         assert chunks == []
 
     @pytest.mark.parametrize("transport_closed", [False, True], ids=lambda p: f"transport_closed=={p}")
-    async def test____send_eof____default(
+    def test____send_eof____default(
         self,
         transport_closed: bool,
-        endpoint: AsyncStreamEndpoint[Any, Any],
+        endpoint: StreamEndpoint[Any, Any],
         mock_stream_transport: MagicMock,
         mock_stream_protocol: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
-        mock_stream_transport.is_closing.return_value = transport_closed
+        mock_stream_transport.is_closed.return_value = transport_closed
 
         # Act
         with (
             pytest.raises(UnsupportedOperation, match=r"^transport does not support sending EOF$")
-            if mock_stream_transport.__class__ is not AsyncStreamTransport
+            if mock_stream_transport.__class__ is not StreamTransport
             else contextlib.nullcontext()
         ):
-            await endpoint.send_eof()
+            endpoint.send_eof()
 
         # Assert
-        if mock_stream_transport.__class__ is AsyncStreamTransport:
-            mock_stream_transport.send_eof.assert_awaited_once_with()
+        if mock_stream_transport.__class__ is StreamTransport:
+            mock_stream_transport.send_eof.assert_called_once_with()
             with pytest.raises(RuntimeError, match=r"^send_eof\(\) has been called earlier$"):
-                await endpoint.send_packet(mocker.sentinel.packet)
+                endpoint.send_packet(mocker.sentinel.packet)
             mock_stream_protocol.generate_chunks.assert_not_called()
             mock_stream_transport.send_all_from_iterable.assert_not_called()
 
     @pytest.mark.parametrize("transport_closed", [False, True], ids=lambda p: f"transport_closed=={p}")
-    @pytest.mark.parametrize("mock_stream_transport", [AsyncStreamTransport], indirect=True)
-    async def test____send_eof____idempotent(
+    @pytest.mark.parametrize("mock_stream_transport", [StreamTransport], indirect=True)
+    def test____send_eof____idempotent(
         self,
         transport_closed: bool,
-        endpoint: AsyncStreamEndpoint[Any, Any],
+        endpoint: StreamEndpoint[Any, Any],
         mock_stream_transport: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
-        mock_stream_transport.is_closing.return_value = transport_closed
-        await endpoint.send_eof()
+        mock_stream_transport.is_closed.return_value = transport_closed
+        endpoint.send_eof()
 
         # Act
-        await endpoint.send_eof()
+        endpoint.send_eof()
 
         # Assert
-        mock_stream_transport.send_eof.assert_awaited_once_with()
+        mock_stream_transport.send_eof.assert_called_once_with()
         with pytest.raises(RuntimeError, match=r"^send_eof\(\) has been called earlier$"):
-            await endpoint.send_packet(mocker.sentinel.packet)
+            endpoint.send_packet(mocker.sentinel.packet)
 
-    async def test____recv_packet____receive_bytes_from_transport(
+    def test____recv_packet____blocking_or_not____receive_bytes_from_transport(
         self,
-        endpoint: AsyncStreamEndpoint[Any, Any],
+        endpoint: StreamEndpoint[Any, Any],
+        recv_timeout: float | None,
+        expected_recv_timeout: float,
         max_recv_size: int,
         mock_stream_transport: MagicMock,
-        consumer_feed: MagicMock,
-        consumer_buffer_updated: MagicMock,
         stream_protocol_mode: Literal["data", "buffer"],
         mocker: MockerFixture,
     ) -> None:
         # Arrange
-        with contextlib.suppress(AttributeError):
+        if hasattr(mock_stream_transport, "recv"):
             mock_stream_transport.recv.side_effect = [b"packet\n"]
-        with contextlib.suppress(AttributeError):
+        if hasattr(mock_stream_transport, "recv_into"):
             mock_stream_transport.recv_into.side_effect = make_recv_into_side_effect([b"packet\n"])
 
         # Act
         packet: Any = mocker.sentinel.packet_not_received
         with (
             pytest.raises(UnsupportedOperation, match=r"^transport does not support receiving data$")
-            if mock_stream_transport.__class__
-            not in (AsyncStreamReadTransport, AsyncBufferedStreamReadTransport, AsyncStreamTransport)
+            if mock_stream_transport.__class__ not in (StreamReadTransport, BufferedStreamReadTransport, StreamTransport)
             else contextlib.nullcontext()
         ):
-            packet = await endpoint.recv_packet()
+            packet = endpoint.recv_packet(timeout=recv_timeout)
 
         # Assert
-        if mock_stream_transport.__class__ is AsyncBufferedStreamReadTransport:
+        if mock_stream_transport.__class__ is BufferedStreamReadTransport:
             if stream_protocol_mode == "buffer":
-                mock_stream_transport.recv_into.assert_awaited_once_with(mocker.ANY)
+                mock_stream_transport.recv_into.assert_called_once_with(mocker.ANY, expected_recv_timeout)
                 mock_stream_transport.recv.assert_not_called()
-                consumer_buffer_updated.assert_called_once_with(mocker.ANY, len(b"packet\n"))
-                consumer_feed.assert_not_called()
             else:
-                mock_stream_transport.recv.assert_awaited_once_with(max_recv_size)
+                mock_stream_transport.recv.assert_called_once_with(max_recv_size, expected_recv_timeout)
                 mock_stream_transport.recv_into.assert_not_called()
-                consumer_feed.assert_called_once_with(mocker.ANY, b"packet\n")
-                consumer_buffer_updated.assert_not_called()
-        elif mock_stream_transport.__class__ in (AsyncStreamReadTransport, AsyncStreamTransport):
-            mock_stream_transport.recv.assert_awaited_once_with(max_recv_size)
-            consumer_feed.assert_called_once_with(mocker.ANY, b"packet\n")
-            consumer_buffer_updated.assert_not_called()
+        elif mock_stream_transport.__class__ in (StreamReadTransport, StreamTransport):
+            mock_stream_transport.recv.assert_called_once_with(max_recv_size, expected_recv_timeout)
             assert packet is mocker.sentinel.packet
         else:
-            consumer_feed.assert_not_called()
             assert packet is mocker.sentinel.packet_not_received
 
-    @pytest.mark.parametrize("mock_stream_transport", [AsyncStreamReadTransport], indirect=True)
-    async def test____recv_packet____partial_data(
-        self,
-        endpoint: AsyncStreamEndpoint[Any, Any],
+    @pytest.mark.parametrize("recv_timeout", [None, math.inf, 123456789], indirect=True)  # Do not test with timeout==0
+    @pytest.mark.parametrize("mock_stream_transport", [StreamReadTransport], indirect=True)
+    def test____recv_packet____blocking____partial_data(
+        self,
+        endpoint: StreamEndpoint[Any, Any],
+        recv_timeout: float | None,
+        expected_recv_timeout: float,
         max_recv_size: int,
         mock_stream_transport: MagicMock,
-        consumer_feed: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         mock_stream_transport.recv.side_effect = [b"pac", b"ket\n"]
 
         # Act
-        packet: Any = await endpoint.recv_packet()
+        packet: Any = endpoint.recv_packet(timeout=recv_timeout)
 
         # Assert
-        assert mock_stream_transport.recv.await_args_list == [mocker.call(max_recv_size) for _ in range(2)]
-        assert consumer_feed.call_args_list == [
-            mocker.call(mocker.ANY, b"pac"),
-            mocker.call(mocker.ANY, b"ket\n"),
-        ]
+        assert mock_stream_transport.recv.call_args_list == [mocker.call(max_recv_size, expected_recv_timeout) for _ in range(2)]
         assert packet is mocker.sentinel.packet
 
-    @pytest.mark.parametrize("mock_stream_transport", [AsyncBufferedStreamReadTransport], indirect=True)
+    @pytest.mark.parametrize("recv_timeout", [None, math.inf, 123456789], indirect=True)  # Do not test with timeout==0
+    @pytest.mark.parametrize("mock_stream_transport", [BufferedStreamReadTransport], indirect=True)
     @pytest.mark.parametrize("stream_protocol_mode", ["buffer"], indirect=True)
-    async def test____recv_packet____buffered____partial_data(
+    def test____recv_packet____buffered____blocking____partial_data(
         self,
-        endpoint: AsyncStreamEndpoint[Any, Any],
+        endpoint: StreamEndpoint[Any, Any],
+        recv_timeout: float | None,
+        expected_recv_timeout: float,
         mock_stream_transport: MagicMock,
-        consumer_buffer_updated: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         mock_stream_transport.recv_into.side_effect = make_recv_into_side_effect([b"pac", b"ket\n"])
 
         # Act
-        packet: Any = await endpoint.recv_packet()
+        packet: Any = endpoint.recv_packet(timeout=recv_timeout)
 
         # Assert
-        assert mock_stream_transport.recv_into.await_args_list == [mocker.call(mocker.ANY) for _ in range(2)]
-        assert consumer_buffer_updated.call_args_list == [
-            mocker.call(mocker.ANY, len(b"pac")),
-            mocker.call(mocker.ANY, len(b"ket\n")),
+        assert mock_stream_transport.recv_into.call_args_list == [
+            mocker.call(mocker.ANY, expected_recv_timeout) for _ in range(2)
         ]
         assert packet is mocker.sentinel.packet
 
-    @pytest.mark.parametrize("mock_stream_transport", [AsyncStreamReadTransport], indirect=True)
-    async def test____recv_packet____extra_data(
+    @pytest.mark.parametrize("recv_timeout", [0], indirect=True)  # Only test with timeout==0
+    @pytest.mark.parametrize(
+        "max_recv_size",
+        [
+            pytest.param(3, id="chunk_matching_bufsize"),
+            pytest.param(1024, id="chunk_not_matching_bufsize"),
+        ],
+        indirect=True,
+    )
+    @pytest.mark.parametrize("mock_stream_transport", [StreamReadTransport], indirect=True)
+    def test____recv_packet____non_blocking____partial_data(
+        self,
+        endpoint: StreamEndpoint[Any, Any],
+        recv_timeout: float | None,
+        expected_recv_timeout: float,
+        max_recv_size: int,
+        mock_stream_transport: MagicMock,
+        mocker: MockerFixture,
+    ) -> None:
+        # Arrange
+        mock_stream_transport.recv.side_effect = [b"pac", b"ket", b"\n"]
+
+        # Act & Assert
+        if max_recv_size == 3:
+            packet: Any = endpoint.recv_packet(timeout=recv_timeout)
+
+            assert mock_stream_transport.recv.call_args_list == [
+                mocker.call(max_recv_size, expected_recv_timeout) for _ in range(3)
+            ]
+            assert packet is mocker.sentinel.packet
+        else:
+            with pytest.raises(TimeoutError):
+                endpoint.recv_packet(timeout=recv_timeout)
+
+            mock_stream_transport.recv.assert_called_once_with(max_recv_size, expected_recv_timeout)
+
+    @pytest.mark.parametrize("recv_timeout", [0], indirect=True)  # Only test with timeout==0
+    @pytest.mark.parametrize(
+        "max_recv_size",
+        [
+            pytest.param(3, id="chunk_matching_bufsize"),
+            pytest.param(1024, id="chunk_not_matching_bufsize"),
+        ],
+        indirect=True,
+    )
+    @pytest.mark.parametrize("mock_stream_transport", [BufferedStreamReadTransport], indirect=True)
+    @pytest.mark.parametrize("stream_protocol_mode", ["buffer"], indirect=True)
+    def test____recv_packet____buffered____non_blocking____partial_data(
+        self,
+        endpoint: StreamEndpoint[Any, Any],
+        recv_timeout: float | None,
+        expected_recv_timeout: float,
+        max_recv_size: int,
+        mock_stream_transport: MagicMock,
+        mocker: MockerFixture,
+    ) -> None:
+        # Arrange
+        mock_stream_transport.recv_into.side_effect = make_recv_into_side_effect([b"pac", b"ket", b"\n"])
+
+        # Act & Assert
+        if max_recv_size == 3:
+            packet: Any = endpoint.recv_packet(timeout=recv_timeout)
+
+            assert mock_stream_transport.recv_into.call_args_list == [
+                mocker.call(mocker.ANY, expected_recv_timeout) for _ in range(3)
+            ]
+            assert packet is mocker.sentinel.packet
+        else:
+            with pytest.raises(TimeoutError):
+                endpoint.recv_packet(timeout=recv_timeout)
+
+            mock_stream_transport.recv_into.assert_called_once_with(mocker.ANY, expected_recv_timeout)
+
+    @pytest.mark.parametrize("mock_stream_transport", [StreamReadTransport], indirect=True)
+    def test____recv_packet____blocking_or_not____extra_data(
         self,
-        endpoint: AsyncStreamEndpoint[Any, Any],
+        endpoint: StreamEndpoint[Any, Any],
+        recv_timeout: float | None,
         mock_stream_transport: MagicMock,
-        consumer_feed: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         mock_stream_transport.recv.side_effect = [b"packet_1\npacket_2\n"]
 
         # Act
-        packet_1: Any = await endpoint.recv_packet()
-        packet_2: Any = await endpoint.recv_packet()
+        packet_1: Any = endpoint.recv_packet(timeout=recv_timeout)
+        packet_2: Any = endpoint.recv_packet(timeout=recv_timeout)
 
         # Assert
-        mock_stream_transport.recv.assert_awaited_once()
-        consumer_feed.assert_called_once_with(mocker.ANY, b"packet_1\npacket_2\n")
+        mock_stream_transport.recv.assert_called_once()
         assert packet_1 is mocker.sentinel.packet_1
         assert packet_2 is mocker.sentinel.packet_2
 
-    @pytest.mark.parametrize("mock_stream_transport", [AsyncBufferedStreamReadTransport], indirect=True)
+    @pytest.mark.parametrize("mock_stream_transport", [BufferedStreamReadTransport], indirect=True)
     @pytest.mark.parametrize("stream_protocol_mode", ["buffer"], indirect=True)
-    async def test____recv_packet____buffered____extra_data(
+    def test____recv_packet____buffered____blocking_or_not____extra_data(
         self,
-        endpoint: AsyncStreamEndpoint[Any, Any],
+        endpoint: StreamEndpoint[Any, Any],
+        recv_timeout: float | None,
         mock_stream_transport: MagicMock,
-        consumer_buffer_updated: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         mock_stream_transport.recv_into.side_effect = make_recv_into_side_effect([b"packet_1\npacket_2\n"])
 
         # Act
-        packet_1: Any = await endpoint.recv_packet()
-        packet_2: Any = await endpoint.recv_packet()
+        packet_1: Any = endpoint.recv_packet(timeout=recv_timeout)
+        packet_2: Any = endpoint.recv_packet(timeout=recv_timeout)
 
         # Assert
-        mock_stream_transport.recv_into.assert_awaited_once()
-        consumer_buffer_updated.assert_called_once_with(mocker.ANY, len(b"packet_1\npacket_2\n"))
+        mock_stream_transport.recv_into.assert_called_once()
         assert packet_1 is mocker.sentinel.packet_1
         assert packet_2 is mocker.sentinel.packet_2
 
-    @pytest.mark.parametrize("mock_stream_transport", [AsyncStreamReadTransport], indirect=True)
-    async def test____recv_packet____eof_error(
+    @pytest.mark.parametrize("mock_stream_transport", [StreamReadTransport], indirect=True)
+    def test____recv_packet____blocking_or_not____eof_error(
         self,
-        endpoint: AsyncStreamEndpoint[Any, Any],
+        endpoint: StreamEndpoint[Any, Any],
+        recv_timeout: float | None,
         mock_stream_transport: MagicMock,
-        consumer_feed: MagicMock,
     ) -> None:
         # Arrange
         mock_stream_transport.recv.side_effect = [b""]
 
         # Act
         with pytest.raises(EOFError, match=r"^end-of-stream$"):
-            _ = await endpoint.recv_packet()
+            _ = endpoint.recv_packet(timeout=recv_timeout)
 
         # Assert
-        mock_stream_transport.recv.assert_awaited_once()
-        consumer_feed.assert_not_called()
+        mock_stream_transport.recv.assert_called_once()
 
-    @pytest.mark.parametrize("mock_stream_transport", [AsyncBufferedStreamReadTransport], indirect=True)
+    @pytest.mark.parametrize("mock_stream_transport", [BufferedStreamReadTransport], indirect=True)
     @pytest.mark.parametrize("stream_protocol_mode", ["buffer"], indirect=True)
-    async def test____recv_packet____buffered____eof_error(
+    def test____recv_packet____buffered____blocking_or_not____eof_error(
         self,
-        endpoint: AsyncStreamEndpoint[Any, Any],
+        endpoint: StreamEndpoint[Any, Any],
+        recv_timeout: float | None,
         mock_stream_transport: MagicMock,
-        consumer_buffer_updated: MagicMock,
     ) -> None:
         # Arrange
         mock_stream_transport.recv_into.side_effect = make_recv_into_side_effect([b""])
 
         # Act
         with pytest.raises(EOFError, match=r"^end-of-stream$"):
-            _ = await endpoint.recv_packet()
+            _ = endpoint.recv_packet(timeout=recv_timeout)
 
         # Assert
-        mock_stream_transport.recv_into.assert_awaited_once()
-        consumer_buffer_updated.assert_not_called()
+        mock_stream_transport.recv_into.assert_called_once()
 
-    @pytest.mark.parametrize("mock_stream_transport", [AsyncStreamReadTransport], indirect=True)
-    async def test____recv_packet____protocol_parse_error(
+    @pytest.mark.parametrize("mock_stream_transport", [StreamReadTransport], indirect=True)
+    def test____recv_packet____blocking_or_not____protocol_parse_error(
         self,
-        endpoint: AsyncStreamEndpoint[Any, Any],
+        endpoint: StreamEndpoint[Any, Any],
+        recv_timeout: float | None,
         mock_stream_transport: MagicMock,
         mock_stream_protocol: MagicMock,
     ) -> None:
         # Arrange
         mock_stream_transport.recv.side_effect = [b"packet\n"]
         expected_error = StreamProtocolParseError(b"", IncrementalDeserializeError("Error", b""))
 
@@ -549,24 +584,25 @@
             yield
             raise expected_error
 
         mock_stream_protocol.build_packet_from_chunks.side_effect = side_effect
 
         # Act
         with pytest.raises(StreamProtocolParseError) as exc_info:
-            _ = await endpoint.recv_packet()
+            _ = endpoint.recv_packet(timeout=recv_timeout)
 
         # Assert
         assert exc_info.value is expected_error
 
-    @pytest.mark.parametrize("mock_stream_transport", [AsyncBufferedStreamReadTransport], indirect=True)
+    @pytest.mark.parametrize("mock_stream_transport", [BufferedStreamReadTransport], indirect=True)
     @pytest.mark.parametrize("stream_protocol_mode", ["buffer"], indirect=True)
-    async def test____recv_packet____buffered____protocol_parse_error(
+    def test____recv_packet____buffered____blocking_or_not____protocol_parse_error(
         self,
-        endpoint: AsyncStreamEndpoint[Any, Any],
+        endpoint: StreamEndpoint[Any, Any],
+        recv_timeout: float | None,
         mock_stream_transport: MagicMock,
         mock_buffered_stream_receiver: MagicMock,
     ) -> None:
         # Arrange
         mock_stream_transport.recv_into.side_effect = make_recv_into_side_effect([b"packet\n"])
         expected_error = StreamProtocolParseError(b"", IncrementalDeserializeError("Error", b""))
 
@@ -574,111 +610,213 @@
             yield
             raise expected_error
 
         mock_buffered_stream_receiver.build_packet_from_buffer.side_effect = side_effect
 
         # Act
         with pytest.raises(StreamProtocolParseError) as exc_info:
-            _ = await endpoint.recv_packet()
+            _ = endpoint.recv_packet(timeout=recv_timeout)
 
         # Assert
         assert exc_info.value is expected_error
 
-    @pytest.mark.parametrize("mock_stream_transport", [AsyncStreamReadTransport], indirect=True)
+    @pytest.mark.parametrize("mock_stream_transport", [StreamReadTransport], indirect=True)
     @pytest.mark.parametrize("before_transport_reading", [False, True], ids=lambda p: f"before_transport_reading=={p}")
-    async def test____recv_packet____protocol_crashed(
+    def test____recv_packet____blocking_or_not____protocol_crashed(
         self,
         before_transport_reading: bool,
-        endpoint: AsyncStreamEndpoint[Any, Any],
+        endpoint: StreamEndpoint[Any, Any],
+        recv_timeout: float | None,
         mock_stream_transport: MagicMock,
         mock_stream_protocol: MagicMock,
     ) -> None:
         # Arrange
         mock_stream_transport.recv.side_effect = [b"packet_1\npacket_2\n"]
         expected_error = Exception("Error")
 
         if before_transport_reading:
-            await endpoint.recv_packet()
+            endpoint.recv_packet()
 
         def side_effect() -> Generator[None, bytes, tuple[Any, bytes]]:
             yield
             raise expected_error
 
         mock_stream_protocol.build_packet_from_chunks.side_effect = side_effect
 
         # Act
         with pytest.raises(RuntimeError, match=r"^protocol\.build_packet_from_chunks\(\) crashed$") as exc_info:
-            _ = await endpoint.recv_packet()
+            _ = endpoint.recv_packet(timeout=recv_timeout)
 
         # Assert
         assert exc_info.value.__cause__ is expected_error
 
-    @pytest.mark.parametrize("mock_stream_transport", [AsyncBufferedStreamReadTransport], indirect=True)
+    @pytest.mark.parametrize("mock_stream_transport", [BufferedStreamReadTransport], indirect=True)
     @pytest.mark.parametrize("stream_protocol_mode", ["buffer"], indirect=True)
     @pytest.mark.parametrize("before_transport_reading", [False, True], ids=lambda p: f"before_transport_reading=={p}")
-    async def test____recv_packet____buffered____protocol_crashed(
+    def test____recv_packet____buffered____blocking_or_not____protocol_crashed(
         self,
         before_transport_reading: bool,
-        endpoint: AsyncStreamEndpoint[Any, Any],
+        endpoint: StreamEndpoint[Any, Any],
+        recv_timeout: float | None,
         mock_stream_transport: MagicMock,
         mock_buffered_stream_receiver: MagicMock,
     ) -> None:
         # Arrange
         mock_stream_transport.recv_into.side_effect = make_recv_into_side_effect([b"packet_1\n", b"packet_2\n"])
         expected_error = Exception("Error")
 
         if before_transport_reading:
-            await endpoint.recv_packet()
+            endpoint.recv_packet()
 
         def side_effect(buffer: memoryview) -> Generator[None, int, tuple[Any, bytes]]:
             yield
             raise expected_error
 
         mock_buffered_stream_receiver.build_packet_from_buffer.side_effect = side_effect
 
         # Act
         with pytest.raises(RuntimeError, match=r"^protocol\.build_packet_from_buffer\(\) crashed$") as exc_info:
-            _ = await endpoint.recv_packet()
+            _ = endpoint.recv_packet(timeout=recv_timeout)
 
         # Assert
         assert exc_info.value.__cause__ is expected_error
 
-    @pytest.mark.parametrize("mock_stream_transport", [AsyncStreamReadTransport], indirect=True)
-    async def test____special_case____recv_packet____eof_error____do_not_try_socket_recv_on_next_call(
+    @pytest.mark.parametrize("mock_stream_transport", [StreamReadTransport], indirect=True)
+    def test____special_case____recv_packet____blocking_or_not____eof_error____do_not_try_socket_recv_on_next_call(
         self,
-        endpoint: AsyncStreamEndpoint[Any, Any],
+        endpoint: StreamEndpoint[Any, Any],
+        recv_timeout: float | None,
         mock_stream_transport: MagicMock,
     ) -> None:
         # Arrange
         mock_stream_transport.recv.side_effect = [b""]
         with pytest.raises(EOFError, match=r"^end-of-stream$"):
-            _ = await endpoint.recv_packet()
+            _ = endpoint.recv_packet(timeout=recv_timeout)
 
         mock_stream_transport.recv.reset_mock()
 
         # Act
         with pytest.raises(EOFError, match=r"^end-of-stream$"):
-            _ = await endpoint.recv_packet()
+            _ = endpoint.recv_packet(timeout=recv_timeout)
 
         # Assert
         mock_stream_transport.recv.assert_not_called()
 
-    @pytest.mark.parametrize("mock_stream_transport", [AsyncBufferedStreamReadTransport], indirect=True)
+    @pytest.mark.parametrize("mock_stream_transport", [BufferedStreamReadTransport], indirect=True)
     @pytest.mark.parametrize("stream_protocol_mode", ["buffer"], indirect=True)
-    async def test____special_case____recv_packet____buffered____eof_error____do_not_try_socket_recv_on_next_call(
+    def test____special_case____recv_packet____buffered____blocking_or_not____eof_error____do_not_try_socket_recv_on_next_call(
         self,
-        endpoint: AsyncStreamEndpoint[Any, Any],
+        endpoint: StreamEndpoint[Any, Any],
+        recv_timeout: float | None,
         mock_stream_transport: MagicMock,
     ) -> None:
         # Arrange
         mock_stream_transport.recv_into.side_effect = make_recv_into_side_effect([b""])
         with pytest.raises(EOFError, match=r"^end-of-stream$"):
-            _ = await endpoint.recv_packet()
+            _ = endpoint.recv_packet(timeout=recv_timeout)
 
         mock_stream_transport.recv_into.reset_mock()
 
         # Act
         with pytest.raises(EOFError, match=r"^end-of-stream$"):
-            _ = await endpoint.recv_packet()
+            _ = endpoint.recv_packet(timeout=recv_timeout)
 
         # Assert
         mock_stream_transport.recv_into.assert_not_called()
+
+    # NOTE: The cases where recv_packet() uses transport.recv() or transport.recv_into() when manual_buffer_allocation == "try"
+    #       are implicitly tested above, because this is the default behavior.
+
+    @pytest.mark.parametrize("mock_stream_transport", [StreamReadTransport, BufferedStreamReadTransport], indirect=True)
+    @pytest.mark.parametrize("stream_protocol_mode", ["data"], indirect=True)
+    def test____manual_buffer_allocation____try____but_stream_protocol_does_not_support_it(
+        self,
+        mock_stream_transport: MagicMock,
+        mock_stream_protocol: MagicMock,
+        max_recv_size: int,
+    ) -> None:
+        # Arrange
+
+        # Act & Assert
+        with warnings.catch_warnings():
+            warnings.simplefilter("error", ManualBufferAllocationWarning)
+            _ = StreamEndpoint(mock_stream_transport, mock_stream_protocol, max_recv_size, manual_buffer_allocation="try")
+
+    @pytest.mark.parametrize("mock_stream_transport", [StreamReadTransport], indirect=True)
+    @pytest.mark.parametrize("stream_protocol_mode", ["buffer"], indirect=True)
+    def test____manual_buffer_allocation____try____but_stream_transport_does_not_support_it(
+        self,
+        mock_stream_transport: MagicMock,
+        mock_stream_protocol: MagicMock,
+        max_recv_size: int,
+    ) -> None:
+        # Arrange
+
+        # Act & Assert
+        with pytest.warns(
+            ManualBufferAllocationWarning,
+            match=r"^The transport implementation .+ does not implement BufferedStreamReadTransport interface$",
+        ):
+            _ = StreamEndpoint(mock_stream_transport, mock_stream_protocol, max_recv_size, manual_buffer_allocation="try")
+
+    @pytest.mark.parametrize("mock_stream_transport", [StreamReadTransport, BufferedStreamReadTransport], indirect=True)
+    def test____manual_buffer_allocation____disabled(
+        self,
+        mock_stream_transport: MagicMock,
+        mock_stream_protocol: MagicMock,
+        max_recv_size: int,
+        mocker: MockerFixture,
+    ) -> None:
+        # Arrange
+        mock_stream_transport.recv.side_effect = [b"packet\n"]
+
+        # Act
+        endpoint: StreamEndpoint[Any, Any]
+        with warnings.catch_warnings():
+            warnings.simplefilter("error", ManualBufferAllocationWarning)
+            endpoint = StreamEndpoint(mock_stream_transport, mock_stream_protocol, max_recv_size, manual_buffer_allocation="no")
+        packet = endpoint.recv_packet()
+
+        # Assert
+        mock_stream_transport.recv.assert_called_once_with(max_recv_size, math.inf)
+        if hasattr(mock_stream_transport, "recv_into"):
+            mock_stream_transport.recv_into.assert_not_called()
+        assert packet is mocker.sentinel.packet
+
+    @pytest.mark.parametrize("mock_stream_transport", [StreamReadTransport, BufferedStreamReadTransport], indirect=True)
+    @pytest.mark.parametrize("stream_protocol_mode", ["data"], indirect=True)
+    def test____manual_buffer_allocation____force____but_stream_protocol_does_not_support_it(
+        self,
+        mock_stream_transport: MagicMock,
+        mock_stream_protocol: MagicMock,
+        max_recv_size: int,
+    ) -> None:
+        # Arrange
+
+        # Act & Assert
+        with (
+            pytest.raises(UnsupportedOperation, match=r"^This protocol does not support the buffer API$"),
+            warnings.catch_warnings(),
+        ):
+            warnings.simplefilter("error", ManualBufferAllocationWarning)
+            _ = StreamEndpoint(mock_stream_transport, mock_stream_protocol, max_recv_size, manual_buffer_allocation="force")
+
+    @pytest.mark.parametrize("mock_stream_transport", [StreamReadTransport], indirect=True)
+    @pytest.mark.parametrize("stream_protocol_mode", ["buffer"], indirect=True)
+    def test____manual_buffer_allocation____force____but_stream_transport_does_not_support_it(
+        self,
+        mock_stream_transport: MagicMock,
+        mock_stream_protocol: MagicMock,
+        max_recv_size: int,
+    ) -> None:
+        # Arrange
+
+        # Act & Assert
+        with (
+            pytest.raises(
+                UnsupportedOperation,
+                match=r"^The transport implementation .+ does not implement BufferedStreamReadTransport interface$",
+            ),
+            warnings.catch_warnings(),
+        ):
+            warnings.simplefilter("error", ManualBufferAllocationWarning)
+            _ = StreamEndpoint(mock_stream_transport, mock_stream_protocol, max_recv_size, manual_buffer_allocation="force")
```

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_async/test_lowlevel_api/test_transports/test_abc.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_async/test_lowlevel_api/test_transports/test_abc.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_serializers/base.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_serializers/base.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_serializers/test_abc.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_serializers/test_abc.py`

 * *Files 2% similar despite different names*

```diff
@@ -168,24 +168,31 @@
         return mocker.patch.object(_AutoSeparatedPacketSerializerForTest, "deserialize")
 
     @pytest.fixture(params=[False, True], ids=lambda p: f"check_separator=={p}")
     @staticmethod
     def check_separator(request: pytest.FixtureRequest) -> bool:
         return getattr(request, "param")
 
+    @pytest.fixture(params=["data", "buffer"])
+    @staticmethod
+    def incremental_deserialize_mode(request: pytest.FixtureRequest) -> str:
+        assert request.param in ("data", "buffer")
+        return request.param
+
     @pytest.mark.parametrize("separator", [b"\n", b".", b"\r\n", b"--"], ids=lambda p: f"separator=={p}")
     def test____properties____right_values(self, separator: bytes, debug_mode: bool) -> None:
         # Arrange
 
         # Act
-        serializer = _AutoSeparatedPacketSerializerForTest(separator=separator, debug=debug_mode)
+        serializer = _AutoSeparatedPacketSerializerForTest(separator=separator, debug=debug_mode, limit=123456789)
 
         # Assert
         assert serializer.separator == separator
         assert serializer.debug is debug_mode
+        assert serializer.buffer_limit == 123456789
 
     def test____dunder_init____empty_separator_bytes(self) -> None:
         # Arrange
 
         # Act & Assert
         with pytest.raises(ValueError, match=r"^Empty separator$"):
             _ = _AutoSeparatedPacketSerializerForTest(b"")
@@ -214,36 +221,33 @@
         # Act
         data = list(serializer.incremental_serialize(mocker.sentinel.packet))
 
         # Assert
         mock_serialize_func.assert_called_once_with(mocker.sentinel.packet)
         assert data == []
 
-    @pytest.mark.parametrize("limit_reached", [False, True], ids=lambda p: f"limit_reached=={p}")
     def test____incremental_serialize____append_separator(
         self,
-        limit_reached: bool,
         check_separator: bool,
         mock_serialize_func: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         serializer = _AutoSeparatedPacketSerializerForTest(
             separator=b"\r\n",
             incremental_serialize_check_separator=check_separator,
-            limit=4 if limit_reached else 42,
         )
         mock_serialize_func.return_value = b"data"
 
         # Act
         data = list(serializer.incremental_serialize(mocker.sentinel.packet))
 
         # Assert
         mock_serialize_func.assert_called_once_with(mocker.sentinel.packet)
-        assert data == ([b"data", b"\r\n"] if limit_reached else [b"data\r\n"])
+        assert data == [b"data\r\n"]
 
     def test____incremental_serialize____keep_already_present_separator(
         self,
         check_separator: bool,
         mock_serialize_func: MagicMock,
         mocker: MockerFixture,
     ) -> None:
@@ -255,15 +259,15 @@
         mock_serialize_func.return_value = b"data\r\n"
 
         # Act
         data = list(serializer.incremental_serialize(mocker.sentinel.packet))
 
         # Assert
         mock_serialize_func.assert_called_once_with(mocker.sentinel.packet)
-        assert data == [b"data\r\n" if check_separator else b"data\r\n\r\n"]
+        assert data == [b"data\r\n"]
 
     def test____incremental_serialize____remove_useless_trailing_separators(
         self,
         check_separator: bool,
         mock_serialize_func: MagicMock,
         mocker: MockerFixture,
     ) -> None:
@@ -275,15 +279,15 @@
         mock_serialize_func.return_value = b"data\r\n\r\n\r\n\r\n"
 
         # Act
         data = list(serializer.incremental_serialize(mocker.sentinel.packet))
 
         # Assert
         mock_serialize_func.assert_called_once_with(mocker.sentinel.packet)
-        assert data == [b"data\r\n" if check_separator else b"data\r\n\r\n\r\n\r\n\r\n"]
+        assert data == [b"data\r\n" if check_separator else b"data\r\n\r\n\r\n\r\n"]
 
     def test____incremental_serialize____does_not_remove_partial_separator_at_end(
         self,
         check_separator: bool,
         mock_serialize_func: MagicMock,
         mocker: MockerFixture,
     ) -> None:
@@ -295,15 +299,15 @@
         mock_serialize_func.return_value = b"data\r\r\n"
 
         # Act
         data = list(serializer.incremental_serialize(mocker.sentinel.packet))
 
         # Assert
         mock_serialize_func.assert_called_once_with(mocker.sentinel.packet)
-        assert data == [b"data\r\r\n" if check_separator else b"data\r\r\n\r\n"]
+        assert data == [b"data\r\r\n" if check_separator else b"data\r\r\n"]
 
     def test____incremental_serialize____error_if_separator_is_within_output(
         self,
         check_separator: bool,
         mock_serialize_func: MagicMock,
         mocker: MockerFixture,
     ) -> None:
@@ -311,114 +315,162 @@
         serializer = _AutoSeparatedPacketSerializerForTest(
             separator=b"\r\n",
             incremental_serialize_check_separator=check_separator,
         )
         mock_serialize_func.return_value = b"data\r\nother"
 
         # Act & Assert
-        with pytest.raises(
-            ValueError,
-            match=r"^b'[\\]r[\\]n' separator found in serialized packet sentinel\.packet which was not at the end$",
-        ) if check_separator else contextlib.nullcontext():
+        with (
+            pytest.raises(
+                ValueError,
+                match=r"^b'[\\]r[\\]n' separator found in serialized packet sentinel\.packet which was not at the end$",
+            )
+            if check_separator
+            else contextlib.nullcontext()
+        ):
             list(serializer.incremental_serialize(mocker.sentinel.packet))
 
     @pytest.mark.parametrize(
         "expected_remaining_data",
         [
             pytest.param(b"", id="without remaining data"),
             pytest.param(b"remaining", id="with remaining data"),
             pytest.param(b"remaining\r\nother", id="with remaining data including separator"),
         ],
     )
     def test____incremental_deserialize____one_shot_chunk(
         self,
         expected_remaining_data: bytes,
         mock_deserialize_func: MagicMock,
+        incremental_deserialize_mode: Literal["data", "buffer"],
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         serializer = _AutoSeparatedPacketSerializerForTest(separator=b"\r\n")
         mock_deserialize_func.return_value = mocker.sentinel.packet
         data_to_test: bytes = b"data\r\n"
 
         # Act
-        consumer = serializer.incremental_deserialize()
-        next(consumer)
-        packet, remaining_data = send_return(consumer, data_to_test + expected_remaining_data)
+        sent_data = data_to_test + expected_remaining_data
+        remaining_data: ReadableBuffer
+        match incremental_deserialize_mode:
+            case "data":
+                data_consumer = serializer.incremental_deserialize()
+                next(data_consumer)
+                packet, remaining_data = send_return(data_consumer, sent_data)
+            case "buffer":
+                buffer = serializer.create_deserializer_buffer(1024)
+                buffered_consumer = serializer.buffered_incremental_deserialize(buffer)
+                start_pos = next(buffered_consumer)
+                packet, remaining_data = send_return(buffered_consumer, write_in_buffer(buffer, sent_data, start_pos=start_pos))
+            case _:
+                pytest.fail("Invalid fixture argument")
 
         # Assert
         mock_deserialize_func.assert_called_once_with(b"data")
-        assert remaining_data == expected_remaining_data
+        assert bytes(remaining_data) == expected_remaining_data
         assert packet is mocker.sentinel.packet
 
     @pytest.mark.parametrize(
         "expected_remaining_data",
         [
             pytest.param(b"", id="without remaining data"),
             pytest.param(b"remaining", id="with remaining data"),
             pytest.param(b"remaining\r\nother", id="with remaining data including separator"),
         ],
     )
     def test____incremental_deserialize____several_chunks(
         self,
         expected_remaining_data: bytes,
         mock_deserialize_func: MagicMock,
+        incremental_deserialize_mode: Literal["data", "buffer"],
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         serializer = _AutoSeparatedPacketSerializerForTest(separator=b"\r\n")
         mock_deserialize_func.return_value = mocker.sentinel.packet
 
         # Act
-        consumer = serializer.incremental_deserialize()
-        next(consumer)
-        consumer.send(b"data\r")
-        packet, remaining_data = send_return(consumer, b"\n" + expected_remaining_data)
+        remaining_data: ReadableBuffer
+        match incremental_deserialize_mode:
+            case "data":
+                data_consumer = serializer.incremental_deserialize()
+                next(data_consumer)
+                data_consumer.send(b"data\r")
+                packet, remaining_data = send_return(data_consumer, b"\n" + expected_remaining_data)
+            case "buffer":
+                buffer = serializer.create_deserializer_buffer(1024)
+                buffered_consumer = serializer.buffered_incremental_deserialize(buffer)
+                start_pos = next(buffered_consumer)
+                start_pos = buffered_consumer.send(write_in_buffer(buffer, b"data\r", start_pos=start_pos))
+                packet, remaining_data = send_return(
+                    buffered_consumer,
+                    write_in_buffer(
+                        buffer,
+                        b"\n" + expected_remaining_data,
+                        start_pos=start_pos,
+                    ),
+                )
+            case _:
+                pytest.fail("Invalid fixture argument")
 
         # Assert
         mock_deserialize_func.assert_called_once_with(b"data")
-        assert remaining_data == expected_remaining_data
+        assert bytes(remaining_data) == expected_remaining_data
         assert packet is mocker.sentinel.packet
 
     @pytest.mark.parametrize(
         "expected_remaining_data",
         [
             pytest.param(b"", id="without remaining data"),
             pytest.param(b"remaining", id="with remaining data"),
             pytest.param(b"remaining\r\nother", id="with remaining data including separator"),
         ],
     )
     def test____incremental_deserialize____translate_deserialize_errors(
         self,
         expected_remaining_data: bytes,
         mock_deserialize_func: MagicMock,
+        incremental_deserialize_mode: Literal["data", "buffer"],
         debug_mode: bool,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         serializer = _AutoSeparatedPacketSerializerForTest(separator=b"\r\n", debug=debug_mode)
         mock_deserialize_func.side_effect = DeserializeError("Bad news", error_info=mocker.sentinel.error_info)
 
         # Act
-        consumer = serializer.incremental_deserialize()
-        next(consumer)
-        with pytest.raises(IncrementalDeserializeError) as exc_info:
-            consumer.send(b"data\r\n" + expected_remaining_data)
-        exception = exc_info.value
+        sent_data = b"data\r\n" + expected_remaining_data
+        match incremental_deserialize_mode:
+            case "data":
+                data_consumer = serializer.incremental_deserialize()
+                next(data_consumer)
+                with pytest.raises(IncrementalDeserializeError) as exc_info:
+                    send_return(data_consumer, sent_data)
+                exception = exc_info.value
+            case "buffer":
+                buffer = serializer.create_deserializer_buffer(1024)
+                buffered_consumer = serializer.buffered_incremental_deserialize(buffer)
+                start_pos = next(buffered_consumer)
+                with pytest.raises(IncrementalDeserializeError) as exc_info:
+                    send_return(buffered_consumer, write_in_buffer(buffer, sent_data, start_pos=start_pos))
+                exception = exc_info.value
+            case _:
+                pytest.fail("Invalid fixture argument")
 
         # Assert
         mock_deserialize_func.assert_called_once_with(b"data")
         assert exception.__cause__ is mock_deserialize_func.side_effect
         assert bytes(exception.remaining_data) == expected_remaining_data
         assert exception.error_info is mocker.sentinel.error_info
 
     @pytest.mark.parametrize("separator_found", [False, True], ids=lambda p: f"separator_found=={p}")
     def test____incremental_deserialize____reached_limit(
         self,
-        separator_found: bytes,
+        separator_found: bool,
         mock_deserialize_func: MagicMock,
         debug_mode: bool,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         serializer = _AutoSeparatedPacketSerializerForTest(separator=b"\r\n", limit=1, debug=debug_mode)
         mock_deserialize_func.return_value = mocker.sentinel.packet
@@ -440,15 +492,15 @@
             assert str(exc_info.value) == "Separator is not found, and chunk exceed the limit"
         assert bytes(exc_info.value.remaining_data) == b""
         assert exc_info.value.error_info is None
 
     @pytest.mark.parametrize("separator_found", [False, True], ids=lambda p: f"separator_found=={p}")
     def test____incremental_deserialize____reached_limit____separator_partially_received(
         self,
-        separator_found: bytes,
+        separator_found: bool,
         mock_deserialize_func: MagicMock,
         debug_mode: bool,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         serializer = _AutoSeparatedPacketSerializerForTest(separator=b"\r\n", limit=1, debug=debug_mode)
         mock_deserialize_func.return_value = mocker.sentinel.packet
@@ -468,14 +520,62 @@
             assert str(exc_info.value) == "Separator is found, but chunk is longer than limit"
             assert bytes(exc_info.value.remaining_data) == b""
         else:
             assert str(exc_info.value) == "Separator is not found, and chunk exceed the limit"
             assert bytes(exc_info.value.remaining_data) == b"\r"
         assert exc_info.value.error_info is None
 
+    def test____buffered_incremental_deserialize____reached_limit(
+        self,
+        mock_deserialize_func: MagicMock,
+        debug_mode: bool,
+        mocker: MockerFixture,
+    ) -> None:
+        # Arrange
+        serializer = _AutoSeparatedPacketSerializerForTest(separator=b"\r\n", limit=1024, debug=debug_mode)
+        mock_deserialize_func.return_value = mocker.sentinel.packet
+        data_to_test: bytes = b"X" * 1023
+
+        # Act
+        buffer = serializer.create_deserializer_buffer(1024)
+        consumer = serializer.buffered_incremental_deserialize(buffer)
+        start_pos = next(consumer)
+        with pytest.raises(LimitOverrunError) as exc_info:
+            consumer.send(write_in_buffer(buffer, data_to_test, start_pos=start_pos))
+
+        # Assert
+        mock_deserialize_func.assert_not_called()
+        assert str(exc_info.value) == "Separator is not found, and chunk exceed the limit"
+        assert bytes(exc_info.value.remaining_data) == b""
+        assert exc_info.value.error_info is None
+
+    def test____buffered_incremental_deserialize____reached_limit____separator_partially_received(
+        self,
+        mock_deserialize_func: MagicMock,
+        debug_mode: bool,
+        mocker: MockerFixture,
+    ) -> None:
+        # Arrange
+        serializer = _AutoSeparatedPacketSerializerForTest(separator=b"\r\n", limit=1024, debug=debug_mode)
+        mock_deserialize_func.return_value = mocker.sentinel.packet
+        data_to_test: bytes = b"X" * 1023 + b"\r"
+
+        # Act
+        buffer = serializer.create_deserializer_buffer(1024)
+        consumer = serializer.buffered_incremental_deserialize(buffer)
+        start_pos = next(consumer)
+        with pytest.raises(LimitOverrunError) as exc_info:
+            consumer.send(write_in_buffer(buffer, data_to_test, start_pos=start_pos))
+
+        # Assert
+        mock_deserialize_func.assert_not_called()
+        assert str(exc_info.value) == "Separator is not found, and chunk exceed the limit"
+        assert bytes(exc_info.value.remaining_data) == b"\r"
+        assert exc_info.value.error_info is None
+
 
 class _FixedSizePacketSerializerForTest(FixedSizePacketSerializer[Any, Any]):
     def serialize(self, packet: Any) -> bytes:
         raise NotImplementedError
 
     def deserialize(self, data: bytes) -> Any:
         raise NotImplementedError
```

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_serializers/test_base64.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_serializers/test_base64.py`

 * *Files 13% similar despite different names*

```diff
@@ -25,39 +25,68 @@
         return mocker.patch(f"base64.{alphabet}_b64encode", autospec=True)
 
     @pytest.fixture(autouse=True)
     @staticmethod
     def mock_b64decode(mocker: MockerFixture, alphabet: Literal["standard", "urlsafe"]) -> MagicMock:
         return mocker.patch(f"base64.{alphabet}_b64decode", autospec=True)
 
-    @pytest.mark.parametrize("method", ["incremental_serialize", "incremental_deserialize"])
+    @pytest.mark.parametrize(
+        "method",
+        [
+            "incremental_serialize",
+            "incremental_deserialize",
+            "create_deserializer_buffer",
+            "buffered_incremental_deserialize",
+        ],
+    )
     def test____base_class____implements_default_methods(self, method: str) -> None:
         # Arrange
         from easynetwork.serializers.base_stream import AutoSeparatedPacketSerializer
 
         # Act & Assert
         assert getattr(Base64EncoderSerializer, method) is getattr(AutoSeparatedPacketSerializer, method)
 
     def test____properties____right_values(self, mock_serializer: MagicMock, debug_mode: bool) -> None:
         # Arrange
 
         # Act
-        serializer: Base64EncoderSerializer[Any, Any] = Base64EncoderSerializer(mock_serializer, debug=debug_mode)
+        serializer: Base64EncoderSerializer[Any, Any] = Base64EncoderSerializer(
+            mock_serializer,
+            debug=debug_mode,
+            limit=123456789,
+        )
 
         # Assert
         assert serializer.debug is debug_mode
+        assert serializer.buffer_limit == 123456789
 
     def test____dunder_init____invalid_serializer(self, mocker: MockerFixture) -> None:
         # Arrange
         mock_not_serializer = mocker.NonCallableMagicMock(spec=object)
 
         # Act
         with pytest.raises(TypeError, match=r"^Expected a serializer instance, got .+$"):
             Base64EncoderSerializer(mock_not_serializer)
 
+    @pytest.mark.parametrize("invalid_checksum", [1, 0, bytearray(b"key")], ids=repr)
+    def test____dunder_init____invalid_checksum(self, mock_serializer: MagicMock, invalid_checksum: Any) -> None:
+        # Arrange
+
+        # Act
+        with pytest.raises(TypeError, match=r"^Invalid checksum argument$"):
+            Base64EncoderSerializer(mock_serializer, checksum=invalid_checksum)
+
+    @pytest.mark.parametrize("invalid_alphabet", ["unknown", 4], ids=repr)
+    def test____dunder_init____invalid_alphabet(self, mock_serializer: MagicMock, invalid_alphabet: Any) -> None:
+        # Arrange
+
+        # Act
+        with pytest.raises(TypeError, match=r"^Invalid alphabet argument$"):
+            Base64EncoderSerializer(mock_serializer, alphabet=invalid_alphabet)
+
     def test____serialize____encode_previously_serialized_data(
         self,
         alphabet: Literal["standard", "urlsafe"],
         mock_serializer: MagicMock,
         mock_b64encode: MagicMock,
         mocker: MockerFixture,
     ) -> None:
```

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_serializers/test_cbor.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_serializers/test_cbor.py`

 * *Files 16% similar despite different names*

```diff
@@ -2,14 +2,15 @@
 
 from typing import TYPE_CHECKING, Any, final
 
 from easynetwork.serializers.cbor import CBORDecoderConfig, CBOREncoderConfig, CBORSerializer
 
 import pytest
 
+from .._utils import mock_import_module_not_found
 from .base import BaseSerializerConfigInstanceCheck
 
 if TYPE_CHECKING:
     from unittest.mock import MagicMock
 
     from pytest_mock import MockerFixture
 
@@ -152,7 +153,29 @@
             mock_file,
             object_hook=mocker.sentinel.object_hook if decoder_config is not None else None,
             tag_hook=mocker.sentinel.tag_hook if decoder_config is not None else None,
             str_errors=mocker.sentinel.str_errors if decoder_config is not None else "strict",
         )
         mock_decoder.decode.assert_called_once_with()
         assert packet is mocker.sentinel.packet
+
+
+class TestCBORSerializerDependencies:
+    def test____dunder_init____cbor2_missing(
+        self,
+        mocker: MockerFixture,
+    ) -> None:
+        # Arrange
+        mock_import: MagicMock = mock_import_module_not_found({"cbor2"}, mocker)
+
+        # Act
+        with pytest.raises(ModuleNotFoundError) as exc_info:
+            try:
+                _ = CBORSerializer()
+            finally:
+                mocker.stop(mock_import)
+
+        # Assert
+        mock_import.assert_any_call("cbor2", mocker.ANY, mocker.ANY, None, 0)
+        assert exc_info.value.args[0] == "cbor dependencies are missing. Consider adding 'cbor' extra"
+        assert exc_info.value.__notes__ == ['example: pip install "easynetwork[cbor]"']
+        assert isinstance(exc_info.value.__cause__, ModuleNotFoundError)
```

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_serializers/test_compressor.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_serializers/test_compressor.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_serializers/test_json.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_serializers/test_json.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 from __future__ import annotations
 
 from collections.abc import Generator
 from typing import TYPE_CHECKING, Any
 
 from easynetwork.exceptions import DeserializeError, IncrementalDeserializeError, LimitOverrunError
-from easynetwork.lowlevel.constants import _DEFAULT_LIMIT as DEFAULT_LIMIT
+from easynetwork.lowlevel.constants import DEFAULT_SERIALIZER_LIMIT as DEFAULT_LIMIT
 from easynetwork.serializers.json import JSONDecoderConfig, JSONEncoderConfig, JSONSerializer, _JSONParser
 from easynetwork.serializers.tools import GeneratorStreamReader
 
 import pytest
 
 from ...tools import send_return
 from .base import BaseSerializerConfigInstanceCheck
@@ -106,18 +106,19 @@
     def mock_generator_stream_reader_cls(mock_generator_stream_reader: MagicMock, mocker: MockerFixture) -> MagicMock:
         return mocker.patch(f"{JSONSerializer.__module__}.GeneratorStreamReader", return_value=mock_generator_stream_reader)
 
     def test____properties____right_values(self, debug_mode: bool) -> None:
         # Arrange
 
         # Act
-        serializer = JSONSerializer(debug=debug_mode)
+        serializer = JSONSerializer(debug=debug_mode, limit=123456789)
 
         # Assert
         assert serializer.debug is debug_mode
+        assert serializer.buffer_limit == 123456789
 
     def test____dunder_init____with_encoder_config(
         self,
         encoder_config: JSONEncoderConfig | None,
         mock_encoder_cls: MagicMock,
         mocker: MockerFixture,
     ) -> None:
@@ -168,163 +169,127 @@
 
     def test____serialize____encode_packet(
         self,
         mock_encoder: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
-        serializer: JSONSerializer = JSONSerializer(
-            encoding=mocker.sentinel.encoding,
-            unicode_errors=mocker.sentinel.str_errors,
-        )
-        mock_string = mock_encoder.encode.return_value = mocker.NonCallableMagicMock()
-        mock_string.encode.return_value = b'{"data":42}'
+        serializer: JSONSerializer = JSONSerializer()
+        mock_encoder.encode.return_value = '{"data":42}'
 
         # Act
         data = serializer.serialize(mocker.sentinel.packet)
 
         # Assert
         mock_encoder.encode.assert_called_once_with(mocker.sentinel.packet)
-        mock_string.encode.assert_called_once_with(mocker.sentinel.encoding, mocker.sentinel.str_errors)
         assert data == b'{"data":42}'
 
     @pytest.mark.parametrize("value", [b'{"data":42}', b"[4]", b'"string"'])
-    @pytest.mark.parametrize("limit_reached", [False, True], ids=lambda p: f"limit_reached=={p}")
     def test____incremental_serialize____encode_packet____with_frames(
         self,
         value: bytes,
         use_lines: bool,
-        limit_reached: bool,
         mock_encoder: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
-        serializer: JSONSerializer = JSONSerializer(
-            encoding=mocker.sentinel.encoding,
-            unicode_errors=mocker.sentinel.str_errors,
-            limit=2 if limit_reached else 1024,
-            use_lines=use_lines,
-        )
-        mock_string = mock_encoder.encode.return_value = mocker.NonCallableMagicMock()
-        mock_string.encode.return_value = value
+        serializer: JSONSerializer = JSONSerializer(use_lines=use_lines)
+        mock_encoder.encode.return_value = value.decode()
 
         # Act
         chunks = list(serializer.incremental_serialize(mocker.sentinel.packet))
 
         # Assert
         mock_encoder.encode.assert_called_once_with(mocker.sentinel.packet)
-        mock_string.encode.assert_called_once_with(mocker.sentinel.encoding, mocker.sentinel.str_errors)
         if use_lines:
-            if limit_reached:
-                assert chunks == [value, b"\n"]
-            else:
-                assert chunks == [value + b"\n"]
+            assert chunks == [value + b"\n"]
         else:
             assert chunks == [value]
 
     @pytest.mark.parametrize("value", [b"12345", b"true", b"false", b"null"])
-    @pytest.mark.parametrize("limit_reached", [False, True], ids=lambda p: f"limit_reached=={p}")
     def test____incremental_serialize____encode_packet____plain_value(
         self,
         value: bytes,
         use_lines: bool,
-        limit_reached: bool,
         mock_encoder: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
-        serializer: JSONSerializer = JSONSerializer(
-            encoding=mocker.sentinel.encoding,
-            unicode_errors=mocker.sentinel.str_errors,
-            limit=2 if limit_reached else 1024,
-            use_lines=use_lines,
-        )
-        mock_string = mock_encoder.encode.return_value = mocker.NonCallableMagicMock()
-        mock_string.encode.return_value = value
+        serializer: JSONSerializer = JSONSerializer(use_lines=use_lines)
+        mock_encoder.encode.return_value = value.decode()
 
         # Act
         chunks = list(serializer.incremental_serialize(mocker.sentinel.packet))
 
         # Assert
         mock_encoder.encode.assert_called_once_with(mocker.sentinel.packet)
-        mock_string.encode.assert_called_once_with(mocker.sentinel.encoding, mocker.sentinel.str_errors)
         assert chunks == [value + b"\n"]
 
     def test____deserialize____decode_data(
         self,
         mock_decoder: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
-        serializer: JSONSerializer = JSONSerializer(
-            encoding=mocker.sentinel.encoding,
-            unicode_errors=mocker.sentinel.str_errors,
-        )
-        mock_bytes = mocker.NonCallableMagicMock()
-        mock_bytes.decode.return_value = mocker.sentinel.document
+        serializer: JSONSerializer = JSONSerializer()
+        data = b'{"data": 42}'
         mock_decoder.decode.return_value = mocker.sentinel.packet
 
         # Act
-        packet = serializer.deserialize(mock_bytes)
+        packet = serializer.deserialize(data)
 
         # Assert
-        mock_bytes.decode.assert_called_once_with(mocker.sentinel.encoding, mocker.sentinel.str_errors)
-        mock_decoder.decode.assert_called_once_with(mocker.sentinel.document)
+        mock_decoder.decode.assert_called_once_with(data.decode())
         assert packet is mocker.sentinel.packet
 
     def test____deserialize____translate_unicode_decode_errors(
         self,
         mock_decoder: MagicMock,
         debug_mode: bool,
-        mocker: MockerFixture,
     ) -> None:
         # Arrange
-        serializer: JSONSerializer = JSONSerializer(debug=debug_mode)
-        mock_bytes = mocker.NonCallableMagicMock()
-        mock_bytes.decode.side_effect = UnicodeDecodeError("some encoding", b"invalid data", 0, 2, "Bad encoding ?")
+        serializer: JSONSerializer = JSONSerializer(encoding="utf-8", debug=debug_mode)
+        data = "é".encode("latin-1")
 
         # Act
         with pytest.raises(DeserializeError) as exc_info:
-            _ = serializer.deserialize(mock_bytes)
+            _ = serializer.deserialize(data)
         exception = exc_info.value
 
         # Assert
-        mock_bytes.decode.assert_called_once()
         mock_decoder.decode.assert_not_called()
-        assert exception.__cause__ is mock_bytes.decode.side_effect
+        assert isinstance(exception.__cause__, UnicodeError)
         if debug_mode:
-            assert exception.error_info == {"data": mock_bytes}
+            assert exception.error_info == {"data": data}
         else:
             assert exception.error_info is None
 
     def test____deserialize____translate_json_decode_errors(
         self,
         mock_decoder: MagicMock,
         debug_mode: bool,
-        mocker: MockerFixture,
     ) -> None:
         # Arrange
         from json import JSONDecodeError
 
         serializer: JSONSerializer = JSONSerializer(debug=debug_mode)
-        mock_bytes = mocker.NonCallableMagicMock()
-        mock_decoder.decode.side_effect = JSONDecodeError("Invalid payload", "invalid\ndocument", 8)
+        data = b"invalid\ndocument"
+        mock_decoder.decode.side_effect = JSONDecodeError("Invalid payload", data.decode(), 8)
 
         # Act
         with pytest.raises(DeserializeError) as exc_info:
-            _ = serializer.deserialize(mock_bytes)
+            _ = serializer.deserialize(data)
         exception = exc_info.value
 
         # Assert
-        mock_bytes.decode.assert_called_once()
         mock_decoder.decode.assert_called_once()
         assert exception.__cause__ is mock_decoder.decode.side_effect
         if debug_mode:
             assert exception.error_info == {
-                "document": "invalid\ndocument",
+                "document": data.decode(),
                 "position": 8,
                 "lineno": 2,
                 "colno": 1,
             }
         else:
             assert exception.error_info is None
 
@@ -336,92 +301,80 @@
         mock_generator_stream_reader_cls: MagicMock,
         mock_generator_stream_reader: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         def raw_parse_side_effect(*args: Any, **kwargs: Any) -> Generator[None, bytes, tuple[bytes, bytes]]:
             data = yield
-            assert data is mocker.sentinel.data
-            return mock_bytes, b"Hello World !"
+            return data, b"Hello World !"
 
         def reader_read_until_side_effect(*args: Any, **kwargs: Any) -> Generator[None, bytes, bytes]:
             data = yield
-            assert data is mocker.sentinel.data
-            return mock_bytes
+            return data
 
-        serializer: JSONSerializer = JSONSerializer(
-            encoding=mocker.sentinel.encoding,
-            unicode_errors=mocker.sentinel.str_errors,
-            use_lines=use_lines,
-        )
-        mock_bytes = mocker.NonCallableMagicMock()
-        mock_string_document = mock_bytes.decode.return_value = mocker.NonCallableMagicMock()
+        serializer: JSONSerializer = JSONSerializer(use_lines=use_lines)
+        data = b'{"data": 42}'
         mock_decoder.decode.return_value = mocker.sentinel.packet
         mock_json_parser.side_effect = raw_parse_side_effect
         mock_generator_stream_reader.read_until.side_effect = reader_read_until_side_effect
         mock_generator_stream_reader.read_all.return_value = b"Hello World !"
 
         # Act
         consumer = serializer.incremental_deserialize()
         next(consumer)
-        packet, remaining_data = send_return(consumer, mocker.sentinel.data)
+        packet, remaining_data = send_return(consumer, data)
 
         # Assert
         if use_lines:
             mock_json_parser.assert_not_called()
             mock_generator_stream_reader_cls.assert_called_once_with()
             mock_generator_stream_reader.read_until.assert_called_once_with(b"\n", limit=DEFAULT_LIMIT)
             mock_generator_stream_reader.read_all.assert_called_once_with()
         else:
             mock_json_parser.assert_called_once_with(limit=DEFAULT_LIMIT)
             mock_generator_stream_reader_cls.assert_not_called()
             mock_generator_stream_reader.read_until.assert_not_called()
             mock_generator_stream_reader.read_all.assert_not_called()
-        mock_bytes.decode.assert_called_once_with(mocker.sentinel.encoding, mocker.sentinel.str_errors)
-        mock_decoder.decode.assert_called_once_with(mock_string_document)
+        mock_decoder.decode.assert_called_once_with(data.decode())
         assert packet is mocker.sentinel.packet
         assert remaining_data == b"Hello World !"
 
     def test____incremental_deserialize____translate_unicode_decode_errors(
         self,
         mock_decoder: MagicMock,
         mock_json_parser: MagicMock,
         debug_mode: bool,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         def raw_parse_side_effect(*args: Any, **kwargs: Any) -> Generator[None, bytes, tuple[bytes, bytes]]:
             data = yield
-            assert data is mocker.sentinel.data
-            return mock_bytes, mocker.sentinel.remaining_data
+            return data, mocker.sentinel.remaining_data
 
         serializer: JSONSerializer = JSONSerializer(
-            encoding=mocker.sentinel.encoding,
-            unicode_errors=mocker.sentinel.str_errors,
+            encoding="utf-8",
             use_lines=False,
             debug=debug_mode,
         )
-        mock_bytes = mocker.NonCallableMagicMock()
-        mock_bytes.decode.side_effect = UnicodeDecodeError("some encoding", b"invalid data", 0, 2, "Bad encoding ?")
+        data = "é".encode("latin-1")
         mock_json_parser.side_effect = raw_parse_side_effect
 
         # Act
         consumer = serializer.incremental_deserialize()
         next(consumer)
         with pytest.raises(IncrementalDeserializeError) as exc_info:
-            _ = consumer.send(mocker.sentinel.data)
+            _ = consumer.send(data)
         exception = exc_info.value
 
         # Assert
-        mock_bytes.decode.assert_called_once()
         mock_decoder.decode.assert_not_called()
         assert exception.remaining_data is mocker.sentinel.remaining_data
-        assert exception.__cause__ is mock_bytes.decode.side_effect
+        assert isinstance(exception.__cause__, UnicodeError)
         if debug_mode:
-            assert exception.error_info == {"data": mock_bytes}
+            assert exception.error_info == {"data": data}
         else:
             assert exception.error_info is None
 
     def test____incremental_deserialize____translate_json_decode_errors(
         self,
         mock_decoder: MagicMock,
         mock_json_parser: MagicMock,
@@ -429,42 +382,38 @@
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         from json import JSONDecodeError
 
         def raw_parse_side_effect(*args: Any, **kwargs: Any) -> Generator[None, bytes, tuple[bytes, bytes]]:
             data = yield
-            assert data is mocker.sentinel.data
-            return mock_bytes, mocker.sentinel.remaining_data
+            return data, mocker.sentinel.remaining_data
 
         serializer: JSONSerializer = JSONSerializer(
-            encoding=mocker.sentinel.encoding,
-            unicode_errors=mocker.sentinel.str_errors,
             use_lines=False,
             debug=debug_mode,
         )
-        mock_bytes = mocker.NonCallableMagicMock()
-        mock_decoder.decode.side_effect = JSONDecodeError("Invalid payload", "invalid\ndocument", 8)
+        data = b"invalid\ndocument"
+        mock_decoder.decode.side_effect = JSONDecodeError("Invalid payload", data.decode(), 8)
         mock_json_parser.side_effect = raw_parse_side_effect
 
         # Act
         consumer = serializer.incremental_deserialize()
         next(consumer)
         with pytest.raises(IncrementalDeserializeError) as exc_info:
-            _ = consumer.send(mocker.sentinel.data)
+            _ = consumer.send(data)
         exception = exc_info.value
 
         # Assert
-        mock_bytes.decode.assert_called_once()
         mock_decoder.decode.assert_called_once()
         assert exception.remaining_data is mocker.sentinel.remaining_data
         assert exception.__cause__ is mock_decoder.decode.side_effect
         if debug_mode:
             assert exception.error_info == {
-                "document": "invalid\ndocument",
+                "document": data.decode(),
                 "position": 8,
                 "lineno": 2,
                 "colno": 1,
             }
         else:
             assert exception.error_info is None
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_serializers/test_msgpack.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_serializers/test_msgpack.py`

 * *Files 14% similar despite different names*

```diff
@@ -3,14 +3,15 @@
 from typing import TYPE_CHECKING, Any, final
 
 from easynetwork.exceptions import DeserializeError
 from easynetwork.serializers.msgpack import MessagePackerConfig, MessagePackSerializer, MessageUnpackerConfig
 
 import pytest
 
+from .._utils import mock_import_module_not_found
 from .base import BaseSerializerConfigInstanceCheck
 
 if TYPE_CHECKING:
     from unittest.mock import MagicMock
 
     from pytest_mock import MockerFixture
 
@@ -197,7 +198,49 @@
 
         # Assert
         assert isinstance(exc_info.value.__cause__, Exception)
         if debug_mode:
             assert exc_info.value.error_info == {"data": mocker.sentinel.data}
         else:
             assert exc_info.value.error_info is None
+
+
+class TestMessagePackSerializerDependencies:
+    def test____dunder_init____msgpack_missing(
+        self,
+        mocker: MockerFixture,
+    ) -> None:
+        # Arrange
+        mock_import: MagicMock = mock_import_module_not_found({"msgpack"}, mocker)
+
+        # Act
+        with pytest.raises(ModuleNotFoundError) as exc_info:
+            try:
+                _ = MessagePackSerializer()
+            finally:
+                mocker.stop(mock_import)
+
+        # Assert
+        mock_import.assert_any_call("msgpack", mocker.ANY, mocker.ANY, None, 0)
+        assert exc_info.value.args[0] == "message-pack dependencies are missing. Consider adding 'msgpack' extra"
+        assert exc_info.value.__notes__ == ['example: pip install "easynetwork[msgpack]"']
+        assert isinstance(exc_info.value.__cause__, ModuleNotFoundError)
+
+    def test____MessageUnpackerConfig____msgpack_missing(
+        self,
+        mocker: MockerFixture,
+    ) -> None:
+        # Arrange
+        mock_import: MagicMock = mock_import_module_not_found({"msgpack"}, mocker)
+
+        # Act
+        with pytest.raises(ModuleNotFoundError) as exc_info:
+            try:
+                _ = MessageUnpackerConfig()
+            finally:
+                mocker.stop(mock_import)
+
+        # Assert
+        mock_import.assert_any_call("msgpack", mocker.ANY, mocker.ANY, ("ExtType",), 0)
+        assert exc_info.value.args[0] == "message-pack dependencies are missing. Consider adding 'msgpack' extra"
+        assert exc_info.value.__notes__ == ['example: pip install "easynetwork[msgpack]"']
+        assert isinstance(exc_info.value.__cause__, ModuleNotFoundError)
```

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_serializers/test_pickle.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_serializers/test_pickle.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_serializers/test_struct.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_serializers/test_struct.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,15 +1,20 @@
 from __future__ import annotations
 
 import collections
 from collections.abc import Iterable
 from typing import TYPE_CHECKING, Any, final
 
 from easynetwork.exceptions import DeserializeError
-from easynetwork.serializers.struct import _ENDIANNESS_CHARACTERS, AbstractStructSerializer, NamedTupleStructSerializer
+from easynetwork.serializers.struct import (
+    _ENDIANNESS_CHARACTERS,
+    AbstractStructSerializer,
+    NamedTupleStructSerializer,
+    StructSerializer,
+)
 
 import pytest
 
 if TYPE_CHECKING:
     from unittest.mock import MagicMock
 
     from pytest_mock import MockerFixture
@@ -146,42 +151,14 @@
         mock_serializer_from_tuple.assert_not_called()
         assert exception.__cause__ is mock_struct_unpack.side_effect
         if debug_mode:
             assert exception.error_info == {"data": mocker.sentinel.data}
         else:
             assert exception.error_info is None
 
-    def test____deserialize____translate_any_exception_raised_by_from_tuple_method(
-        self,
-        mock_struct: MagicMock,
-        mock_serializer_from_tuple: MagicMock,
-        debug_mode: bool,
-        mocker: MockerFixture,
-    ) -> None:
-        # Arrange
-        serializer = _StructSerializerForTest("format", debug=debug_mode)
-        mock_struct_unpack: MagicMock = mock_struct.unpack
-        mock_struct_unpack.return_value = mocker.sentinel.packet_tuple
-        mock_serializer_from_tuple.side_effect = Exception()
-
-        # Act
-
-        with pytest.raises(DeserializeError) as exc_info:
-            _ = serializer.deserialize(mocker.sentinel.data)
-        exception = exc_info.value
-
-        # Assert
-        mock_struct_unpack.assert_called_once_with(mocker.sentinel.data)
-        mock_serializer_from_tuple.assert_called_once_with(mocker.sentinel.packet_tuple)
-        assert exception.__cause__ is mock_serializer_from_tuple.side_effect
-        if debug_mode:
-            assert exception.error_info == {"unpacked_struct": mocker.sentinel.packet_tuple}
-        else:
-            assert exception.error_info is None
-
 
 class TestNamedTupleStructSerializer(BaseTestStructBasedSerializer):
     @pytest.mark.parametrize("method", ["serialize", "incremental_serialize", "deserialize", "incremental_deserialize"])
     def test____base_class____implements_default_methods(self, method: str) -> None:
         # Arrange
 
         # Act & Assert
@@ -421,14 +398,36 @@
         # Assert
         assert isinstance(result, namedtuple_cls)
         assert isinstance(result.x, str)
         assert isinstance(result.y, str)
         assert result.x == "1234"
         assert result.y == "56789"
 
+    def test____from_tuple____construct_namedtuple____with_strings_and_encoding____decode_error(
+        self,
+        debug_mode: bool,
+    ) -> None:
+        # Arrange
+        namedtuple_cls = collections.namedtuple("namedtuple_cls", ["data"])
+        serializer = NamedTupleStructSerializer(namedtuple_cls, {"data": "10s"}, encoding="utf-8", debug=debug_mode)
+        packet_tuple = ("é".encode("latin-1").ljust(10, b"\0"),)
+
+        # Act
+        with pytest.raises(
+            DeserializeError, match=r"^UnicodeError when building packet from unpacked struct value: .+$"
+        ) as exc_info:
+            _ = serializer.from_tuple(packet_tuple)
+
+        # Assert
+        assert isinstance(exc_info.value.__cause__, UnicodeError)
+        if debug_mode:
+            assert exc_info.value.error_info == {"unpacked_struct": packet_tuple}
+        else:
+            assert exc_info.value.error_info is None
+
     @pytest.mark.parametrize(
         "strip_string_trailing_nul_bytes",
         [True, False],
         ids=lambda boolean: f"strip_string_trailing_nul_bytes=={boolean}",
     )
     @pytest.mark.parametrize(
         "encoding",
@@ -465,7 +464,66 @@
             assert result.x == expected_x.decode(encoding)
             assert result.y == expected_y.decode(encoding)
         else:
             assert isinstance(result.x, bytes)
             assert isinstance(result.y, bytes)
             assert result.x == expected_x
             assert result.y == expected_y
+
+
+class TestStructSerializer(BaseTestStructBasedSerializer):
+    @pytest.mark.parametrize("method", ["serialize", "incremental_serialize", "deserialize", "incremental_deserialize"])
+    def test____base_class____implements_default_methods(self, method: str) -> None:
+        # Arrange
+
+        # Act & Assert
+        assert getattr(StructSerializer, method) is getattr(AbstractStructSerializer, method)
+
+    @pytest.mark.parametrize("endianness", sorted(_ENDIANNESS_CHARACTERS.union([""])), ids=repr)
+    def test____dunder_init____format_with_endianness(
+        self,
+        endianness: str,
+        mock_struct_cls: MagicMock,
+        mock_struct: MagicMock,
+    ) -> None:
+        # Arrange
+        format = "format"
+        expected_format = f"{endianness}{format}" if endianness else f"!{format}"
+
+        # Act
+        serializer = StructSerializer(f"{endianness}{format}")
+
+        # Assert
+        mock_struct_cls.assert_called_once_with(expected_format)
+        assert serializer.struct is mock_struct
+        assert serializer.packet_size == 123456789
+
+    def test____iter_values____return_given_instance(self) -> None:
+        # Arrange
+        serializer = StructSerializer("format")
+        packet = (10, b"data")
+
+        # Act
+        iterable = serializer.iter_values(packet)
+
+        # Assert
+        assert iterable is packet
+
+    def test____iter_values____error_not_tuple(self) -> None:
+        # Arrange
+        serializer = StructSerializer("format")
+        packet = [10, b"data"]
+
+        # Act & Assert
+        with pytest.raises(TypeError, match=r"^Expected a tuple instance, got \[10, b'data'\]$"):
+            _ = serializer.iter_values(packet)  # type: ignore[arg-type]
+
+    def test____from_tuple____return_given_instance(self) -> None:
+        # Arrange
+        serializer = StructSerializer("format")
+        packet_tuple = (10, b"data")
+
+        # Act
+        packet = serializer.from_tuple(packet_tuple)
+
+        # Assert
+        assert packet is packet_tuple
```

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_serializers/test_tools.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_serializers/test_tools.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_sync/test_client/base.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_sync/test_client/base.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_sync/test_client/conftest.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_sync/test_client/conftest.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_sync/test_client/test_abc.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_sync/test_client/test_abc.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 from __future__ import annotations
 
 from collections import deque
 from typing import TYPE_CHECKING, Any, final
 
-from easynetwork.api_sync.client.abc import AbstractNetworkClient
+from easynetwork.clients.abc import AbstractNetworkClient
 from easynetwork.lowlevel.socket import SocketAddress
 
 import pytest
 
 if TYPE_CHECKING:
     from pytest_mock import MockerFixture
```

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_sync/test_client/test_tcp.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_sync/test_client/test_tcp.py`

 * *Files 1% similar despite different names*

```diff
@@ -5,15 +5,15 @@
 import os
 from collections.abc import Generator
 from selectors import EVENT_READ, EVENT_WRITE
 from socket import AF_INET6, IPPROTO_TCP, SHUT_RDWR, SHUT_WR, SO_KEEPALIVE, SOL_SOCKET, TCP_NODELAY
 from ssl import SSLEOFError, SSLError, SSLErrorNumber, SSLWantReadError, SSLWantWriteError, SSLZeroReturnError
 from typing import TYPE_CHECKING, Any
 
-from easynetwork.api_sync.client.tcp import TCPNetworkClient
+from easynetwork.clients.tcp import TCPNetworkClient
 from easynetwork.exceptions import ClientClosedError, IncrementalDeserializeError, UnsupportedOperation
 from easynetwork.lowlevel._stream import StreamDataConsumer
 from easynetwork.lowlevel.constants import (
     CLOSED_SOCKET_ERRNOS,
     DEFAULT_STREAM_BUFSIZE,
     SSL_HANDSHAKE_TIMEOUT,
     SSL_SHUTDOWN_TIMEOUT,
@@ -28,19 +28,14 @@
     from pytest_mock import MockerFixture
 
 from ..._utils import DummyLock
 from ...base import UNSUPPORTED_FAMILIES, MixinTestSocketSendMSG
 from .base import BaseTestClient
 
 
-@pytest.fixture(autouse=True)
-def remove_ssl_OP_IGNORE_UNEXPECTED_EOF(monkeypatch: pytest.MonkeyPatch) -> None:
-    monkeypatch.delattr("ssl.OP_IGNORE_UNEXPECTED_EOF", raising=False)
-
-
 class TestTCPNetworkClient(BaseTestClient, MixinTestSocketSendMSG):
     @pytest.fixture(scope="class", params=["AF_INET", "AF_INET6"])
     @staticmethod
     def socket_family(request: Any) -> Any:
         import socket
 
         return getattr(socket, request.param)
@@ -191,63 +186,65 @@
     @pytest.fixture  # DO NOT set autouse=True
     @staticmethod
     def setup_consumer_mock(mock_stream_data_consumer: MagicMock, mocker: MockerFixture) -> None:
         bytes_buffer: bytes = b""
 
         sentinel = mocker.sentinel
 
-        def feed_side_effect(chunk: bytes) -> None:
-            nonlocal bytes_buffer
-            bytes_buffer += chunk
-
-        def next_side_effect() -> Any:
+        def next_side_effect(chunk: bytes | None) -> Any:
             nonlocal bytes_buffer
+            if chunk is not None:
+                bytes_buffer += chunk
             data, separator, bytes_buffer = bytes_buffer.partition(b"\n")
             if not separator:
                 assert not bytes_buffer
                 bytes_buffer = data
                 raise StopIteration
             return getattr(sentinel, data.decode("ascii"))
 
         def get_buffer_side_effect() -> memoryview:
             nonlocal bytes_buffer
 
             return memoryview(bytes_buffer)
 
-        mock_stream_data_consumer.feed.side_effect = feed_side_effect
-        mock_stream_data_consumer.__iter__.side_effect = lambda: mock_stream_data_consumer
-        mock_stream_data_consumer.__next__.side_effect = next_side_effect
+        mock_stream_data_consumer.next.side_effect = next_side_effect
         mock_stream_data_consumer.get_buffer.side_effect = get_buffer_side_effect
 
     @pytest.fixture
     @staticmethod
     def max_recv_size(request: Any) -> int | None:
         return getattr(request, "param", None)
 
     @pytest.fixture
     @staticmethod
     def ssl_shutdown_timeout(request: Any) -> float | None:
         return getattr(request, "param", None)
 
     @pytest.fixture
     @staticmethod
+    def ssl_standard_compatible(request: Any) -> bool | None:
+        return getattr(request, "param", None)
+
+    @pytest.fixture
+    @staticmethod
     def ssl_shared_lock(request: Any) -> bool | None:
         return getattr(request, "param", None)
 
     @pytest.fixture
     @staticmethod
     def retry_interval(request: Any) -> float:
         return getattr(request, "param", float("+inf"))
 
     @pytest.fixture(params=["REMOTE_ADDRESS", "EXTERNAL_SOCKET"])
     @staticmethod
     def client(
         request: Any,
         max_recv_size: int | None,
         ssl_shutdown_timeout: float | None,
+        ssl_standard_compatible: bool | None,
         retry_interval: float,
         remote_address: tuple[str, int],
         mock_tcp_socket: MagicMock,
         mock_ssl_socket: MagicMock,
         mock_stream_protocol: MagicMock,
         ssl_context: MagicMock | None,
         server_hostname: Any | None,
@@ -261,25 +258,27 @@
                     return TCPNetworkClient(
                         remote_address,
                         mock_stream_protocol,
                         ssl=ssl_context,
                         server_hostname=server_hostname,
                         max_recv_size=max_recv_size,
                         ssl_shutdown_timeout=ssl_shutdown_timeout,
+                        ssl_standard_compatible=ssl_standard_compatible,
                         ssl_shared_lock=ssl_shared_lock,
                         retry_interval=retry_interval,
                     )
                 case "EXTERNAL_SOCKET":
                     return TCPNetworkClient(
                         mock_tcp_socket,
                         mock_stream_protocol,
                         ssl=ssl_context,
                         server_hostname=server_hostname,
                         max_recv_size=max_recv_size,
                         ssl_shutdown_timeout=ssl_shutdown_timeout,
+                        ssl_standard_compatible=ssl_standard_compatible,
                         ssl_shared_lock=ssl_shared_lock,
                         retry_interval=retry_interval,
                     )
                 case invalid:
                     pytest.fail(f"Invalid fixture param: Got {invalid!r}")
         finally:
             mock_tcp_socket.reset_mock()
@@ -514,14 +513,45 @@
         assert exc_info.value.errno == enotconn_exception.errno
         mock_socket_create_connection.assert_not_called()
         mock_ssl_create_default_context.assert_not_called()
         if ssl_context:
             ssl_context.wrap_socket.assert_not_called()
         assert mock_tcp_socket.mock_calls == [mocker.call.getpeername(), mocker.call.close()]
 
+    def test____dunder_init____invalid_first_argument____invalid_object(
+        self,
+        mock_stream_protocol: MagicMock,
+        mocker: MockerFixture,
+    ) -> None:
+        # Arrange
+        invalid_object = mocker.NonCallableMagicMock(spec=object)
+
+        # Act & Assert
+        with pytest.raises(TypeError, match=r"^Invalid arguments$"):
+            _ = TCPNetworkClient(
+                invalid_object,
+                protocol=mock_stream_protocol,
+            )
+
+    def test____dunder_init____invalid_first_argument____invalid_host_port_pair(
+        self,
+        mock_stream_protocol: MagicMock,
+        mocker: MockerFixture,
+    ) -> None:
+        # Arrange
+        invalid_host = mocker.NonCallableMagicMock(spec=object)
+        invalid_port = mocker.NonCallableMagicMock(spec=object)
+
+        # Act & Assert
+        with pytest.raises(TypeError, match=r"^Invalid arguments$"):
+            _ = TCPNetworkClient(
+                (invalid_host, invalid_port),
+                protocol=mock_stream_protocol,
+            )
+
     @pytest.mark.parametrize("use_socket", [False, True], ids=lambda p: f"use_socket=={p}")
     def test____dunder_init____protocol____invalid_value(
         self,
         use_socket: bool,
         remote_address: tuple[str, int],
         mock_tcp_socket: MagicMock,
         mock_datagram_protocol: MagicMock,
@@ -591,15 +621,15 @@
             )
 
         # Assert
         assert client.max_recv_size == expected_size
 
     @pytest.mark.parametrize("max_recv_size", [0, -1, 10.4], ids=lambda p: f"max_recv_size=={p}")
     @pytest.mark.parametrize("use_socket", [False, True], ids=lambda p: f"use_socket=={p}")
-    def test____dunder_init____max_size____invalid_value(
+    def test____dunder_init____max_recv_size____invalid_value(
         self,
         max_recv_size: Any,
         use_socket: bool,
         remote_address: tuple[str, int],
         mock_tcp_socket: MagicMock,
         mock_stream_protocol: MagicMock,
         ssl_context: MagicMock | None,
@@ -622,14 +652,47 @@
                     remote_address,
                     protocol=mock_stream_protocol,
                     max_recv_size=max_recv_size,
                     ssl=ssl_context,
                     server_hostname=server_hostname,
                 )
 
+    @pytest.mark.parametrize("manual_buffer_allocation", ["unknown", ""], ids=lambda p: f"manual_buffer_allocation=={p!r}")
+    @pytest.mark.parametrize("use_socket", [False, True], ids=lambda p: f"use_socket=={p}")
+    def test____dunder_init____manual_buffer_allocation____invalid_value(
+        self,
+        manual_buffer_allocation: Any,
+        use_socket: bool,
+        remote_address: tuple[str, int],
+        mock_tcp_socket: MagicMock,
+        mock_stream_protocol: MagicMock,
+        ssl_context: MagicMock | None,
+        server_hostname: Any | None,
+    ) -> None:
+        # Arrange
+
+        # Act & Assert
+        with pytest.raises(ValueError, match=r'^"manual_buffer_allocation" must be "try", "no" or "force"$'):
+            if use_socket:
+                _ = TCPNetworkClient(
+                    mock_tcp_socket,
+                    protocol=mock_stream_protocol,
+                    ssl=ssl_context,
+                    server_hostname=server_hostname,
+                    manual_buffer_allocation=manual_buffer_allocation,
+                )
+            else:
+                _ = TCPNetworkClient(
+                    remote_address,
+                    protocol=mock_stream_protocol,
+                    ssl=ssl_context,
+                    server_hostname=server_hostname,
+                    manual_buffer_allocation=manual_buffer_allocation,
+                )
+
     @pytest.mark.parametrize("retry_interval", [0, -12.34])
     @pytest.mark.parametrize("use_socket", [False, True], ids=lambda p: f"use_socket=={p}")
     def test____dunder_init____retry_interval____invalid_value(
         self,
         retry_interval: float,
         use_socket: bool,
         remote_address: tuple[str, int],
@@ -663,14 +726,15 @@
     @pytest.mark.parametrize("use_socket", [False, True], ids=lambda p: f"use_socket=={p}")
     @pytest.mark.parametrize(
         "ssl_parameter",
         [
             "server_hostname",
             "ssl_handshake_timeout",
             "ssl_shutdown_timeout",
+            "ssl_standard_compatible",
             "ssl_shared_lock",
         ],
     )
     def test____dunder_init____ssl____useless_parameter_if_no_context(
         self,
         ssl_parameter: str,
         use_socket: bool,
@@ -941,14 +1005,45 @@
             mock_selector_register.assert_not_called()
             mock_selector_select.assert_not_called()
         else:
             assert len(mock_ssl_socket.do_handshake.call_args_list) == 2
             mock_selector_register.assert_called_once_with(socket_fileno, would_block_event)
             mock_selector_select.assert_called_once_with(expected_timeout)
 
+    @pytest.mark.parametrize("use_ssl", ["USE_SSL"], indirect=True)
+    @pytest.mark.parametrize("ssl_standard_compatible", [False, True], indirect=True, ids=lambda p: f"standard_compatible=={p}")
+    def test____dunder_init____ssl____standard_compatible(
+        self,
+        ssl_standard_compatible: bool,
+        remote_address: tuple[str, int],
+        mock_ssl_context: MagicMock,
+        mock_tcp_socket: MagicMock,
+        mock_stream_protocol: MagicMock,
+    ) -> None:
+        # Arrange
+
+        # Act
+        _ = TCPNetworkClient(
+            remote_address,
+            protocol=mock_stream_protocol,
+            ssl=mock_ssl_context,
+            server_hostname="server_hostname",
+            ssl_standard_compatible=ssl_standard_compatible,
+        )
+
+        # Assert
+        mock_ssl_context.wrap_socket.assert_called_once_with(
+            mock_tcp_socket,
+            server_side=False,
+            server_hostname="server_hostname",
+            do_handshake_on_connect=False,
+            suppress_ragged_eofs=not ssl_standard_compatible,
+            session=None,
+        )
+
     def test____close____default(
         self,
         client: TCPNetworkClient[Any, Any],
         use_ssl: bool,
         mock_used_socket: MagicMock,
         mock_ssl_socket: MagicMock,
     ) -> None:
@@ -1068,14 +1163,34 @@
 
         # Assert
         assert client.is_closed()
 
         mock_used_socket.shutdown.assert_called_once_with(SHUT_RDWR)
         mock_used_socket.close.assert_called_once_with()
 
+    @pytest.mark.parametrize("use_ssl", ["USE_SSL"], indirect=True)
+    @pytest.mark.parametrize("ssl_standard_compatible", [False], indirect=True, ids=lambda p: f"standard_compatible=={p}")
+    def test____close____ssl____do_not_call_unwrap(
+        self,
+        client: TCPNetworkClient[Any, Any],
+        mock_used_socket: MagicMock,
+        mock_ssl_socket: MagicMock,
+    ) -> None:
+        # Arrange
+        assert not client.is_closed()
+
+        # Act
+        client.close()
+
+        # Assert
+        assert client.is_closed()
+        mock_ssl_socket.unwrap.assert_not_called()
+        mock_used_socket.shutdown.assert_called_once_with(SHUT_RDWR)
+        mock_used_socket.close.assert_called_once_with()
+
     def test____get_local_address____ask_for_address(
         self,
         client: TCPNetworkClient[Any, Any],
         socket_family: int,
         local_address: tuple[str, int],
         mock_used_socket: MagicMock,
     ) -> None:
@@ -1288,15 +1403,15 @@
         mock_selector_select.assert_not_called()
         mock_stream_protocol.generate_chunks.assert_not_called()
         mock_used_socket_send.assert_not_called()
         mock_used_socket.getsockopt.assert_not_called()
 
     @pytest.mark.usefixtures("setup_producer_mock")
     @pytest.mark.parametrize("use_ssl", ["USE_SSL"], indirect=True)
-    @pytest.mark.parametrize("ssl_eof_error", [SSLEOFError, SSLZeroReturnError])
+    @pytest.mark.parametrize("ssl_eof_error", [SSLEOFError])
     def test____send_packet____ssl____eof_error(
         self,
         client: TCPNetworkClient[Any, Any],
         ssl_eof_error: type[Exception],
         mock_used_socket: MagicMock,
         mock_used_socket_send: MagicMock,
         mock_stream_protocol: MagicMock,
@@ -1499,15 +1614,15 @@
         mock_used_socket.settimeout.assert_not_called()
         mock_used_socket.setblocking.assert_not_called()
 
         mock_selector_register.assert_not_called()
         mock_selector_select.assert_not_called()
 
         mock_used_socket.recv.assert_called_once_with(DEFAULT_STREAM_BUFSIZE)
-        mock_stream_data_consumer.feed.assert_called_once_with(b"packet\n")
+        assert mock_stream_data_consumer.next.call_args_list == [mocker.call(None), mocker.call(b"packet\n")]
         assert packet is mocker.sentinel.packet
 
     @pytest.mark.parametrize("recv_timeout", [None, float("+inf"), 123456789], indirect=True)  # Do not test with timeout==0
     @pytest.mark.usefixtures("setup_consumer_mock")
     def test____recv_packet____blocking____partial_data(
         self,
         client: TCPNetworkClient[Any, Any],
@@ -1522,15 +1637,15 @@
         # Act
         packet: Any = client.recv_packet(timeout=recv_timeout)
 
         # Assert
         mock_used_socket.settimeout.assert_not_called()
         mock_used_socket.setblocking.assert_not_called()
         assert mock_used_socket.recv.call_args_list == [mocker.call(DEFAULT_STREAM_BUFSIZE) for _ in range(2)]
-        assert mock_stream_data_consumer.feed.call_args_list == [mocker.call(b"pac"), mocker.call(b"ket\n")]
+        assert mock_stream_data_consumer.next.call_args_list == [mocker.call(None), mocker.call(b"pac"), mocker.call(b"ket\n")]
         assert packet is mocker.sentinel.packet
 
     @pytest.mark.parametrize("recv_timeout", [0], indirect=True)  # Only test with timeout==0
     @pytest.mark.parametrize(
         "max_recv_size",
         [
             pytest.param(3, id="chunk_matching_bufsize"),
@@ -1552,22 +1667,27 @@
         mock_used_socket.recv.side_effect = [b"pac", b"ket", b"\n"]
 
         # Act & Assert
         if max_recv_size == 3:
             packet: Any = client.recv_packet(timeout=recv_timeout)
 
             assert mock_used_socket.recv.call_args_list == [mocker.call(max_recv_size) for _ in range(3)]
-            assert mock_stream_data_consumer.feed.call_args_list == [mocker.call(b"pac"), mocker.call(b"ket"), mocker.call(b"\n")]
+            assert mock_stream_data_consumer.next.call_args_list == [
+                mocker.call(None),
+                mocker.call(b"pac"),
+                mocker.call(b"ket"),
+                mocker.call(b"\n"),
+            ]
             assert packet is mocker.sentinel.packet
         else:
             with pytest.raises(TimeoutError):
                 client.recv_packet(timeout=recv_timeout)
 
             mock_used_socket.recv.assert_called_once_with(max_recv_size)
-            mock_stream_data_consumer.feed.assert_called_once_with(b"pac")
+            assert mock_stream_data_consumer.next.call_args_list == [mocker.call(None), mocker.call(b"pac")]
 
         mock_used_socket.settimeout.assert_not_called()
         mock_used_socket.setblocking.assert_not_called()
 
     @pytest.mark.usefixtures("setup_consumer_mock")
     def test____recv_packet____blocking_or_not____extra_data(
         self,
@@ -1584,15 +1704,19 @@
         packet_1: Any = client.recv_packet(timeout=recv_timeout)
         packet_2: Any = client.recv_packet(timeout=recv_timeout)
 
         # Assert
         mock_used_socket.recv.assert_called_once()
         mock_used_socket.settimeout.assert_not_called()
         mock_used_socket.setblocking.assert_not_called()
-        mock_stream_data_consumer.feed.assert_called_once_with(b"packet_1\npacket_2\n")
+        assert mock_stream_data_consumer.next.call_args_list == [
+            mocker.call(None),
+            mocker.call(b"packet_1\npacket_2\n"),
+            mocker.call(None),
+        ]
         assert packet_1 is mocker.sentinel.packet_1
         assert packet_2 is mocker.sentinel.packet_2
 
     @pytest.mark.usefixtures("setup_consumer_mock")
     def test____recv_packet____blocking_or_not____eof_error____default(
         self,
         client: TCPNetworkClient[Any, Any],
@@ -1607,15 +1731,15 @@
         with pytest.raises(ConnectionAbortedError):
             _ = client.recv_packet(timeout=recv_timeout)
 
         # Assert
         mock_used_socket.settimeout.assert_not_called()
         mock_used_socket.setblocking.assert_not_called()
         mock_used_socket.recv.assert_called_once_with(DEFAULT_STREAM_BUFSIZE)
-        mock_stream_data_consumer.feed.assert_not_called()
+        mock_stream_data_consumer.next.assert_called_once_with(None)
 
     @pytest.mark.usefixtures("setup_consumer_mock")
     def test____recv_packet____blocking_or_not____eof_error____convert_connection_errors(
         self,
         client: TCPNetworkClient[Any, Any],
         recv_timeout: float | None,
         mock_used_socket: MagicMock,
@@ -1628,15 +1752,15 @@
         with pytest.raises(ConnectionAbortedError):
             _ = client.recv_packet(timeout=recv_timeout)
 
         # Assert
         mock_used_socket.settimeout.assert_not_called()
         mock_used_socket.setblocking.assert_not_called()
         mock_used_socket.recv.assert_called_once_with(DEFAULT_STREAM_BUFSIZE)
-        mock_stream_data_consumer.feed.assert_not_called()
+        mock_stream_data_consumer.next.assert_called_once_with(None)
 
     @pytest.mark.usefixtures("setup_consumer_mock")
     @pytest.mark.parametrize("closed_socket_errno", sorted(CLOSED_SOCKET_ERRNOS), ids=errno.errorcode.__getitem__)
     def test____recv_packet____blocking_or_not____eof_error____convert_closed_socket_errors(
         self,
         closed_socket_errno: int,
         client: TCPNetworkClient[Any, Any],
@@ -1651,41 +1775,42 @@
         with pytest.raises(ClientClosedError):
             _ = client.recv_packet(timeout=recv_timeout)
 
         # Assert
         mock_used_socket.settimeout.assert_not_called()
         mock_used_socket.setblocking.assert_not_called()
         mock_used_socket.recv.assert_called_once_with(DEFAULT_STREAM_BUFSIZE)
-        mock_stream_data_consumer.feed.assert_not_called()
+        mock_stream_data_consumer.next.assert_called_once_with(None)
 
     @pytest.mark.usefixtures("setup_consumer_mock")
     def test____recv_packet____blocking_or_not____protocol_parse_error(
         self,
         client: TCPNetworkClient[Any, Any],
         recv_timeout: float | None,
         mock_used_socket: MagicMock,
         mock_stream_data_consumer: MagicMock,
+        mocker: MockerFixture,
     ) -> None:
         # Arrange
         from easynetwork.exceptions import StreamProtocolParseError
 
         mock_used_socket.recv.side_effect = [b"packet\n"]
         expected_error = StreamProtocolParseError(b"", IncrementalDeserializeError("Sorry", b""))
-        mock_stream_data_consumer.__next__.side_effect = [StopIteration, expected_error]
+        mock_stream_data_consumer.next.side_effect = [StopIteration, expected_error]
 
         # Act
         with pytest.raises(StreamProtocolParseError) as exc_info:
             _ = client.recv_packet(timeout=recv_timeout)
         exception = exc_info.value
 
         # Assert
         mock_used_socket.settimeout.assert_not_called()
         mock_used_socket.setblocking.assert_not_called()
         mock_used_socket.recv.assert_called_once_with(DEFAULT_STREAM_BUFSIZE)
-        mock_stream_data_consumer.feed.assert_called_once_with(b"packet\n")
+        assert mock_stream_data_consumer.next.call_args_list == [mocker.call(None), mocker.call(b"packet\n")]
         assert exception is expected_error
 
     @pytest.mark.usefixtures("setup_consumer_mock")
     def test____recv_packet____blocking_or_not____closed_client_error(
         self,
         client: TCPNetworkClient[Any, Any],
         recv_timeout: float | None,
@@ -1703,15 +1828,15 @@
             _ = client.recv_packet(timeout=recv_timeout)
 
         # Assert
         mock_used_socket.settimeout.assert_not_called()
         mock_used_socket.setblocking.assert_not_called()
         mock_selector_register.assert_not_called()
         mock_selector_select.assert_not_called()
-        mock_stream_data_consumer.feed.assert_not_called()
+        mock_stream_data_consumer.next.assert_not_called()
         mock_used_socket.recv.assert_not_called()
 
     @pytest.mark.usefixtures("setup_consumer_mock")
     @pytest.mark.parametrize("use_ssl", ["USE_SSL"], indirect=True)
     @pytest.mark.parametrize("ssl_eof_error", [SSLEOFError, SSLZeroReturnError])
     def test____recv_packet____blocking_or_not____ssl____eof_error(
         self,
@@ -1728,15 +1853,15 @@
         with pytest.raises(ConnectionAbortedError):
             _ = client.recv_packet(timeout=recv_timeout)
 
         # Assert
         mock_used_socket.settimeout.assert_not_called()
         mock_used_socket.setblocking.assert_not_called()
         mock_used_socket.recv.assert_called_once_with(DEFAULT_STREAM_BUFSIZE)
-        mock_stream_data_consumer.feed.assert_not_called()
+        mock_stream_data_consumer.next.assert_called_once_with(None)
 
     @pytest.mark.usefixtures("setup_consumer_mock")
     @pytest.mark.parametrize("use_ssl", ["USE_SSL"], indirect=True)
     def test____recv_packet____blocking_or_not____ssl____unrelated_ssl_error(
         self,
         client: TCPNetworkClient[Any, Any],
         recv_timeout: float | None,
@@ -1751,15 +1876,15 @@
             _ = client.recv_packet(timeout=recv_timeout)
 
         # Assert
         assert exc_info.value is mock_used_socket.recv.side_effect
         mock_used_socket.settimeout.assert_not_called()
         mock_used_socket.setblocking.assert_not_called()
         mock_used_socket.recv.assert_called_once_with(DEFAULT_STREAM_BUFSIZE)
-        mock_stream_data_consumer.feed.assert_not_called()
+        mock_stream_data_consumer.next.assert_called_once_with(None)
 
     @pytest.mark.parametrize(
         "recv_timeout",
         [
             pytest.param(0, id="null timeout"),
             pytest.param(123456789, id="strictly positive timeout"),
         ],
@@ -1789,15 +1914,15 @@
             assert len(mock_used_socket.recv.call_args_list) == 1
             mock_selector_register.assert_not_called()
             mock_selector_select.assert_not_called()
         else:
             assert len(mock_used_socket.recv.call_args_list) == 2
             mock_selector_register.assert_called_with(socket_fileno, EVENT_READ)
             mock_selector_select.assert_any_call(recv_timeout)
-        mock_stream_data_consumer.feed.assert_not_called()
+        mock_stream_data_consumer.next.assert_called_once_with(None)
 
     @pytest.mark.parametrize(
         "recv_timeout",
         [
             pytest.param(0, id="null timeout"),
             pytest.param(123456789, id="strictly positive timeout"),
         ],
@@ -1828,21 +1953,24 @@
         self.selector_timeout_after_n_calls(mock_selector_select, mocker, nb_calls=0)
 
         # Act
         packets = list(client.iter_received_packets(timeout=recv_timeout))
 
         # Assert
         assert mock_used_socket.recv.call_args_list == [mocker.call(max_recv_size) for _ in range(7)]
-        assert mock_stream_data_consumer.feed.call_args_list == [
+        assert mock_stream_data_consumer.next.call_args_list == [
+            mocker.call(None),
             mocker.call(b"pac"),
             mocker.call(b"ket"),
             mocker.call(b"_1\np"),
+            mocker.call(None),
             mocker.call(b"ack"),
             mocker.call(b"et_"),
             mocker.call(b"2\n"),
+            mocker.call(None),
         ]
         assert packets == [mocker.sentinel.packet_1, mocker.sentinel.packet_2]
 
     @pytest.mark.parametrize("max_recv_size", [3], indirect=True)  # Needed for timeout==0
     @pytest.mark.usefixtures("setup_consumer_mock")
     def test____iter_received_packets____yields_available_packets_until_eof(
         self,
@@ -1857,21 +1985,24 @@
         mock_used_socket.recv.side_effect = [b"pac", b"ket", b"_1\np", b"ack", b"et_", b"2\n", b""]
 
         # Act
         packets = list(client.iter_received_packets(timeout=recv_timeout))
 
         # Assert
         assert mock_used_socket.recv.call_args_list == [mocker.call(max_recv_size) for _ in range(7)]
-        assert mock_stream_data_consumer.feed.call_args_list == [
+        assert mock_stream_data_consumer.next.call_args_list == [
+            mocker.call(None),
             mocker.call(b"pac"),
             mocker.call(b"ket"),
             mocker.call(b"_1\np"),
+            mocker.call(None),
             mocker.call(b"ack"),
             mocker.call(b"et_"),
             mocker.call(b"2\n"),
+            mocker.call(None),
         ]
         assert packets == [mocker.sentinel.packet_1, mocker.sentinel.packet_2]
 
     @pytest.mark.parametrize("max_recv_size", [3], indirect=True)  # Needed for timeout==0
     @pytest.mark.usefixtures("setup_consumer_mock")
     def test____iter_received_packets____yields_available_packets_until_error(
         self,
@@ -1886,43 +2017,46 @@
         mock_used_socket.recv.side_effect = [b"pac", b"ket", b"_1\np", b"ack", b"et_", b"2\n", OSError]
 
         # Act
         packets = list(client.iter_received_packets(timeout=recv_timeout))
 
         # Assert
         assert mock_used_socket.recv.call_args_list == [mocker.call(max_recv_size) for _ in range(7)]
-        assert mock_stream_data_consumer.feed.call_args_list == [
+        assert mock_stream_data_consumer.next.call_args_list == [
+            mocker.call(None),
             mocker.call(b"pac"),
             mocker.call(b"ket"),
             mocker.call(b"_1\np"),
+            mocker.call(None),
             mocker.call(b"ack"),
             mocker.call(b"et_"),
             mocker.call(b"2\n"),
+            mocker.call(None),
         ]
         assert packets == [mocker.sentinel.packet_1, mocker.sentinel.packet_2]
 
     @pytest.mark.usefixtures("setup_consumer_mock")
     def test____iter_received_packets____protocol_parse_error(
         self,
         client: TCPNetworkClient[Any, Any],
         recv_timeout: float | None,
         mock_stream_data_consumer: MagicMock,
     ) -> None:
         # Arrange
         from easynetwork.exceptions import StreamProtocolParseError
 
-        mock_stream_data_consumer.__next__.side_effect = StreamProtocolParseError(b"", IncrementalDeserializeError("Sorry", b""))
+        mock_stream_data_consumer.next.side_effect = StreamProtocolParseError(b"", IncrementalDeserializeError("Sorry", b""))
 
         # Act
         with pytest.raises(StreamProtocolParseError) as exc_info:
             _ = next(client.iter_received_packets(timeout=recv_timeout))
         exception = exc_info.value
 
         # Assert
-        assert exception is mock_stream_data_consumer.__next__.side_effect
+        assert exception is mock_stream_data_consumer.next.side_effect
 
     @pytest.mark.parametrize("several_generators", [False, True], ids=lambda t: f"several_generators=={t}")
     @pytest.mark.usefixtures("setup_consumer_mock")
     def test____iter_received_packets____avoid_unnecessary_socket_recv_call(
         self,
         client: TCPNetworkClient[Any, Any],
         several_generators: bool,
@@ -1941,15 +2075,19 @@
         else:
             iterator = client.iter_received_packets(timeout=recv_timeout)
             packet_1 = next(iterator)
             packet_2 = next(iterator)
 
         # Assert
         mock_used_socket.recv.assert_called_once_with(DEFAULT_STREAM_BUFSIZE)
-        mock_stream_data_consumer.feed.assert_called_once_with(b"packet_1\npacket_2\n")
+        assert mock_stream_data_consumer.next.call_args_list == [
+            mocker.call(None),
+            mocker.call(b"packet_1\npacket_2\n"),
+            mocker.call(None),
+        ]
         assert packet_1 is mocker.sentinel.packet_1
         assert packet_2 is mocker.sentinel.packet_2
 
     @pytest.mark.usefixtures("setup_consumer_mock")
     def test____iter_received_packets____closed_client_during_iteration(
         self,
         client: TCPNetworkClient[Any, Any],
```

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_sync/test_client/test_udp.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_sync/test_client/test_udp.py`

 * *Files 2% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 import errno
 import os
 from collections.abc import Callable, Sequence
 from selectors import EVENT_READ, EVENT_WRITE
 from socket import AF_INET, AF_INET6, AF_UNSPEC, AI_PASSIVE, IPPROTO_UDP, SO_ERROR, SOCK_DGRAM, SOL_SOCKET, AddressFamily
 from typing import TYPE_CHECKING, Any, Literal, assert_never
 
-from easynetwork.api_sync.client.udp import UDPNetworkClient, _create_udp_socket as create_udp_socket
+from easynetwork.clients.udp import UDPNetworkClient, _create_udp_socket as create_udp_socket
 from easynetwork.exceptions import ClientClosedError, DatagramProtocolParseError, DeserializeError
 from easynetwork.lowlevel._utils import error_from_errno
 from easynetwork.lowlevel.constants import CLOSED_SOCKET_ERRNOS, MAX_DATAGRAM_BUFSIZE
 from easynetwork.lowlevel.socket import IPv4SocketAddress, IPv6SocketAddress
 
 import pytest
 
@@ -337,14 +337,45 @@
         # Act & Assert
         with pytest.raises(ValueError, match=r"^A 'SOCK_DGRAM' socket is expected$"):
             _ = UDPNetworkClient(
                 mock_tcp_socket,
                 protocol=mock_datagram_protocol,
             )
 
+    def test____dunder_init____invalid_first_argument____invalid_object(
+        self,
+        mock_datagram_protocol: MagicMock,
+        mocker: MockerFixture,
+    ) -> None:
+        # Arrange
+        invalid_object = mocker.NonCallableMagicMock(spec=object)
+
+        # Act & Assert
+        with pytest.raises(TypeError, match=r"^Invalid arguments$"):
+            _ = UDPNetworkClient(
+                invalid_object,
+                protocol=mock_datagram_protocol,
+            )
+
+    def test____dunder_init____invalid_first_argument____invalid_host_port_pair(
+        self,
+        mock_datagram_protocol: MagicMock,
+        mocker: MockerFixture,
+    ) -> None:
+        # Arrange
+        invalid_host = mocker.NonCallableMagicMock(spec=object)
+        invalid_port = mocker.NonCallableMagicMock(spec=object)
+
+        # Act & Assert
+        with pytest.raises(TypeError, match=r"^Invalid arguments$"):
+            _ = UDPNetworkClient(
+                (invalid_host, invalid_port),
+                protocol=mock_datagram_protocol,
+            )
+
     def test____dunder_init____protocol____invalid_value(
         self,
         mock_udp_socket: MagicMock,
         mock_stream_protocol: MagicMock,
     ) -> None:
         # Arrange
 
@@ -845,17 +876,17 @@
 
     @pytest.fixture(autouse=True)
     @staticmethod
     def mock_getaddrinfo(socket_families: Sequence[int], mocker: MockerFixture) -> MagicMock:
         return mocker.patch(
             "socket.getaddrinfo",
             autospec=True,
-            side_effect=lambda host, port, *args, **kwargs: datagram_addrinfo_list(port, socket_families)
-            if socket_families
-            else [],
+            side_effect=lambda host, port, *args, **kwargs: (
+                datagram_addrinfo_list(port, socket_families) if socket_families else []
+            ),
         )
 
     @pytest.fixture
     @staticmethod
     def mock_socket_ipv4(mock_udp_socket_factory: Callable[[], MagicMock]) -> MagicMock:
         return mock_udp_socket_factory()
```

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_sync/test_lowlevel_api/test_endpoints/test_datagram.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_sync/test_lowlevel_api/test_endpoints/test_datagram.py`

 * *Files 3% similar despite different names*

```diff
@@ -6,48 +6,37 @@
 
 from easynetwork.exceptions import DatagramProtocolParseError, DeserializeError, UnsupportedOperation
 from easynetwork.lowlevel.api_sync.endpoints.datagram import DatagramEndpoint
 from easynetwork.lowlevel.api_sync.transports.abc import DatagramReadTransport, DatagramTransport, DatagramWriteTransport
 
 import pytest
 
+from ....base import BaseTestWithDatagramProtocol
+
 if TYPE_CHECKING:
     from unittest.mock import MagicMock
 
     from pytest_mock import MockerFixture
 
 
-class TestDatagramEndpoint:
+class TestDatagramEndpoint(BaseTestWithDatagramProtocol):
     @pytest.fixture(params=[DatagramReadTransport, DatagramWriteTransport, DatagramTransport])
     @staticmethod
     def mock_datagram_transport(request: pytest.FixtureRequest, mocker: MockerFixture) -> MagicMock:
         mock_datagram_transport = mocker.NonCallableMagicMock(spec=request.param)
         mock_datagram_transport.is_closed.return_value = False
 
         def close_side_effect() -> None:
             mock_datagram_transport.is_closed.return_value = True
 
         mock_datagram_transport.close.side_effect = close_side_effect
         return mock_datagram_transport
 
     @pytest.fixture
     @staticmethod
-    def mock_datagram_protocol(mock_datagram_protocol: MagicMock, mocker: MockerFixture) -> MagicMock:
-        def make_datagram_side_effect(packet: Any) -> bytes:
-            return str(packet).encode("ascii").removeprefix(b"sentinel.")
-
-        def build_packet_from_datagram_side_effect(data: bytes) -> Any:
-            return getattr(mocker.sentinel, data.decode("ascii"))
-
-        mock_datagram_protocol.make_datagram.side_effect = make_datagram_side_effect
-        mock_datagram_protocol.build_packet_from_datagram.side_effect = build_packet_from_datagram_side_effect
-        return mock_datagram_protocol
-
-    @pytest.fixture
-    @staticmethod
     def endpoint(mock_datagram_transport: MagicMock, mock_datagram_protocol: MagicMock) -> DatagramEndpoint[Any, Any]:
         return DatagramEndpoint(mock_datagram_transport, mock_datagram_protocol)
 
     @pytest.fixture(
         params=[
             pytest.param(None, id="blocking (None)"),
             pytest.param(math.inf, id="blocking (+inf)"),
@@ -133,15 +122,15 @@
 
         # Act
         endpoint.close()
 
         # Assert
         mock_datagram_transport.close.assert_called_once_with()
 
-    def test____get_extra_info____default(
+    def test____extra_attributes____default(
         self,
         endpoint: DatagramEndpoint[Any, Any],
         mock_datagram_transport: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         mock_datagram_transport.extra_attributes = {mocker.sentinel.name: lambda: mocker.sentinel.extra_info}
```

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_sync/test_lowlevel_api/test_transports/test_abc.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_sync/test_lowlevel_api/test_transports/test_abc.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_sync/test_lowlevel_api/test_transports/test_selector.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_sync/test_lowlevel_api/test_transports/test_selector.py`

 * *Files 2% similar despite different names*

```diff
@@ -137,21 +137,22 @@
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         callback = mocker.stub()
         callback.side_effect = [blocking_error(mocker.sentinel.fd), mocker.sentinel.result]
 
         # Act
-        result = SelectorBaseTransport._retry(mock_transport, callback, math.inf)
+        result, timeout = SelectorBaseTransport._retry(mock_transport, callback, math.inf)
 
         # Assert
         assert callback.call_args_list == [mocker.call() for _ in range(2)]
         mock_selector_register.assert_called_once_with(mocker.sentinel.fd, selector_event)
         mock_selector_select.assert_called_once_with()
         assert result is mocker.sentinel.result
+        assert timeout == math.inf
 
     @pytest.mark.parametrize(
         ["blocking_error", "selector_event"],
         [
             pytest.param(WouldBlockOnRead, EVENT_READ),
             pytest.param(WouldBlockOnWrite, EVENT_WRITE),
         ],
@@ -263,26 +264,29 @@
         mock_time_perfcounter.side_effect = [
             now,
             now + timeout,
         ]
 
         # Act
         result = None
+        reduced_timeout = None
         with pytest.raises(TimeoutError) if not available else contextlib.nullcontext():
-            result = SelectorBaseTransport._retry(mock_transport, callback, timeout)
+            result, reduced_timeout = SelectorBaseTransport._retry(mock_transport, callback, timeout)
 
         # Assert
         mock_selector_register.assert_called_once_with(mocker.sentinel.fd, selector_event)
         mock_selector_select.assert_called_once_with(timeout)
         if available:
             assert callback.call_args_list == [mocker.call() for _ in range(2)]
             assert result is mocker.sentinel.result
+            assert reduced_timeout == 0.0
         else:
             callback.assert_called_once_with()
             assert result is None
+            assert reduced_timeout is None
 
     @pytest.mark.parametrize(
         ["blocking_error", "selector_event"],
         [
             pytest.param(WouldBlockOnRead, EVENT_READ),
             pytest.param(WouldBlockOnWrite, EVENT_WRITE),
         ],
@@ -321,26 +325,29 @@
             now + retry_interval * 2,
             now + retry_interval * 2,
             now + retry_interval * 2 + 0.5,
         ]
 
         # Act
         result = None
+        reduced_timeout = None
         with pytest.raises(TimeoutError) if not available else contextlib.nullcontext():
-            result = SelectorBaseTransport._retry(mock_transport, callback, timeout)
+            result, reduced_timeout = SelectorBaseTransport._retry(mock_transport, callback, timeout)
 
         # Assert
         assert mock_selector_register.call_args_list == [mocker.call(mocker.sentinel.fd, selector_event) for _ in range(3)]
         assert mock_selector_select.call_args_list == [mocker.call(retry_interval) for _ in range(2)] + [mocker.call(0.5)]
         if available:
             assert callback.call_args_list == [mocker.call() for _ in range(4)]
             assert result is mocker.sentinel.result
+            assert reduced_timeout == 0.0
         else:
             assert callback.call_args_list == [mocker.call() for _ in range(3)]
             assert result is None
+            assert reduced_timeout is None
 
 
 def _retry_side_effect(callback: Callable[[], Any], timeout: float) -> Any:
     while True:
         try:
             return callback()
         except (WouldBlockOnRead, WouldBlockOnWrite):
@@ -359,15 +366,15 @@
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         mock_transport._retry.side_effect = _retry_side_effect
         mock_transport.recv_noblock.side_effect = [
             WouldBlockOnRead(mocker.sentinel.fd),
             WouldBlockOnWrite(mocker.sentinel.fd),
-            mocker.sentinel.bytes,
+            (mocker.sentinel.bytes, mocker.sentinel.timeout),
         ]
 
         # Act
         data = SelectorStreamTransport.recv(mock_transport, mocker.sentinel.bufsize, mocker.sentinel.timeout)
 
         # Assert
         mock_transport._retry.assert_called_once_with(mocker.ANY, mocker.sentinel.timeout)
@@ -380,15 +387,15 @@
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         mock_transport._retry.side_effect = _retry_side_effect
         mock_transport.send_noblock.side_effect = [
             WouldBlockOnRead(mocker.sentinel.fd),
             WouldBlockOnWrite(mocker.sentinel.fd),
-            mocker.sentinel.nb_sent_bytes,
+            (mocker.sentinel.nb_sent_bytes, mocker.sentinel.timeout),
         ]
 
         # Act
         sent = SelectorStreamTransport.send(mock_transport, mocker.sentinel.data, mocker.sentinel.timeout)
 
         # Assert
         mock_transport._retry.assert_called_once_with(mocker.ANY, mocker.sentinel.timeout)
@@ -408,15 +415,15 @@
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         mock_transport._retry.side_effect = _retry_side_effect
         mock_transport.recv_noblock_into.side_effect = [
             WouldBlockOnRead(mocker.sentinel.fd),
             WouldBlockOnWrite(mocker.sentinel.fd),
-            mocker.sentinel.nb_bytes_written,
+            (mocker.sentinel.nb_bytes_written, mocker.sentinel.timeout),
         ]
 
         # Act
         nbytes = SelectorBufferedStreamReadTransport.recv_into(mock_transport, mocker.sentinel.buffer, mocker.sentinel.timeout)
 
         # Assert
         mock_transport._retry.assert_called_once_with(mocker.ANY, mocker.sentinel.timeout)
@@ -436,15 +443,15 @@
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         mock_transport._retry.side_effect = _retry_side_effect
         mock_transport.recv_noblock.side_effect = [
             WouldBlockOnRead(mocker.sentinel.fd),
             WouldBlockOnWrite(mocker.sentinel.fd),
-            mocker.sentinel.bytes,
+            (mocker.sentinel.bytes, mocker.sentinel.timeout),
         ]
 
         # Act
         data = SelectorDatagramTransport.recv(mock_transport, mocker.sentinel.timeout)
 
         # Assert
         mock_transport._retry.assert_called_once_with(mocker.ANY, mocker.sentinel.timeout)
@@ -457,15 +464,15 @@
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         mock_transport._retry.side_effect = _retry_side_effect
         mock_transport.send_noblock.side_effect = [
             WouldBlockOnRead(mocker.sentinel.fd),
             WouldBlockOnWrite(mocker.sentinel.fd),
-            None,
+            (None, mocker.sentinel.timeout),
         ]
 
         # Act
         SelectorDatagramTransport.send(mock_transport, mocker.sentinel.data, mocker.sentinel.timeout)
 
         # Assert
         mock_transport._retry.assert_called_once_with(mocker.ANY, mocker.sentinel.timeout)
```

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_sync/test_lowlevel_api/test_transports/test_socket.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_sync/test_lowlevel_api/test_transports/test_socket.py`

 * *Files 2% similar despite different names*

```diff
@@ -28,18 +28,18 @@
 if TYPE_CHECKING:
     from unittest.mock import MagicMock
 
     from _typeshed import ReadableBuffer
     from pytest_mock import MockerFixture
 
 
-def _retry_side_effect(callback: Callable[[], Any], timeout: float) -> Any:
+def _retry_side_effect(callback: Callable[[], Any], timeout: float) -> tuple[Any, float]:
     while True:
         try:
-            return callback()
+            return callback(), timeout
         except (WouldBlockOnRead, WouldBlockOnWrite):
             pass
 
 
 class TestSocketStreamTransport(MixinTestSocketSendMSG):
     @pytest.fixture(autouse=True)
     @staticmethod
@@ -509,14 +509,38 @@
 
         # Act & Assert
         with pytest.raises(TypedAttributeLookupError):
             transport.extra(extra_attribute)
         mock_get_address.assert_called_once()
         assert transport.extra(extra_attribute, mocker.sentinel.default_value) is mocker.sentinel.default_value
 
+    @pytest.mark.parametrize(
+        ["extra_attribute", "called_socket_method"],
+        [
+            pytest.param(SocketAttribute.sockname, "getsockname", id="socket.getsockname()"),
+            pytest.param(SocketAttribute.peername, "getpeername", id="socket.getpeername()"),
+        ],
+    )
+    def test____extra_attributes____address_lookup_on_closed_socket(
+        self,
+        extra_attribute: Any,
+        called_socket_method: str,
+        transport: SocketStreamTransport,
+        mock_tcp_socket: MagicMock,
+    ) -> None:
+        # Arrange
+        mock_get_address: MagicMock = getattr(mock_tcp_socket, called_socket_method)
+        transport.close()
+        assert mock_tcp_socket.fileno.return_value == -1
+
+        # Act & Assert
+        with pytest.raises(TypedAttributeLookupError):
+            transport.extra(extra_attribute)
+        mock_get_address.assert_not_called()
+
 
 class TestSSLStreamTransport:
     @pytest.fixture(autouse=True)
     @staticmethod
     def mock_transport_retry(mocker: MockerFixture) -> MagicMock:
         mock_transport_retry = mocker.patch.object(SSLStreamTransport, "_retry")
         mock_transport_retry.side_effect = _retry_side_effect
@@ -836,14 +860,31 @@
 
         # Assert
         mock_ssl_socket.recv.assert_called_once_with(mocker.sentinel.bufsize)
         mock_ssl_socket.fileno.assert_called_once()
         assert isinstance(exc_info.value, (WouldBlockOnRead, WouldBlockOnWrite))
         assert exc_info.value.fileno is mock_ssl_socket.fileno.return_value
 
+    def test____recv_noblock____SSLZeroReturnError(
+        self,
+        transport: SSLStreamTransport,
+        mock_ssl_socket: MagicMock,
+        mocker: MockerFixture,
+    ) -> None:
+        # Arrange
+        mock_ssl_socket.recv.side_effect = ssl.SSLZeroReturnError
+
+        # Act
+        result = transport.recv_noblock(mocker.sentinel.bufsize)
+
+        # Assert
+        mock_ssl_socket.recv.assert_called_once_with(mocker.sentinel.bufsize)
+        mock_ssl_socket.fileno.assert_not_called()
+        assert result == b""
+
     def test____recv_noblock_into____default(
         self,
         transport: SSLStreamTransport,
         mock_ssl_socket: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
@@ -882,14 +923,31 @@
 
         # Assert
         mock_ssl_socket.recv_into.assert_called_once_with(mocker.sentinel.buffer)
         mock_ssl_socket.fileno.assert_called_once()
         assert isinstance(exc_info.value, (WouldBlockOnRead, WouldBlockOnWrite))
         assert exc_info.value.fileno is mock_ssl_socket.fileno.return_value
 
+    def test____recv_noblock_into____SSLZeroReturnError(
+        self,
+        transport: SSLStreamTransport,
+        mock_ssl_socket: MagicMock,
+        mocker: MockerFixture,
+    ) -> None:
+        # Arrange
+        mock_ssl_socket.recv_into.side_effect = ssl.SSLZeroReturnError
+
+        # Act
+        result = transport.recv_noblock_into(mocker.sentinel.buffer)
+
+        # Assert
+        mock_ssl_socket.recv_into.assert_called_once_with(mocker.sentinel.buffer)
+        mock_ssl_socket.fileno.assert_not_called()
+        assert result == 0
+
     def test____send_noblock____default(
         self,
         transport: SSLStreamTransport,
         mock_ssl_socket: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
@@ -928,14 +986,32 @@
 
         # Assert
         mock_ssl_socket.send.assert_called_once_with(mocker.sentinel.data)
         mock_ssl_socket.fileno.assert_called_once()
         assert isinstance(exc_info.value, (WouldBlockOnRead, WouldBlockOnWrite))
         assert exc_info.value.fileno is mock_ssl_socket.fileno.return_value
 
+    def test____send_noblock____SSLZeroReturnError(
+        self,
+        transport: SSLStreamTransport,
+        mock_ssl_socket: MagicMock,
+        mocker: MockerFixture,
+    ) -> None:
+        # Arrange
+        mock_ssl_socket.send.side_effect = ssl.SSLZeroReturnError
+
+        # Act
+        with pytest.raises(ConnectionError) as exc_info:
+            transport.send_noblock(mocker.sentinel.data)
+
+        # Assert
+        mock_ssl_socket.send.assert_called_once_with(mocker.sentinel.data)
+        mock_ssl_socket.fileno.assert_not_called()
+        assert exc_info.value.errno == errno.ECONNRESET
+
     def test____send_eof____default(
         self,
         transport: SSLStreamTransport,
         mock_ssl_socket: MagicMock,
     ) -> None:
         # Arrange
 
@@ -971,14 +1047,38 @@
             transport.extra(extra_attribute)
         mock_get_address.assert_called_once()
         assert transport.extra(extra_attribute, mocker.sentinel.default_value) is mocker.sentinel.default_value
 
     @pytest.mark.parametrize(
         ["extra_attribute", "called_socket_method"],
         [
+            pytest.param(SocketAttribute.sockname, "getsockname", id="socket.getsockname()"),
+            pytest.param(SocketAttribute.peername, "getpeername", id="socket.getpeername()"),
+        ],
+    )
+    def test____extra_attributes____address_lookup_on_closed_socket(
+        self,
+        extra_attribute: Any,
+        called_socket_method: str,
+        transport: SSLStreamTransport,
+        mock_ssl_socket: MagicMock,
+    ) -> None:
+        # Arrange
+        mock_get_address: MagicMock = getattr(mock_ssl_socket, called_socket_method)
+        transport.close()
+        assert mock_ssl_socket.fileno.return_value == -1
+
+        # Act & Assert
+        with pytest.raises(TypedAttributeLookupError):
+            transport.extra(extra_attribute)
+        mock_get_address.assert_not_called()
+
+    @pytest.mark.parametrize(
+        ["extra_attribute", "called_socket_method"],
+        [
             pytest.param(TLSAttribute.peercert, "getpeercert", id="socket.getpeercert()"),
             pytest.param(TLSAttribute.cipher, "cipher", id="socket.cipher()"),
             pytest.param(TLSAttribute.compression, "compression", id="socket.compression()"),
             pytest.param(TLSAttribute.tls_version, "version", id="socket.version()"),
         ],
     )
     def test____extra_attributes____ssl_object_values_not_available(
@@ -1245,7 +1345,31 @@
         mock_get_address.side_effect = OSError(os_error, os.strerror(os_error))
 
         # Act & Assert
         with pytest.raises(TypedAttributeLookupError):
             transport.extra(extra_attribute)
         mock_get_address.assert_called_once()
         assert transport.extra(extra_attribute, mocker.sentinel.default_value) is mocker.sentinel.default_value
+
+    @pytest.mark.parametrize(
+        ["extra_attribute", "called_socket_method"],
+        [
+            pytest.param(SocketAttribute.sockname, "getsockname", id="socket.getsockname()"),
+            pytest.param(SocketAttribute.peername, "getpeername", id="socket.getpeername()"),
+        ],
+    )
+    def test____extra_attributes____address_lookup_on_closed_socket(
+        self,
+        extra_attribute: Any,
+        called_socket_method: str,
+        transport: SocketDatagramTransport,
+        mock_udp_socket: MagicMock,
+    ) -> None:
+        # Arrange
+        mock_get_address: MagicMock = getattr(mock_udp_socket, called_socket_method)
+        transport.close()
+        assert mock_udp_socket.fileno.return_value == -1
+
+        # Act & Assert
+        with pytest.raises(TypedAttributeLookupError):
+            transport.extra(extra_attribute)
+        mock_get_address.assert_not_called()
```

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_tools/test_asyncgen.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_tools/test_asyncgen.py`

 * *Files 13% similar despite different names*

```diff
@@ -24,43 +24,57 @@
         mock_generator.athrow.return_value = None
         mock_generator.aclose.return_value = None
         return mock_generator
 
     async def test____SendAction____send_value(self, mock_generator: MagicMock, mocker: MockerFixture) -> None:
         # Arrange
         mock_generator.asend.return_value = mocker.sentinel.to_yield_from_send
-        action: SendAction[Any, Any] = SendAction(mocker.sentinel.value)
+        action: SendAction[Any] = SendAction(mocker.sentinel.value)
 
         # Act
-        to_yield = await action.asend(mock_generator)
+        to_yield: Any = await action.asend(mock_generator)
 
         # Assert
         assert mock_generator.mock_calls == [mocker.call.asend(mocker.sentinel.value)]
         mock_generator.asend.assert_awaited_once_with(mocker.sentinel.value)
         assert to_yield is mocker.sentinel.to_yield_from_send
 
     async def test____ThrowAction____throw_exception(self, mock_generator: MagicMock, mocker: MockerFixture) -> None:
         # Arrange
         exc = ValueError("abc")
         mock_generator.athrow.return_value = mocker.sentinel.to_yield_from_throw
-        action: ThrowAction[Any] = ThrowAction(exc)
+        action: ThrowAction = ThrowAction(exc)
 
         # Act
-        to_yield = await action.asend(mock_generator)
+        to_yield: Any = await action.asend(mock_generator)
 
         # Assert
         assert mock_generator.mock_calls == [mocker.call.athrow(exc)]
         mock_generator.athrow.assert_awaited_once_with(exc)
         assert to_yield is mocker.sentinel.to_yield_from_throw
 
+    async def test____ThrowAction____close_generator(self, mock_generator: MagicMock, mocker: MockerFixture) -> None:
+        # Arrange
+        exc = GeneratorExit()
+        action: ThrowAction = ThrowAction(exc)
+
+        # Act
+        with pytest.raises(GeneratorExit) as exc_info:
+            await action.asend(mock_generator)
+
+        # Assert
+        assert exc_info.value is exc
+        assert mock_generator.mock_calls == [mocker.call.aclose()]
+        mock_generator.aclose.assert_awaited_once_with()
+
     # Test taken from "outcome" project (https://github.com/python-trio/outcome)
-    async def test____ErrorAction____does_not_create_reference_cycles(self, mock_generator: MagicMock) -> None:
+    async def test____ThrowAction____does_not_create_reference_cycles(self, mock_generator: MagicMock) -> None:
         # Arrange
         exc = ValueError("abc")
-        action: ThrowAction[Any] = ThrowAction(exc)
+        action: ThrowAction = ThrowAction(exc)
         mock_generator.athrow.side_effect = exc
 
         # Act
         with pytest.raises(ValueError):
             await action.asend(mock_generator)
 
         # Assert
```

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_tools/test_lock.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_tools/test_lock.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_tools/test_socket.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_tools/test_socket.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_tools/test_stream.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_tools/test_stream.py`

 * *Files 7% similar despite different names*

```diff
@@ -27,175 +27,37 @@
         # Arrange
         from easynetwork.protocol import DatagramProtocol
 
         # Act & Assert
         with pytest.raises(TypeError, match=r"^Expected a StreamProtocol object, got .*$"):
             _ = StreamDataProducer(DatagramProtocol(mock_serializer))  # type: ignore[arg-type]
 
-    def test____dunder_iter____return_self(self, producer: StreamDataProducer[Any]) -> None:
-        # Arrange
-
-        # Act
-        iterator = iter(producer)
-
-        # Assert
-        assert iterator is producer
-
-    def test____next____no_packets(
-        self,
-        producer: StreamDataProducer[Any],
-        mock_stream_protocol: MagicMock,
-    ) -> None:
-        # Arrange
-        mock_generate_chunks_func: MagicMock = mock_stream_protocol.generate_chunks
-
-        # Act
-        with pytest.raises(StopIteration):
-            _ = next(producer)
-
-        # Assert
-        mock_generate_chunks_func.assert_not_called()
-
-    def test____next____return_one_generator_chunk(
-        self,
-        producer: StreamDataProducer[Any],
-        mock_stream_protocol: MagicMock,
-        mocker: MockerFixture,
-    ) -> None:
-        # Arrange
-        def side_effect(_: Any) -> Generator[bytes, None, None]:
-            yield b"chunk"
-            yield b"never yielded"
-
-        mock_generate_chunks_func: MagicMock = mock_stream_protocol.generate_chunks
-        mock_generate_chunks_func.side_effect = side_effect
-        producer.enqueue(mocker.sentinel.packet)
-
-        # Act
-        chunk: bytes = next(producer)
-
-        # Assert
-        mock_generate_chunks_func.assert_called_once_with(mocker.sentinel.packet)
-        assert chunk == b"chunk"
-
-    def test____next____reuse_generator(
-        self,
-        producer: StreamDataProducer[Any],
-        mock_stream_protocol: MagicMock,
-        mocker: MockerFixture,
-    ) -> None:
-        # Arrange
-        def side_effect(_: Any) -> Generator[bytes, None, None]:
-            yield b"chunk"
-            yield b"2nd chunk"
-
-        mock_generate_chunks_func: MagicMock = mock_stream_protocol.generate_chunks
-        mock_generate_chunks_func.side_effect = side_effect
-        producer.enqueue(mocker.sentinel.packet)
-
-        # Act
-        chunk: bytes = next(producer)
-        second_chunk: bytes = next(producer)
-
-        # Assert
-        mock_generate_chunks_func.assert_called_once_with(mocker.sentinel.packet)
-        assert chunk == b"chunk"
-        assert second_chunk == b"2nd chunk"
-
-    def test____next____ignore_empty_yielded_bytes(
+    def test____generate____yield_generator_chunk(
         self,
         producer: StreamDataProducer[Any],
         mock_stream_protocol: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         def side_effect(_: Any) -> Generator[bytes, None, None]:
-            yield b""
-            yield b"2nd chunk"
+            yield b"chunk 1"
+            yield b"chunk 2"
 
         mock_generate_chunks_func: MagicMock = mock_stream_protocol.generate_chunks
         mock_generate_chunks_func.side_effect = side_effect
-        producer.enqueue(mocker.sentinel.packet)
 
         # Act
-        chunk: bytes = next(producer)
+        chunks: list[bytes] = list(producer.generate(mocker.sentinel.packet))
 
         # Assert
         mock_generate_chunks_func.assert_called_once_with(mocker.sentinel.packet)
-        assert chunk == b"2nd chunk"
-
-    def test____next____go_to_next_queued_packet_if_actual_generator_is_exhausted(
-        self,
-        producer: StreamDataProducer[Any],
-        mock_stream_protocol: MagicMock,
-        mocker: MockerFixture,
-    ) -> None:
-        # Arrange
-        def side_effect(_: Any) -> Generator[bytes, None, None]:
-            yield b"chunk"
-
-        mock_generate_chunks_func: MagicMock = mock_stream_protocol.generate_chunks
-        mock_generate_chunks_func.side_effect = side_effect
-        producer.enqueue(mocker.sentinel.packet_for_test_arrange, mocker.sentinel.second_packet)
-        next(producer)
-        mock_generate_chunks_func.reset_mock()  # Needed to call assert_called_once() later
-
-        # Act
-        chunk: bytes = next(producer)
-
-        # Assert
-        mock_generate_chunks_func.assert_called_once_with(mocker.sentinel.second_packet)
-        assert chunk == b"chunk"
-
-    def test____next____actual_generator_is_exhausted_and_there_is_no_queued_packet(
-        self,
-        producer: StreamDataProducer[Any],
-        mock_stream_protocol: MagicMock,
-        mocker: MockerFixture,
-    ) -> None:
-        # Arrange
-        def side_effect(_: Any) -> Generator[bytes, None, None]:
-            yield b"chunk"
-
-        mock_generate_chunks_func: MagicMock = mock_stream_protocol.generate_chunks
-        mock_generate_chunks_func.side_effect = side_effect
-        producer.enqueue(mocker.sentinel.packet_for_test_arrange)
-        next(producer)
-        mock_generate_chunks_func.reset_mock()  # Needed to call assert_not_called() later
-
-        # Act
-        with pytest.raises(StopIteration):
-            _ = next(producer)
-
-        # Assert
-        mock_generate_chunks_func.assert_not_called()
-
-    def test____next____convert_bytearrays(
-        self,
-        producer: StreamDataProducer[Any],
-        mock_stream_protocol: MagicMock,
-        mocker: MockerFixture,
-    ) -> None:
-        # Arrange
-        def side_effect(_: Any) -> Generator[bytes, None, None]:
-            yield bytearray(b"chunk")
-
-        mock_generate_chunks_func: MagicMock = mock_stream_protocol.generate_chunks
-        mock_generate_chunks_func.side_effect = side_effect
-        producer.enqueue(mocker.sentinel.packet_for_test_arrange)
-
-        # Act
-        chunk = next(producer)
-
-        # Assert
-        assert isinstance(chunk, bytes)
-        assert chunk == b"chunk"
+        assert chunks == [b"chunk 1", b"chunk 2"]
 
     @pytest.mark.parametrize("before_yielding", [False, True], ids=lambda p: f"before_yielding=={p}")
-    def test____next____generator_raised(
+    def test____generate____generator_raised(
         self,
         before_yielding: bool,
         producer: StreamDataProducer[Any],
         mock_stream_protocol: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
@@ -205,109 +67,26 @@
             if before_yielding:
                 raise expected_error
             yield b"chunk"
             raise expected_error
 
         mock_generate_chunks_func: MagicMock = mock_stream_protocol.generate_chunks
         mock_generate_chunks_func.side_effect = side_effect
-        producer.enqueue(mocker.sentinel.packet_for_test_arrange)
+        generator = producer.generate(mocker.sentinel.packet_for_test_arrange)
 
         if not before_yielding:
-            next(producer)
+            next(generator)
 
         # Act
         with pytest.raises(RuntimeError, match=r"^protocol\.generate_chunks\(\) crashed$") as exc_info:
-            next(producer)
+            next(generator)
 
         # Assert
         assert exc_info.value.__cause__ is expected_error
 
-    def test____pending_packets____empty_producer(self, producer: StreamDataProducer[Any]) -> None:
-        # Arrange
-
-        # Act & Assert
-        assert not producer.pending_packets()
-
-    def test____pending_packets____queued_packet(
-        self,
-        producer: StreamDataProducer[Any],
-        mock_stream_protocol: MagicMock,
-        mocker: MockerFixture,
-    ) -> None:
-        # Arrange
-        def side_effect(_: Any) -> Generator[bytes, None, None]:
-            yield b"chunk"
-            yield b"2nd chunk"
-
-        mock_generate_chunks_func: MagicMock = mock_stream_protocol.generate_chunks
-        mock_generate_chunks_func.side_effect = side_effect
-        producer.enqueue(mocker.sentinel.packet)
-
-        # Act & Assert
-        assert producer.pending_packets()
-        next(producer)
-        assert producer.pending_packets()
-        next(producer)
-        assert producer.pending_packets()  # Generator still alive
-        with pytest.raises(StopIteration):
-            next(producer)
-        assert not producer.pending_packets()
-
-    def test____enqueue____no_args(self, producer: StreamDataProducer[Any]) -> None:
-        # Arrange
-
-        # Act
-        producer.enqueue()
-
-        # Assert
-        ## There is no exceptions ? Nice !
-
-    def test____clear____remove_queued_packets(
-        self,
-        producer: StreamDataProducer[Any],
-        mocker: MockerFixture,
-    ) -> None:
-        # Arrange
-        producer.enqueue(mocker.sentinel.packet)
-        assert producer.pending_packets()
-
-        # Act
-        producer.clear()
-
-        # Assert
-        assert not producer.pending_packets()
-        with pytest.raises(StopIteration):
-            next(producer)
-
-    def test____clear____close_current_generator(
-        self,
-        producer: StreamDataProducer[Any],
-        mock_stream_protocol: MagicMock,
-        mocker: MockerFixture,
-    ) -> None:
-        # Arrange
-        def side_effect(_: Any) -> Generator[bytes, None, None]:
-            with pytest.raises(GeneratorExit) as exc_info:  # Exception raised in generator when calling generator.close()
-                yield b"chunk"
-            raise exc_info.value  # re-raise exception
-
-        mock_generate_chunks_func: MagicMock = mock_stream_protocol.generate_chunks
-        mock_generate_chunks_func.side_effect = side_effect
-        producer.enqueue(mocker.sentinel.packet)
-        assert next(producer) == b"chunk"
-        assert producer.pending_packets()
-
-        # Act
-        producer.clear()
-
-        # Assert
-        assert not producer.pending_packets()
-        with pytest.raises(StopIteration):
-            next(producer)
-
 
 class TestStreamDataConsumer:
     @pytest.fixture
     @staticmethod
     def consumer(mock_stream_protocol: MagicMock) -> StreamDataConsumer[Any]:
         return StreamDataConsumer(mock_stream_protocol)
 
@@ -315,58 +94,25 @@
         # Arrange
         from easynetwork.protocol import DatagramProtocol
 
         # Act & Assert
         with pytest.raises(TypeError, match=r"^Expected a StreamProtocol object, got .*$"):
             _ = StreamDataConsumer(DatagramProtocol(mock_serializer))  # type: ignore[arg-type]
 
-    def test____dunder_iter____return_self(self, consumer: StreamDataConsumer[Any]) -> None:
-        # Arrange
-
-        # Act
-        iterator = iter(consumer)
-
-        # Assert
-        assert iterator is consumer
-
-    def test____feed____convert_bytearrays(
-        self,
-        consumer: StreamDataConsumer[Any],
-        mock_stream_protocol: MagicMock,
-        mocker: MockerFixture,
-    ) -> None:
-        # Arrange
-        def side_effect() -> Generator[None, bytes, tuple[Any, bytes]]:
-            data = yield
-            assert isinstance(data, bytes)
-            assert data == b"Hello"
-            return mocker.sentinel.packet, bytearray(b"World")
-
-        mock_build_packet_from_chunks_func: MagicMock = mock_stream_protocol.build_packet_from_chunks
-        mock_build_packet_from_chunks_func.side_effect = side_effect
-        consumer.feed(bytearray(b"Hello"))
-
-        # Act
-        packet = next(consumer)
-
-        # Assert
-        assert packet is mocker.sentinel.packet
-
     def test____next____no_buffer(
         self,
         consumer: StreamDataConsumer[Any],
         mock_stream_protocol: MagicMock,
     ) -> None:
         # Arrange
         mock_build_packet_from_chunks_func: MagicMock = mock_stream_protocol.build_packet_from_chunks
-        consumer.feed(b"")  # 0 + 0 == 0
 
         # Act
         with pytest.raises(StopIteration):
-            next(consumer)
+            consumer.next(None)
 
         # Assert
         mock_build_packet_from_chunks_func.assert_not_called()
 
     def test____next____oneshot(
         self,
         consumer: StreamDataConsumer[Any],
@@ -377,19 +123,17 @@
         def side_effect() -> Generator[None, bytes, tuple[Any, bytes]]:
             data = yield
             assert data == b"Hello"
             return mocker.sentinel.packet, b"World"
 
         mock_build_packet_from_chunks_func: MagicMock = mock_stream_protocol.build_packet_from_chunks
         mock_build_packet_from_chunks_func.side_effect = side_effect
-        consumer.feed(b"Hello")
-        assert consumer.get_buffer() == b"Hello"
 
         # Act
-        packet = next(consumer)
+        packet = consumer.next(b"Hello")
 
         # Assert
         mock_build_packet_from_chunks_func.assert_called_once_with()
         assert packet is mocker.sentinel.packet
         assert consumer.get_buffer() == b"World"
 
     def test____next____several_attempts(
@@ -406,72 +150,111 @@
             assert data == b"World"
             return mocker.sentinel.packet, b"Bye"
 
         mock_build_packet_from_chunks_func: MagicMock = mock_stream_protocol.build_packet_from_chunks
         mock_build_packet_from_chunks_func.side_effect = side_effect
 
         # Act & Assert
-        consumer.feed(b"Hello")
         with pytest.raises(StopIteration):
-            next(consumer)
+            consumer.next(b"Hello")
         mock_build_packet_from_chunks_func.assert_called_once_with()
         assert consumer.get_buffer() == b""
 
         mock_build_packet_from_chunks_func.reset_mock()
-        consumer.feed(b"World")
-        packet = next(consumer)
+        packet = consumer.next(b"World")
         mock_build_packet_from_chunks_func.assert_not_called()
         assert packet is mocker.sentinel.packet
         assert consumer.get_buffer() == b"Bye"
 
-    def test____next____concatenate_feed_buffer(
+    @pytest.mark.parametrize("sent_buffer", [None, b"", b"Hello"], ids=repr)
+    def test____next____reuse_previously_returned_buffer(
         self,
+        sent_buffer: bytes | None,
         consumer: StreamDataConsumer[Any],
         mock_stream_protocol: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
-        def side_effect() -> Generator[None, bytes, tuple[Any, bytes]]:
+        def setup_side_effect() -> Generator[None, bytes, tuple[Any, bytes]]:
+            data = yield
+            assert data == b"Hello"
+            return mocker.sentinel.packet, b"World"
+
+        def test_side_effect() -> Generator[None, bytes, tuple[Any, bytes]]:
             data = yield
-            assert data == b"HelloWorld"
+            if sent_buffer is None:
+                assert data == b"World"
+            else:
+                assert data == b"World" + sent_buffer
             return mocker.sentinel.packet, b"Bye"
 
         mock_build_packet_from_chunks_func: MagicMock = mock_stream_protocol.build_packet_from_chunks
-        mock_build_packet_from_chunks_func.side_effect = side_effect
+        mock_build_packet_from_chunks_func.side_effect = setup_side_effect
+        assert consumer.next(b"Hello") is mocker.sentinel.packet
+        assert consumer.get_buffer() == b"World"
+        mock_build_packet_from_chunks_func.reset_mock()
+        mock_build_packet_from_chunks_func.side_effect = test_side_effect
 
         # Act
-        consumer.feed(b"Hello")
-        consumer.feed(b"World")
-        assert consumer.get_buffer() == b"HelloWorld"
-        packet = next(consumer)
+        packet = consumer.next(sent_buffer)
 
         # Assert
         mock_build_packet_from_chunks_func.assert_called_once_with()
         assert packet is mocker.sentinel.packet
         assert consumer.get_buffer() == b"Bye"
 
+    @pytest.mark.parametrize("buffer", [None, b""], ids=repr)
+    def test____next____ignore_empty_buffer(
+        self,
+        buffer: bytes | None,
+        consumer: StreamDataConsumer[Any],
+        mock_stream_protocol: MagicMock,
+        mocker: MockerFixture,
+    ) -> None:
+        # Arrange
+        rest_checkpoint = mocker.stub()
+
+        def side_effect() -> Generator[None, bytes, tuple[Any, bytes]]:
+            data = yield
+            assert data == b"Hello"
+            rest = yield
+            rest_checkpoint(rest)
+            return mocker.sentinel.packet, b"World"
+
+        mock_build_packet_from_chunks_func: MagicMock = mock_stream_protocol.build_packet_from_chunks
+        mock_build_packet_from_chunks_func.side_effect = side_effect
+        with pytest.raises(StopIteration):
+            consumer.next(b"Hello")
+        assert consumer.get_buffer() == b""
+        rest_checkpoint.assert_not_called()
+
+        # Act
+        with pytest.raises(StopIteration):
+            consumer.next(buffer)
+
+        # Assert
+        rest_checkpoint.assert_not_called()
+
     def test____next____protocol_parse_error(
         self,
         consumer: StreamDataConsumer[Any],
         mock_stream_protocol: MagicMock,
     ) -> None:
         # Arrange
         def side_effect() -> Generator[None, bytes, tuple[Any, bytes]]:
             data = yield
             assert data == b"Hello"
             raise StreamProtocolParseError(b"World", IncrementalDeserializeError("Error occurred", b""))
 
         mock_build_packet_from_chunks_func: MagicMock = mock_stream_protocol.build_packet_from_chunks
         mock_build_packet_from_chunks_func.side_effect = side_effect
-        consumer.feed(b"Hello")
-        assert consumer.get_buffer() == b"Hello"
 
         # Act
         with pytest.raises(StreamProtocolParseError) as exc_info:
-            next(consumer)
+            consumer.next(b"Hello")
         exception = exc_info.value
 
         # Assert
         mock_build_packet_from_chunks_func.assert_called_once_with()
         assert consumer.get_buffer() == b"World"
         assert bytes(exception.remaining_data) == b"World"
 
@@ -484,20 +267,18 @@
         def side_effect() -> Generator[None, bytes, tuple[Any, bytes]]:
             if False:
                 yield  # type: ignore[unreachable]
             return 42, b"42"
 
         mock_build_packet_from_chunks_func: MagicMock = mock_stream_protocol.build_packet_from_chunks
         mock_build_packet_from_chunks_func.side_effect = side_effect
-        consumer.feed(b"Hello")
-        assert consumer.get_buffer() == b"Hello"
 
         # Act
         with pytest.raises(RuntimeError, match=r"^protocol\.build_packet_from_chunks\(\) did not yield$"):
-            next(consumer)
+            consumer.next(b"Hello")
 
         # Assert
         mock_build_packet_from_chunks_func.assert_called_once_with()
         assert consumer.get_buffer() == b"Hello"
 
     @pytest.mark.parametrize("before_yielding", [False, True], ids=lambda p: f"before_yielding=={p}")
     def test____next____generator_raised(
@@ -513,59 +294,49 @@
             if before_yielding:
                 raise expected_error
             yield
             raise expected_error
 
         mock_build_packet_from_chunks_func: MagicMock = mock_stream_protocol.build_packet_from_chunks
         mock_build_packet_from_chunks_func.side_effect = side_effect
-        consumer.feed(b"Hello")
-        assert consumer.get_buffer() == b"Hello"
 
         # Act
         with pytest.raises(RuntimeError, match=r"^protocol\.build_packet_from_chunks\(\) crashed$") as exc_info:
-            next(consumer)
+            consumer.next(b"Hello")
 
         # Assert
         assert exc_info.value.__cause__ is expected_error
 
-    def test____clear____flush_pending_buffer(self, consumer: StreamDataConsumer[Any]) -> None:
-        # Arrange
-        consumer.feed(b"Hello")
-        assert consumer.get_buffer() == b"Hello"
-
-        # Act
-        consumer.clear()
-
-        # Assert
-        assert consumer.get_buffer() == b""
-
     def test____clear____close_current_generator(
         self,
         consumer: StreamDataConsumer[Any],
         mock_stream_protocol: MagicMock,
+        mocker: MockerFixture,
     ) -> None:
         # Arrange
+        generator_exit_checkpoint = mocker.stub()
+
         def side_effect() -> Generator[None, bytes, tuple[Any, bytes]]:
             assert (yield) == b"Hello"
             with pytest.raises(GeneratorExit) as exc_info:
                 yield
+            generator_exit_checkpoint()
             raise exc_info.value
 
         mock_build_packet_from_chunks_func: MagicMock = mock_stream_protocol.build_packet_from_chunks
         mock_build_packet_from_chunks_func.side_effect = side_effect
-        consumer.feed(b"Hello")
         with pytest.raises(StopIteration):
-            next(consumer)
-        consumer.feed(b"World")
-        assert consumer.get_buffer() == b"World"
+            consumer.next(b"Hello")
+        generator_exit_checkpoint.assert_not_called()
 
         # Act
         consumer.clear()
 
         # Assert
+        generator_exit_checkpoint.assert_called_once_with()
         assert consumer.get_buffer() == b""
 
 
 class TestBufferedStreamDataConsumer:
     @pytest.fixture
     @staticmethod
     def mock_stream_protocol(mock_stream_protocol: MagicMock, mock_buffered_stream_receiver: MagicMock) -> MagicMock:
@@ -585,19 +356,23 @@
 
     @pytest.fixture
     @staticmethod
     def consumer(mock_stream_protocol: MagicMock, sizehint: int) -> BufferedStreamDataConsumer[Any]:
         return BufferedStreamDataConsumer(mock_stream_protocol, sizehint)
 
     @staticmethod
-    def write_in_consumer(consumer: BufferedStreamDataConsumer[Any], data: bytes | bytearray | memoryview) -> None:
+    def write_in_consumer(
+        consumer: BufferedStreamDataConsumer[Any],
+        data: bytes | bytearray | memoryview,
+        start_idx: int = 0,
+    ) -> int:
         nbytes = len(data)
-        with memoryview(consumer.get_write_buffer()) as buffer:
+        with memoryview(consumer.get_write_buffer())[start_idx:] as buffer:
             buffer[:nbytes] = data
-        consumer.buffer_updated(nbytes)
+        return nbytes
 
     def test____dunder_init____invalid_protocol(self, mock_serializer: MagicMock, sizehint: int) -> None:
         # Arrange
         from easynetwork.protocol import DatagramProtocol
 
         # Act & Assert
         with pytest.raises(TypeError, match=r"^Expected a StreamProtocol object, got .*$"):
@@ -607,23 +382,14 @@
     def test____dunder_init____invalid_sizehint(self, mock_stream_protocol: MagicMock, sizehint: int) -> None:
         # Arrange
 
         # Act & Assert
         with pytest.raises(ValueError, match=rf"^buffer_size_hint={sizehint}$"):
             _ = BufferedStreamDataConsumer(mock_stream_protocol, sizehint)
 
-    def test____dunder_iter____return_self(self, consumer: BufferedStreamDataConsumer[Any]) -> None:
-        # Arrange
-
-        # Act
-        iterator = iter(consumer)
-
-        # Assert
-        assert iterator is consumer
-
     def test____get_write_buffer____protocol_create_buffer_validation____readonly_buffer(
         self,
         consumer: BufferedStreamDataConsumer[Any],
         mock_buffered_stream_receiver: MagicMock,
     ) -> None:
         # Arrange
         mock_buffered_stream_receiver.create_buffer.side_effect = [b"read-only buffer"]
@@ -679,82 +445,82 @@
         mock_build_packet_from_buffer_func: MagicMock = mock_buffered_stream_receiver.build_packet_from_buffer
         mock_build_packet_from_buffer_func.side_effect = side_effect
 
         # Act & Assert
         with pytest.raises(RuntimeError, match=r"^The start position is set to the end of the buffer$"):
             consumer.get_write_buffer()
 
-    def test____buffer_updated____error_negative_nbytes(
+    def test____next____error_negative_nbytes(
         self,
         consumer: BufferedStreamDataConsumer[Any],
         mock_buffered_stream_receiver: MagicMock,
     ) -> None:
         # Arrange
         def side_effect(buffer: memoryview) -> Generator[int, int, tuple[Any, bytes]]:
             yield 0
             pytest.fail("Should not arrive here")
 
         mock_build_packet_from_buffer_func: MagicMock = mock_buffered_stream_receiver.build_packet_from_buffer
         mock_build_packet_from_buffer_func.side_effect = side_effect
         consumer.get_write_buffer()
 
         # Act & Assert
-        with pytest.raises(ValueError, match=r"^Negative value given$"):
-            consumer.buffer_updated(-1)
+        with pytest.raises(RuntimeError, match=r"^Invalid value given$"):
+            consumer.next(-1)
 
-    def test____buffer_updated____get_buffer_not_called(
+    def test____next____get_buffer_not_called(
         self,
         consumer: BufferedStreamDataConsumer[Any],
         mock_buffered_stream_receiver: MagicMock,
     ) -> None:
         # Arrange
         def side_effect(buffer: memoryview) -> Generator[int, int, tuple[Any, bytes]]:
             yield 0
             pytest.fail("Should not arrive here")
 
         mock_build_packet_from_buffer_func: MagicMock = mock_buffered_stream_receiver.build_packet_from_buffer
         mock_build_packet_from_buffer_func.side_effect = side_effect
         # consumer.get_write_buffer()
 
         # Act & Assert
-        with pytest.raises(RuntimeError, match=r"^buffer_updated\(\) has been called whilst get_buffer\(\) was never called$"):
-            consumer.buffer_updated(4)
+        with pytest.raises(RuntimeError, match=r"^next\(\) has been called whilst get_write_buffer\(\) was never called$"):
+            consumer.next(4)
 
-    def test____buffer_updated____nbytes_too_big(
+    def test____next____nbytes_too_big(
         self,
         consumer: BufferedStreamDataConsumer[Any],
         mock_buffered_stream_receiver: MagicMock,
     ) -> None:
         # Arrange
         def side_effect(buffer: memoryview) -> Generator[int, int, tuple[Any, bytes]]:
             yield -10
             pytest.fail("Should not arrive here")
 
         mock_build_packet_from_buffer_func: MagicMock = mock_buffered_stream_receiver.build_packet_from_buffer
         mock_build_packet_from_buffer_func.side_effect = side_effect
         assert len(memoryview(consumer.get_write_buffer())) == 10
 
         # Act & Assert
-        with pytest.raises(RuntimeError, match=r"^nbytes > buffer_view\.nbytes$"):
-            consumer.buffer_updated(12)
+        with pytest.raises(RuntimeError, match=r"^Invalid value given$"):
+            consumer.next(12)
 
     def test____next____no_buffer(
         self,
         consumer: BufferedStreamDataConsumer[Any],
         mock_buffered_stream_receiver: MagicMock,
     ) -> None:
         # Arrange
         mock_buffered_stream_receiver.create_buffer.assert_not_called()
         mock_buffered_stream_receiver.build_packet_from_buffer.assert_not_called()
         assert consumer.buffer_size == 0
         assert consumer.get_value() is None
 
         # Act
         with pytest.raises(StopIteration):
-            next(consumer)
+            consumer.next(None)
 
         # Assert
         mock_buffered_stream_receiver.create_buffer.assert_not_called()
         mock_buffered_stream_receiver.build_packet_from_buffer.assert_not_called()
 
     @pytest.mark.parametrize("remainder_type", ["buffer_view", "external"])
     def test____next____oneshot(
@@ -777,21 +543,21 @@
                     return mocker.sentinel.packet, b""
                 case _:
                     assert_never(remainder_type)
 
         mock_build_packet_from_buffer_func: MagicMock = mock_buffered_stream_receiver.build_packet_from_buffer
         mock_build_packet_from_buffer_func.side_effect = side_effect
         assert consumer.get_value() is None
-        self.write_in_consumer(consumer, b"Hello world")
+        nb_updated_bytes = self.write_in_consumer(consumer, b"Hello world")
         assert consumer.buffer_size > 0
-        assert consumer.get_value() == b"Hello world"
+        assert consumer.get_value() == b""
         mock_build_packet_from_buffer_func.reset_mock()
 
         # Act
-        packet = next(consumer)
+        packet = consumer.next(nb_updated_bytes)
 
         # Assert
         assert packet is mocker.sentinel.packet
         mock_build_packet_from_buffer_func.assert_not_called()
         assert consumer.get_value() == b""
 
     @pytest.mark.parametrize("remainder_type", ["buffer_view", "external"])
@@ -815,44 +581,46 @@
                     return mocker.sentinel.packet, b"world"
                 case _:
                     assert_never(remainder_type)
 
         mock_build_packet_from_buffer_func: MagicMock = mock_buffered_stream_receiver.build_packet_from_buffer
         mock_build_packet_from_buffer_func.side_effect = side_effect
         assert consumer.get_value() is None
-        self.write_in_consumer(consumer, b"Hello world")
+        nb_updated_bytes = self.write_in_consumer(consumer, b"Hello world")
         assert consumer.buffer_size > 0
-        assert consumer.get_value() == b"Hello world"
+        assert consumer.get_value() == b""
 
         # Act
-        packet = next(consumer)
+        packet = consumer.next(nb_updated_bytes)
 
         # Assert
         assert packet is mocker.sentinel.packet
         assert consumer.get_value() == b"world"
 
     def test____next____remainder_buffer_overlapping(
         self,
         consumer: BufferedStreamDataConsumer[Any],
         zero_or_none: int | None,
         mock_buffered_stream_receiver: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         def side_effect(buffer: memoryview) -> Generator[int | None, int, tuple[Any, ReadableBuffer]]:
+            for i in range(buffer.nbytes):
+                buffer[i] = 0
             nbytes = yield zero_or_none
-            assert buffer[:nbytes] == b"Hello world"
+            assert bytes(buffer[:nbytes]) == b"Hello world"
             return mocker.sentinel.packet, buffer[2:nbytes]
 
         mock_build_packet_from_buffer_func: MagicMock = mock_buffered_stream_receiver.build_packet_from_buffer
         mock_build_packet_from_buffer_func.side_effect = side_effect
-        self.write_in_consumer(consumer, b"Hello world")
+        nb_updated_bytes = self.write_in_consumer(consumer, b"Hello world")
 
         # Act
-        packet = next(consumer)
+        packet = consumer.next(nb_updated_bytes)
 
         # Assert
         assert packet is mocker.sentinel.packet
         assert consumer.get_value() == b"llo world"
 
     def test____next____several_attempts(
         self,
@@ -860,65 +628,101 @@
         zero_or_none: int | None,
         mock_buffered_stream_receiver: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         def side_effect(buffer: memoryview) -> Generator[int | None, int, tuple[Any, ReadableBuffer]]:
             nbytes = yield zero_or_none
-            assert buffer[:nbytes] == b"Hello"
+            assert bytes(buffer[:nbytes]) == b"Hello"
             nbytes = yield zero_or_none
-            assert buffer[:nbytes] == b"World"
+            assert bytes(buffer[:nbytes]) == b"World"
             return mocker.sentinel.packet, b"Bye"
 
         mock_build_packet_from_buffer_func: MagicMock = mock_buffered_stream_receiver.build_packet_from_buffer
         mock_build_packet_from_buffer_func.side_effect = side_effect
 
         # Act & Assert
-        self.write_in_consumer(consumer, b"Hello")
+        nb_updated_bytes = self.write_in_consumer(consumer, b"Hello")
         mock_build_packet_from_buffer_func.assert_called_once_with(mocker.ANY)
         with pytest.raises(StopIteration):
-            next(consumer)
+            consumer.next(nb_updated_bytes)
         assert consumer.get_value() == b""
 
         mock_build_packet_from_buffer_func.reset_mock()
-        self.write_in_consumer(consumer, b"")
+        nb_updated_bytes = self.write_in_consumer(consumer, b"")
         with pytest.raises(StopIteration):
-            next(consumer)
+            consumer.next(nb_updated_bytes)
         mock_build_packet_from_buffer_func.assert_not_called()
         assert consumer.get_value() == b""
 
         mock_build_packet_from_buffer_func.reset_mock()
-        self.write_in_consumer(consumer, b"World")
+        nb_updated_bytes = self.write_in_consumer(consumer, b"World")
         mock_build_packet_from_buffer_func.assert_not_called()
-        packet = next(consumer)
+        packet = consumer.next(nb_updated_bytes)
         assert packet is mocker.sentinel.packet
         assert consumer.get_value() == b"Bye"
 
     def test____next____move_buffer_start_after_updating_it(
         self,
         consumer: BufferedStreamDataConsumer[Any],
         zero_or_none: int | None,
         mock_buffered_stream_receiver: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         def side_effect(buffer: memoryview) -> Generator[int | None, int, tuple[Any, ReadableBuffer]]:
             nbytes = yield zero_or_none
-            assert buffer[:nbytes] == b"HelloWorld"
+            assert bytes(buffer[:nbytes]) == b"HelloWorld"
             return mocker.sentinel.packet, b"Bye"
 
         mock_build_packet_from_buffer_func: MagicMock = mock_buffered_stream_receiver.build_packet_from_buffer
         mock_build_packet_from_buffer_func.side_effect = side_effect
 
         # Act
-        self.write_in_consumer(consumer, b"Hello")
-        self.write_in_consumer(consumer, b"World")
-        assert consumer.get_value() == b"HelloWorld"
+        nb_updated_bytes = 0
+        nb_updated_bytes += self.write_in_consumer(consumer, b"Hello", nb_updated_bytes)
+        nb_updated_bytes += self.write_in_consumer(consumer, b"World", nb_updated_bytes)
+        assert consumer.get_value() == b""
         mock_build_packet_from_buffer_func.assert_called_once_with(mocker.ANY)
-        packet = next(consumer)
+        packet = consumer.next(nb_updated_bytes)
+
+        # Assert
+        assert packet is mocker.sentinel.packet
+        assert consumer.get_value() == b"Bye"
+
+    def test____next____move_buffer_start_in_case_of_remainder(
+        self,
+        consumer: BufferedStreamDataConsumer[Any],
+        zero_or_none: int | None,
+        mock_buffered_stream_receiver: MagicMock,
+        mocker: MockerFixture,
+    ) -> None:
+        # Arrange
+        def setup_side_effect(buffer: memoryview) -> Generator[int | None, int, tuple[Any, ReadableBuffer]]:
+            nbytes = yield zero_or_none
+            assert bytes(buffer[:nbytes]) == b"Hello"
+            return mocker.sentinel.packet_in_setup, b"World"
+
+        def test_side_effect(buffer: memoryview) -> Generator[int | None, int, tuple[Any, ReadableBuffer]]:
+            nbytes = yield zero_or_none
+            assert bytes(buffer[:nbytes]) == b"WorldHello"
+            return mocker.sentinel.packet, b"Bye"
+
+        mock_build_packet_from_buffer_func: MagicMock = mock_buffered_stream_receiver.build_packet_from_buffer
+        mock_build_packet_from_buffer_func.side_effect = setup_side_effect
+        nb_updated_bytes = self.write_in_consumer(consumer, b"Hello")
+        mock_build_packet_from_buffer_func.reset_mock()
+        mock_build_packet_from_buffer_func.side_effect = test_side_effect
+        packet = consumer.next(nb_updated_bytes)
+        assert packet is mocker.sentinel.packet_in_setup
+        assert consumer.get_value() == b"World"
+
+        # Act
+        nb_updated_bytes = self.write_in_consumer(consumer, b"Hello")
+        packet = consumer.next(nb_updated_bytes)
 
         # Assert
         assert packet is mocker.sentinel.packet
         assert consumer.get_value() == b"Bye"
 
     def test____next____buffer_start_on_first_yield(
         self,
@@ -926,112 +730,113 @@
         mock_buffered_stream_receiver: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         def side_effect(buffer: memoryview) -> Generator[int | None, int, tuple[Any, ReadableBuffer]]:
             buffer[:5] = b"Hello"
             nbytes = yield 5
-            assert buffer[: 5 + nbytes] == b"HelloWorld"
+            assert bytes(buffer[: 5 + nbytes]) == b"HelloWorld"
             return mocker.sentinel.packet, b"Bye"
 
         mock_build_packet_from_buffer_func: MagicMock = mock_buffered_stream_receiver.build_packet_from_buffer
         mock_build_packet_from_buffer_func.side_effect = side_effect
 
         # Act & Assert
         consumer.get_write_buffer()
         assert consumer.get_value() == b"Hello"
-        self.write_in_consumer(consumer, b"World")
-        assert consumer.get_value() == b"HelloWorld"
-        packet = next(consumer)
+        nb_updated_bytes = self.write_in_consumer(consumer, b"World")
+        assert consumer.get_value() == b"Hello"
+        packet = consumer.next(nb_updated_bytes)
         assert packet is mocker.sentinel.packet
         assert consumer.get_value() == b"HelloBye"
 
     def test____next____buffer_start_on_first_yield____several_writes(
         self,
         consumer: BufferedStreamDataConsumer[Any],
         mock_buffered_stream_receiver: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         def side_effect(buffer: memoryview) -> Generator[int | None, int, tuple[Any, ReadableBuffer]]:
             buffer[:5] = b"Hello"
             nbytes = yield 5
-            assert buffer[: 5 + nbytes] == b"HelloWorld!"
+            assert bytes(buffer[: 5 + nbytes]) == b"HelloWorld!"
             return mocker.sentinel.packet, b"Bye"
 
         mock_build_packet_from_buffer_func: MagicMock = mock_buffered_stream_receiver.build_packet_from_buffer
         mock_build_packet_from_buffer_func.side_effect = side_effect
 
         # Act & Assert
         consumer.get_write_buffer()
         assert consumer.get_value() == b"Hello"
-        self.write_in_consumer(consumer, b"World")
-        self.write_in_consumer(consumer, b"!")
-        assert consumer.get_value() == b"HelloWorld!"
-        packet = next(consumer)
+        nb_updated_bytes = 0
+        nb_updated_bytes += self.write_in_consumer(consumer, b"World", nb_updated_bytes)
+        nb_updated_bytes += self.write_in_consumer(consumer, b"!", nb_updated_bytes)
+        assert consumer.get_value() == b"Hello"
+        packet = consumer.next(nb_updated_bytes)
         assert packet is mocker.sentinel.packet
         assert consumer.get_value() == b"HelloBye"
 
     def test____next____buffer_start_on_subsequent_yields(
         self,
         consumer: BufferedStreamDataConsumer[Any],
         mock_buffered_stream_receiver: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         def side_effect(buffer: memoryview) -> Generator[int | None, int, tuple[Any, ReadableBuffer]]:
             nbytes = yield 0
-            assert buffer[:nbytes] == b"Hello"
+            assert bytes(buffer[:nbytes]) == b"Hello"
             nbytes += yield nbytes
-            assert buffer[:nbytes] == b"HelloWorld"
+            assert bytes(buffer[:nbytes]) == b"HelloWorld"
             return mocker.sentinel.packet, b"Bye"
 
         mock_build_packet_from_buffer_func: MagicMock = mock_buffered_stream_receiver.build_packet_from_buffer
         mock_build_packet_from_buffer_func.side_effect = side_effect
 
         # Act & Assert
-        self.write_in_consumer(consumer, b"Hello")
+        nb_updated_bytes = self.write_in_consumer(consumer, b"Hello")
         with pytest.raises(StopIteration):
-            next(consumer)
-        self.write_in_consumer(consumer, b"World")
-        packet = next(consumer)
+            consumer.next(nb_updated_bytes)
+        nb_updated_bytes = self.write_in_consumer(consumer, b"World")
+        packet = consumer.next(nb_updated_bytes)
         assert packet is mocker.sentinel.packet
         assert consumer.get_value() == b"Bye"
 
     def test____next____negative_buffer_start(
         self,
         consumer: BufferedStreamDataConsumer[Any],
         mock_buffered_stream_receiver: MagicMock,
         mocker: MockerFixture,
     ) -> None:
         # Arrange
         def side_effect(buffer: memoryview) -> Generator[int | None, int, tuple[Any, ReadableBuffer]]:
             nbytes = yield -10
             assert nbytes == 5
-            assert buffer[-10:-5] == b"Hello"
+            assert bytes(buffer[-10:-5]) == b"Hello"
             nbytes = yield -5
             assert nbytes == 5
-            assert buffer[-5:] == b"World"
-            assert buffer[-10:] == b"HelloWorld"
+            assert bytes(buffer[-5:]) == b"World"
+            assert bytes(buffer[-10:]) == b"HelloWorld"
             return mocker.sentinel.packet, b"Bye"
 
         mock_build_packet_from_buffer_func: MagicMock = mock_buffered_stream_receiver.build_packet_from_buffer
         mock_build_packet_from_buffer_func.side_effect = side_effect
 
         # Act & Assert
-        self.write_in_consumer(consumer, b"Hello")
+        nb_updated_bytes = self.write_in_consumer(consumer, b"Hello")
         with pytest.raises(StopIteration):
-            next(consumer)
-        self.write_in_consumer(consumer, b"World")
-        packet = next(consumer)
+            consumer.next(nb_updated_bytes)
+        nb_updated_bytes = self.write_in_consumer(consumer, b"World")
+        packet = consumer.next(nb_updated_bytes)
         assert packet is mocker.sentinel.packet
         full_buffer_value = consumer.get_value(full=True)
         truncated_buffer_value = consumer.get_value(full=False)
         assert full_buffer_value is not None and truncated_buffer_value is not None
-        assert full_buffer_value[-10:-7] == b"Bye"
+        assert bytes(full_buffer_value[-10:-7]) == b"Bye"
         assert truncated_buffer_value.endswith(b"Bye")
 
     def test____next____not_a_byte_buffer(
         self,
         consumer: BufferedStreamDataConsumer[Any],
         mock_buffered_stream_receiver: MagicMock,
         sizehint: int,
@@ -1057,49 +862,49 @@
 
         # Act & Assert
         buffer = consumer.get_write_buffer()
         assert memoryview(buffer).format == "B"
         assert memoryview(buffer).itemsize == 1
         assert memoryview(buffer).nbytes == sizehint
         assert consumer.buffer_size == sizehint
-        self.write_in_consumer(consumer, struct.pack("@I", 42))
+        nb_updated_bytes = self.write_in_consumer(consumer, struct.pack("@I", 42))
         with pytest.raises(StopIteration):
-            next(consumer)
-        self.write_in_consumer(consumer, struct.pack("@I", 987))
-        packet = next(consumer)
+            consumer.next(nb_updated_bytes)
+        nb_updated_bytes = self.write_in_consumer(consumer, struct.pack("@I", 987))
+        packet = consumer.next(nb_updated_bytes)
         assert isinstance(packet, tuple)
         assert packet == (42, 987)
 
     @pytest.mark.parametrize("remainder_type", ["buffer_view", "external"])
     def test____next____protocol_parse_error(
         self,
         remainder_type: Literal["buffer_view", "external"],
         consumer: BufferedStreamDataConsumer[Any],
         zero_or_none: int | None,
         mock_buffered_stream_receiver: MagicMock,
     ) -> None:
         # Arrange
         def side_effect(buffer: memoryview) -> Generator[int | None, int, tuple[Any, ReadableBuffer]]:
             nbytes = yield zero_or_none
-            assert buffer[:nbytes] == b"Hello world"
+            assert bytes(buffer[:nbytes]) == b"Hello world"
             match remainder_type:
                 case "buffer_view":
                     raise StreamProtocolParseError(buffer[6:nbytes], IncrementalDeserializeError("Error occurred", b""))
                 case "external":
                     raise StreamProtocolParseError(b"world", IncrementalDeserializeError("Error occurred", b""))
                 case _:
                     assert_never(remainder_type)
 
         mock_build_packet_from_buffer_func: MagicMock = mock_buffered_stream_receiver.build_packet_from_buffer
         mock_build_packet_from_buffer_func.side_effect = side_effect
-        self.write_in_consumer(consumer, b"Hello world")
+        nb_updated_bytes = self.write_in_consumer(consumer, b"Hello world")
 
         # Act
         with pytest.raises(StreamProtocolParseError) as exc_info:
-            next(consumer)
+            consumer.next(nb_updated_bytes)
         exception = exc_info.value
 
         # Assert
         assert consumer.get_value() == b"world"
         assert bytes(exception.remaining_data) == b"world"
 
     def test____next____generator_did_not_yield(
@@ -1117,15 +922,15 @@
         mock_build_packet_from_buffer_func.side_effect = side_effect
 
         # Act
         with pytest.raises(RuntimeError, match=r"^protocol\.build_packet_from_buffer\(\) did not yield$"):
             self.write_in_consumer(consumer, b"Hello")
 
         with pytest.raises(StopIteration):
-            next(consumer)
+            consumer.next(None)
 
         # Assert
         assert consumer.get_value() == b""
 
     @pytest.mark.parametrize("before_yielding", [False, True], ids=lambda p: f"before_yielding=={p}")
     def test____next____generator_raised(
         self,
@@ -1149,21 +954,21 @@
         if before_yielding:
             with pytest.raises(RuntimeError, match=r"^protocol\.build_packet_from_buffer\(\) crashed$") as exc_info:
                 self.write_in_consumer(consumer, b"Hello")
 
             assert consumer.get_value() is None
 
             with pytest.raises(StopIteration):
-                next(consumer)
+                consumer.next(None)
 
         else:
-            self.write_in_consumer(consumer, b"Hello")
+            nb_updated_bytes = self.write_in_consumer(consumer, b"Hello")
 
             with pytest.raises(RuntimeError, match=r"^protocol\.build_packet_from_buffer\(\) crashed$") as exc_info:
-                next(consumer)
+                consumer.next(nb_updated_bytes)
 
             assert consumer.get_value() is None
 
         # Assert
         assert exc_info.value.__cause__ is expected_error
 
     def test____clear____never_used_consumer(
@@ -1187,40 +992,44 @@
         def side_effect(buffer: memoryview) -> Generator[int, int, tuple[Any, bytes]]:
             yield 0
             pytest.fail("Should not arrive here")
 
         mock_build_packet_from_buffer_func: MagicMock = mock_buffered_stream_receiver.build_packet_from_buffer
         mock_build_packet_from_buffer_func.side_effect = side_effect
         self.write_in_consumer(consumer, b"Hello world")
-        assert consumer.get_value() == b"Hello world"
 
         # Act
         consumer.clear()
 
         # Assert
         assert consumer.get_value() is None
 
     def test____clear____close_current_generator(
         self,
         consumer: BufferedStreamDataConsumer[Any],
         mock_buffered_stream_receiver: MagicMock,
+        mocker: MockerFixture,
     ) -> None:
         # Arrange
+        generator_exit_checkpoint = mocker.stub()
+
         def side_effect(buffer: memoryview) -> Generator[None, int, tuple[Any, bytes]]:
             yield
             with pytest.raises(GeneratorExit) as exc_info:
                 yield
+            generator_exit_checkpoint()
             raise exc_info.value
 
         mock_build_packet_from_buffer_func: MagicMock = mock_buffered_stream_receiver.build_packet_from_buffer
         mock_build_packet_from_buffer_func.side_effect = side_effect
-        self.write_in_consumer(consumer, b"Hello")
+        nb_updated_bytes = self.write_in_consumer(consumer, b"Hello")
         with pytest.raises(StopIteration):
-            next(consumer)
+            consumer.next(nb_updated_bytes)
         self.write_in_consumer(consumer, b"World")
-        assert consumer.get_value() == b"World"
+        generator_exit_checkpoint.assert_not_called()
 
         # Act
         consumer.clear()
 
         # Assert
+        generator_exit_checkpoint.assert_called_once_with()
         assert consumer.get_value() is None
```

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_tools/test_typedattr.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_tools/test_typedattr.py`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/tests/unit_test/test_tools/test_utils.py` & `easynetwork-1.0.0rc8/tests/unit_test/test_tools/test_utils.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 from __future__ import annotations
 
 import contextlib
+import itertools
 import math
 import os
 import ssl
 import threading
 from collections import deque
 from collections.abc import Callable
 from errno import EINVAL, ENOTCONN, errorcode as errno_errorcode
@@ -20,20 +21,23 @@
     check_real_socket_state,
     check_socket_family,
     check_socket_is_connected,
     check_socket_no_ssl,
     ensure_datagram_socket_bound,
     error_from_errno,
     exception_with_notes,
+    get_callable_name,
     is_socket_connected,
     is_ssl_eof_error,
     is_ssl_socket,
     iter_bytes,
+    iterate_exceptions,
     lock_with_timeout,
     make_callback,
+    missing_extra_deps,
     prepend_argument,
     remove_traceback_frames_in_place,
     replace_kwargs,
     set_reuseport,
     supports_socket_sendmsg,
     validate_timeout_delay,
 )
@@ -116,15 +120,28 @@
     cb = make_callback(stub, mocker.sentinel.arg1, mocker.sentinel.arg2, kw1=mocker.sentinel.kw1, kw2=mocker.sentinel.kw2)
 
     # Assert
     assert cb() is mocker.sentinel.ret_val
     stub.assert_called_once_with(mocker.sentinel.arg1, mocker.sentinel.arg2, kw1=mocker.sentinel.kw1, kw2=mocker.sentinel.kw2)
 
 
-def test____prepend_argument____add_positional_argument(mocker: MockerFixture) -> None:
+def test____prepend_argument____add_positional_argument____direct_call(mocker: MockerFixture) -> None:
+    # Arrange
+    stub = mocker.stub()
+    stub.return_value = mocker.sentinel.ret_val
+
+    # Act
+    cb = prepend_argument(mocker.sentinel.first_arg, stub)
+
+    # Assert
+    assert cb(mocker.sentinel.arg1, kw1=mocker.sentinel.kw1) is mocker.sentinel.ret_val
+    stub.assert_called_once_with(mocker.sentinel.first_arg, mocker.sentinel.arg1, kw1=mocker.sentinel.kw1)
+
+
+def test____prepend_argument____add_positional_argument____decorator(mocker: MockerFixture) -> None:
     # Arrange
     stub = mocker.stub()
     stub.return_value = mocker.sentinel.ret_val
 
     # Act
     cb = prepend_argument(mocker.sentinel.first_arg)(stub)
 
@@ -150,14 +167,65 @@
         mocker.sentinel.first_arg,
         mocker.sentinel.second_arg,
         mocker.sentinel.arg1,
         kw1=mocker.sentinel.kw1,
     )
 
 
+@pytest.mark.parametrize("module", ["package.module", None], ids=lambda p: f"module=={p!r}")
+def test____get_callable_name____qualname(module: str | None, mocker: MockerFixture) -> None:
+    # Arrange
+    func = mocker.stub()
+    func.__name__ = "func"
+    func.__qualname__ = "namespace.func"
+    func.__module__ = module or ""
+
+    # Act
+    name = get_callable_name(func)
+
+    # Assert
+    if module:
+        assert name == f"{module}.namespace.func"
+    else:
+        assert name == "namespace.func"
+
+
+@pytest.mark.parametrize("module", ["package.module", None], ids=lambda p: f"module=={p!r}")
+def test____get_callable_name____name_without_qualname(module: str | None, mocker: MockerFixture) -> None:
+    # Arrange
+    func = mocker.stub()
+    func.__name__ = "func"
+    del func.__qualname__
+    func.__module__ = module or ""
+
+    # Act
+    name = get_callable_name(func)
+
+    # Assert
+    if module:
+        assert name == f"{module}.func"
+    else:
+        assert name == "func"
+
+
+@pytest.mark.parametrize("module", ["package.module", None], ids=lambda p: f"module=={p!r}")
+def test____get_callable_name____neither_name_nor_qualname(module: str | None, mocker: MockerFixture) -> None:
+    # Arrange
+    func = mocker.stub()
+    del func.__name__
+    del func.__qualname__
+    func.__module__ = module or ""
+
+    # Act
+    name = get_callable_name(func)
+
+    # Assert
+    assert name == ""
+
+
 def test____error_from_errno____returns_OSError(mocker: MockerFixture) -> None:
     # Arrange
     errno: int = 123456
     mock_strerror = mocker.patch("os.strerror", return_value="errno message")
 
     # Act
     exception = error_from_errno(errno)
@@ -165,14 +233,38 @@
     # Assert
     assert isinstance(exception, OSError)
     assert exception.errno == errno
     assert exception.strerror == "errno message"
     mock_strerror.assert_called_once_with(errno)
 
 
+def test____missing_extra_deps____returns_ModuleNotFoundError() -> None:
+    # Arrange
+
+    # Act
+    exception = missing_extra_deps("cbor")
+
+    # Assert
+    assert isinstance(exception, ModuleNotFoundError)
+    assert exception.args[0] == "cbor dependencies are missing. Consider adding 'cbor' extra"
+    assert exception.__notes__ == ['example: pip install "easynetwork[cbor]"']
+
+
+def test____missing_extra_deps____returns_ModuleNotFoundError____specify_feature_name() -> None:
+    # Arrange
+
+    # Act
+    exception = missing_extra_deps("msgpack", feature_name="message-pack")
+
+    # Assert
+    assert isinstance(exception, ModuleNotFoundError)
+    assert exception.args[0] == "message-pack dependencies are missing. Consider adding 'msgpack' extra"
+    assert exception.__notes__ == ['example: pip install "easynetwork[msgpack]"']
+
+
 def test____check_real_socket_state____socket_without_error(mock_tcp_socket: MagicMock, mocker: MockerFixture) -> None:
     # Arrange
     mock_tcp_socket.getsockopt.return_value = 0
     mock_error_from_errno = mocker.patch(f"{error_from_errno.__module__}.{error_from_errno.__qualname__}")
 
     # Act
     check_real_socket_state(mock_tcp_socket)
@@ -599,14 +691,48 @@
         assert len(list(traceback.walk_tb(exception.__traceback__))) == 3 - n
     elif n < 0:
         assert len(list(traceback.walk_tb(exception.__traceback__))) == 3
     else:  # n > 3
         assert len(list(traceback.walk_tb(exception.__traceback__))) == 0
 
 
+def test____iterate_exceptions____yield_exception() -> None:
+    # Arrange
+    exception = BaseException()
+
+    # Act
+    all_exceptions = list(iterate_exceptions(exception))
+
+    # Assert
+    assert all_exceptions == [exception]
+
+
+def test____iterate_exceptions____yield_exceptions_in_group() -> None:
+    # Arrange
+    excgrp = BaseExceptionGroup("", [BaseException(), Exception()])
+
+    # Act
+    all_exceptions = list(iterate_exceptions(excgrp))
+
+    # Assert
+    assert all_exceptions == list(excgrp.exceptions)
+
+
+def test____iterate_exceptions____recursive_yield_exceptions_in_group() -> None:
+    # Arrange
+    sub_excgrp1 = BaseExceptionGroup("", [BaseException(), Exception()])
+    sub_excgrp2 = BaseExceptionGroup("", [BaseException(), Exception()])
+
+    # Act
+    all_exceptions = list(iterate_exceptions(BaseExceptionGroup("", [sub_excgrp1, sub_excgrp2])))
+
+    # Assert
+    assert all_exceptions == list(itertools.chain(sub_excgrp1.exceptions, sub_excgrp2.exceptions))
+
+
 def test____ElapsedTime____catch_elapsed_time(mocker: MockerFixture) -> None:
     # Arrange
     now: float = 798546132.0
     mocker.patch("time.perf_counter", autospec=True, side_effect=[now, now + 12.0])
 
     # Act
     with ElapsedTime() as elapsed:
```

### Comparing `easynetwork-1.0.0rc7/.gitignore` & `easynetwork-1.0.0rc8/.gitignore`

 * *Files 10% similar despite different names*

```diff
@@ -1,16 +1,18 @@
 # Workspace
-.vscode/*
-!.vscode/settings.example.json
+.vscode/settings.json
 
 # Auto-generated files at build time
 src/easynetwork/version.py
 
-# Compiled requirements
-requirements*.txt
+# Benchmark reports
+benchmark_reports
+benchmark*.json
+benchmark*.html
+benchmark*.svg
 
 # Byte-compiled / optimized / DLL files
 __pycache__/
 *.py[cod]
 *$py.class
 
 # pdm
```

### Comparing `easynetwork-1.0.0rc7/LICENSE` & `easynetwork-1.0.0rc8/LICENSE`

 * *Files identical despite different names*

### Comparing `easynetwork-1.0.0rc7/README.md` & `easynetwork-1.0.0rc8/README.md`

 * *Files 22% similar despite different names*

```diff
@@ -37,34 +37,35 @@
 
 ## Overview
 EasyNetwork completely encapsulates the socket handling, providing you with a higher level interface
 that allows an application/software to completely handle the logic part with Python objects,
 without worrying about how to process, send or receive data over the network.
 
 The communication protocol can be whatever you want, be it JSON, Pickle, ASCII, structure, base64 encoded,
-compressed, encrypted, or any other format that is not part of the standard library.
+compressed, or any other format that is not part of the standard library.
 You choose the data format and the library takes care of the rest.
 
+This project is especially useful for simple **message** exchange between clients and servers.
+
 Works with TCP and UDP.
 
+Interested ? Here is the documentation : https://easynetwork.readthedocs.io/
+
 ## Usage
 ### TCP Echo server with JSON data
 ```py
 import asyncio
 import logging
 from collections.abc import AsyncGenerator
 from typing import Any, TypeAlias
 
-from easynetwork.api_async.server import (
-    AsyncStreamClient,
-    AsyncStreamRequestHandler,
-    AsyncTCPNetworkServer,
-)
 from easynetwork.protocol import StreamProtocol
 from easynetwork.serializers import JSONSerializer
+from easynetwork.servers import AsyncTCPNetworkServer
+from easynetwork.servers.handlers import AsyncStreamClient, AsyncStreamRequestHandler
 
 # These TypeAliases are there to help you understand
 # where requests and responses are used in the code
 RequestType: TypeAlias = Any
 ResponseType: TypeAlias = Any
 
 
@@ -116,15 +117,15 @@
     asyncio.run(main())
 ```
 
 ### TCP Echo client with JSON data
 ```py
 from typing import Any, TypeAlias
 
-from easynetwork.api_sync.client import TCPNetworkClient
+from easynetwork.clients import TCPNetworkClient
 from easynetwork.protocol import StreamProtocol
 from easynetwork.serializers import JSONSerializer
 
 RequestType: TypeAlias = Any
 ResponseType: TypeAlias = Any
 
 
@@ -145,28 +146,18 @@
 ```
 
 <details markdown="1">
 <summary>Asynchronous version ( with <code>async def</code> )</summary>
 
 ```py
 import asyncio
-from typing import Any, TypeAlias
-
-from easynetwork.api_async.client import AsyncTCPNetworkClient
-from easynetwork.protocol import StreamProtocol
-from easynetwork.serializers import JSONSerializer
-
-RequestType: TypeAlias = Any
-ResponseType: TypeAlias = Any
 
+from easynetwork.clients import AsyncTCPNetworkClient
 
-class JSONProtocol(StreamProtocol[RequestType, ResponseType]):
-    def __init__(self) -> None:
-        super().__init__(JSONSerializer())
-
+...
 
 async def main() -> None:
     async with AsyncTCPNetworkClient(("localhost", 9000), JSONProtocol()) as client:
         await client.send_packet({"data": {"my_body": ["as json"]}})
         response = await client.recv_packet()  # response should be the sent dictionary
         print(response)  # prints {'data': {'my_body': ['as json']}}
 
@@ -175,7 +166,31 @@
     asyncio.run(main())
 ```
 
 </details>
 
 ## License
 This project is licensed under the terms of the [Apache Software License 2.0](https://github.com/francis-clairicia/EasyNetwork/blob/main/LICENSE).
+
+### `easynetwork.lowlevel.typed_attr`
+
+AnyIO's typed attributes incorporated in `easynetwork.lowlevel.typed_attr` from [anyio 4.2](https://github.com/agronholm/anyio/tree/4.2.0), which is distributed under the MIT license:
+```
+Copyright (c) 2018 Alex Grönholm
+
+Permission is hereby granted, free of charge, to any person obtaining a copy of
+this software and associated documentation files (the "Software"), to deal in
+the Software without restriction, including without limitation the rights to
+use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of
+the Software, and to permit persons to whom the Software is furnished to do so,
+subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS
+FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR
+COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER
+IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+```
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `easynetwork-1.0.0rc7/pyproject.toml` & `easynetwork-1.0.0rc8/pyproject.toml`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 [build-system]
-requires = ["hatchling >=1.13,<2", "hatch-vcs~=0.4.0"]
+requires = ["hatchling >=1.20,<2", "hatch-vcs~=0.4.0"]
 build-backend = "hatchling.build"
 
 [project]
 name = "easynetwork"
 description = "The easiest way to use sockets in Python"
 authors = [
     {name = "FrankySnow9", email = "clairicia.rcj.francis@gmail.com"}
@@ -51,107 +51,113 @@
 [project.optional-dependencies]
 cbor = [
     "cbor2>=5.5,<6",
 ]
 msgpack = [
     "msgpack>=1.0.7,<2",
 ]
-encryption = [
-    "cryptography>=39",
-]
 sniffio = [
-    "sniffio>=1.3.0",
+    "sniffio>=1.3.0,<2",
 ]
 
 ############################ pdm configuration ############################
 
 [tool.pdm.dev-dependencies]
 dev = [
     "bump2version>=1.0.1",
-    "hatch~=1.7.0",
+    "hatch>=1.8",
     "hatch-vcs~=0.4.0",
 ]
 build = [
     "build>=1.0.3",
-    "twine>=4.0.2",
 ]
 pre-commit = [
     "pre-commit>=2.20.0",
 ]
 uvloop = [
-    "uvloop~=0.19.0; sys_platform!='win32'",
+    "uvloop~=0.19.0; os_name=='posix'",
 ]
 tox = [
     "tox<5,>=4.0.8",
-    "tox-pdm~=0.7.0",
+    "tox-pdm~=0.7.2",
 ]
 format = [
     "isort!=5.11.0,>=5.10.1",
     "black>=22.6.0",
 ]
 mypy = [
-    "mypy~=1.7.0",
+    "mypy~=1.8",
 ]
 types-msgpack = [
     "msgpack-types>=0.2.0",
 ]
-types-encryption = [
-    "types-cryptography>=3.3.23.2",
-]
 flake8 = [
     "flake8>=6.0",
-    "flake8-pyi>=22.11.0",
     "flake8-dunder-all @ git+https://github.com/francis-clairicia/flake8-dunder-all.git@v0.1.0",
 ]
 bandit = [
     "bandit>=1.7.5",
 ]
 test = [
     "pytest~=7.4,>=7.4.2",
     "pytest-xdist~=3.3",
     "pytest-mock~=3.10",
     "pytest-cov~=4.0",
     "pytest-asyncio~=0.21.0",
     "trove-classifiers>=2023.11.9",
     "trustme~=1.0",
+    "pytest-retry~=1.6",
 ]
 coverage = [
     "coverage~=7.0",
 ]
 doc = [
     "sphinx>=6.2.1,<7",
     "sphinx-rtd-theme>=1.2.2",
     "sphinx-tabs>=3.4.1",
     "sphinx-toolbox>=3.5.0",
     "enum-tools[sphinx]>=0.10.0",
 ]
+micro-benchmark = [
+    "pytest~=7.4",
+    "pytest-benchmark[histogram]~=4.0",
+]
+benchmark-servers = [
+    "build~=1.0",
+    "docker~=7.0",
+    "plotly~=5.18",
+]
+benchmark-servers-deps = [
+    "uvloop~=0.19.0; os_name=='posix'",
+    "asyncio-dgram~=2.1",
+]
 
 ############################ hatchling configuration ############################
 
 [tool.hatch.build]
 exclude = [
   ".github/",
   ".vscode/",
-  ".bumpversion.cfg",
   ".readthedocs.yaml",
 ]
 
 [tool.hatch.build.targets.wheel]
 packages = ["src/easynetwork"]
 
 [tool.hatch.version]
 source = "vcs"
+fallback-version = "1.0.0rc8"
 
 [tool.hatch.version.raw-options]  # setuptools_scm options
 local_scheme =  "no-local-version"
 
 [tool.hatch.build.hooks.vcs]
 version-file = "src/easynetwork/version.py"
 template = '''
-# Copyright 2021-2023, Francis Clairicia-Rose-Claire-Josephine
+# Copyright 2021-2024, Francis Clairicia-Rose-Claire-Josephine
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
@@ -211,19 +217,24 @@
 no_implicit_optional = true
 warn_redundant_casts = true
 warn_unused_ignores = true
 warn_no_return = true
 warn_unreachable = true
 no_implicit_reexport = true
 strict_equality = true
+local_partial_types = true
 enable_error_code = ["truthy-bool", "ignore-without-code", "unused-awaitable"]
 
+[[tool.mypy.overrides]]
+module = ["docker.*", "plotly.*", "pytest_benchmark.*"]
+ignore_missing_imports = true
+
 [tool.pytest.ini_options]
 asyncio_mode = "strict"  # Avoid some unwanted behaviour
-addopts = "--strict-markers -p 'no:anyio'"  # hatch CLI dependencies installs anyio
+addopts = "--strict-markers -p 'no:anyio' -p 'no:benchmark'"  # hatch CLI dependencies installs anyio
 minversion = "7.1.2"
 testpaths = ["tests"]
 norecursedirs = ["scripts"]
 markers = [
     "slow: marks tests as slow",
     "functional: marks tests as functional tests",
     "unit: marks tests as unit tests",
@@ -267,7 +278,17 @@
     # Don't complain about type hinting code:
     "if (typing\\.)?TYPE_CHECKING:",
     "@(typing\\.)?overload",
 
     # Don't complain about abstract methods, they aren't run:
     "@(abc\\.)?abstractmethod",
 ]
+partial_branches = [
+    # Have to re-enable the standard pragma
+    "pragma: no branch",
+
+    # Don't complain about code in debug mode, they are always run in unit tests
+    "if __debug__:",
+]
+
+[tool.coverage.html]
+show_contexts = "${COVERAGE_SHOWCONTEXTS-false}"
```

### Comparing `easynetwork-1.0.0rc7/PKG-INFO` & `easynetwork-1.0.0rc8/PKG-INFO`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
-Metadata-Version: 2.1
+Metadata-Version: 2.3
 Name: easynetwork
-Version: 1.0.0rc7
+Version: 1.0.0rc8
 Summary: The easiest way to use sockets in Python
 Project-URL: Documentation, https://easynetwork.readthedocs.io/
 Project-URL: Issue Tracker, https://github.com/francis-clairicia/EasyNetwork/issues
 Project-URL: Release Notes, https://github.com/francis-clairicia/EasyNetwork/releases
 Project-URL: Source Code, https://github.com/francis-clairicia/EasyNetwork
 Author-email: FrankySnow9 <clairicia.rcj.francis@gmail.com>
 License-File: LICENSE
@@ -24,20 +24,18 @@
 Classifier: Topic :: Internet
 Classifier: Topic :: Software Development :: Libraries :: Python Modules
 Classifier: Topic :: System :: Networking
 Classifier: Typing :: Typed
 Requires-Python: >=3.11
 Provides-Extra: cbor
 Requires-Dist: cbor2<6,>=5.5; extra == 'cbor'
-Provides-Extra: encryption
-Requires-Dist: cryptography>=39; extra == 'encryption'
 Provides-Extra: msgpack
 Requires-Dist: msgpack<2,>=1.0.7; extra == 'msgpack'
 Provides-Extra: sniffio
-Requires-Dist: sniffio>=1.3.0; extra == 'sniffio'
+Requires-Dist: sniffio<2,>=1.3.0; extra == 'sniffio'
 Description-Content-Type: text/markdown
 
 # EasyNetwork
 
 The easiest way to use sockets in Python!
 
 [![PyPI](https://img.shields.io/pypi/v/easynetwork)](https://pypi.org/project/easynetwork/)
@@ -75,34 +73,35 @@
 
 ## Overview
 EasyNetwork completely encapsulates the socket handling, providing you with a higher level interface
 that allows an application/software to completely handle the logic part with Python objects,
 without worrying about how to process, send or receive data over the network.
 
 The communication protocol can be whatever you want, be it JSON, Pickle, ASCII, structure, base64 encoded,
-compressed, encrypted, or any other format that is not part of the standard library.
+compressed, or any other format that is not part of the standard library.
 You choose the data format and the library takes care of the rest.
 
+This project is especially useful for simple **message** exchange between clients and servers.
+
 Works with TCP and UDP.
 
+Interested ? Here is the documentation : https://easynetwork.readthedocs.io/
+
 ## Usage
 ### TCP Echo server with JSON data
 ```py
 import asyncio
 import logging
 from collections.abc import AsyncGenerator
 from typing import Any, TypeAlias
 
-from easynetwork.api_async.server import (
-    AsyncStreamClient,
-    AsyncStreamRequestHandler,
-    AsyncTCPNetworkServer,
-)
 from easynetwork.protocol import StreamProtocol
 from easynetwork.serializers import JSONSerializer
+from easynetwork.servers import AsyncTCPNetworkServer
+from easynetwork.servers.handlers import AsyncStreamClient, AsyncStreamRequestHandler
 
 # These TypeAliases are there to help you understand
 # where requests and responses are used in the code
 RequestType: TypeAlias = Any
 ResponseType: TypeAlias = Any
 
 
@@ -154,15 +153,15 @@
     asyncio.run(main())
 ```
 
 ### TCP Echo client with JSON data
 ```py
 from typing import Any, TypeAlias
 
-from easynetwork.api_sync.client import TCPNetworkClient
+from easynetwork.clients import TCPNetworkClient
 from easynetwork.protocol import StreamProtocol
 from easynetwork.serializers import JSONSerializer
 
 RequestType: TypeAlias = Any
 ResponseType: TypeAlias = Any
 
 
@@ -183,28 +182,18 @@
 ```
 
 <details markdown="1">
 <summary>Asynchronous version ( with <code>async def</code> )</summary>
 
 ```py
 import asyncio
-from typing import Any, TypeAlias
-
-from easynetwork.api_async.client import AsyncTCPNetworkClient
-from easynetwork.protocol import StreamProtocol
-from easynetwork.serializers import JSONSerializer
-
-RequestType: TypeAlias = Any
-ResponseType: TypeAlias = Any
 
+from easynetwork.clients import AsyncTCPNetworkClient
 
-class JSONProtocol(StreamProtocol[RequestType, ResponseType]):
-    def __init__(self) -> None:
-        super().__init__(JSONSerializer())
-
+...
 
 async def main() -> None:
     async with AsyncTCPNetworkClient(("localhost", 9000), JSONProtocol()) as client:
         await client.send_packet({"data": {"my_body": ["as json"]}})
         response = await client.recv_packet()  # response should be the sent dictionary
         print(response)  # prints {'data': {'my_body': ['as json']}}
 
@@ -213,7 +202,31 @@
     asyncio.run(main())
 ```
 
 </details>
 
 ## License
 This project is licensed under the terms of the [Apache Software License 2.0](https://github.com/francis-clairicia/EasyNetwork/blob/main/LICENSE).
+
+### `easynetwork.lowlevel.typed_attr`
+
+AnyIO's typed attributes incorporated in `easynetwork.lowlevel.typed_attr` from [anyio 4.2](https://github.com/agronholm/anyio/tree/4.2.0), which is distributed under the MIT license:
+```
+Copyright (c) 2018 Alex Grönholm
+
+Permission is hereby granted, free of charge, to any person obtaining a copy of
+this software and associated documentation files (the "Software"), to deal in
+the Software without restriction, including without limitation the rights to
+use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of
+the Software, and to permit persons to whom the Software is furnished to do so,
+subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS
+FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR
+COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER
+IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+```
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

