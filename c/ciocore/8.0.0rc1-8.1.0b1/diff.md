# Comparing `tmp/ciocore-8.0.0rc1-py2.py3-none-any.whl.zip` & `tmp/ciocore-8.1.0b1-py2.py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,129 +1,53 @@
-Zip file size: 843715 bytes, number of entries: 127
--rw-r--r--  2.0 unx       10 b- defN 24-Apr-06 00:12 ciocore/VERSION
--rw-r--r--  2.0 unx      646 b- defN 24-Apr-06 00:11 ciocore/__init__.py
--rw-r--r--  2.0 unx    24599 b- defN 24-Apr-06 00:11 ciocore/api_client.py
--rw-r--r--  2.0 unx    15509 b- defN 24-Apr-06 00:11 ciocore/cli.py
--rw-r--r--  2.0 unx    12978 b- defN 24-Apr-06 00:11 ciocore/client_db.py
--rw-r--r--  2.0 unx    14982 b- defN 24-Apr-06 00:11 ciocore/common.py
--rw-r--r--  2.0 unx      256 b- defN 24-Apr-06 00:11 ciocore/compat.py
--rw-r--r--  2.0 unx     9382 b- defN 24-Apr-06 00:11 ciocore/conductor_submit.py
--rw-r--r--  2.0 unx     8973 b- defN 24-Apr-06 00:11 ciocore/config.py
--rw-r--r--  2.0 unx     7027 b- defN 24-Apr-06 00:11 ciocore/data.py
--rw-r--r--  2.0 unx     1504 b- defN 24-Apr-06 00:11 ciocore/exceptions.py
--rw-r--r--  2.0 unx    17176 b- defN 24-Apr-06 00:11 ciocore/file_utils.py
--rw-r--r--  2.0 unx    17047 b- defN 24-Apr-06 00:11 ciocore/hardware_set.py
--rw-r--r--  2.0 unx    15251 b- defN 24-Apr-06 00:11 ciocore/loggeria.py
--rw-r--r--  2.0 unx     7808 b- defN 24-Apr-06 00:11 ciocore/package_environment.py
--rw-r--r--  2.0 unx     5574 b- defN 24-Apr-06 00:11 ciocore/package_query.py
--rw-r--r--  2.0 unx    15649 b- defN 24-Apr-06 00:11 ciocore/package_tree.py
--rw-r--r--  2.0 unx     1000 b- defN 24-Apr-06 00:11 ciocore/post_install.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-06 00:11 ciocore/retry.py
--rw-r--r--  2.0 unx     2344 b- defN 24-Apr-06 00:11 ciocore/validator.py
--rw-r--r--  2.0 unx    21743 b- defN 24-Apr-06 00:11 ciocore/worker.py
--rw-r--r--  2.0 unx      606 b- defN 24-Apr-06 00:11 ciocore/auth/__init__.py
--rw-r--r--  2.0 unx     3892 b- defN 24-Apr-06 00:11 ciocore/auth/server.py
--rw-r--r--  2.0 unx    17207 b- defN 24-Apr-06 00:12 ciocore/docsite/404.html
--rw-r--r--  2.0 unx    20945 b- defN 24-Apr-06 00:12 ciocore/docsite/index.html
--rw-r--r--  2.0 unx     2825 b- defN 24-Apr-06 00:12 ciocore/docsite/logo.png
--rw-r--r--  2.0 unx      758 b- defN 24-Apr-06 00:12 ciocore/docsite/objects.inv
--rw-r--r--  2.0 unx      109 b- defN 24-Apr-06 00:12 ciocore/docsite/sitemap.xml
--rw-r--r--  2.0 unx      127 b- defN 24-Apr-06 00:12 ciocore/docsite/sitemap.xml.gz
--rw-r--r--  2.0 unx   170218 b- defN 24-Apr-06 00:12 ciocore/docsite/apidoc/api_client/index.html
--rw-r--r--  2.0 unx    26171 b- defN 24-Apr-06 00:12 ciocore/docsite/apidoc/apidoc/index.html
--rw-r--r--  2.0 unx    72559 b- defN 24-Apr-06 00:12 ciocore/docsite/apidoc/config/index.html
--rw-r--r--  2.0 unx    50850 b- defN 24-Apr-06 00:12 ciocore/docsite/apidoc/data/index.html
--rw-r--r--  2.0 unx   123042 b- defN 24-Apr-06 00:12 ciocore/docsite/apidoc/hardware_set/index.html
--rw-r--r--  2.0 unx    69248 b- defN 24-Apr-06 00:12 ciocore/docsite/apidoc/package_environment/index.html
--rw-r--r--  2.0 unx   109393 b- defN 24-Apr-06 00:12 ciocore/docsite/apidoc/package_tree/index.html
--rw-r--r--  2.0 unx      341 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/_mkdocstrings.css
--rw-r--r--  2.0 unx     1870 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/images/favicon.png
--rw-r--r--  2.0 unx    97250 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/javascripts/bundle.4e31edb1.min.js
--rw-r--r--  2.0 unx   890964 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/javascripts/bundle.4e31edb1.min.js.map
--rw-r--r--  2.0 unx    22878 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/javascripts/lunr/tinyseg.js
--rw-r--r--  2.0 unx   677463 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/javascripts/lunr/wordcut.js
--rw-r--r--  2.0 unx    17074 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/javascripts/lunr/min/lunr.ar.min.js
--rw-r--r--  2.0 unx     4654 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/javascripts/lunr/min/lunr.da.min.js
--rw-r--r--  2.0 unx     6119 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/javascripts/lunr/min/lunr.de.min.js
--rw-r--r--  2.0 unx     6208 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/javascripts/lunr/min/lunr.du.min.js
--rw-r--r--  2.0 unx    11499 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/javascripts/lunr/min/lunr.es.min.js
--rw-r--r--  2.0 unx     9342 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/javascripts/lunr/min/lunr.fi.min.js
--rw-r--r--  2.0 unx    10669 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/javascripts/lunr/min/lunr.fr.min.js
--rw-r--r--  2.0 unx     3383 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/javascripts/lunr/min/lunr.hi.min.js
--rw-r--r--  2.0 unx     9437 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/javascripts/lunr/min/lunr.hu.min.js
--rw-r--r--  2.0 unx     1264 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/javascripts/lunr/min/lunr.hy.min.js
--rw-r--r--  2.0 unx    11232 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/javascripts/lunr/min/lunr.it.min.js
--rw-r--r--  2.0 unx     2313 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/javascripts/lunr/min/lunr.ja.min.js
--rw-r--r--  2.0 unx       36 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/javascripts/lunr/min/lunr.jp.min.js
--rw-r--r--  2.0 unx     3494 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/javascripts/lunr/min/lunr.kn.min.js
--rw-r--r--  2.0 unx     7972 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/javascripts/lunr/min/lunr.ko.min.js
--rw-r--r--  2.0 unx      817 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/javascripts/lunr/min/lunr.multi.min.js
--rw-r--r--  2.0 unx     6026 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/javascripts/lunr/min/lunr.nl.min.js
--rw-r--r--  2.0 unx     4754 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/javascripts/lunr/min/lunr.no.min.js
--rw-r--r--  2.0 unx    10171 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/javascripts/lunr/min/lunr.pt.min.js
--rw-r--r--  2.0 unx    10958 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/javascripts/lunr/min/lunr.ro.min.js
--rw-r--r--  2.0 unx    10331 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/javascripts/lunr/min/lunr.ru.min.js
--rw-r--r--  2.0 unx     4901 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/javascripts/lunr/min/lunr.sa.min.js
--rw-r--r--  2.0 unx     3647 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/javascripts/lunr/min/lunr.stemmer.support.min.js
--rw-r--r--  2.0 unx     4523 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/javascripts/lunr/min/lunr.sv.min.js
--rw-r--r--  2.0 unx     2406 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/javascripts/lunr/min/lunr.ta.min.js
--rw-r--r--  2.0 unx     2330 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/javascripts/lunr/min/lunr.te.min.js
--rw-r--r--  2.0 unx     1031 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/javascripts/lunr/min/lunr.th.min.js
--rw-r--r--  2.0 unx    15009 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/javascripts/lunr/min/lunr.tr.min.js
--rw-r--r--  2.0 unx      784 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/javascripts/lunr/min/lunr.vi.min.js
--rw-r--r--  2.0 unx     2158 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/javascripts/lunr/min/lunr.zh.min.js
--rw-r--r--  2.0 unx    39431 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/javascripts/workers/search.dfff1995.min.js
--rw-r--r--  2.0 unx   214982 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/javascripts/workers/search.dfff1995.min.js.map
--rw-r--r--  2.0 unx   122334 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/stylesheets/main.83068744.min.css
--rw-r--r--  2.0 unx    42278 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/stylesheets/main.83068744.min.css.map
--rw-r--r--  2.0 unx    12245 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/stylesheets/palette.ecc896b0.min.css
--rw-r--r--  2.0 unx     3639 b- defN 24-Apr-06 00:12 ciocore/docsite/assets/stylesheets/palette.ecc896b0.min.css.map
--rw-r--r--  2.0 unx    20051 b- defN 24-Apr-06 00:12 ciocore/docsite/cmdline/docs/index.html
--rw-r--r--  2.0 unx    23171 b- defN 24-Apr-06 00:12 ciocore/docsite/cmdline/downloader/index.html
--rw-r--r--  2.0 unx    20923 b- defN 24-Apr-06 00:12 ciocore/docsite/cmdline/packages/index.html
--rw-r--r--  2.0 unx    25123 b- defN 24-Apr-06 00:12 ciocore/docsite/cmdline/uploader/index.html
--rw-r--r--  2.0 unx    20100 b- defN 24-Apr-06 00:12 ciocore/docsite/how-to-guides/index.html
--rw-r--r--  2.0 unx   182633 b- defN 24-Apr-06 00:12 ciocore/docsite/search/search_index.json
--rw-r--r--  2.0 unx      354 b- defN 24-Apr-06 00:12 ciocore/docsite/stylesheets/extra.css
--rw-r--r--  2.0 unx     3235 b- defN 24-Apr-06 00:12 ciocore/docsite/stylesheets/tables.css
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-06 00:11 ciocore/downloader/__init__.py
--rw-r--r--  2.0 unx    25784 b- defN 24-Apr-06 00:11 ciocore/downloader/base_downloader.py
--rw-r--r--  2.0 unx     1612 b- defN 24-Apr-06 00:11 ciocore/downloader/download_runner_base.py
--rw-r--r--  2.0 unx     5697 b- defN 24-Apr-06 00:11 ciocore/downloader/job_downloader.py
--rw-r--r--  2.0 unx    51158 b- defN 24-Apr-06 00:11 ciocore/downloader/legacy_downloader.py
--rw-r--r--  2.0 unx     2105 b- defN 24-Apr-06 00:11 ciocore/downloader/log.py
--rw-r--r--  2.0 unx     2865 b- defN 24-Apr-06 00:11 ciocore/downloader/logging_download_runner.py
--rw-r--r--  2.0 unx     2105 b- defN 24-Apr-06 00:11 ciocore/downloader/perpetual_downloader.py
--rw-r--r--  2.0 unx     2903 b- defN 24-Apr-06 00:11 ciocore/downloader/registry.py
--rw-r--r--  2.0 unx     4501 b- defN 24-Apr-06 00:11 ciocore/downloader/reporter.py
--rw-r--r--  2.0 unx       24 b- defN 24-Apr-06 00:11 ciocore/uploader/__init__.py
--rw-r--r--  2.0 unx    37353 b- defN 24-Apr-06 00:11 ciocore/uploader/_uploader.py
--rw-r--r--  2.0 unx     5309 b- defN 24-Apr-06 00:11 ciocore/uploader/upload_stats/__init__.py
--rw-r--r--  2.0 unx     2402 b- defN 24-Apr-06 00:11 ciocore/uploader/upload_stats/stats_formats.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-06 00:11 tests/__init__.py
--rw-r--r--  2.0 unx     1119 b- defN 24-Apr-06 00:11 tests/extra_env_fixtures.py
--rw-r--r--  2.0 unx     4320 b- defN 24-Apr-06 00:11 tests/instance_type_fixtures.py
--rw-r--r--  2.0 unx     5070 b- defN 24-Apr-06 00:11 tests/package_fixtures.py
--rw-r--r--  2.0 unx      138 b- defN 24-Apr-06 00:11 tests/project_fixtures.py
--rw-r--r--  2.0 unx     6757 b- defN 24-Apr-06 00:11 tests/test_api_client.py
--rw-r--r--  2.0 unx     3603 b- defN 24-Apr-06 00:11 tests/test_base_downloader.py
--rw-r--r--  2.0 unx     6088 b- defN 24-Apr-06 00:11 tests/test_cli.py
--rw-r--r--  2.0 unx      763 b- defN 24-Apr-06 00:11 tests/test_common.py
--rw-r--r--  2.0 unx    13340 b- defN 24-Apr-06 00:11 tests/test_config.py
--rw-r--r--  2.0 unx     5433 b- defN 24-Apr-06 00:11 tests/test_data.py
--rw-r--r--  2.0 unx     4721 b- defN 24-Apr-06 00:11 tests/test_downloader.py
--rw-r--r--  2.0 unx     4408 b- defN 24-Apr-06 00:11 tests/test_hardware_set.py
--rw-r--r--  2.0 unx      507 b- defN 24-Apr-06 00:11 tests/test_imports_2and3.py
--rw-r--r--  2.0 unx     7025 b- defN 24-Apr-06 00:11 tests/test_job_downloader.py
--rw-r--r--  2.0 unx     5893 b- defN 24-Apr-06 00:11 tests/test_package_environment.py
--rw-r--r--  2.0 unx     6780 b- defN 24-Apr-06 00:11 tests/test_package_tree.py
--rw-r--r--  2.0 unx     4940 b- defN 24-Apr-06 00:11 tests/test_submit.py
--rw-r--r--  2.0 unx     2914 b- defN 24-Apr-06 00:11 tests/test_uploader.py
--rw-r--r--  2.0 unx     1526 b- defN 24-Apr-06 00:11 tests/test_validator.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-06 00:11 tests/mocks/__init__.py
--rw-r--r--  2.0 unx      215 b- defN 24-Apr-06 00:11 tests/mocks/glob.py
--rw-r--r--  2.0 unx    18196 b- defN 24-Apr-06 00:12 ciocore-8.0.0rc1.dist-info/METADATA
--rw-r--r--  2.0 unx      110 b- defN 24-Apr-06 00:12 ciocore-8.0.0rc1.dist-info/WHEEL
--rw-r--r--  2.0 unx       47 b- defN 24-Apr-06 00:12 ciocore-8.0.0rc1.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       14 b- defN 24-Apr-06 00:12 ciocore-8.0.0rc1.dist-info/top_level.txt
--rw-rw-r--  2.0 unx    12091 b- defN 24-Apr-06 00:12 ciocore-8.0.0rc1.dist-info/RECORD
-127 files, 3733051 bytes uncompressed, 824263 bytes compressed:  77.9%
+Zip file size: 107876 bytes, number of entries: 51
+-rw-r--r--  2.0 unx       12 b- defN 23-Dec-08 16:44 ciocore/VERSION
+-rw-r--r--  2.0 unx      241 b- defN 23-Dec-08 16:43 ciocore/__about__.py
+-rw-r--r--  2.0 unx       25 b- defN 23-Dec-08 16:43 ciocore/__init__.py
+-rw-r--r--  2.0 unx    17986 b- defN 23-Dec-08 16:43 ciocore/api_client.py
+-rw-r--r--  2.0 unx    12978 b- defN 23-Dec-08 16:43 ciocore/client_db.py
+-rw-r--r--  2.0 unx    14725 b- defN 23-Dec-08 16:43 ciocore/common.py
+-rw-r--r--  2.0 unx      256 b- defN 23-Dec-08 16:43 ciocore/compat.py
+-rw-r--r--  2.0 unx     9382 b- defN 23-Dec-08 16:43 ciocore/conductor_submit.py
+-rw-r--r--  2.0 unx     5998 b- defN 23-Dec-08 16:43 ciocore/config.py
+-rw-r--r--  2.0 unx     8655 b- defN 23-Dec-08 16:43 ciocore/data.py
+-rw-r--r--  2.0 unx    51228 b- defN 23-Dec-08 16:43 ciocore/downloader.py
+-rw-r--r--  2.0 unx     1504 b- defN 23-Dec-08 16:43 ciocore/exceptions.py
+-rw-r--r--  2.0 unx    17174 b- defN 23-Dec-08 16:43 ciocore/file_utils.py
+-rw-r--r--  2.0 unx    17051 b- defN 23-Dec-08 16:43 ciocore/hardware_set.py
+-rw-r--r--  2.0 unx    15251 b- defN 23-Dec-08 16:43 ciocore/loggeria.py
+-rw-r--r--  2.0 unx     7238 b- defN 23-Dec-08 16:43 ciocore/package_environment.py
+-rw-r--r--  2.0 unx    16277 b- defN 23-Dec-08 16:43 ciocore/package_tree.py
+-rw-r--r--  2.0 unx     1000 b- defN 23-Dec-08 16:43 ciocore/post_install.py
+-rw-r--r--  2.0 unx     2344 b- defN 23-Dec-08 16:43 ciocore/validator.py
+-rw-r--r--  2.0 unx    21743 b- defN 23-Dec-08 16:43 ciocore/worker.py
+-rw-r--r--  2.0 unx      606 b- defN 23-Dec-08 16:43 ciocore/auth/__init__.py
+-rw-r--r--  2.0 unx     3892 b- defN 23-Dec-08 16:43 ciocore/auth/server.py
+-rw-r--r--  2.0 unx       26 b- defN 23-Dec-08 16:43 ciocore/cli/__init__.py
+-rw-r--r--  2.0 unx    11096 b- defN 23-Dec-08 16:43 ciocore/cli/conductor.py
+-rw-r--r--  2.0 unx       24 b- defN 23-Dec-08 16:43 ciocore/uploader/__init__.py
+-rw-r--r--  2.0 unx    37687 b- defN 23-Dec-08 16:43 ciocore/uploader/_uploader.py
+-rw-r--r--  2.0 unx     5309 b- defN 23-Dec-08 16:43 ciocore/uploader/upload_stats/__init__.py
+-rw-r--r--  2.0 unx     2402 b- defN 23-Dec-08 16:43 ciocore/uploader/upload_stats/stats_formats.py
+-rwxr-xr-x  2.0 unx      409 b- defN 23-Dec-08 16:44 ciocore-8.1.0b1.data/scripts/conductor
+-rwxr-xr-x  2.0 unx      447 b- defN 23-Dec-08 16:43 ciocore-8.1.0b1.data/scripts/conductor.bat
+-rw-r--r--  2.0 unx        0 b- defN 23-Dec-08 16:43 tests/__init__.py
+-rw-r--r--  2.0 unx     3630 b- defN 23-Dec-08 16:43 tests/instance_type_fixtures.py
+-rw-r--r--  2.0 unx     5070 b- defN 23-Dec-08 16:43 tests/package_fixtures.py
+-rw-r--r--  2.0 unx     1402 b- defN 23-Dec-08 16:43 tests/test_api_client.py
+-rw-r--r--  2.0 unx      747 b- defN 23-Dec-08 16:43 tests/test_common.py
+-rw-r--r--  2.0 unx    12863 b- defN 23-Dec-08 16:43 tests/test_config.py
+-rw-r--r--  2.0 unx     5809 b- defN 23-Dec-08 16:43 tests/test_data.py
+-rw-r--r--  2.0 unx     3107 b- defN 23-Dec-08 16:43 tests/test_hardware_set.py
+-rw-r--r--  2.0 unx      507 b- defN 23-Dec-08 16:43 tests/test_imports_2and3.py
+-rw-r--r--  2.0 unx     5893 b- defN 23-Dec-08 16:43 tests/test_package_environment.py
+-rw-r--r--  2.0 unx     6780 b- defN 23-Dec-08 16:43 tests/test_package_tree.py
+-rw-r--r--  2.0 unx     4940 b- defN 23-Dec-08 16:43 tests/test_submit.py
+-rw-r--r--  2.0 unx     2914 b- defN 23-Dec-08 16:43 tests/test_uploader.py
+-rw-r--r--  2.0 unx     1526 b- defN 23-Dec-08 16:43 tests/test_validator.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Dec-08 16:43 tests/mocks/__init__.py
+-rw-r--r--  2.0 unx      943 b- defN 23-Dec-08 16:43 tests/mocks/api_client_mock.py
+-rw-r--r--  2.0 unx      215 b- defN 23-Dec-08 16:43 tests/mocks/glob.py
+-rw-r--r--  2.0 unx    15228 b- defN 23-Dec-08 16:44 ciocore-8.1.0b1.dist-info/METADATA
+-rw-r--r--  2.0 unx      110 b- defN 23-Dec-08 16:44 ciocore-8.1.0b1.dist-info/WHEEL
+-rw-r--r--  2.0 unx       14 b- defN 23-Dec-08 16:44 ciocore-8.1.0b1.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     4103 b- defN 23-Dec-08 16:44 ciocore-8.1.0b1.dist-info/RECORD
+51 files, 358767 bytes uncompressed, 101452 bytes compressed:  71.7%
```

## zipnote {}

```diff
@@ -1,17 +1,17 @@
 Filename: ciocore/VERSION
 Comment: 
 
-Filename: ciocore/__init__.py
+Filename: ciocore/__about__.py
 Comment: 
 
-Filename: ciocore/api_client.py
+Filename: ciocore/__init__.py
 Comment: 
 
-Filename: ciocore/cli.py
+Filename: ciocore/api_client.py
 Comment: 
 
 Filename: ciocore/client_db.py
 Comment: 
 
 Filename: ciocore/common.py
 Comment: 
@@ -24,14 +24,17 @@
 
 Filename: ciocore/config.py
 Comment: 
 
 Filename: ciocore/data.py
 Comment: 
 
+Filename: ciocore/downloader.py
+Comment: 
+
 Filename: ciocore/exceptions.py
 Comment: 
 
 Filename: ciocore/file_utils.py
 Comment: 
 
 Filename: ciocore/hardware_set.py
@@ -39,314 +42,83 @@
 
 Filename: ciocore/loggeria.py
 Comment: 
 
 Filename: ciocore/package_environment.py
 Comment: 
 
-Filename: ciocore/package_query.py
-Comment: 
-
 Filename: ciocore/package_tree.py
 Comment: 
 
 Filename: ciocore/post_install.py
 Comment: 
 
-Filename: ciocore/retry.py
-Comment: 
-
 Filename: ciocore/validator.py
 Comment: 
 
 Filename: ciocore/worker.py
 Comment: 
 
 Filename: ciocore/auth/__init__.py
 Comment: 
 
 Filename: ciocore/auth/server.py
 Comment: 
 
-Filename: ciocore/docsite/404.html
-Comment: 
-
-Filename: ciocore/docsite/index.html
-Comment: 
-
-Filename: ciocore/docsite/logo.png
-Comment: 
-
-Filename: ciocore/docsite/objects.inv
-Comment: 
-
-Filename: ciocore/docsite/sitemap.xml
-Comment: 
-
-Filename: ciocore/docsite/sitemap.xml.gz
-Comment: 
-
-Filename: ciocore/docsite/apidoc/api_client/index.html
-Comment: 
-
-Filename: ciocore/docsite/apidoc/apidoc/index.html
-Comment: 
-
-Filename: ciocore/docsite/apidoc/config/index.html
-Comment: 
-
-Filename: ciocore/docsite/apidoc/data/index.html
-Comment: 
-
-Filename: ciocore/docsite/apidoc/hardware_set/index.html
-Comment: 
-
-Filename: ciocore/docsite/apidoc/package_environment/index.html
-Comment: 
-
-Filename: ciocore/docsite/apidoc/package_tree/index.html
-Comment: 
-
-Filename: ciocore/docsite/assets/_mkdocstrings.css
-Comment: 
-
-Filename: ciocore/docsite/assets/images/favicon.png
-Comment: 
-
-Filename: ciocore/docsite/assets/javascripts/bundle.4e31edb1.min.js
-Comment: 
-
-Filename: ciocore/docsite/assets/javascripts/bundle.4e31edb1.min.js.map
-Comment: 
-
-Filename: ciocore/docsite/assets/javascripts/lunr/tinyseg.js
-Comment: 
-
-Filename: ciocore/docsite/assets/javascripts/lunr/wordcut.js
-Comment: 
-
-Filename: ciocore/docsite/assets/javascripts/lunr/min/lunr.ar.min.js
-Comment: 
-
-Filename: ciocore/docsite/assets/javascripts/lunr/min/lunr.da.min.js
-Comment: 
-
-Filename: ciocore/docsite/assets/javascripts/lunr/min/lunr.de.min.js
-Comment: 
-
-Filename: ciocore/docsite/assets/javascripts/lunr/min/lunr.du.min.js
-Comment: 
-
-Filename: ciocore/docsite/assets/javascripts/lunr/min/lunr.es.min.js
-Comment: 
-
-Filename: ciocore/docsite/assets/javascripts/lunr/min/lunr.fi.min.js
-Comment: 
-
-Filename: ciocore/docsite/assets/javascripts/lunr/min/lunr.fr.min.js
-Comment: 
-
-Filename: ciocore/docsite/assets/javascripts/lunr/min/lunr.hi.min.js
-Comment: 
-
-Filename: ciocore/docsite/assets/javascripts/lunr/min/lunr.hu.min.js
-Comment: 
-
-Filename: ciocore/docsite/assets/javascripts/lunr/min/lunr.hy.min.js
-Comment: 
-
-Filename: ciocore/docsite/assets/javascripts/lunr/min/lunr.it.min.js
-Comment: 
-
-Filename: ciocore/docsite/assets/javascripts/lunr/min/lunr.ja.min.js
-Comment: 
-
-Filename: ciocore/docsite/assets/javascripts/lunr/min/lunr.jp.min.js
-Comment: 
-
-Filename: ciocore/docsite/assets/javascripts/lunr/min/lunr.kn.min.js
-Comment: 
-
-Filename: ciocore/docsite/assets/javascripts/lunr/min/lunr.ko.min.js
-Comment: 
-
-Filename: ciocore/docsite/assets/javascripts/lunr/min/lunr.multi.min.js
-Comment: 
-
-Filename: ciocore/docsite/assets/javascripts/lunr/min/lunr.nl.min.js
-Comment: 
-
-Filename: ciocore/docsite/assets/javascripts/lunr/min/lunr.no.min.js
-Comment: 
-
-Filename: ciocore/docsite/assets/javascripts/lunr/min/lunr.pt.min.js
+Filename: ciocore/cli/__init__.py
 Comment: 
 
-Filename: ciocore/docsite/assets/javascripts/lunr/min/lunr.ro.min.js
-Comment: 
-
-Filename: ciocore/docsite/assets/javascripts/lunr/min/lunr.ru.min.js
-Comment: 
-
-Filename: ciocore/docsite/assets/javascripts/lunr/min/lunr.sa.min.js
-Comment: 
-
-Filename: ciocore/docsite/assets/javascripts/lunr/min/lunr.stemmer.support.min.js
-Comment: 
-
-Filename: ciocore/docsite/assets/javascripts/lunr/min/lunr.sv.min.js
-Comment: 
-
-Filename: ciocore/docsite/assets/javascripts/lunr/min/lunr.ta.min.js
-Comment: 
-
-Filename: ciocore/docsite/assets/javascripts/lunr/min/lunr.te.min.js
-Comment: 
-
-Filename: ciocore/docsite/assets/javascripts/lunr/min/lunr.th.min.js
-Comment: 
-
-Filename: ciocore/docsite/assets/javascripts/lunr/min/lunr.tr.min.js
-Comment: 
-
-Filename: ciocore/docsite/assets/javascripts/lunr/min/lunr.vi.min.js
-Comment: 
-
-Filename: ciocore/docsite/assets/javascripts/lunr/min/lunr.zh.min.js
-Comment: 
-
-Filename: ciocore/docsite/assets/javascripts/workers/search.dfff1995.min.js
-Comment: 
-
-Filename: ciocore/docsite/assets/javascripts/workers/search.dfff1995.min.js.map
-Comment: 
-
-Filename: ciocore/docsite/assets/stylesheets/main.83068744.min.css
-Comment: 
-
-Filename: ciocore/docsite/assets/stylesheets/main.83068744.min.css.map
-Comment: 
-
-Filename: ciocore/docsite/assets/stylesheets/palette.ecc896b0.min.css
-Comment: 
-
-Filename: ciocore/docsite/assets/stylesheets/palette.ecc896b0.min.css.map
-Comment: 
-
-Filename: ciocore/docsite/cmdline/docs/index.html
-Comment: 
-
-Filename: ciocore/docsite/cmdline/downloader/index.html
-Comment: 
-
-Filename: ciocore/docsite/cmdline/packages/index.html
-Comment: 
-
-Filename: ciocore/docsite/cmdline/uploader/index.html
-Comment: 
-
-Filename: ciocore/docsite/how-to-guides/index.html
-Comment: 
-
-Filename: ciocore/docsite/search/search_index.json
-Comment: 
-
-Filename: ciocore/docsite/stylesheets/extra.css
-Comment: 
-
-Filename: ciocore/docsite/stylesheets/tables.css
-Comment: 
-
-Filename: ciocore/downloader/__init__.py
-Comment: 
-
-Filename: ciocore/downloader/base_downloader.py
-Comment: 
-
-Filename: ciocore/downloader/download_runner_base.py
-Comment: 
-
-Filename: ciocore/downloader/job_downloader.py
-Comment: 
-
-Filename: ciocore/downloader/legacy_downloader.py
-Comment: 
-
-Filename: ciocore/downloader/log.py
-Comment: 
-
-Filename: ciocore/downloader/logging_download_runner.py
-Comment: 
-
-Filename: ciocore/downloader/perpetual_downloader.py
-Comment: 
-
-Filename: ciocore/downloader/registry.py
-Comment: 
-
-Filename: ciocore/downloader/reporter.py
+Filename: ciocore/cli/conductor.py
 Comment: 
 
 Filename: ciocore/uploader/__init__.py
 Comment: 
 
 Filename: ciocore/uploader/_uploader.py
 Comment: 
 
 Filename: ciocore/uploader/upload_stats/__init__.py
 Comment: 
 
 Filename: ciocore/uploader/upload_stats/stats_formats.py
 Comment: 
 
-Filename: tests/__init__.py
+Filename: ciocore-8.1.0b1.data/scripts/conductor
 Comment: 
 
-Filename: tests/extra_env_fixtures.py
+Filename: ciocore-8.1.0b1.data/scripts/conductor.bat
 Comment: 
 
-Filename: tests/instance_type_fixtures.py
+Filename: tests/__init__.py
 Comment: 
 
-Filename: tests/package_fixtures.py
+Filename: tests/instance_type_fixtures.py
 Comment: 
 
-Filename: tests/project_fixtures.py
+Filename: tests/package_fixtures.py
 Comment: 
 
 Filename: tests/test_api_client.py
 Comment: 
 
-Filename: tests/test_base_downloader.py
-Comment: 
-
-Filename: tests/test_cli.py
-Comment: 
-
 Filename: tests/test_common.py
 Comment: 
 
 Filename: tests/test_config.py
 Comment: 
 
 Filename: tests/test_data.py
 Comment: 
 
-Filename: tests/test_downloader.py
-Comment: 
-
 Filename: tests/test_hardware_set.py
 Comment: 
 
 Filename: tests/test_imports_2and3.py
 Comment: 
 
-Filename: tests/test_job_downloader.py
-Comment: 
-
 Filename: tests/test_package_environment.py
 Comment: 
 
 Filename: tests/test_package_tree.py
 Comment: 
 
 Filename: tests/test_submit.py
@@ -357,26 +129,26 @@
 
 Filename: tests/test_validator.py
 Comment: 
 
 Filename: tests/mocks/__init__.py
 Comment: 
 
-Filename: tests/mocks/glob.py
+Filename: tests/mocks/api_client_mock.py
 Comment: 
 
-Filename: ciocore-8.0.0rc1.dist-info/METADATA
+Filename: tests/mocks/glob.py
 Comment: 
 
-Filename: ciocore-8.0.0rc1.dist-info/WHEEL
+Filename: ciocore-8.1.0b1.dist-info/METADATA
 Comment: 
 
-Filename: ciocore-8.0.0rc1.dist-info/entry_points.txt
+Filename: ciocore-8.1.0b1.dist-info/WHEEL
 Comment: 
 
-Filename: ciocore-8.0.0rc1.dist-info/top_level.txt
+Filename: ciocore-8.1.0b1.dist-info/top_level.txt
 Comment: 
 
-Filename: ciocore-8.0.0rc1.dist-info/RECORD
+Filename: ciocore-8.1.0b1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## ciocore/VERSION

```diff
@@ -1 +1 @@
-8.0.0-rc.1
+8.1.0-beta.1
```

## ciocore/__init__.py

```diff
@@ -1,23 +1 @@
-import os
-
-# Get the version from the VERSION file
-# The VERSION file may be in the current directory or (in dev) one directory up
-
-
-def read_version(filepath):
-    try:
-        with open(filepath, "r", encoding="utf-8") as version_file:
-            return version_file.read().strip()
-    except IOError:
-        return None
-
-
-file_path = os.path.join(os.path.abspath(os.path.dirname(__file__)), "VERSION")
-version = read_version(file_path)
-if not version:
-    file_path = os.path.join(
-        os.path.abspath(os.path.dirname(os.path.dirname(__file__))), "VERSION"
-    )
-    version = read_version(file_path)
-if not version:
-    version = "dev"
+from .__about__ import *
```

## ciocore/api_client.py

```diff
@@ -1,33 +1,33 @@
-"""
-The api_client module is used to make requests to the Conductor API.
-"""
-
 import base64
 import importlib
 import json
 import jwt
 import logging
 import os
 import platform
 import requests
 import socket
 import time
 import sys
-import platform
-import hashlib
-
-from urllib import parse
 
+try:
+    from urllib import parse
+except ImportError:
+    import urlparse as parse
+    
 import ciocore
 
 from ciocore import config
 from ciocore import common, auth
 
-logger = logging.getLogger(__name__)
+
+from ciocore.common import CONDUCTOR_LOGGER_NAME
+
+logger = logging.getLogger(CONDUCTOR_LOGGER_NAME)
 
 # A convenience tuple of network exceptions that can/should likely be retried by the retry decorator
 try:
     CONNECTION_EXCEPTIONS = (
         requests.exceptions.HTTPError,
         requests.exceptions.ConnectionError,
         requests.exceptions.Timeout,
@@ -36,139 +36,110 @@
     CONNECTION_EXCEPTIONS = (
         requests.HTTPError,
         requests.ConnectionError,
         requests.Timeout,
     )
 
 
-def truncate_middle(s, max_length):
-    """
-    Truncate the string `s` to `max_length` by removing characters from the middle.
-
-    :param s: The original string to be truncated.
-    :type s: str
-    :param max_length: The maximum allowed length of the string after truncation.
-    :type max_length: int
-    :return: The truncated string.
-    :rtype: str
-    """
-
-    if len(s) <= max_length:
-        # String is already at or below the maximum length, return it as is
-        return s
-
-    # Calculate the number of characters to keep from the start and end of the string
-    num_keep_front = (max_length // 2)
-    num_keep_end = max_length - num_keep_front - 1  # -1 for the ellipsis
-    
-    # Construct the truncated string
-    return s[:num_keep_front] + '~' + s[-num_keep_end:]
-
 
 # TODO: appspot_dot_com_cert = os.path.join(common.base_dir(),'auth','appspot_dot_com_cert2') load
 # appspot.com cert into requests lib verify = appspot_dot_com_cert
 
-class ApiClient:
-    """
-    The ApiClient class is a wrapper around the requests library that handles authentication and retries.
-    """
 
+class ApiClient:
     http_verbs = ["PUT", "POST", "GET", "DELETE", "HEAD", "PATCH"]
-
+    
     USER_AGENT_TEMPLATE = "client {client_name}/{client_version} (ciocore {ciocore_version}; {runtime} {runtime_version}; {platform} {platform_details}; {hostname} {pid}; {python_path})"
     USER_AGENT_MAX_PATH_LENGTH = 1024
-
+    
     user_agent_header = None
 
     def __init__(self):
         logger.debug("")
 
     def _make_request(self, verb, conductor_url, headers, params, data, raise_on_error=True):
         response = requests.request(
             method=verb, url=conductor_url, headers=headers, params=params, data=data
         )
 
-        logger.debug(f"verb: {verb}")
-        logger.debug(f"conductor_url: {conductor_url}")
-        logger.debug(f"headers: {headers}")
-        logger.debug(f"params: {params}")
-        logger.debug(f"data: {data}")
+        logger.debug("verb: %s", verb)
+        logger.debug("conductor_url: %s", conductor_url)
+        logger.debug("headers: %s", headers)
+        logger.debug("params: %s", params)
+        logger.debug("data: %s", data)
 
         # If we get 300s/400s debug out the response. TODO(lws): REMOVE THIS
-        if 300 <= response.status_code < 500:
+        if response.status_code and 300 <= response.status_code < 500:
             logger.debug("*****  ERROR!!  *****")
-            logger.debug(f"Reason: {response.reason}")
-            logger.debug(f"Text: {response.text}")
+            logger.debug("Reason: %s" % response.reason)
+            logger.debug("Text: %s" % response.text)
 
         # trigger an exception to be raised for 4XX or 5XX http responses
         if raise_on_error:
             response.raise_for_status()
 
         return response
 
     def make_prepared_request(
         self,
         verb,
         url,
         headers=None,
         params=None,
-        json_payload=None,
+        json=None,
         data=None,
         stream=False,
         remove_headers_list=None,
         raise_on_error=True,
         tries=5,
     ):
+
         """
-        Make a request to the Conductor API.
+        Primarily used to removed enforced headers by requests.Request. Requests 2.x will add
+        Transfer-Encoding: chunked with file like object that is 0 bytes, causing s3 failures (501)
+        - https://github.com/psf/requests/issues/4215#issuecomment-319521235
+
+        To get around this bug make_prepared_request has functionality to remove the enforced header
+        that would occur when using requests.request(...). Requests 3.x resolves this issue, when
+        client is built to use Requests 3.x this function can be deprecated.
+
+        args:
+            verb: (str) of HTTP verbs
+            url: (str) url
+            headers: (dict)
+            params: (dict)
+            json: (dict)
+            data: (varies)
+            stream: (bool)
+            remove_headers_list: list of headers to remove i.e ["Transfer-Encoding"]
+            raise_on_error: (bool)
+            tries: (int) number of attempts to perform request
+
 
-        Deprecated:
-            Primarily used to removed enforced headers by requests.Request. Requests 2.x will add
-            Transfer-Encoding: chunked with file like object that is 0 bytes, causing s3 failures (501)
-            - https://github.com/psf/requests/issues/4215#issuecomment-319521235
-
-            To get around this bug make_prepared_request has functionality to remove the enforced header
-            that would occur when using requests.request(...). Requests 3.x resolves this issue, when
-            client is built to use Requests 3.x this function can be removed.
-
-        Returns:
-            requests.Response: The response object.
-
-        Args:
-            verb (str): The HTTP verb to use.
-            url (str): The URL to make the request to.
-            headers (dict): A dictionary of headers to send with the request.
-            params (dict): A dictionary of query parameters to send with the request.
-            json (dict): A JSON payload to send with the request.
-            stream (bool): Whether or not to stream the response.
-            remove_headers_list (list): A list of headers to remove from the request.
-            raise_on_error (bool): Whether or not to raise an exception if the request fails.
-            tries (int): The number of times to retry the request.
+        return: request.Response
         """
 
         req = requests.Request(
             method=verb,
             url=url,
             headers=headers,
             params=params,
-            json=json_payload,
+            json=json,
             data=data,
         )
         prepped = req.prepare()
 
         if remove_headers_list:
             for header in remove_headers_list:
                 prepped.headers.pop(header, None)
 
         # Create a retry wrapper function
-        retry_wrapper = common.DecRetry(
-            retry_exceptions=CONNECTION_EXCEPTIONS, tries=tries
-        )
+        retry_wrapper = common.DecRetry(retry_exceptions=CONNECTION_EXCEPTIONS, tries=tries)
 
-        # requests sessions potentially not thread-safe, but need to removed enforced
+        # requests sessions potentially not thread-safe, but need to potentially removed enforced
         # headers by using a prepared request.create which can only be done through an
         # request.Session object. Create Session object per call of make_prepared_request, it will
         # not benefit from connection pooling reuse. https://github.com/psf/requests/issues/1871
         session = requests.Session()
 
         # wrap the request function with the retry wrapper
         wrapped_func = retry_wrapper(session.send)
@@ -194,48 +165,34 @@
         headers=None,
         params=None,
         data=None,
         verb=None,
         conductor_url=None,
         raise_on_error=True,
         tries=5,
-        use_api_key=False,
+        use_api_key=False
     ):
         """
-        Make a request to the Conductor API.
-
-        Args:
-            uri_path (str): The path to the resource to request.
-            headers (dict): A dictionary of headers to send with the request.
-            params (dict): A dictionary of query parameters to send with the request.
-            data (dict): A dictionary of data to send with the request.
-            verb (str): The HTTP verb to use.
-            conductor_url (str): The Conductor URL.
-            raise_on_error (bool): Whether or not to raise an exception if the request fails.
-            tries (int): The number of times to retry the request.
-            use`_api_key (bool): Whether or not to use the API key for authentication.
-
-        Returns:
-            tuple(str, int): The response text and status code.
+        verb: PUT, POST, GET, DELETE, HEAD, PATCH
         """
-        cfg = config.get()
+        cfg = config.config().config
         # TODO: set Content Content-Type to json if data arg
         if not headers:
             headers = {"Content-Type": "application/json", "Accept": "application/json"}
 
         logger.debug("read_conductor_credentials({})".format(use_api_key))
         bearer_token = read_conductor_credentials(use_api_key)
         if not bearer_token:
             raise Exception("Error: Could not get conductor credentials!")
 
         headers["Authorization"] = "Bearer %s" % bearer_token
-
+        
         if not ApiClient.user_agent_header:
             self.register_client("ciocore")
-
+        
         headers["User-Agent"] = ApiClient.user_agent_header
 
         # Construct URL
         if not conductor_url:
             conductor_url = parse.urljoin(cfg["url"], uri_path)
 
         if not verb:
@@ -243,77 +200,123 @@
                 verb = "POST"
             else:
                 verb = "GET"
 
         assert verb in self.http_verbs, "Invalid http verb: %s" % verb
 
         # Create a retry wrapper function
-        retry_wrapper = common.DecRetry(
-            retry_exceptions=CONNECTION_EXCEPTIONS, tries=tries
-        )
+        retry_wrapper = common.DecRetry(retry_exceptions=CONNECTION_EXCEPTIONS, tries=tries)
 
         # wrap the request function with the retry wrapper
         wrapped_func = retry_wrapper(self._make_request)
 
         # call the wrapped request function
         response = wrapped_func(
             verb, conductor_url, headers, params, data, raise_on_error=raise_on_error
         )
 
         return response.text, response.status_code
-
+    
     @classmethod
-    def register_client(cls, client_name, client_version=None):
-        """
-        Generates the http User Agent header that includes helpful debug info.
-        """
-
-        # Use the provided client_version.
-        if not client_version:
+    def _get_user_agent_header(cls, client_name, client_version=None):
+        '''
+        Generates the http User Agent header that includes helpful debug info. 
+        
+        The final component is the path to the python executable (MD5 hex).
+        
+        ex: 'ciomaya/0.3.7 (ciocore 4.3.2; python 3.9.5; linux 3.10.0-1160.53.1.el7.x86_64; 0ee7123c2365d7a0d126de5a70f19727)'
+        
+        :param client_name: The name of the client to be used in the header. If it's importable it
+                            will be queried for its __version__ (unless client_version is supplied)
+        :type client_name: str
+        
+        :param client_version: The version to use in the header if client_name can't be queried for
+                               __version__ (or it needs to be overridden)
+        :type client_version: str [default = None]
+        
+        :return: The value for the User Agent header
+        :rtype: str        
+        '''
+        
+        try:
+            client_module = importlib.import_module(client_name)
+                    
+        except ImportError:
+            logger.warning("Unable to import module '%s'. Won't query for version details.", client_name)
             client_version = 'unknown'
-
-        python_version = platform.python_version()
-        system_info = platform.system()
-        release_info = platform.release()
         
+        if not client_version:
+            try:
+                client_version = client_module.__version__
+                
+            except AttributeError:
+                logger.warning("Module '%s' has no __version__ attribute. Setting version to 'N/A' in the user agent", client_name)
+                client_version = 'unknown'
+        
+        ciocore_version = ciocore.__version__
+        
+        python_path = sys.executable
+        
+        # If the length of the path is longer than allowed, truncate the middle
+        if len(python_path) > cls.USER_AGENT_MAX_PATH_LENGTH:
+            
+            first_half = int(cls.USER_AGENT_MAX_PATH_LENGTH/2)
+            second_half = len(python_path) - int(cls.USER_AGENT_MAX_PATH_LENGTH/2)
+            
+            print(first_half, second_half)
+            python_path = "{}...{}".format(python_path[0:first_half], python_path[second_half:-1])
+        
+        python_path_encoded = base64.b64encode(python_path.encode('utf-8')).decode('utf-8')
+        
+        if platform.system() == "Linux":
+            platform_details = platform.release()
+            
+        elif platform.system() == "Windows":
+            platform_details = platform.version()
+            
+        elif platform.system() == "Darwin":
+            platform_details = platform.mac_ver()[0]
+            
+        else:
+            raise ValueError("Unrecognized platform '{}'".format(platform.release()))
+        
+        pid = base64.b64encode(str(os.getpid()).encode('utf-8')).decode('utf-8')
+        hostname = base64.b64encode(socket.gethostname().encode('utf-8')).decode('utf-8')
 
-        # Get the MD5 hex digest of the path to the python executable
-        python_executable_path = truncate_middle(sys.executable.encode('utf-8'), cls.USER_AGENT_MAX_PATH_LENGTH)
-        md5_hash = hashlib.md5(python_executable_path).hexdigest()
-
-        user_agent = (
-            f"{client_name}/{client_version} "
-            f"(python {python_version}; {system_info} {release_info}; {md5_hash})"
-        )
-        cls.user_agent_header = user_agent
+        return cls.USER_AGENT_TEMPLATE.format(client_name=client_name,
+                                              client_version=client_version,
+                                              ciocore_version=ciocore_version,
+                                              runtime='python',
+                                              runtime_version=platform.python_version(),
+                                              platform=sys.platform,
+                                              platform_details=platform_details,
+                                              pid=pid,
+                                              hostname=hostname, 
+                                              python_path=python_path_encoded
+                                              )
+        
+    @classmethod
+    def register_client(cls, client_name, client_version=None):
+        cls.user_agent_header = cls._get_user_agent_header(client_name, client_version)
+        
         
-        return user_agent
-    
 
 def read_conductor_credentials(use_api_key=False):
     """
-    Read the conductor credentials file.
-    
-    If the credentials file exists, it will contain a bearer token from either
-    the user or the API key.
-    
-    If the credentials file doesn't exist, or is
-    expired, or is from a different domain, we try to fetch a new one in the API key scenario or
-    prompt the user to log in. 
-    
-    Args: 
-        use_api_key (bool): Whether or not to try to use the API key
+    Read the conductor credentials file, if it exists. This will contain a bearer token from either
+    the user or the API key (if that's desired). If the credentials file doesn't exist, or is
+    expired, or is from a different domain, try and fetch a new one in the API key scenario or
+    prompt the user to log in. Args: use_api_key: Whether or not to use the API key
 
-    Returns: 
-        A Bearer token in the event of a success or None
+    Returns: A Bearer token in the event of a success or None if things couldn't get figured out
 
     """
 
-    cfg = config.get()
-
+    # TODO: use config.get(). Below call is deprecated
+    cfg = config.config().config
     logger.debug("Reading conductor credentials...")
     if use_api_key:
         if not cfg.get("api_key"):
             use_api_key = False
         if use_api_key and not cfg["api_key"].get("client_id"):
             use_api_key = False
     logger.debug("use_api_key = %s" % use_api_key)
@@ -329,48 +332,46 @@
 
             #  Exchange the API key for a bearer token
             logger.debug("Attempting to get API key bearer token")
             get_api_key_bearer_token(creds_file)
 
         else:
             auth.run(creds_file, cfg["url"])
+
     if not os.path.exists(creds_file):
         return None
 
     logger.debug("Reading credentials file...")
-    with open(creds_file, "r", encoding="utf-8") as fp:
+    with open(creds_file) as fp:
         file_contents = json.loads(fp.read())
+
     expiration = file_contents.get("expiration")
+
     same_domain = creds_same_domain(file_contents)
+
     if same_domain and expiration and expiration >= int(time.time()):
         return file_contents["access_token"]
+
     logger.debug("Credentials have expired or are from a different domain")
+
     if use_api_key:
         logger.debug("Refreshing API key bearer token!")
         get_api_key_bearer_token(creds_file)
     else:
         logger.debug("Sending to auth page...")
         auth.run(creds_file, cfg["url"])
+
     #  Re-read the creds file, since it has been re-upped
-    with open(creds_file, "r", encoding="utf-8") as fp:
+    with open(creds_file) as fp:
         file_contents = json.loads(fp.read())
         return file_contents["access_token"]
 
 
 def get_api_key_bearer_token(creds_file=None):
-    """
-    Get a bearer token from the API key.
-    
-    Args:
-        creds_file (str): The path to the credentials file. If not provided, the bearer token will not be written to disk.
-        
-    Returns:
-        A dictionary containing the bearer token and other information.
-    """
-    cfg = config.get()
+    cfg = config.config().config
     url = "{}/api/oauth_jwt".format(cfg["url"])
     response = requests.get(
         url,
         params={
             "grant_type": "client_credentials",
             "scope": "owner admin user",
             "client_id": cfg["api_key"]["client_id"],
@@ -394,54 +395,34 @@
 
         with open(creds_file, "w") as fp:
             fp.write(json.dumps(credentials_dict))
     return
 
 
 def get_creds_path(api_key=False):
-    """
-    Get the path to the credentials file.
-    
-    Args:
-        api_key (bool): Whether or not to use the API key.
-        
-    Returns:
-        str: The path to the credentials file.
-    """
+
     creds_dir = os.path.join(os.path.expanduser("~"), ".config", "conductor")
     if api_key:
         creds_file = os.path.join(creds_dir, "api_key_credentials")
     else:
         creds_file = os.path.join(creds_dir, "credentials")
     return creds_file
 
 
 def get_bearer_token(refresh=False):
     """
     Return the bearer token.
 
-    Args:
-        refresh (bool): Whether or not to refresh the token.
-        
     TODO: Thread safe multiproc caching, like it used to be pre-python3.7.
     """
     return read_conductor_credentials(True)
 
 
 def creds_same_domain(creds):
-    """
-    Check if the creds are for the same domain as the config.
-    
-    Args:
-        creds (dict): The credentials dictionary.
-        
-    Returns:
-        bool: Whether or not the creds are for the same domain as the config.
-    """
-    cfg = config.get()
+    cfg = config.config().config
     """Ensure the creds file refers to the domain in config"""
     token = creds.get("access_token")
     if not token:
         return False
 
     decoded = jwt.decode(creds["access_token"], verify=False)
     audience_domain = decoded.get("aud")
@@ -450,84 +431,58 @@
         and audience_domain.rpartition("/")[-1] == cfg["api_url"].rpartition("/")[-1]
     )
 
 
 def account_id_from_jwt(token):
     """
     Fetch the accounts id from a jwt token value.
-    
-    Args:
-        token (str): The jwt token.
-    
-    Returns:
-        str: The account id.
     """
     payload = jwt.decode(token, verify=False)
     return payload.get("account")
 
 
 def account_name_from_jwt(token):
     """
     Fetch the accounts name from a jwt token value.
-    
-    Args:
-        token (str): The jwt token.
-        
-    Returns:
-        str: The account name.
     """
+    cfg = config.config().config
     account_id = account_id_from_jwt(token)
-    cfg = config.get()
     if account_id:
         url = "%s/api/v1/accounts/%s" % (cfg["api_url"], account_id)
         response = requests.get(url, headers={"authorization": "Bearer %s" % token})
         if response.status_code == 200:
             response_dict = json.loads(response.text)
             return response_dict["data"]["name"]
     return None
 
 
 def request_instance_types(as_dict=False):
     """
     Get the list of available instances types.
-    
-    Args:
-        as_dict (bool): Whether or not to return the instance types as a dictionary.
-    
-    Returns:
-        list: The list of instance types.
     """
     api = ApiClient()
     response, response_code = api.make_request(
         "api/v1/instance-types", use_api_key=True, raise_on_error=False
     )
     if response_code not in (200,):
         msg = "Failed to get instance types"
         msg += "\nError %s ...\n%s" % (response_code, response)
         raise Exception(msg)
 
     instance_types = json.loads(response).get("data", [])
     logger.debug("Found available instance types: %s", instance_types)
 
     if as_dict:
-        return dict(
-            [(instance["description"], instance) for instance in instance_types]
-        )
+        return dict([(instance["description"], instance) for instance in instance_types])
     return instance_types
 
 
 def request_projects(statuses=("active",)):
     """
-    Query Conductor for all client Projects that are in the given status(es).
-    
-    Args:
-        statuses (tuple): The statuses to filter for.
-    
-    Returns:
-        list: The list of project names.
+    Query Conductor for all client Projects that are in the given state(s)
     """
     api = ApiClient()
 
     logger.debug("statuses: %s", statuses)
 
     uri = "api/v1/projects/"
 
@@ -548,241 +503,20 @@
             projects.append(project["name"])
     return projects
 
 
 def request_software_packages():
     """
     Query Conductor for all software packages for the currently available sidecar.
-    
-    Returns:
-        list: The list of software packages.
     """
     api = ApiClient()
 
     uri = "api/v1/ee/packages?all=true,"
     response, response_code = api.make_request(
         uri_path=uri, verb="GET", raise_on_error=False, use_api_key=True
     )
 
     if response_code not in [200]:
         msg = "Failed to get software packages for latest sidecar"
         msg += "\nError %s ...\n%s" % (response_code, response)
         raise Exception(msg)
-    
-    software = json.loads(response).get("data", [])
-    software = [sw for sw in software if not ("3dsmax" in sw["product"] and sw["platform"] == "linux")]
-    return software
-
-def request_extra_environment():
-    """
-    Query Conductor for extra environment.
-    """
-    api = ApiClient()
-
-    uri = "api/v1/integrations/env-vars-configs"
-    response, response_code = api.make_request(
-        uri_path=uri, verb="GET", raise_on_error=False, use_api_key=True
-    )
-
-    if response_code not in [200]:
-        msg = "Failed to get extra environment"
-        msg += "\nError %s ...\n%s" % (response_code, response)
-        raise Exception(msg)
-
-    all_accounts = json.loads(response).get("data", [])
-
-    token = read_conductor_credentials(True)
-    if not token:
-        raise Exception("Error: Could not get conductor credentials!")
-    account_id =  str(account_id_from_jwt(token))
-
-    if not account_id:
-        raise Exception("Error: Could not get account id from jwt!")
-    account_env = next((account for account in all_accounts if account["account_id"] == account_id), None)
-    if not account_env:
-        raise Exception("Error: Could not get account environment!")
-    return account_env.get("env", [])
-
-
-
-
-def get_jobs(first_jid, last_jid=None):
-    """
-    Query Conductor for all jobs between the given job ids.
-
-    Returns:
-        list: The list of jobs.
-
-    Raises:
-        Exception: If the request fails.
-
-    Examples:
-        >>> from ciocore import api_client
-        >>> jobs = api_client.get_jobs(1959)
-        >>> len(jobs)
-        1
-        >>> jobs[0]["jid"]
-        '01959'
-        >>> jobs = api_client.get_jobs(1959, 1961)
-        >>> len(jobs)
-        3
-    """
-    if last_jid is None:
-        last_jid = first_jid
-    low = str(int(first_jid) - 1).zfill(5)
-    high = str(int(last_jid) + 1).zfill(5)
-    api = ApiClient()
-    uri = "api/v1/jobs"
-
-    response, response_code = api.make_request(
-        uri_path=uri,
-        verb="GET",
-        raise_on_error=False,
-        use_api_key=True,
-        params={"filter": f"jid_gt_{low},jid_lt_{high}"},
-    )
-
-    if response_code not in [200]:
-        msg = f"Failed to get jobs {first_jid}-{last_jid}"
-        msg += "\nError %s ...\n%s" % (response_code, response)
-        raise Exception(msg)
-    jobs = json.loads(response).get("data")
-    return jobs
-
-
-def get_log(job_id, task_id):
-    """
-    Get the log for the given job and task.
-
-    Args:
-        job_id (str): The job id.
-        task_id (str): The task id.
-
-    Returns:
-        list: A list of logs.
-
-    Raises:
-        Exception: If the request fails.
-
-    Examples:
-        >>> from ciocore import api_client
-        >>> logs = api_client.get_log(1959, 0)
-        {
-            "logs": [
-                {
-                    "container_id": "j-5669544198668288-5619559933149184-5095331660038144-stde",
-                    "instance_name": "renderer-5669544198668288-170062309438-62994",
-                    "log": [
-                        "Blender 2.93.0 (hash 84da05a8b806 built 2021-06-02 11:29:24)",
-                        ...
-                        ...
-                        "Saved: '/var/folders/8r/46lmjdmj50x_0swd9klwptzm0000gq/T/blender_bmw/renders/render_0001.png'",
-                        " Time: 00:29.22 (Saving: 00:00.32)",
-                        "",
-                        "",
-                        "Blender quit"
-                ],
-                "timestamp": "1.700623521101516E9"
-                }
-            ],
-            "new_num_lines": [
-                144
-            ],
-            "status_description": "",
-            "task_status": "success"
-        }
-    """
-    job_id = str(job_id).zfill(5)
-    task_id = str(task_id).zfill(3)
-
-    api = ApiClient()
-    uri = f"get_log_file?job={job_id}&task={task_id}&num_lines[]=0"
-
-    response, response_code = api.make_request(
-        uri_path=uri, verb="GET", raise_on_error=False, use_api_key=True
-    )
-
-    if response_code not in [200]:
-        msg = f"Failed to get log for job {job_id} task {task_id}"
-        msg += "\nError %s ...\n%s" % (response_code, response)
-        raise Exception(msg)
-
-    return response
-
-
-def kill_jobs(*job_ids):
-    """
-    Kill the given jobs.
-    
-    Args:
-        job_ids (list): The list of job ids.
-    
-    Returns:
-        dict: The response.
-    
-    Examples:
-        >>> from ciocore import api_client
-        >>> api_client.kill_jobs("03095","03094")
-        {'body': 'success', 'message': "Jobs [u'03095', u'03094'] have been kill."}
-
-    """
-    job_ids = [str(job_id).zfill(5) for job_id in job_ids]
-    api = ApiClient()
-    payload = {
-        "action": "kill",
-        "jobids": job_ids,
-    }
-    response, response_code = api.make_request(
-        uri_path="jobs_multi", 
-        verb="PUT", 
-        raise_on_error=False, 
-        use_api_key=True, 
-        data=json.dumps(payload)
-    )
-    
-    if response_code not in [200]:
-        msg = f"Failed to kill jobs {job_ids}"
-        msg += "\nError %s ...\n%s" % (response_code, response)
-        raise Exception(msg)
-    
-    return json.loads(response)
-
-
-def kill_tasks(job_id, *task_ids):
-    """
-    Kill the given tasks.
-    
-    Args:
-        job_id (str): The job id.
-        task_ids (list): The list of task ids.
-        
-    Returns:
-        dict: The response.
-        
-    Examples:
-        >>> from ciocore import api_client
-        >>> api_client.kill_tasks("03096", *range(50,56))
-        {'body': 'success', 'message': ' 6 Tasks set to "kill"\n\t050\n\t051\n\t052\n\t053\n\t054\n\t055'}
-    """
-    
-    job_id = str(job_id).zfill(5)
-    task_ids = [str(task_id).zfill(3) for task_id in task_ids]
-    api = ApiClient()
-    payload = {
-        "action": "kill",
-        "jobid": job_id,
-        "taskids": task_ids,
-    }
-    response, response_code = api.make_request(
-        uri_path="tasks_multi", 
-        verb="PUT", 
-        raise_on_error=False, 
-        use_api_key=True, 
-        data=json.dumps(payload)
-    )
-    
-    if response_code not in [200]:
-        msg = f"Failed to kill tasks {task_ids} of job {job_id}"
-        msg += "\nError %s ...\n%s" % (response_code, response)
-        raise Exception(msg)
-    
-    return json.loads(response)
+    return json.loads(response).get("data", [])
```

## ciocore/common.py

```diff
@@ -360,24 +360,15 @@
                 "Cannot read file '%s'. Is it readable by the user running the" " uploader?",
                 filepath,
             )
             raise
 
         while len(file_buffer) > 0:
             hash_obj.update(file_buffer)
-            try:
-                file_buffer = file_obj.read(MD5_BLOCKSIZE)
-            except OSError:
-                logger.log(
-                    log_level,
-                    "Cannot read file '%s'. Is it corrupted or unavailable?",
-                    filepath,
-                )
-                raise
-
+            file_buffer = file_obj.read(MD5_BLOCKSIZE)
             curtime = time.time()
             bytes_processed = buffer_count * MD5_BLOCKSIZE
             percentage_processed = int((bytes_processed / float(file_size)) * 100)
 
             if poll_seconds and curtime - last_time >= poll_seconds:
                 logger.log(
                     log_level,
```

## ciocore/config.py

```diff
@@ -1,180 +1,146 @@
 """
-Config is a configuration object implemented as a module-level singleton.
+A configuration object implemented as a module-level singleton.
 
-Configuration variables can be shared by importing the module. If there are changes in environment variables or other sources, the config can be refreshed.
+Configuration variables can be shared by importing the module. If there are changes in environment variables or other sources, the config can be refreshed. See [config()](#config)
 """
 
 import logging
 import os
-# import multiprocessing
+import multiprocessing
 import base64
 import json
 import re
-import platform
 
 from ciocore.common import CONDUCTOR_LOGGER_NAME
 
 logger = logging.getLogger(CONDUCTOR_LOGGER_NAME)
 
-# https://stackoverflow.com/a/3809435/179412
+#https://stackoverflow.com/a/3809435/179412
 
-
-USER_DIRS = {
-    "Linux": os.path.expanduser(os.path.join("~", ".conductor")),
-    "Darwin": os.path.expanduser(os.path.join("~",".conductor")),
-    "Windows": os.path.expanduser(os.path.join("~", "AppData", "Local", "Conductor")),
-}
-
-DEFAULT_USER_DIR = USER_DIRS.get(platform.system(), USER_DIRS["Linux"])
-
-URL_REGEX = re.compile(
-    r"https?:\/\/(www\.)?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b([-a-zA-Z0-9()@:%_\+.~#?&//=]*)"
-)
+URL_REGEX=re.compile(r"https?:\/\/(www\.)?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b([-a-zA-Z0-9()@:%_\+.~#?&//=]*)")
 
 __config__ = None
 
-
 def config(force=False):
     """
+    DEPRECATED! Use [get()](#get) instead.
+
     Instantiate a config object if necessary.
 
-    Deprecated:
-        Use [get()](#get) instead.
+    Keyword Arguments:
 
-    Args:
-        force (bool): Discards any existing config object and instantiate a new one -- Defaults to `False`.
+    * **`force`** --  Discards any existing config object and instantiate a new one -- Defaults to `False`.
 
     Returns:
-        dict: A dictionary containing configuration values.
+
+    * [Config()](#Config) object containing a dictionary of configuration variables.
     """
 
     global __config__
     if force or not __config__:
         __config__ = Config()
     return __config__
 
-
 def get(force=False):
     """
     Instantiate a config object if necessary and return the dictionary.
 
-    Args:
-        force (bool): Discards any existing config object and instantiate a new one -- Defaults to `False`.
+    Keyword Arguments:
+
+    * **`force`** -- Discards any existing config object and instantiate a new one -- Defaults to `False`.
 
     Returns:
-        dict: A dictionary containing configuration values.
 
-    Example:
-        >>> from ciocore import config
-        >>> config.get()
-        {
-          'thread_count': 16,
-          'priority': 5,
-          'md5_caching': True,
-          'log_level': 'INFO',
-          'url': 'https://dashboard.conductortech.com',
-          'auth_url': 'https://dashboard.conductortech.com',
-          'api_url': 'https://api.conductortech.com',
-          'api_key': None,
-          'downloader_page_size': 50,
-        }
+    * A dictionary containing configuration.
+\
+    ???+ example
+        ``` python
+        from ciocore import config
+
+        print(config.get())
+
+        # Result:
+        # {
+        #   'thread_count': 16,
+        #   'priority': 5,
+        #   'md5_caching': True,
+        #   'log_level': 'INFO',
+        #   'url': 'https://dashboard.conductortech.com',
+        #   'auth_url': 'https://dashboard.conductortech.com',
+        #   'api_url': 'https://api.conductortech.com', 
+        #   'api_key': None
+        # }
+        ```
     """
     global __config__
     if force or not __config__:
         __config__ = Config()
     return __config__.config
 
-
 class Config(object):
     def __init__(self):
         """
-        Initialize the config object.
+        Create a configuration dictionary.
 
-        A config object is a dictionary containing configuration values. It is a singleton, so there is only one instance of it. It is instantiated the first time it is needed. It can be refreshed by calling get() with the `force` keyword argument set to `True`.
-
-        A Config object has the following properties:
-
-        * `thread_count` The number of threads to use for downloading files. Defaults to the number of CPUs on the system times 2. It can be overridden by the `CONDUCTOR_THREAD_COUNT` environment variable.
-        * `priority` Set the priority for submissions. Defaults to 5. It can be overridden by the `CONDUCTOR_PRIORITY` environment variable.
-        * `md5_caching` Whether to cache MD5s. Defaults to `True`. It can be overridden by the `CONDUCTOR_MD5_CACHING` environment variable. Cachine MD5s significantly improves submission performance, but on rare occasions it can cause submissions to fail. If you experience this, set `md5_caching` to `False`.
-        * `log_level` The logging level. Defaults to `INFO`. It can be overridden by the `CONDUCTOR_LOG_LEVEL` environment variable.
-        * `url` The URL of the Conductor dashboard. Defaults to `https://dashboard.conductortech.com`. It can be overridden by the `CONDUCTOR_URL` environment variable.
-        * `auth_url` The URL of the Conductor dashboard. Defaults to `https://dashboard.conductortech.com`. It can be overridden by the `CONDUCTOR_AUTH_URL` environment variable. This is deprecated. Use `url` instead.
-        * `api_url` The URL of the Conductor API. Defaults to `https://api.conductortech.com`. It can be overridden by the `CONDUCTOR_API_URL` environment variable.
-        * `api_key` The API key. The API key can be acquired from the Conductor dashboard, and can be stored in an environment variable or a file. In both cases the API KEY can be a JSON object or a base64 encoded JSON object. If it is base64 encoded, it can be a string or bytes. If it is a string, it will be decoded as ASCII. If it is bytes, it will be decoded as UTF-8.
-            * Environment variable: The `CONDUCTOR_API_KEY` variable can hold the API KEY directly.
-            * File: The `CONDUCTOR_API_KEY_PATH` variable can hold the path to a file containing the API KEY.
-        * `downloader_page_size` The number of files to request from the Conductor API at a time. Defaults to 50. It can be overridden by the `CONDUCTOR_DOWNLOADER_PAGE_SIZE` environment variable.
-
-        Returns:
-            Config: A config object.
 
         Raises:
-            ValueError -- Invalid inputs, such as badly formed URLs.
-        """
-        default_downloader_page_size = 50
 
-     
+        * **`ValueError`** -- Invalid inputs, such as badly formed URLs.
+        """
 
         try:
-            default_thread_count = min( os.cpu_count() - 1, 15)
+            default_thread_count = min(multiprocessing.cpu_count() * 2, 16)
         except NotImplementedError:
-            default_thread_count = 15
+            default_thread_count = 16
 
         url = os.environ.get("CONDUCTOR_URL", "https://dashboard.conductortech.com")
 
         if not URL_REGEX.match(url):
             raise ValueError("CONDUCTOR_URL is not valid '{}'".format(url))
-
+ 
         api_url = os.environ.get("CONDUCTOR_API_URL", url.replace("dashboard", "api"))
         if not URL_REGEX.match(api_url):
             raise ValueError("CONDUCTOR_API_URL is not valid '{}'".format(api_url))
-
+ 
         falsy = ["false", "no", "off", "0"]
 
         log_level = os.environ.get("CONDUCTOR_LOG_LEVEL", "INFO")
         if log_level not in ["CRITICAL", "ERROR", "WARNING", "INFO", "DEBUG"]:
             log_level = "INFO"
 
         self.config = {
-            "thread_count": int(
-                os.environ.get("CONDUCTOR_THREAD_COUNT", default_thread_count)
-            ),
-            "downloader_page_size": int(
-                os.environ.get(
-                    "CONDUCTOR_DOWNLOADER_PAGE_SIZE", default_downloader_page_size
-                )
-            ),
+            "thread_count": int(os.environ.get("CONDUCTOR_THREAD_COUNT", default_thread_count)),
             "priority": int(os.environ.get("CONDUCTOR_PRIORITY", 5)),
             "md5_caching": False
             if os.environ.get("CONDUCTOR_MD5_CACHING", "True").lower() in falsy
             else True,
             "log_level": log_level,
             "url": url,
-            # Keep "auth_url" for backwwards compatibillity only.
-            # Clients should use "url" moving forward.
+            # Keep "auth_url" for backwwards compatibillity only. 
+            # Clients should use "url" moving forward. 
             # Remove "auth_url" on the next major version bump.
             "auth_url": url,
             "api_url": api_url,
             "api_key": self.get_api_key_from_variable() or self.get_api_key_from_file(),
-            "user_dir": os.environ.get('CONDUCTOR_USER_DIR', DEFAULT_USER_DIR)
+            "user_dir": os.environ.get('CONDUCTOR_USER_DIR', os.path.expanduser(os.path.join("~", ".conductor")))
         }
 
     @staticmethod
     def get_api_key_from_variable():
         """
-        Attempt to get an API key from the `CONDUCTOR_API_KEY` environment variable.
+        Attempt to get an API key from the CONDUCTOR_API_KEY environment variable.
 
         Raises:
-            ValueError: An error occurred while reading or loading the key into JSON.
+
+        * **`ValueError`** -- An error occurred while loading the key into JSON.
 
         Returns:
-            str: JSON object containing the key - base 64 decoded if necessary.
 
+        * JSON object containing the key - base 64 decoded if necessary.
         """
         api_key = os.environ.get("CONDUCTOR_API_KEY")
         if not api_key:
             return
         logger.info("Attempting to read API key from CONDUCTOR_API_KEY")
         try:
             return json.loads(api_key.replace("\n", "").replace("\r", ""))
@@ -192,19 +158,20 @@
 
     @staticmethod
     def get_api_key_from_file():
         """
         Attempt to get an API key from the file in the CONDUCTOR_API_KEY_PATH environment variable.
 
         Raises:
-            ValueError: An error occurred while reading or loading the key into JSON.
+
+        * **`ValueError`** --  An error occurred while reading or loading the key into JSON.
 
         Returns:
-            str: JSON object containing the key - base 64 decoded if necessary.
 
+        * JSON object containing the key - base 64 decoded if necessary.
         """
         api_key_path = os.environ.get("CONDUCTOR_API_KEY_PATH")
         if not api_key_path:
             return
         logger.info("Attempting to read API key from CONDUCTOR_API_KEY_PATH")
         try:
             with open(api_key_path, "r") as fp:
```

## ciocore/data.py

```diff
@@ -1,20 +1,26 @@
 """
-This module is a singleton that provides the data from Conductor endpoints. Specifically, it provides projects, instance types, and software package data.
+This module is a singleton that provides the data from Conductor endpoints. Specifically, it provides projects, instance types, and software package data. It is also possible to use cached fixture data for development purposes.
 
 Since the data is stored at the module level, you can access it from anywhere in your code without the need to pass it around.
 """
 
+import json
+import os
 from ciocore.package_tree import PackageTree
 from ciocore import api_client
 from ciocore.hardware_set import HardwareSet
 import copy
 
+
+
+
 __data__ = {}
 __products__ = None
+__fixtures_dir__ = None
 __platforms__ = None
 
 def init(*products, **kwargs):
     """
     Initialize the module and let it know what host products to provide.
 
     Args:
@@ -52,15 +58,15 @@
     __platforms__ = set(kwargs.get("platforms", ["windows", "linux"]))
 
 def data(force=False):
     """
     Provide projects, instance types, and software package data.
 
     Keyword Args:
-        force: (bool) If `True`, then force the system to fetch fresh data -- Defaults to `False`.
+        force: (bool) If `True`, then fetch fresh data -- Defaults to `False`.
 
     Raises:
         ValueError:  Module was not initialized with [init()](/data/#ciocore.data.init).
 
     Returns:
         dict: Keys are `projects`, `instance_types`, `software`.
 
@@ -105,38 +111,44 @@
     global __platforms__
 
     if __products__ is None:
         raise ValueError(
             'Data must be initialized before use, e.g. data.init("maya-io") or data.init().'
         )
     products_copy = copy.copy(__products__)
-
+    # products_copy = __products__.copy()
     if force:
         clear()
         init(*products_copy)
 
     if __data__ == {}:
         # PROJECTS
-        __data__["projects"] = sorted(api_client.request_projects())
-        # INST_TYPES
-        instance_types = api_client.request_instance_types()
-        # SOFTWARE
-        software = api_client.request_software_packages()
+        projects_json = _get_json_fixture("projects")
+        if projects_json:
+            __data__["projects"] = projects_json
+        else:
+            __data__["projects"] = sorted(api_client.request_projects())
 
-        # EXTRA ENV VARS
-        extra_env_vars = []
-        try:
-            extra_env_vars = api_client.request_extra_environment()
-        except Exception:
-            pass   
-        __data__["extra_environment"] = extra_env_vars
+        # INST_TYPES
+        instance_types = _get_json_fixture("instance_types")
+        if not instance_types:
+            instance_types = api_client.request_instance_types()
 
-        # PLATFORMS
         it_platforms = set([it["operating_system"] for it in instance_types])
         valid_platforms = it_platforms.intersection(__platforms__)
+
+        # SOFTWARE
+        software = _get_json_fixture("software")
+        if not software:
+            software = api_client.request_software_packages()
+
+        # This is a temporary hack to remove 3dsmax on linux from the software list, since it is a
+        # miscategorization
+        software = [sw for sw in software if not ("3dsmax" in sw["product"] and sw["platform"] == "linux")]
+
         kwargs = {"platforms": valid_platforms}
 
         # If there's only one product, it's possible to initialize the software tree with a plugin.
         # So we set the product kwarg. Otherwise, we set the host_products kwarg
         host_products = __products__
         if len(__products__) == 1:
             host_products = []
@@ -148,21 +160,22 @@
             __data__["software"] = software_tree
             # Revisit instance types to filter out any that are not needed for any software package.
             sw_platforms = software_tree.platforms()
 
             instance_types = [
                 it for it in instance_types if it["operating_system"] in sw_platforms
             ]
-        # Then adjust __platforms__ to match the instance types that are represented.
+
         __platforms__ = set([it["operating_system"] for it in instance_types])
 
         __data__["instance_types"] = HardwareSet(instance_types)
 
     return __data__
 
+
 def valid():
     """
     Check validity.
 
     Returns:
         bool: True if `projects`, `instance_types`, and `software` are valid.
 
@@ -199,17 +212,57 @@
     """
 
     Returns:
         list(str): The product names. An empty list signifies all products.
     """
     return __products__
 
-def set_fixtures_dir(_):
-    """Deprecated. """
-    pass
+
+def set_fixtures_dir(path):
+    """
+    Specify a directory in which to find JSON files representing the three sets of data to provide.
+    The individual filenames are:
+
+    * `projects.json`
+    * `instance_types.json`
+    * `software.json`
+
+    These files could be used in an environment where machines can't access the internet. They are
+    also useful as a cache for developers who need to reload often as it avoids waiting for the
+    network.
+
+    In order to get the content for the fixtures files, use the following Example
+
+    Examples:
+        >>> from ciocore import api_client
+        >>> projects = api_client.request_projects()
+        >>> instance_types = api_client.request_instance_types()
+        >>> software = api_client.request_software_packages()
+
+    Write that data as JSON to the filenames listed above.
+
+    Arguments:
+        path (str): Directory in which to find the above files.
+
+    """
+
+    global __fixtures_dir__
+    __fixtures_dir__ = path or ""
+
+
+def _get_json_fixture(resource):
+    if __fixtures_dir__:
+        cache_path = os.path.join(__fixtures_dir__, "{}.json".format(resource))
+        if os.path.isfile(cache_path):
+            try:
+                with open(cache_path) as f:
+                    return json.load(f)
+            except BaseException:
+                pass
+
 
 def platforms():
     """
     The set of platforms that both software and instance types are valid on.
 
     Returns:
         set: A set containing platforms: windows and/or linux.
```

## ciocore/file_utils.py

```diff
@@ -252,21 +252,21 @@
 
         # IF the directory is NOT considered a root directory (such as "/" or "G:\\" then return it
         if _is_valid_path(dirpath):
             return dirpath
 
 
 def _is_valid_path(path_str):
-    r"""
+    """
     This is dirty/inaccurate helper function to determine whether the given "path" is considered
     valid. If so, return True.
 
     If the given path_str is any of the following characters, then it's to be considered invalid:
 
-        On linux/mac:
+        On linux\mac:
                 /
                 //
                 lettered drive (e.g. x:\)
 
         On windows:
             \
             \\
@@ -449,15 +449,15 @@
     tx_filepath = filepath_base + ".tx"
     if existing_only and not os.path.isfile(tx_filepath):
         return ""
     return tx_filepath
 
 
 def strip_drive_letter(filepath):
-    r"""
+    """
     If the given filepath has a drive letter, remove it and return the rest of the path
 
         C:\cat.txt         -->    \cat.txt
         Z:\cat.txt         -->    \cat.txt
         c:/cat.txt         -->    /cat.txt
         z:/cat.txt         -->    /cat.txt
         //cat.txt          -->    //cat.txt
```

## ciocore/hardware_set.py

```diff
@@ -62,14 +62,15 @@
     `content` is a list of instance types in the category. The order is used to sort the categories. The order of the instance types within a category is determined by the number of cores and memory.
 
     If all instance_types have not been assigned any categories, then the structure is built with two default categories: CPU and GPU.
     """
 
     def __init__(self, instance_types):
         """Initialize the HardwareSet with a list of instance types.
+
         Typically, you would access the HardwareSet through the ciocore.data.data() function, which initializes it for you. However, you can also initialize it directly with a list of instance types straight from ciocore.api_client. The difference being that the latter contains all instance types, whereas the former contains only the instance types compatible with the products you have specified, as well as being cached.
 
         Args:
             instance_types (list): A list of instance types.
 
         Returns:
             HardwareSet: The initialized HardwareSet.
@@ -157,14 +158,15 @@
         """Find an instance type by its name (sku).
 
         Args:
             name (str): The name of the instance type.
 
         Returns:
             dict: The instance type or None if not found.
+
         Example:
             >>> from ciocore import data as coredata
             >>> coredata.init()
             >>> hardware_set = coredata.data()["instance_types"]
             >>> hardware_set.find("n2-highmem-80")
             {
                 'cores': 80,
@@ -191,14 +193,15 @@
         """Find a category by label.
 
         Args:
             label (str): The label of the category.
 
         Returns:
             dict: The category or None if not found.
+
         Example:
             >>> from ciocore import data as coredata
             >>> coredata.init()
             >>> hardware_set = coredata.data()["instance_types"]
             >>> hardware_set.find_category("High cores")
             {
                 "label": "Low cores",
@@ -345,14 +348,15 @@
         instance_types = cls._rewrite_descriptions(instance_types)
         categories = [
             category
             for it in instance_types
             for category in (it.get("categories") or [])
         ]
         result = {}
+
         for it in instance_types:
             is_gpu = it.get("gpu", False)
             if categories:
                 if it.get("categories") in [[], None]:
                     continue
             else:
                 # make our own categories GPU/CPU
```

## ciocore/package_environment.py

```diff
@@ -3,81 +3,107 @@
 """
 
 class PackageEnvironment(object):
 
 
     def __init__(self, env_list=None, platform=None):
         """
-        Encapsulate a list of environment variables.
+        Create a list of environment variables.
 
-        Typically, one would initialize a PackageEnvironment with a package, and then modify by adding more packages or lists of variables. Extra variables can be added by the customer, or programmatically such as during asset scraping.
+        Typically, one would initialize a PackageEnvironment with a package, and then modify by
+        adding more packages or lists of variables. Extra variables can be added by the customer, or
+        programmatically such as during asset scraping.
+
+        Keyword Arguments:
+
+        * **`env_list`** -- Either a Package that contains an "environment" property, or a list of
+          environment objects. -- Defaults to `None`.
+        * **`platform`** -- If env_list is not a package, then the platform must be provided. --
+          Defaults to `None`.
+
+        On instantiation, delegete args to the [extend()](#extend) method.
 
-        Args:
-            env_list (object|list): An object that provides a list of dictionaries with properties: `name`, `value`, and `merge_policy`.
-            platform (str): If the env_list is a regular list, then this is required.
-            
-        Args are delegated to [extend()](/package_environment/#ciocore.package_environment.PackageEnvironment.extend).
         """
         self.platform = None
         self._env = {}
        
         self.extend(env_list, platform)
 
 
     def extend(self, env_list, platform=None):
         """
         Extend the Package environment with the given variable specifications.
 
-        Args:
-            env_list (object|list): Either:
-                * A list of dictionaries with properties: `name`, `value`, and `merge_policy`.
-                * An object with an `environment` key that contains a list of the dictionaries described above. The latter is the structure of a package. Therefore we can initialize or extend a PackageEnvironment with a package.
-            platform (str): Defaults to `None`. If env_list is a package, then the platform is taken from the package and the `platform` keyword is ignored. If env_list is a list, then if this is the first add, a platform should be specified, otherwise it will default to linux.
-                
-        The first time data is added to a PackageEnvironment, the platform is set in stone. Subsequent `adds` that try to change the platform are considered an error.
-
-        Each variable to be added specifies a merge_policy: `append`, `prepend`, or `exclusive` `append` and `prepend` can be thought of as lists= types. Once an individual variable has been initialized as a list, it can't be changed to `exclusive`. This means:
-    
-        1. It's not possible to overwrite variables that have been added as `append` or `prepend`.
-        2. Exclusive variables are always overwritten by subsequent adds.
+        Arguments:
 
-        Raises:
-            ValueError: Either an attempt to change the platform once initialized, or an invalid merge policy.
+        * **`env_list`** --  Either:
+        1. A list of dictionaries with properties: `name`, `value`, and `merge_policy`.
+        2. An object with an `environment` key that contains a list of the dictionaries described
+           above. The latter is the structure of a package. Therefore we can initialize or extend a
+           PackageEnvironment with a package.
+
+
+        Keyword Arguments:
 
-            
-        Example:
-            >>> from ciocore import api_client, package_tree, package_environment
-            >>> packages = api_client.request_software_packages()
-            >>> pt = package_tree.PackageTree(packages, product="cinema4d")
-            >>> one_dcc_name = pt.supported_host_names()[0]
-            cinema4d 21.209.RB305619 linux
+        * **`platform`** -- Defaults to `None`.
+            If env_list is a package, then the platform is taken from the package and the `platform`
+            keyword is ignored. If env_list is a list, then if this is the first add, a platform
+            should be specified, otherwise it willl default to linux.
         
-            >>> pkg = pt.find_by_name(one_dcc_name)
-            >>> pe = package_environment.PackageEnvironment(pkg)
-            >>> print(dict(pe))
-            {
-            "PATH": "/opt/maxon/cinema4d/21/cinema4d21.209vRB305619/bin",
-            "g_licenseServerRLM": "conductor-rlm:6112",
-            "LD_LIBRARY_PATH": "/opt/maxon/cinema4d/21/cinema4d21.209vRB305619/lib64:/opt/maxon/cinema4d/21/cinema4d21.209vRB305619/bin/resource/modules/python/libs/linux64/python.linux64.framework/lib64:/opt/maxon/cinema4d/21/cinema4d21.209vRB305619/bin/resource/modules/embree.module/libs/linux64",
-            "PYTHONPATH": "/opt/maxon/cinema4d/21/cinema4d21.209vRB305619/bin/resource/modules/python/libs/linux64/python.linux64.framework/lib64/python2.7/lib-dynload",
-            }
+        The first time data is added to a PackageEnvironment, the platform is set in stone.
+        Subsequent `adds` that try to change the platform are considered an error.
 
-            >>> extra_env = [
+        Each variable to be added specifies a merge_policy: `append` or `exclusive`. Once an
+        individual variable has been initialized with a merge policy, it can't be changed. This
+        means: 1. It's not possible to overwrite variables that have been set as append. 2.
+        Exclusive variables are always overwritten by subsequent adds.
+
+        Raises:
+
+        * **`ValueError`** -- Attempt to change the platform once initialized.
+        * **`ValueError`** -- Unknown merge policy
+        .
+
+        ???+ example
+            ``` python
+
+            from ciocore import api_client, package_tree, package_environment
+            packages = api_client.request_software_packages()
+            pt = package_tree.PackageTree(packages, product="cinema4d")
+            one_dcc_name = pt.supported_host_names()[0]
+            # 'cinema4d 21.209.RB305619 linux'
+            pkg = pt.find_by_name(one_dcc_name)
+            pe = package_environment.PackageEnvironment(pkg)
+            print(dict(pe))
+
+            # Result:
+            # {
+            # "PATH": "/opt/maxon/cinema4d/21/cinema4d21.209vRB305619/bin",
+            # "g_licenseServerRLM": "conductor-rlm:6112",
+            # "LD_LIBRARY_PATH": "/opt/maxon/cinema4d/21/cinema4d21.209vRB305619/lib64:/opt/maxon/cinema4d/21/cinema4d21.209vRB305619/bin/resource/modules/python/libs/linux64/python.linux64.framework/lib64:/opt/maxon/cinema4d/21/cinema4d21.209vRB305619/bin/resource/modules/embree.module/libs/linux64",
+            # "PYTHONPATH": "/opt/maxon/cinema4d/21/cinema4d21.209vRB305619/bin/resource/modules/python/libs/linux64/python.linux64.framework/lib64/python2.7/lib-dynload",
+            # }
+
+            extra_env = [
                 {"name":"PATH", "value": "/my/custom/scripts", "merge_policy":"append"},
                 {"name":"DEBUG_MODE", "value": "1", "merge_policy":"exclusive"}
             ]
-            >>> pe.extend(extra_env)
-            >>> print(dict(pe))
-            {
-                "PATH": "/opt/maxon/cinema4d/21/cinema4d21.209vRB305619/bin:/my/custom/scripts",
-                "g_licenseServerRLM": "conductor-rlm:6112",
-                "LD_LIBRARY_PATH": "/opt/maxon/cinema4d/21/cinema4d21.209vRB305619/lib64:/opt/maxon/cinema4d/21/cinema4d21.209vRB305619/bin/resource/modules/python/libs/linux64/python.linux64.framework/lib64:/opt/maxon/cinema4d/21/cinema4d21.209vRB305619/bin/resource/modules/embree.module/libs/linux64",
-                "PYTHONPATH": "/opt/maxon/cinema4d/21/cinema4d21.209vRB305619/bin/resource/modules/python/libs/linux64/python.linux64.framework/lib64/python2.7/lib-dynload",
-                "DEBUG_MODE": "1",
-            }
+
+            pe.extend(extra_env)
+            print(dict(pe))
+
+            # Result:
+            # {
+            #     "PATH": "/opt/maxon/cinema4d/21/cinema4d21.209vRB305619/bin:/my/custom/scripts",
+            #     "g_licenseServerRLM": "conductor-rlm:6112",
+            #     "LD_LIBRARY_PATH": "/opt/maxon/cinema4d/21/cinema4d21.209vRB305619/lib64:/opt/maxon/cinema4d/21/cinema4d21.209vRB305619/bin/resource/modules/python/libs/linux64/python.linux64.framework/lib64:/opt/maxon/cinema4d/21/cinema4d21.209vRB305619/bin/resource/modules/embree.module/libs/linux64",
+            #     "PYTHONPATH": "/opt/maxon/cinema4d/21/cinema4d21.209vRB305619/bin/resource/modules/python/libs/linux64/python.linux64.framework/lib64/python2.7/lib-dynload",
+            #     "DEBUG_MODE": "1",
+            # }
+            ```
         """
 
         if not env_list:
             return
 
         try:
             others = env_list["environment"]
@@ -91,21 +117,19 @@
         elif requested_platform and requested_platform != self.platform:
             raise ValueError("Can't change platform once initialized.")
 
         for var in others:
             name = var["name"]
             value = var["value"]
             policy = var["merge_policy"]
-            if policy not in ["append", "prepend", "exclusive"]:
+            if policy not in ["append", "exclusive"]:
                 raise ValueError("Unexpected merge policy: %s" % policy)
 
             if policy == "append":
                 self._append(name, value)
-            elif policy == "prepend":
-                self._prepend(name, value)
             else:
                 self._set(name, value)
 
 
     def _set(self, name, value):
         """Set the value of an exclusive variable.
 
@@ -131,30 +155,14 @@
                 raise ValueError(
                     "Can't change merge policy for '{}' from 'exclusive' to 'append'.".format(name)
                 )
         else:
             self._env[name] = []
         self._env[name].append(value)
 
-    def _prepend(self, name, value):
-        """Set the value of an append/prepend variable.
-
-        Can be appended to with subsequent adds.
-
-        It is an error if the variable has already been declared with policy=exclusive.
-        """
-        if self._env.get(name):
-            if not isinstance(self._env[name], list):
-                raise ValueError(
-                    "Can't change merge policy for '{}' from 'exclusive' to 'prepend'.".format(name)
-                )
-        else:
-            self._env[name] = []
-        self._env[name].insert(0, value)
-        
     def __iter__(self):
         """Cast the object as a dict."""
         sep = ";" if self.platform == "windows" else ":"
         for key in self._env:
             var = self._env[key]
             if isinstance(var, list):
                 yield key, sep.join(var)
```

## ciocore/package_tree.py

```diff
@@ -1,62 +1,225 @@
 """
-A class to provide available packages as DAG structure. In reality however, the structure is just two levels deep: **hosts** and **plugins**. 
+A class to provide available packages as DAG structure. 
 
-* DCCs such as **Maya** and **Cinema4D** are top-level host packages. 
-* Renderers and other plugins are children of those hosts.
-
-Methods are provided to traverse the tree to find packages by name, version, platform and so on. If you are writing submission tools there's no need to create a Package tree directly. It is recommended to use the singleton module: [ciocore.data](/data/).
-
-The only functions you should need from this module are:
-
-* [supported_host_names()](/package_tree/#ciocore.package_tree.PackageTree.supported_host_names)
-* [supported_plugins()](/package_tree/#ciocore.package_tree.PackageTree.supported_plugins)
+In reality, the structure is just two levels deep: **hosts** and **plugins**. DCCs such as **Maya** and
+**Cinema4D** are top-level host packages. Renderers and other plugins are children of those hosts.
+Methods are provided to traverse the tree to find packages by name, version, platform and so on. 
+
+If you are writing submission tools there's no need to create a Package tree directly.
+It is recommended to use the singleton module: [ciocore.data](/developer/ciocore/data/). The only
+functions you should need from this module are: [supported_host_names()](#supported_host_names) and [supported_plugins()](#supported_plugins)
 """
 
 import copy
 import json
 
+from ciocore.package_environment import PackageEnvironment
+
 WINDOWS = "windows"
 LINUX = "linux"
 
 #: The set of supported platforms, currently windows and linux.
 PLATFORMS = {WINDOWS, LINUX}
 
+
+def to_name(pkg):
+    """
+    Generate Name like `houdini 16.5.323 linux` or `maya 2016.SP3 linux`.
+
+    This name is derived from the product and version fields in a package. Note: It is not
+    necessarily possible to go the other way and extract version fields from the name..
+
+    Arguments:
+
+    * **`pkg`** -- An object with product, platform , and all version fields.
+
+    Returns:
+
+    * The package name.
+
+    ???+ example
+        ``` python
+
+        from ciocore import api_client, package_tree
+        packages = api_client.request_software_packages()
+        package_tree.to_name(packages[0])
+
+        # Result:
+        # redshift-maya 3.0.64 linux
+
+        ```
+    """
+
+    version_parts = [
+        pkg["major_version"],
+        pkg["minor_version"],
+        pkg["release_version"],
+        pkg["build_version"],
+    ]
+    version_string = (".").join([p for p in version_parts if p])
+    if pkg["platform"] not in PLATFORMS:
+        raise KeyError("Invalid platform: {}".format(pkg["platform"]))
+    return " ".join(filter(None, [pkg["product"], version_string, pkg["platform"]]))
+
+
+def _build_tree(packages, package):
+    """Build a tree of dependent software plugins.
+
+    Add a children key, and For each ID in the `plugins` key, add the package to which it refers to
+    children. Recurse until no more plugins are left.
+    """
+
+    for child_id in package.get("plugins", []):
+        child_package = next((c for c in packages if c["package_id"] == child_id), None)
+        if child_package:
+            child_package = _build_tree(packages, child_package)
+            package["children"].append(child_package)
+    package.pop("plugins", None)
+    return package
+
+
+
+
+def _find_by_name(branch, name, limit=None, depth=0):
+    """Given a name made by `to_name` find the package.
+
+    Name is typically part of a path. Limit will limit the search depth and is useful when you know
+    the package should be a direct child and not any descndent.
+    """
+    if not branch:
+        return None
+
+    if  branch.get("product") and name == to_name(branch):
+        return branch
+    depth += 1
+
+    if (not limit) or depth <= limit:
+        for child_branch in branch["children"]:
+            result = _find_by_name(child_branch, name, limit, depth)
+            if result:
+                return result
+    return None
+
+
+def _find_by_path(tree, path):
+    """Find the package uniquely described by this path.
+
+    This method loops through parts of the path name and searches the tree for each part.  When it
+    finds a matching package, we use that package as the root of the tree for the next search. As we
+    are searching for an exact path match, we limit the search to one level deep each time.
+    """
+
+    if not path:
+        return None
+
+
+    result = None
+    for name in [p for p in path.split("/") if p]:
+        tree = _find_by_name(tree, name, 1)
+        result = tree
+    return result
+
+
+def _to_path_list(tree, **kw):
+    """Get paths to all nodes.
+
+    This means starting at the level of the given tree, get all the paths to intermediate and leaf
+    nodes. This is useful for populating a chooser to choose packages fully qualified by path.
+    Houdini's tree widget for example, takes the below format unchanged and generates the
+    appropriate UI. 
+
+    * 'houdini 16.0.736 linux'
+    * 'houdini 16.0.736 linux/arnold-houdini 2.0.1 linux'
+    * 'houdini 16.0.736 linux/arnold-houdini 2.0.1 linux/al-shaders 1.0 linux'
+    """
+    parent_name = kw.get("parent_name", "")
+    paths = kw.get("paths", [])
+
+    for child_tree in tree["children"]:
+        name = ("/").join([n for n in [parent_name, to_name(child_tree)] if n])
+        paths.append(name)
+        paths = _to_path_list(child_tree, paths=paths, parent_name=name)
+    return paths
+
+
+def _clean_package(package):
+    """Remove some unwanted keys.
+
+    TODO - Some of these may turn out to be wanted after all.
+    """
+    pkg = copy.deepcopy(package)
+    for att in [
+        "build_id",
+        "time_updated",
+        "description",
+        "updated_at",
+        "time_created",
+        "plugin_hosts",
+        "relative_path",
+    ]:
+        pkg.pop(att, None)
+
+    pkg["children"] = []
+    return pkg
+
+
 class PackageTree(object):
     def __init__(self, packages,  *host_products, **kwargs):
-        """Build the tree with a list of packages.
-
-        Args:
-            packages (list): List of packages direct from the [Conductor packages endpoint](https://dashboard.conductortech.com/api/v1/ee/packages).
+        """
+        Build the tree with a list of packages.
 
-            *host_products: Filter the tree to contain only top-level host packages of products specified in this list and their plugins. If there are no host_products specified, and the product keyword is omitted, the tree contains all packages.
+        Arguments:
 
-        Keyword Args:
-            product (str): Build the tree from versions of a single product and its compatible plugins. Defaults to `None`, in which case the tree is built from all packages. It is an error to specify both host_products and product. If a nonexistent product is given, the PackageTree is empty. By specifying `product`, you can build the object based on a single plugin product.
-            platforms (set): Build the tree from versions for a specific platform. Defaults to the set `{"linux", "windows"}`.
+        * **`packages`** -- List of packages direct from the [Conductor packages
+        endpoint](https://dashboard.conductortech.com/api/v1/ee/packages).
+      
+      
+        * **`*host_products`** -- Filter the tree to contain only top-level host packages of
+        products specified in this list and their plugins. If there are no host_products specified,
+        and the product keyword is omitted, the tree contains all packages.
+
+        Keyword Arguments:
+
+        * **`product`** -- Build the tree from versions of a single product and its compatible
+            plugins -- Defaults to `None`, in which case the tree is built from all packages. It is
+            an error to specify both host_products and product. If a nonexistent product is given,
+            the PackageTree is empty. By specifying `product`, you can build the object based on a
+            single plugin product.
+        * **`platforms`** -- Build the tree from versions for a specific platform -- Defaults to the
+          set `{"linux", "windows"}`.
 
         Raises:
-            KeyError: An invalid platform was provided.
-            ValueError: Cannot choose both product and host_products.
 
-        Example:
-            >>> from ciocore import api_client, package_tree
+        * **`KeyError`** -- An invalid platform was provided.
+        * **`ValueError`** --  Cannot choose both product and host_products.
+
+
+        ???+ example
+            ``` python
+
+            from ciocore import api_client, package_tree
+
             # Request packages as a flat list from Conductor.
-            >>> packages = api_client.request_software_packages()
+            packages = api_client.request_software_packages()
+
             # Build tree of dependencies from packages list
-            >>> pt = package_tree.PackageTree(packages, "cinema4d", "maya-io")
-            >>> for path in pt.to_path_list():
-            >>>     print(path)
-            cinema4d 22.118.RB320081 linux
-            cinema4d 22.118.RB320081 linux/redshift-cinema4d 3.0.43 linux
-            cinema4d 22.118.RB320081 linux/redshift-cinema4d 3.0.45 linux
-            maya-io 2022.SP3 linux
+            pt = package_tree.PackageTree(packages, "cinema4d", "maya-io")
+            for path in pt.to_path_list():
+                print(path)
+
+            # Result
+            # cinema4d 22.118.RB320081 linux
+            # cinema4d 22.118.RB320081 linux/redshift-cinema4d 3.0.43 linux
+            # cinema4d 22.118.RB320081 linux/redshift-cinema4d 3.0.45 linux
+            # maya-io 2022.SP3 linux
+            # ...
+            ```
         """
 
-
         platforms = kwargs.get("platforms",PLATFORMS)
         product=kwargs.get("product")
 
         unknown_platforms = set(platforms) - PLATFORMS
         if unknown_platforms:
             raise KeyError("Unrecognized platform {}".format(" ".join(unknown_platforms)))
 
@@ -72,367 +235,281 @@
             for p in packages:
                 if not p["plugin_host_product"]:
                     if p["product"] in host_products or not host_products:
                         root_ids.append(p["package_id"])
 
         self._tree = _build_tree(packages, {"children": [], "plugins": root_ids})
 
-    def supported_host_names(self):
-        """
-        All host names from the software tree.
-
-        These names can be used to populate a dropdown menu. Then a single selection from that menu
-        can be used to retrieve the complete package in order to generate an environment dictionary
-        and get package IDs.
-
-        Returns:
-            list(str): Fully qualified DCC hosts of the form: `product version platform`.
-
-        Example:
-            >>> from ciocore import api_client, package_tree
-            >>> packages = api_client.request_software_packages()
-            >>> pt = package_tree.PackageTree(packages, product="cinema4d")
-            >>> pt.supported_host_names()
-            cinema4d 21.209.RB305619 linux
-            cinema4d 22.116.RB316423 linux
-            cinema4d 22.118.RB320081 linux
-            cinema4d 23.110.RB330286 linux
-            cinema4d 24.111 linux
-            cinema4d 24.111 windows
+    def find_by_name(self, name, limit=None):
         """
+        Search the tree for a product with the given name.
 
-        paths = []
-        for pkg in self._tree["children"]:
-            paths.append(to_name(pkg))
-        return sorted(paths)
+        Arguments:
 
-    def supported_plugins(self, host):
-        """
-        Find the plugins that are children of the given DCC host.
+        * **`name`** -- This name is the name originally constructed from the package using
+          to_name(). It must be an exact match with product, version, platform.
+          For example: `maya 2018.0 windows`
 
-        The result does not contain platform information since we assume that plugins are compatible with the DCC host that was used to request them.
+        Keyword Arguments:
 
-        Args:
-            host (str): Name of the DCC host, typically one of the entries returned by [supported_host_names()](/package_tree/#ciocore.package_tree.PackageTree.supported_host_names).
+        * **`limit`** -- Limit the search depth -- Defaults to `None`.
 
         Returns:
-            list(dict): Each entry contains a plugin product and a list of versions.
 
-        Example:
-            >>> from ciocore import api_client, package_tree 
-            >>> packages = api_client.request_software_packages() 
-            >>> pt = package_tree.PackageTree(packages, product="cinema4d") 
-            >>> name = pt.supported_host_names()[0]
-            >>> pt.supported_plugins(name)
-            [
-                {
-                    "plugin": "arnold-cinema4d",
-                    "versions": [
-                        "3.3.2.100",
-                        "3.3.3.0"
-                    ]
-                },
-                {
-                    "plugin": "redshift-cinema4d",
-                    "versions": [
-                        "2.6.54",
-                        "2.6.56",
-                        "3.0.21",
-                        "3.0.22",
-                    ],
-                },
-            ]
-        """
+        * The package that matches.
 
-        try:
-            subtree = self.find_by_name(host)
-            plugin_versions = _to_path_list(subtree)
-        except TypeError:
-            return []
+        ???+ example
+            ``` python
 
-        if not plugin_versions:
-            return []
+            from ciocore import api_client, package_tree
+            packages = api_client.request_software_packages()
 
-        plugin_dict = {}
-        for plugin, version, _ in [pv.split(" ") for pv in plugin_versions]:
-            if plugin not in plugin_dict:
-                plugin_dict[plugin] = []
-            plugin_dict[plugin].append(version)
-
-        # convert to list so it can be sorted
-        plugins = []
-        for key in plugin_dict:
-            plugins.append({"plugin": key, "versions": sorted(plugin_dict[key])})
-
-        return sorted(plugins, key=lambda k: k["plugin"])
-
-    def find_by_name(self, name, limit=None):
-        """
-        Search the tree for a product with the given name.
-
-        Args:
-            name (str): The name constructed from the package using to_name(). It must be an exact match with product, version, and platform. For example: `maya 2018.0 windows`
-
-        Keyword Args:
-            limit (int): Limit the search depth. Defaults to `None`.
-
-        Returns:
-            object: The package that matches.
-
-        Example:
-            >>> from ciocore import api_client, package_tree
-            >>> packages = api_client.request_software_packages()
-            >>> pt = package_tree.PackageTree(packages, product="cinema4d")
-            >>> pt.find_by_name("redshift-cinema4d 3.0.64 linux")
-            {
-                'platform': 'linux',
-                'plugin_host_product': 'cinema4d',
-                'product': 'redshift-cinema4d',
-                'major_version': '3',
-                'release_version': '64',
-                'vendor': 'maxon',
-                'children': [],
-                ...
-            }
+            pt = package_tree.PackageTree(packages, product="cinema4d")
+            pt.find_by_name("redshift-cinema4d 3.0.64 linux")
+
+            # Result:
+            # {
+            #     'platform': 'linux',
+            #     'plugin_host_product': 'cinema4d',
+            #     'product': 'redshift-cinema4d',
+            #     'major_version': '3',
+            #     'release_version': '64',
+            #     'vendor': 'maxon',
+            #     'children': [],
+            #     ...
+            # }
+            ```
         """
 
         return _find_by_name(self._tree, name, limit, 0)
 
     def find_by_path(self, path):
         """
         Find the package uniquely described by the given path.
 
         The path is of the form returned by the to_path_list() method.
 
-        Args:
-            path (str): The path
+        Arguments:
+
+        * **`path`** -- The path
 
         Returns:
-            object: The package or None if no package exists with the given path.
 
-        Example:
-            >>> from ciocore import api_client, package_tree, package_environment
-            >>> packages = api_client.request_software_packages()
-            >>> pt = package_tree.PackageTree(packages, product="cinema4d")
-            >>> pt.find_by_path("cinema4d 24.111 linux/redshift-cinema4d 3.0.62 linux")
-            {
-                'platform': 'linux',
-                'plugin_host_product': 'cinema4d',
-                'product': 'redshift-cinema4d',
-                'major_version': '3',
-                'release_version': '62',
-                'vendor': 'maxon',
-                'children': [],
-                'plugin_host_version': "24",
-                ...
-            }
+        * The package or None if no package exists with the given path.
+
+        ???+ example
+            ``` python
+
+            from ciocore import api_client, package_tree, package_environment
+            packages = api_client.request_software_packages()
+            pt = package_tree.PackageTree(packages, product="cinema4d")
+            pt.find_by_path("cinema4d 24.111 linux/redshift-cinema4d 3.0.62 linux")
+
+            # Result:
+            # {
+            #     'platform': 'linux',
+            #     'plugin_host_product': 'cinema4d',
+            #     'product': 'redshift-cinema4d',
+            #     'major_version': '3',
+            #     'release_version': '62',
+            #     'vendor': 'maxon',
+            #     'children': [],
+            #     'plugin_host_version': "24",
+            #     ...
+            # }
+            ```
         """
-        return _find_by_path(self._tree, path)
 
+        return _find_by_path(self._tree, path)
 
+ 
     def to_path_list(self, name=None):
         """
         Get paths to all nodes.
-
+        
         This is useful for populating a chooser to choose packages fully qualified by path.
-        Houdini's tree widget, for example, takes the below format unchanged and generates the
-        appropriate UI.
+        Houdini's tree widget for example, takes the below format unchanged and generates the
+        appropriate UI. 
 
-        Args:
-            name (str): Get paths below the tree represented by the name. Defaults to None (root node).
+        Keyword Arguments:
+
+        * **`name`** -- Get paths below the tree represented by the name. -- Defaults to `None` (root node).
 
         Returns:
-            list(str): Paths to all nodes in the tree.
 
-        Example:
-            >>> from ciocore import api_client, package_tree
-            >>> packages = api_client.request_software_packages()
-            >>> pt = package_tree.PackageTree(packages, product="cinema4d")
-            >>> pt.to_path_list()
-            cinema4d 22.118.RB320081 linux
-            cinema4d 22.118.RB320081 linux/redshift-cinema4d 3.0.43 linux
-            cinema4d 22.118.RB320081 linux/redshift-cinema4d 3.0.45 linux
-            cinema4d 22.118.RB320081 linux/redshift-cinema4d 3.0.22 linux
-            cinema4d 22.118.RB320081 linux/arnold-cinema4d 3.3.2.100 linux
+        * _description_.
+
+        ???+ example
+            ``` python
+
+            from ciocore import api_client, package_tree
+            packages = api_client.request_software_packages()
+            pt = package_tree.PackageTree(packages, product="cinema4d")
+
+            pt.to_path_list()
+            # Result:
+            # cinema4d 22.118.RB320081 linux
+            # cinema4d 22.118.RB320081 linux/redshift-cinema4d 3.0.43 linux
+            # cinema4d 22.118.RB320081 linux/redshift-cinema4d 3.0.45 linux
+            # cinema4d 22.118.RB320081 linux/redshift-cinema4d 3.0.22 linux
+            # cinema4d 22.118.RB320081 linux/arnold-cinema4d 3.3.2.100 linux
             ...
 
-            >>> pt.to_path_list(name="cinema4d 24.111 linux")
-            redshift-cinema4d 3.0.57 linux
-            redshift-cinema4d 3.0.62 linux
-            redshift-cinema4d 3.0.45 linux
-            redshift-cinema4d 3.0.64 linux
+            pt.to_path_list(name="cinema4d 24.111 linux")
+            # Result:
+            # redshift-cinema4d 3.0.57 linux
+            # redshift-cinema4d 3.0.62 linux
+            # redshift-cinema4d 3.0.45 linux
+            # redshift-cinema4d 3.0.64 linux
+
+
+            ```
         """
         if name:
             subtree = self.find_by_name(name)
             return _to_path_list(subtree)
         return _to_path_list(self._tree)
 
     def platforms(self):
         """
-        Get the platforms represented by packages in the tree.
+        Get the set of platforms represented by packages in the tree.
 
-        Returns:
-            set: The set of platforms.
+        Returns a set.
         """
 
         # No need to recurse. Plugins are assumed to be compatible with the host.
         return set([host["platform"] for host in self._tree["children"]])
 
     def json(self):
         """
-        The whole tree of softwware as json.
+        The whole tree as json.
 
         Returns:
-            str: JSON.
+
+        * JSON object.
 
         """
         return json.dumps(self._tree)
 
     def __bool__(self):
         return True if self._tree["children"] else False
 
     def __nonzero__(self):
         # Python 2.7
         return self.__bool__()
 
     def as_dict(self):
         """
         Returns:
-            dict: The underlying software dictionary.
+
+        * The underlying dictionary.
 
         """
         return self._tree
 
+    def supported_host_names(self):
+        """
+        All host names from the software tree.
 
-def to_name(pkg):
-    """
-    Generate a name like `houdini 16.5.323 linux` or `maya 2016.SP3 linux`.
-
-    This name is derived from the product and version fields in a package. Note: It is not
-    necessarily possible to go the other way and extract version fields from the name.
-
-    Args:
-        pkg (object): An object with product, platform, and all version fields.
-
-    Returns:
-        str: The package name.
-
-    Examples:
-        >>> from ciocore import api_client, package_tree
-        >>> packages = api_client.request_software_packages()
-        >>> package_tree.to_name(packages[0])
-        redshift-maya 3.0.64 linux
-
-    """ 
-
-    version_parts = [
-        pkg["major_version"],
-        pkg["minor_version"],
-        pkg["release_version"],
-        pkg["build_version"],
-    ]
-    version_string = (".").join([p for p in version_parts if p])
-    if pkg["platform"] not in PLATFORMS:
-        raise KeyError("Invalid platform: {}".format(pkg["platform"]))
-    return " ".join(filter(None, [pkg["product"], version_string, pkg["platform"]]))
-
-
-def _build_tree(packages, package):
-    """Build a tree of dependent software plugins.
-
-    Add a children key, and For each ID in the `plugins` key, add the package to which it refers to
-    children. Recurse until no more plugins are left.
-    """
-
-    for child_id in package.get("plugins", []):
-        child_package = next((c for c in packages if c["package_id"] == child_id), None)
-        if child_package:
-            child_package = _build_tree(packages, child_package)
-            package["children"].append(child_package)
-    package.pop("plugins", None)
-    return package
-
+        These names can be used to populate a dropdown menu. Then a single selection from that menu
+        can be used to retrieve the complete package in order to generate an environment dictionary
+        and get package IDs.
 
+        Returns:
 
+        * A list of fully qualified DCC hosts of the form: `product`, `version`, `platform`.
 
-def _find_by_name(branch, name, limit=None, depth=0):
-    """Given a name made by `to_name` find the package.
+        ???+ example
+            ``` python
 
-    Name is typically part of a path. Limit will limit the search depth and is useful when you know
-    the package should be a direct child and not any descndent.
-    """
-    if not branch:
-        return None
-
-    if  branch.get("product") and name == to_name(branch):
-        return branch
-    depth += 1
-
-    if (not limit) or depth <= limit:
-        for child_branch in branch["children"]:
-            result = _find_by_name(child_branch, name, limit, depth)
-            if result:
-                return result
-    return None
+                from ciocore import api_client, package_tree
+                packages = api_client.request_software_packages()
+                pt = package_tree.PackageTree(packages,
+                product="cinema4d")
+
+                pt.supported_host_names()
+
+                # Result:
+                # cinema4d 21.209.RB305619 linux
+                # cinema4d 22.116.RB316423 linux
+                # cinema4d 22.118.RB320081 linux
+                # cinema4d 23.110.RB330286 linux
+                # cinema4d 24.111 linux
+                # cinema4d 24.111 windows
 
+            ```
+        """
 
-def _find_by_path(tree, path):
-    """Find the package uniquely described by this path.
+        paths = []
+        for pkg in self._tree["children"]:
+            paths.append(to_name(pkg))
+        return sorted(paths)
 
-    This method loops through parts of the path name and searches the tree for each part.  When it
-    finds a matching package, we use that package as the root of the tree for the next search. As we
-    are searching for an exact path match, we limit the search to one level deep each time.
-    """
+    def supported_plugins(self, host):
+        """
+        Find the plugins that are children of the given DCC host.
 
-    if not path:
-        return None
+        The result does not contain platform information since we assume that plugins are compatible with the DCC host that was used to
+        request them.
 
+        Arguments:
 
-    result = None
-    for name in [p for p in path.split("/") if p]:
-        tree = _find_by_name(tree, name, 1)
-        result = tree
-    return result
+        * **`host`** -- Name of the DCC host, typically one of the entries returned by supported_host_names().
 
+        Returns:
 
-def _to_path_list(tree, **kw):
-    """Get paths to all nodes.
+        * A list of dictionaries, where each entry contains a plugin product and a list of versions.
 
-    This means starting at the level of the given tree, get all the paths to intermediate and leaf
-    nodes. This is useful for populating a chooser to choose packages fully qualified by path.
-    Houdini's tree widget for example, takes the below format unchanged and generates the
-    appropriate UI. 
+        ???+ example
+            ``` python
 
-    * 'houdini 16.0.736 linux'
-    * 'houdini 16.0.736 linux/arnold-houdini 2.0.1 linux'
-    * 'houdini 16.0.736 linux/arnold-houdini 2.0.1 linux/al-shaders 1.0 linux'
-    """
-    parent_name = kw.get("parent_name", "")
-    paths = kw.get("paths", [])
+            from ciocore import api_client, package_tree packages =
+            api_client.request_software_packages() pt = package_tree.PackageTree(packages,
+            product="cinema4d")
+            name = pt.supported_host_names()[0]
+
+            pt.supported_plugins(name)
+            # Result:
+            #
+            # [
+            #     {
+            #         "plugin": "arnold-cinema4d",
+            #         "versions": [
+            #             "3.3.2.100",
+            #             "3.3.3.0"
+            #         ]
+            #     },
+            #     {
+            #         "plugin": "redshift-cinema4d",
+            #         "versions": [
+            #             "2.6.54",
+            #             "2.6.56",
+            #             "3.0.21",
+            #             "3.0.22",
+            #         ],
+            #     },
+            # ]
 
-    for child_tree in tree["children"]:
-        name = ("/").join([n for n in [parent_name, to_name(child_tree)] if n])
-        paths.append(name)
-        paths = _to_path_list(child_tree, paths=paths, parent_name=name)
-    return paths
+            ```
+        """
 
+        try:
+            subtree = self.find_by_name(host)
+            plugin_versions = _to_path_list(subtree)
+        except TypeError:
+            return []
 
-def _clean_package(package):
-    """Remove some unwanted keys.
+        if not plugin_versions:
+            return []
 
-    TODO - Some of these may turn out to be wanted after all.
-    """
-    pkg = copy.deepcopy(package)
-    for att in [
-        "build_id",
-        "time_updated",
-        "description",
-        "updated_at",
-        "time_created",
-        "plugin_hosts",
-        "relative_path",
-    ]:
-        pkg.pop(att, None)
+        plugin_dict = {}
+        for plugin, version, _ in [pv.split(" ") for pv in plugin_versions]:
+            if plugin not in plugin_dict:
+                plugin_dict[plugin] = []
+            plugin_dict[plugin].append(version)
 
-    pkg["children"] = []
-    return pkg
+        # convert to list so it can be sorted
+        plugins = []
+        for key in plugin_dict:
+            plugins.append({"plugin": key, "versions": sorted(plugin_dict[key])})
 
+        return sorted(plugins, key=lambda k: k["plugin"])
```

## ciocore/uploader/_uploader.py

```diff
@@ -11,23 +11,16 @@
 try:
     import Queue as queue
 except ImportError:
     import queue
 
 import ciocore
 from ciocore import config
-from ciocore import (
-    api_client,
-    client_db,
-    common,
-    file_utils,
-    loggeria,
-    worker,
-    exceptions,
-)
+from ciocore import api_client, client_db, common, file_utils, loggeria, worker, exceptions
+from ciocore.common import CONDUCTOR_LOGGER_NAME, SIGINT_EXIT
 
 from .upload_stats import UploadStats
 
 logger = logging.getLogger("{}.uploader".format(loggeria.CONDUCTOR_LOGGER_NAME))
 
 SINGLEPART = "singlepart"
 MULTIPART = "multipart"
@@ -49,43 +42,39 @@
         logger.debug("job is %s", job)
         filename, submission_time_md5 = job
         filename = str(filename)
         current_md5, cache_hit = self.get_md5(filename)
 
         # if a submission time md5 was provided then check against it
         if submission_time_md5:
-            logger.info(
-                "Enforcing md5 match: %s for: %s", submission_time_md5, filename
-            )
+            logger.info("Enforcing md5 match: %s for: %s", submission_time_md5, filename)
             if current_md5 != submission_time_md5:
                 message = "MD5 of %s has changed since submission\n" % filename
                 message += "submitted md5: %s\n" % submission_time_md5
                 message += "current md5:   %s\n" % current_md5
-                message += (
-                    "This is likely due to the file being written to after the user"
-                )
+                message += "This is likely due to the file being written to after the user"
                 message += " submitted the job but before it got uploaded to conductor"
                 logger.error(message)
                 raise Exception(message)
         self.metric_store.set_dict("file_md5s", filename, current_md5)
         self.metric_store.set_dict("file_md5s_cache_hit", filename, cache_hit)
         size_bytes = os.path.getsize(filename)
-
+        
         return (filename, current_md5, size_bytes)
 
     def get_md5(self, filepath):
         """
         For the given filepath, return a tuple of its md5 and whether the cache was used.
 
         Use the sqlite db cache to retrive this (if the cache is valid), otherwise generate the md5
         from scratch
         """
-
+        
         cache_hit = True
-
+        
         # If md5 caching is disable, then just generate the md5 from scratch
         if not self.md5_caching:
             cache_hit = False
             return common.generate_md5(filepath, poll_seconds=5), cache_hit
 
         # Otherwise attempt to use the md5 cache
         file_info = get_file_info(filepath)
@@ -103,17 +92,15 @@
         logger.debug("Using md5 cache for file: %s", filepath)
         return file_cache["md5"], cache_hit
 
     def cache_file_info(self, file_info):
         """
         Store the given file_info into the database
         """
-        client_db.FilesDB.add_file(
-            file_info, db_filepath=self.database_filepath, thread_safe=True
-        )
+        client_db.FilesDB.add_file(file_info, db_filepath=self.database_filepath, thread_safe=True)
 
 
 class MD5OutputWorker(worker.ThreadWorker):
     """
     This worker will batch the computed md5's into self.batch_size chunks. It will send a partial
     batch after waiting self.wait_time seconds
     """
@@ -136,80 +123,80 @@
         if self.batch:
             logger.debug("sending batch: %s", self.batch)
             self.put_job(self.batch)
             self.batch = []
 
     @common.dec_catch_exception(raise_=True)
     def target(self, thread_int):
+
         while not common.SIGINT_EXIT:
+            
             job = None
 
             try:
                 logger.debug("Worker querying for job")
                 job = self.in_queue.get(block=True, timeout=self.wait_time)
                 logger.debug("Got job")
-                queue_size = self.in_queue.qsize()
-
+                queue_size = self.in_queue.qsize()                       
+            
             except:
+                
                 logger.debug("No jobs available")
-
+                
                 if self._job_counter.value >= self.task_count:
+                    
                     if self.batch:
                         self.ship_batch()
-
+                    
                     logger.debug("Worker has completed all of its tasks (%s)", job)
                     self.thread_complete_counter.decrement()
                     break
-
+                
                 elif self._job_counter.value == 0:
                     logger.debug("Worker waiting for first job")
-
+                
                 time.sleep(1)
                 continue
-
-            logger.debug("Worker got job %s", job)
+                
+            logger.debug("Worker got job %S", job)
             self._job_counter.increment()
-            logger.debug(
-                "Processing Job '%s' #%s on %s. %s tasks remaining in queue",
-                job,
-                self._job_counter.value,
-                self,
-                queue_size,
-            )
+            logger.debug("Processing Job '%s' #%s on %s. %s tasks remaining in queue", job,
+                                                                                       self._job_counter.value, 
+                                                                                       self, 
+                                                                                       queue_size)
 
             try:
                 self.check_for_poison_pill(job)
-
+                
                 # add file info to the batch list
                 self.batch.append(
                     {
                         "path": job[0],
                         "hash": job[1],
                         "size": job[2],
                     }
                 )
-
+    
                 # if the batch is self.batch_size, ship it
                 if len(self.batch) == self.batch_size:
                     self.ship_batch()
-
+    
                 # mark this task as done
                 self.mark_done()
-
+                
             except Exception as exception:
                 logger.exception('CAUGHT EXCEPTION on job "%s" [%s]:\n', job, self)
 
                 # if there is no error queue to dump data into, then simply raise the exception
                 if self.error_queue is None:
                     raise
-
+                
                 self.error_queue.put(sys.exc_info())
                 # exit the while loop to stop the thread
-                break
-
+                break                
 
 class HttpBatchWorker(worker.ThreadWorker):
     """
     This worker receives a batched list of files (path, hash, size) and makes an batched http api
     call which returns a mixture of multiPartURLs (if any) and singlePartURLs (if any), as well
     as the account's KMS key name if available.
 
@@ -274,38 +261,36 @@
         )
 
         if response_code == 200:
             url_list = json.loads(response_str)
             return url_list
         if response_code == 204:
             return None
-        raise Exception(
-            "%s Failed request to: %s\n%s" % (response_code, uri_path, response_str)
-        )
-
+        raise Exception("%s Failed request to: %s\n%s" % (response_code, uri_path, response_str))
+    
     def do_work(self, job, thread_int):
         logger.debug("getting upload urls for %s", job)
         result = self.make_request(job)
 
         # Determine which files have already been uploaded by looking at the difference between
         # the file paths in job and the file paths returned by the request. Only files that need
         # to be uploaded are returned by the request
-        incoming_file_paths = set([item["path"] for item in job])
+        incoming_file_paths = set([ item['path'] for item in job] )
 
         if result:
+            
             for item_type in result.values():
                 for item in item_type:
-                    incoming_file_paths.remove(item["filePath"])
+                    incoming_file_paths.remove(item['filePath'])
 
         for path in incoming_file_paths:
             self.metric_store.increment("already_uploaded", True, path)
-
+        
         return result
 
-
 """
 This worker subscribes to a queue of list of file uploads (multipart and singlepart).
 
 For each item on the queue, it uses the HttpBatchWorker response payload fileSize (bytes) to be
 uploaded, and aggregates the total size for all uploads.
 
 It then places a tuple of (filepath, file_size, upload, type of upload(multipart or singlepart))
@@ -330,32 +315,32 @@
 
         if job:
             # iterate through singlepart urls
             for singlepart_upload in job.get("singlePartURLs", []):
                 path = singlepart_upload["filePath"]
                 file_size = singlepart_upload["fileSize"]
                 upload_url = singlepart_upload["preSignedURL"]
-
+                
                 self.metric_store.increment("bytes_to_upload", file_size, path)
                 self.metric_store.increment("num_files_to_upload")
                 logger.debug("Singlepart, adding task %s", path)
-
+    
                 self.put_job((path, file_size, upload_url, SINGLEPART))
-
+    
             # iterate through multipart
             for multipart_upload in job.get("multiPartURLs", []):
                 path = multipart_upload["filePath"]
                 file_size = multipart_upload["fileSize"]
-
+                
                 self.metric_store.increment("bytes_to_upload", file_size, path)
                 self.metric_store.increment("num_files_to_upload")
                 logger.debug("Multipart, adding task %s", path)
                 self.put_job((path, file_size, multipart_upload, MULTIPART))
 
-        # make sure we return None, so no message is automatically added to the out_queue
+        # make sure we return None, so no message is automatically added to the out_queue        
         return None
 
 
 class UploadWorker(worker.ThreadWorker):
     """
     This worker receives a either (filepath: signed_upload_url) pair or (filepath: multipart (dict))
     and performs an upload of the specified file to the provided url.
@@ -378,64 +363,58 @@
                 # TODO: can we wrap this in a retry?
                 yield data
 
                 # report upload progress
                 self.metric_store.increment("bytes_uploaded", len(data), filename)
 
     def do_work(self, job, thread_int):
+        
         if job:
             kms_key_name = None
-
+            
             try:
                 filename = job[0]
                 file_size = job[1]
                 upload = job[2]
                 upload_type = job[3]
 
             except Exception:
                 logger.error("Issue with job (%s): %s", len(job), job)
                 raise
-
+            
             if len(job) > 4:
                 kms_key_name = job[4]
-
+    
             md5 = self.metric_store.get_dict("file_md5s", filename)
-
+    
             try:
                 if upload_type == SINGLEPART:
-                    return self.do_singlepart_upload(
-                        upload, filename, file_size, md5, kms_key_name
-                    )
+                    return self.do_singlepart_upload(upload, filename, file_size, md5, kms_key_name)
                 elif upload_type == MULTIPART:
                     return self.do_multipart_upload(upload, filename, md5)
-
-                raise Exception(
-                    "upload_type is '%s' expected %s or %s"
-                    % (upload_type, SINGLEPART, MULTIPART)
-                )
-
+    
+                raise Exception("upload_type is '{}' expected %s or %s" % (upload_type, SINGLEPART, MULTIPART))
+            
             except Exception as err_msg:
+                
                 real_md5 = common.get_base64_md5(filename)
 
                 if isinstance(err_msg, requests.exceptions.HTTPError):
-                    error_message = f"Upload of {filename} failed with a response code {err_msg.response.status_code} ({err_msg.response.reason}) (expected '{md5}', got '{real_md5}')"
+                    error_message = "Upload of {} failed with a response code {} ({}) (expected '{}', got '{}')".format(filename, err_msg.response.status_code, err_msg.response.reason, md5, real_md5)
+                
                 else:
-                    error_message = (
-                        f"Upload of {filename} failed. (expected '{md5}', got '{real_md5}') {str(err_msg)}"
-                    )
-
+                    error_message = "Upload of {} failed. (expected '{}', got '{}') {}".format(filename,  md5, real_md5, str(err_msg))
+                
                 logger.error(error_message)
                 raise exceptions.UploadError(error_message)
-
+            
         return worker.EMPTY_JOB
 
     @common.DecRetry(retry_exceptions=api_client.CONNECTION_EXCEPTIONS, tries=5)
-    def do_singlepart_upload(
-        self, upload_url, filename, file_size, md5, kms_key_name=None
-    ):
+    def do_singlepart_upload(self, upload_url, filename, file_size, md5, kms_key_name=None):
         """
         Note that for GCS we don't rely on the make_request's own retry mechanism because we need to
         recreate the chunked_reader generator before retrying the request. Instead, we wrap this
         method in a retry decorator.
 
         We cannot reuse make_request method for S3 because it adds auth and Transfer-Encoding
         headers that S3 does not accept.
@@ -467,16 +446,18 @@
                 response.close()
 
                 # report upload progress
                 self.metric_store.increment("bytes_uploaded", file_size, filename)
 
                 return response
         else:
-            headers = {"Content-MD5": md5, "Content-Type": "application/octet-stream"}
-
+            headers = {"Content-MD5": md5, 
+                       "Content-Type": "application/octet-stream"
+            }
+            
             if kms_key_name:
                 headers["x-goog-encryption-kms-key-name"] = kms_key_name
 
             return self.api_client.make_request(
                 conductor_url=upload_url,
                 headers=headers,
                 data=self.chunked_reader(filename),
@@ -558,101 +539,106 @@
             self.metric_store.increment("bytes_uploaded", content_length, filename)
 
             # close response object to add back to pool
             # https://requests.readthedocs.io/en/master/user/advanced/#body-content-workflow
             response.close()
 
             return response.headers
-
+        
     def is_complete(self):
         # Get the number of files already uploaded as they are not passed to the Upload
         # worker
         file_store = self.metric_store.get("files")
-
+        
         if isinstance(file_store, dict):
-            already_completed_uploads = len(
-                [x for x in file_store.values() if x["already_uploaded"]]
-            )
+            already_completed_uploads = len([x for x in file_store.values() if x['already_uploaded']])
             queue_size = self.out_queue.qsize()
-            logger.debug(
-                "Is complete? out_queue_size=%s, completed_uploads=%s, task_count=%s",
-                queue_size,
-                already_completed_uploads,
-                self.task_count,
-            )
-
+            logger.debug("Is complete? out_queue_size=%s, completed_uploads=%s, task_count=%s", queue_size,
+                                                                                                already_completed_uploads,
+                                                                                                self.task_count)
+        
             return (queue_size + already_completed_uploads) >= self.task_count
-
+        
         else:
             logger.debug("Is complete?: files not initialized yet")
             return False
 
 
 class Uploader(object):
-    sleep_time = 10
 
+    sleep_time = 10
+    
     CLIENT_NAME = "Uploader"
 
     def __init__(self, args=None):
         logger.debug("Uploader.__init__")
         self.api_client = api_client.ApiClient()
         self.args = args or {}
         logger.debug("args: %s", self.args)
 
         self.location = self.args.get("location")
         self.project = self.args.get("project")
         self.progress_callback = None
         self.cancel = False
         self.error_messages = []
         self.num_files_to_process = 0
-
+        
         self.report_status_thread = None
         self.monitor_status_thread = None
-
+        
     def emit_progress(self, upload_stats):
-        if self.progress_callback:
+        
+        if self.progress_callback:        
             self.progress_callback(upload_stats)
 
     def prepare_workers(self):
         logger.debug("preparing workers...")
-
+        
         if isinstance(threading.current_thread(), threading._MainThread):
             common.register_sigint_signal_handler()
         self.manager = None
 
     def create_manager(self, project=None):
+       
         job_description = [
             (
                 MD5Worker,
                 [],
                 {
                     "thread_count": self.args["thread_count"],
                     "database_filepath": self.args["database_filepath"],
                     "md5_caching": self.args["md5_caching"],
                 },
             ),
-            (MD5OutputWorker, [], {"thread_count": 1}),
+            (   
+                MD5OutputWorker, [], {"thread_count": 1}
+            ),
             (
                 HttpBatchWorker,
                 [],
                 {"thread_count": self.args["thread_count"], "project": project},
             ),
-            (FileStatWorker, [], {"thread_count": 1}),
-            (UploadWorker, [], {"thread_count": self.args["thread_count"]}),
+            (   
+                FileStatWorker, [], {"thread_count": 1}
+            ),
+            (
+                UploadWorker, [], {"thread_count": self.args["thread_count"]}
+            ),
         ]
 
         manager = worker.JobManager(job_description)
         manager.start()
         return manager
 
     @common.dec_catch_exception(raise_=True)
     def report_status(self):
         logger.debug("started report_status thread")
         update_interval = 15
         while True:
+
             # don't report status if we are doing a local_upload
             if not self.upload_id:
                 logger.debug("not updating status as we were not provided an upload_id")
                 return
 
             if self.working:
                 bytes_to_upload = self.manager.metric_store.get("bytes_to_upload")
@@ -671,339 +657,300 @@
                         use_api_key=True,
                     )
 
                 except Exception:
                     logger.error("could not report status:")
                     logger.error(traceback.print_exc())
                     logger.error(traceback.format_exc())
-
+                    
             else:
                 break
 
             time.sleep(update_interval)
 
     def create_report_status_thread(self):
         logger.debug("creating reporter thread")
-        self.report_status_thread = threading.Thread(
-            name="ReporterThread", target=self.report_status
-        )
+        self.report_status_thread = threading.Thread(name="ReporterThread", target=self.report_status)
         self.report_status_thread.daemon = True
         self.report_status_thread.start()
 
     @common.dec_catch_exception(raise_=True)
     def monitor_status(self, progress_handler):
         logger.debug("starting monitor_status thread")
         update_interval = 5
 
         def sleep():
             time.sleep(update_interval)
 
         while True:
             if self.working:
                 try:
-                    upload_stats = UploadStats.create(
-                        self.manager.metric_store,
-                        self.num_files_to_process,
-                        self.job_start_time,
-                    )
+                    upload_stats = UploadStats.create (self.manager.metric_store, self.num_files_to_process, self.job_start_time)
                     progress_handler(upload_stats)
                 except Exception as e:
                     print(e)
                     print(traceback.format_exc())
-
+                    
             else:
                 break
             sleep()
 
     def create_monitor_status_thread(self):
         logger.debug("creating console status thread")
-        self.monitor_status_thread = threading.Thread(
-            name="PrintStatusThread",
-            target=self.monitor_status,
-            args=(self.emit_progress,),
-        )
+        self.monitor_status_thread = threading.Thread(name="PrintStatusThread", target=self.monitor_status, args=(self.emit_progress,))
 
         # make sure threads don't stop the program from exiting
         self.monitor_status_thread.daemon = True
 
         # start thread
         self.monitor_status_thread.start()
 
     def mark_upload_finished(self, upload_id, upload_files):
-        data = {
-            "upload_id": upload_id,
-            "status": "server_pending",
-            "upload_files": upload_files,
-        }
+
+        data = {"upload_id": upload_id, "status": "server_pending", "upload_files": upload_files}
 
         self.api_client.make_request(
-            "/uploads/%s/finish" % upload_id,
-            data=json.dumps(data),
-            verb="POST",
-            use_api_key=True,
+            "/uploads/%s/finish" % upload_id, data=json.dumps(data), verb="POST", use_api_key=True
         )
         return True
 
     def mark_upload_failed(self, error_message, upload_id):
         logger.error("Upload failed: \n%s" % error_message)
 
         # report error_message to the app
         self.api_client.make_request(
-            "/uploads/%s/fail" % upload_id,
-            data=error_message,
-            verb="POST",
-            use_api_key=True,
+            "/uploads/%s/fail" % upload_id, data=error_message, verb="POST", use_api_key=True
         )
 
         return True
 
-    def assets_only(self, *paths):
-        processed_filepaths = file_utils.process_upload_filepaths(paths)
-        file_map = {path: None for path in processed_filepaths}
-        self.handle_upload_response(project=None, upload_files=file_map)
-
     def handle_upload_response(self, project, upload_files, upload_id=None):
         """
         This is a really confusing method and should probably be split into to clear logic
         branches: one that is called when in daemon mode, and one that is not. If not called in
         daemon mode (local_upload=True), then md5_only is True and project is not None.Otherwise
         we're in daemon mode, where the project information is not required because the daemon will
         only be fed uploads by the app which have valid projects attached to them.
         """
         try:
+
             logger.info("%s", "  NEXT UPLOAD  ".center(30, "#"))
             logger.info("project: %s", project)
             logger.info("upload_id is %s", upload_id)
             logger.info(
                 "upload_files %s:(truncated)\n\t%s",
                 len(upload_files),
                 "\n\t".join(list(upload_files)[:5]),
             )
 
             # reset counters
             self.num_files_to_process = len(upload_files)
-            logger.debug("Processing %s files", self.num_files_to_process)
+            logger.debug( "Processing %s files", self.num_files_to_process)
             self.job_start_time = datetime.datetime.now()
             self.upload_id = upload_id
             self.job_failed = False
 
             # signal the reporter to start working
             self.working = True
 
             self.prepare_workers()
-
+            
             # create worker pools
             self.manager = self.create_manager(project)
 
             # create reporters
             logger.debug("creating report status thread...")
             self.create_report_status_thread()
 
             # load tasks into worker pools
             for path in upload_files:
                 md5 = upload_files[path]
                 self.manager.add_task((path, md5))
 
             logger.info("creating console status thread...")
-            self.create_monitor_status_thread()
+            self.create_monitor_status_thread()                
 
-            # wait for work to finish
+            #wait for work to finish
             while not self.manager.is_complete():
                 logger.debug("Manager is running, cancel requested?: %s", self.cancel)
-
+                
                 if self.cancel or self.manager.error or common.SIGINT_EXIT:
                     self.error_messages = self.manager.stop_work()
                     logger.debug("Manager sucesfully stopped")
                     break
-
+                
                 time.sleep(5)
-
+            
             # Shutdown the manager once all jobs are done
-            if not self.cancel and not self.manager.error:
-                logger.debug("Waiting for Manager to join")
+            if not self.cancel and not self.manager.error:    
+                logger.debug("Waiting for Manager to join")            
                 self.manager.join()
 
-            upload_stats = UploadStats.create(
-                self.manager.metric_store,
-                self.num_files_to_process,
-                self.job_start_time,
-            )
+            upload_stats = UploadStats.create(self.manager.metric_store, self.num_files_to_process, self.job_start_time)
             logger.info(upload_stats.get_formatted_text())
-            self.emit_progress(upload_stats)
-
+            self.emit_progress(upload_stats)            
+            
             logger.debug("error_message: %s", self.error_messages)
 
             # signal to the reporter to stop working
             self.working = False
             logger.info("done uploading files")
-
+            
             logger.debug("Waiting for reporter status thread to join")
             self.report_status_thread.join()
-
+            
             logger.debug("Waiting for print status thread to join")
-            self.monitor_status_thread.join()
+            self.monitor_status_thread.join()            
 
             #  Despite storing lots of data about new uploads, we will only send back the things
             #  that have changed, to keep payloads small.
             finished_upload_files = {}
             if self.upload_id and not self.error_messages:
                 md5s = self.return_md5s()
                 for path in md5s:
-                    finished_upload_files[path] = {"source": path, "md5": md5s[path]}
+                    finished_upload_files[path] =  {"source": path, "md5": md5s[path]}
 
                 self.mark_upload_finished(self.upload_id, finished_upload_files)
 
         except:
             self.error_messages.append(sys.exc_info())
 
     def main(self, run_one_loop=False):
+        
         def show_ouput(upload_stats):
             print(upload_stats.get_formatted_text())
             logger.info("File Progress: %s", upload_stats.file_progress)
-
+            
         self.progress_callback = show_ouput
-
+        
         logger.info("Uploader Started. Checking for uploads...")
 
         waiting_for_uploads_flag = False
 
         while not common.SIGINT_EXIT:
             try:
                 # TODO: we should pass args as url params, not http data
                 data = {}
                 data["location"] = self.location
                 logger.debug("Data: %s", data)
                 resp_str, resp_code = self.api_client.make_request(
-                    "/uploads/client/next",
-                    data=json.dumps(data),
-                    verb="PUT",
-                    use_api_key=True,
+                    "/uploads/client/next", data=json.dumps(data), verb="PUT", use_api_key=True
                 )
                 if resp_code == 204:
+
                     if not waiting_for_uploads_flag:
                         sys.stdout.write("\nWaiting for jobs to upload ")
                         sys.stdout.flush()
 
                     logger.debug("no files to upload")
                     sys.stdout.write(".")
                     sys.stdout.flush()
                     time.sleep(self.sleep_time)
                     waiting_for_uploads_flag = True
                     continue
-
+                
                 elif resp_code != 201:
-                    logger.error(
-                        "received invalid response code from app %s", resp_code
-                    )
+                    logger.error("received invalid response code from app %s", resp_code)
                     logger.error("response is %s", resp_str)
                     time.sleep(self.sleep_time)
                     continue
-
+                
                 print("")  # to make a newline after the 204 loop
 
                 try:
                     json_data = json.loads(resp_str)
                     upload = json_data.get("data", {})
-
+                
                 except ValueError:
                     logger.error("response was not valid json: %s", resp_str)
                     time.sleep(self.sleep_time)
                     continue
 
                 upload_files = upload["upload_files"]
                 upload_id = upload["id"]
                 project = upload["project"]
 
                 self.handle_upload_response(project, upload_files, upload_id)
-
+                
                 logger.debug("Upload of entity %s completed.", upload_id)
-                upload_stats = UploadStats.create(
-                    self.manager.metric_store,
-                    self.num_files_to_process,
-                    self.job_start_time,
-                )
+                upload_stats = UploadStats.create (self.manager.metric_store, self.num_files_to_process, self.job_start_time)
                 show_ouput(upload_stats)
                 logger.debug(self.manager.worker_queue_status_text())
-
+                
                 error_messages = []
-
+                
                 for exception in self.error_messages:
                     error_messages.append(str(exception[1]))
-
+                    
                 if error_messages:
-                    self.mark_upload_failed(
-                        error_message="\n".join(error_messages), upload_id=upload_id
-                    )
-
+                    self.mark_upload_failed(error_message="\n".join(error_messages), upload_id=upload_id)
+                
                     log_file = loggeria.LOG_PATH
                     sys.stderr.write("Error uploading files:\n")
-
+                    
                     for err_msg in error_messages:
                         sys.stderr.write("\t%s\n", err_msg)
 
-                    sys.stderr.write("\nSee log %s for more details\n\n", log_file)
-
+                    sys.stderr.write("\nSee log %s for more details\n\n", log_file) 
+                
                 self.error_messages = []
 
                 waiting_for_uploads_flag = False
 
             except KeyboardInterrupt:
                 logger.info("ctrl-c exit")
                 break
-            except Exception as err_msg:
-                logger.exception("Caught exception:\n%s", err_msg)
+            except Exception as errMsg:
+                logger.exception("Caught exception:\n%s", errMsg)
                 time.sleep(self.sleep_time)
                 continue
 
         logger.info("exiting uploader")
 
     def return_md5s(self):
         """
         Return a dictionary of the filepaths and their md5s that were generated
         upon uploading
         """
         return self.manager.metric_store.get_dict("file_md5s")
 
-
 def run_uploader(args):
     """
     Start the uploader process. This process will run indefinitely, polling
     the Conductor cloud app for files that need to be uploaded.
     """
     # convert the Namespace object to a dictionary
     args_dict = vars(args)
     cfg = config.config().config
-
-    api_client.ApiClient.register_client(
-        client_name=Uploader.CLIENT_NAME, client_version=ciocore.version
-    )
+    
+    api_client.ApiClient.register_client(client_name = Uploader.CLIENT_NAME, client_version=ciocore.__version__) 
 
     # Set up logging
     log_level_name = args_dict.get("log_level") or cfg["log_level"]
-
+    
     loggeria.setup_conductor_logging(
         logger_level=loggeria.LEVEL_MAP.get(log_level_name),
         log_dirpath=args_dict.get("log_dir"),
         log_filename="conductor_uploader.log",
-        disable_console_logging=not args_dict["log_to_console"],
-        use_system_log=False,
-    )
+        disable_console_logging = not args_dict['log_to_console'],
+        use_system_log=False
+    )    
 
     print("Logging to %s", loggeria.LOG_PATH)
-
+    
     logger.debug("Uploader parsed_args is %s", args_dict)
-
+    
     resolved_args = resolve_args(args_dict)
     uploader = Uploader(resolved_args)
-
+    
     if args.paths:
-        processed_filepaths = file_utils.process_upload_filepaths(args.paths[0])
+        processed_filepaths =  file_utils.process_upload_filepaths(args.paths[0])
         file_map = {path: None for path in processed_filepaths}
         uploader.handle_upload_response(project=None, upload_files=file_map)
-
+    
     else:
         uploader.main()
 
 
 def get_file_info(filepath):
     """
     For the given filepath return the following information in a dictionary:
@@ -1021,30 +968,30 @@
 
 
 def resolve_args(args):
     """
     Resolve all arguments, reconciling differences between command line args and config.yml args.
     See resolve_arg function.
     """
-
+    
     args["md5_caching"] = resolve_arg("md5_caching", args)
     args["database_filepath"] = resolve_arg("database_filepath", args)
     args["location"] = resolve_arg("location", args)
     args["thread_count"] = resolve_arg("thread_count", args)
 
     return args
 
-
 def resolve_arg(key, args):
     """
     If the key doesn't exist (or is None), grab it from the config.
     """
-
+    
     cfg = config.config().config
-    config_value = cfg.get(key)
-
+    config_value = cfg.get(key) 
+    
     value = args.get(key, config_value)
-
+    
     if value is None:
         value = config_value
-
+        
     return value
+
```

## tests/instance_type_fixtures.py

```diff
@@ -51,113 +51,79 @@
         "name": "n1-highmem-32-w",
         "operating_system": "windows",
     },
 ]
 
 ALL_INSTANCE_TYPES = WIN_INSTANCE_TYPES + LIN_INSTANCE_TYPES
 
-AWS_INSTANCE_TYPES = [
- {
-      "cloud": "aws",
-      "cpu": 72,
-      "memory": 144,
-      "name": "c5.18xlarge",
-      "orchestrator": "batch",
-      "operating_system": "linux",
-      "cores": 72,
-       "description": "c5  18xlarge",
-  },
-  {
-      "cloud": "aws",
-      "cpu": 2,
-      "memory": 8,
-      "name": "m5.large",
-      "orchestrator": "batch",
-      "operating_system": "linux",
-      "cores": 2,
-      "description": "m large",
-  },
-  {
-      "cloud": "aws",
-      "cpu": 4,
-      "memory": 16,
-      "name": "m5.xlarge",
-      "orchestrator": "batch",
-      "operating_system": "linux",
-      "cores": 4,
-      "description": "m5 xlarge",
-  }
-]
-
-
 
 CW_INSTANCE_TYPES = [
     {
         "cores": 4,
         "memory": 16,
-        "name": "cw-a-4-16",
+        "name": "a-4-16",
         "categories": [{"label": "low", "order": 1}, {"label": "extra", "order": 9}],
         "operating_system": "linux",
         "description": "Desc 4 C 16 M A",
     },
     {
         "cores": 8,
         "memory": 16,
-        "name": "cw-b-8-16",
+        "name": "b-8-16",
         "categories": [{"label": "low", "order": 1}],
         "operating_system": "linux",
         "description": "Desc 8 C 16 M B",
     },
     {
         "cores": 4,
         "memory": 32,
-        "name": "cw-c-4-32",
+        "name": "c-4-32",
         "categories": [{"label": "mid", "order": 2}, {"label": "extra", "order": 9}],
         "operating_system": "linux",
         "description": "Desc 4 C 32 M C",
     },
     {
         "cores": 8,
         "memory": 32,
-        "name": "cw-d-8-32",
+        "name": "d-8-32",
         "categories": [{"label": "mid", "order": 2}],
         "operating_system": "linux",
         "description": "Desc 8 C 32 M D",
     },
     {
         "cores": 4,
         "memory": 64,
-        "name": "cw-e-4-32",
+        "name": "e-4-32",
         "categories": [{"label": "high", "order": 3}, {"label": "extra", "order": 9}],
         "operating_system": "linux",
         "description": "Desc 4 C 32 M E",
     },
     {
         "cores": 8,
         "memory": 64,
-        "name": "cw-f-8-32",
+        "name": "f-8-32",
         "categories": [{"label": "high", "order": 3}],
         "operating_system": "linux",
         "description": "Desc 8 C 32 M F",
     },
     {
         "cores": 8,
         "memory": 64,
-        "name": "cw-g-8-32",
+        "name": "g-8-32",
         "categories": None,
         "operating_system": "linux",
         "description": "Desc 8 C 32 M G",
     },
 ]
 
 CW_INSTANCE_TYPES_WITH_GPUS = [
     {
         "cores": 8,
         "memory": 64,
-        "name": "cw-f-8-32-gpu",
+        "name": "f-8-32-gpu",
         "categories": [{"label": "high", "order": 3}],
         "operating_system": "linux",
         "description": "Desc 8 C 32 M F gpu",
         "gpu": {
             "gpu_count": 8, "gpu_model": "V100", "gpu_memory": "128GB"
         },
     },
```

## tests/test_api_client.py

```diff
@@ -1,22 +1,21 @@
 """ test data
 
    isort:skip_file
 """
-import sys
+ 
 import unittest
-import json
+
 try:
     from unittest import mock
 except ImportError:
     import mock
 
 from ciocore import api_client
 
-from ciocore.api_client import request_extra_environment
 
 class ApiClientTest(unittest.TestCase):
     @staticmethod
     def path_exists_side_effect(arg):
         if "missing" in arg:
             return False
         else:
@@ -42,125 +41,7 @@
             self.assertEqual(fn, "/users/joebloggs/.config/conductor/credentials")
 
     def test_get_api_key_creds_path(self):
         with mock.patch.dict("os.environ", self.env):
             fn = api_client.get_creds_path(api_key=True).replace("\\", "/")
             self.assertEqual(fn, "/users/joebloggs/.config/conductor/api_key_credentials")
 
-
-class TestTruncateMiddle(unittest.TestCase):
-
-    def test_truncation_not_needed(self):
-        self.assertEqual(api_client.truncate_middle("short", 10), "short")
-
-    def test_truncation_with_even_max_length(self):
-        self.assertEqual(api_client.truncate_middle("1234567890ABCDEF", 8), "1234~DEF")
-
-    def test_truncation_with_odd_max_length(self):
-        self.assertEqual(api_client.truncate_middle("1234567890ABCDEF", 9), "1234~CDEF")
-
-    def test_empty_string(self):
-        self.assertEqual(api_client.truncate_middle("", 5), "")
-
-    def test_non_string_input(self):
-        with self.assertRaises(TypeError):
-            api_client.truncate_middle(12345, 5)
-
-class TestRegisterClient(unittest.TestCase):
-    USER_AGENT_MAX_PATH_LENGTH = 10  # Example max length for testing
-
-    @classmethod
-    def setUpClass(cls):
-        cls.original_executable = sys.executable
-        sys.executable = '/usr/bin/python3'  # Example path for testing
-
-    @classmethod
-    def tearDownClass(cls):
-        sys.executable = cls.original_executable
-
-    def test_register_client_with_version(self):
-        client_name = 'ApiClient'
-        client_version = '1.0'
-
-        with mock.patch('platform.python_version', return_value='3.8.2'), \
-             mock.patch('platform.system', return_value='Linux'), \
-             mock.patch('platform.release', return_value='5.4.0-42-generic'):
-            user_agent = api_client.ApiClient.register_client(client_name, client_version)
-
-        expected_user_agent = (
-            f"ApiClient/1.0 (python 3.8.2; Linux 5.4.0-42-generic; "
-        )
-        self.assertTrue(user_agent.startswith(expected_user_agent))
-
-    def test_register_client_without_version(self):
-        client_name = 'ApiClient'
-
-        with mock.patch('platform.python_version', return_value='3.8.2'), \
-             mock.patch('platform.system', return_value='Linux'), \
-             mock.patch('platform.release', return_value='5.4.0-42-generic'):
-            user_agent =  api_client.ApiClient.register_client(client_name)
-
-
-        expected_user_agent = (
-            f"ApiClient/unknown (python 3.8.2; Linux 5.4.0-42-generic; "
-        )
-
-        self.assertTrue(user_agent.startswith(expected_user_agent))
-
-
-class TestRequestExtraEnvironment(unittest.TestCase):
-
-    def setUp(self):
-        self.api_response_data = {
-            "data": [
-                {"account_id": "123", "env": ["VAR1=value1", "VAR2=value2"]},
-                {"account_id": "456", "env": ["VAR3=value3", "VAR4=value4"]}
-            ]
-        }
-        self.response_ok = mock.MagicMock(status_code=200, text=json.dumps(self.api_response_data))
-        self.response_error = mock.MagicMock(status_code=500, text=json.dumps({"error": "Internal Server Error"}))
-
-    @mock.patch("ciocore.api_client.ApiClient")
-    @mock.patch("ciocore.api_client.read_conductor_credentials")
-    @mock.patch("ciocore.api_client.account_id_from_jwt")
-    def test_request_extra_environment_success(self, mock_account_id_from_jwt, mock_read_conductor_credentials, mock_ApiClient):
-        # Set up mocks for successful execution
-        mock_read_conductor_credentials.return_value = "valid_token"
-        mock_account_id_from_jwt.return_value = "123"
-        mock_api_instance = mock_ApiClient.return_value
-        mock_api_instance.make_request.return_value = (self.response_ok.text, self.response_ok.status_code)
-
-        result = request_extra_environment()
-
-        self.assertEqual(result, ["VAR1=value1", "VAR2=value2"])
-        mock_ApiClient.assert_called_once()
-        mock_read_conductor_credentials.assert_called_once_with(True)
-        mock_account_id_from_jwt.assert_called_once_with("valid_token")
-
-    @mock.patch("ciocore.api_client.ApiClient")
-    def test_request_extra_environment_api_failure(self, mock_ApiClient):
-        # Set up mock for API failure
-        mock_api_instance = mock_ApiClient.return_value
-        mock_api_instance.make_request.return_value = (self.response_error.text, self.response_error.status_code)
-
-        # Assert exception raised when the API call fails
-        with self.assertRaises(Exception) as context:
-            request_extra_environment()
-
-        self.assertIn('Failed to get extra environment', str(context.exception))
-        mock_ApiClient.assert_called_once()
-
-    @mock.patch("ciocore.api_client.ApiClient")
-    @mock.patch("ciocore.api_client.read_conductor_credentials")
-    @mock.patch("ciocore.api_client.account_id_from_jwt")
-    def test_request_extra_environment_no_account_env(self, mock_account_id_from_jwt, mock_read_conductor_credentials, mock_ApiClient):
-        # Set up mocks to simulate valid token and account ID but no matching environment
-        mock_read_conductor_credentials.return_value = "valid_token"
-        mock_account_id_from_jwt.return_value = "invalid_id"  # This won't match any 'account_id' in response
-        mock_api_instance = mock_ApiClient.return_value
-        mock_api_instance.make_request.return_value = (self.response_ok.text, self.response_ok.status_code)
-
-        with self.assertRaises(Exception) as context:
-            request_extra_environment()
-
-        self.assertEqual("Error: Could not get account environment!", str(context.exception))
-        mock_ApiClient.assert_called_once()
```

## tests/test_common.py

```diff
@@ -6,22 +6,22 @@
 import unittest
 
 FILES_PATH = os.path.join(os.path.dirname(__file__), "files")
 
 
 class TestMd5(unittest.TestCase):
  
-    # def test_get_base64_md5_is_correct_md5(self):
-    #     from ciocore import common
-    #     fn1 = os.path.join(FILES_PATH, "one")
-    #     md5=common.get_base64_md5(fn1)
-    #     if os.name == "nt":
-    #         self.assertEqual(md5, "w8F8opHbdwHIozghPc63XA==")
-    #     else:
-    #         self.assertEqual(md5, "9iVbsBxkj+lncU1SqJ6OnA==")
+    def test_get_base64_md5_is_correct_md5(self):
+        from ciocore import common
+        fn1 = os.path.join(FILES_PATH, "one")
+        md5=common.get_base64_md5(fn1)
+        if os.name == "nt":
+            self.assertEqual(md5, "w8F8opHbdwHIozghPc63XA==")
+        else:
+            self.assertEqual(md5, "9iVbsBxkj+lncU1SqJ6OnA==")
 
 
     def test_get_base64_md5_is_correct_type(self):
         from ciocore import common
         from builtins import str
         fn1 = os.path.join(FILES_PATH, "one")
         md5=common.get_base64_md5(fn1)
```

## tests/test_config.py

```diff
@@ -25,15 +25,15 @@
     def up(self, **overrides):
         """
         Call this setup before every test.
 
         It Mocks the 2 api_key functions, the multiprocessing package, and the os environment.
         """
         self.cpu_count_patcher = mock.patch(
-            "os.cpu_count", return_value=overrides.get("cpu_count", 4)
+            "multiprocessing.cpu_count", return_value=overrides.get("cpu_count", 4)
         )
         self.mock_cpu_count = self.cpu_count_patcher.start()
         if overrides.get("cpu_count_raise", False):
             self.mock_cpu_count.side_effect = NotImplementedError()
 
         self.get_api_key_from_variable_patcher = mock.patch.object(
             config.Config,
@@ -60,45 +60,31 @@
         self.get_api_key_from_file_patcher.stop()
         self.env_patcher.stop()
 
     def test_create(self):
         cfg = config.Config()
         self.assertEqual(cfg.__class__.__name__, "Config")
 
-    # DOWNLOADER PAGE SIZE
-    def test_downloader_page_size_default_to_50(self):
-        self.up()
-        cfg = config.Config()
-        self.assertEqual(cfg.config["downloader_page_size"], 50)
-        self.down()
-        
-    def test_downloader_page_size_variable_overrides_default(self):
-        self.up(env={"CONDUCTOR_DOWNLOADER_PAGE_SIZE": "10"})
-        cfg = config.Config()
-        self.assertEqual(cfg.config["downloader_page_size"], 10)
-        self.down()
-        
-        
     # THREAD COUNT
-    def test_thread_count_is_cpu_count_minus_one(self):
-        self.up(cpu_count=4)
+    def test_thread_count_is_double_cpu_count(self):
+        self.up(cpu_count=2)
         cfg = config.Config()
-        self.assertEqual(cfg.config["thread_count"], 3)
+        self.assertEqual(cfg.config["thread_count"], 4)
         self.down()
 
-    def test_thread_count_is_max_15(self):
-        self.up(cpu_count=20)
+    def test_thread_count_is_max_16(self):
+        self.up(cpu_count=12)
         cfg = config.Config()
-        self.assertEqual(cfg.config["thread_count"], 15)
+        self.assertEqual(cfg.config["thread_count"], 16)
         self.down()
 
-    def test_thread_count_is_15_if_an_exception_is_thrown(self):
+    def test_thread_count_is_16_if_an_exception_is_thrown(self):
         self.up(cpu_count_raise=True)
         cfg = config.Config()
-        self.assertEqual(cfg.config["thread_count"], 15)
+        self.assertEqual(cfg.config["thread_count"], 16)
         self.down()
 
     def test_thread_count_variable_overrides_default(self):
         self.up(env={"CONDUCTOR_THREAD_COUNT": "10"})
         cfg = config.Config()
         self.assertEqual(cfg.config["thread_count"], 10)
         self.down()
```

## tests/test_data.py

```diff
@@ -1,163 +1,179 @@
 """ test data
 
    isort:skip_file
 """
 
 import unittest
-
 try:
     from unittest import mock
 except ImportError:
     import mock
 
+
 from ciocore import data
 
+PROJECTS = [
+    "Deadpool",
+    "Harry Potter & the chamber of secrets",
+    "Captain Corelli's Mandolin",
+    "Gone with the Wind",
+]
+
+from package_fixtures import *
+from instance_type_fixtures import *
 
-from project_fixtures import PROJECTS
-from package_fixtures import SOFTWARE_DATA
-from instance_type_fixtures import LIN_INSTANCE_TYPES, ALL_INSTANCE_TYPES
 
-class TestDataAllInstanceTypes(unittest.TestCase):
+class TestDataSingleton(unittest.TestCase):
     def setUp(self):
-        self.default_cache = {
-            "projects": PROJECTS,
-            "instance_types": ALL_INSTANCE_TYPES,
-            "packages": SOFTWARE_DATA,
-        }
-
-        # self.get_data_patcher = mock.patch.object(DataCache, "get_data", return_value=self.default_cache)
-
-        # self.account_id_patcher = mock.patch(
-        #     "ciocore.data.get_account_id", return_value="1234"
-        # )
+        projects_patcher = mock.patch(
+            "ciocore.api_client.request_projects", return_value=PROJECTS
+        )
+        software_packages_patcher = mock.patch(
+            "ciocore.api_client.request_software_packages", return_value=SOFTWARE_DATA
+        )
 
-        # self.mock_get_data = self.get_data_patcher.start()
-        # self.mock_account_id = self.account_id_patcher.start()
+        self.mock_projects = projects_patcher.start()
+        self.mock_software_packages = software_packages_patcher.start()
 
+        self.addCleanup(projects_patcher.stop)
+        self.addCleanup(software_packages_patcher.stop)
         data.__data__ = {}
         data.__products__ = None
 
-    def tearDown(self):
-        # self.get_data_patcher.stop()
-        # self.account_id_patcher.stop()
-        pass
-
-    # def override_default_cache(self, **kwargs):
-    #     return_val = {}
-    #     return_val.update(self.default_cache)
-    #     return_val.update(kwargs)
-        # self.mock_get_data.return_value = return_val
-
-    # def test_smoke(self):
-    #     self.assertEqual(1, 1)
-
-    # def test_init_empty_projects_global(self):
-    #     data.init()
-    #     self.assertEqual(data.products(), [])
-
-    # def test_init_stores_all_args(self):
-    #     data.init("a", "b", "c")
-    #     self.assertEqual(data.products(), ["a", "b", "c"])
-
-    # def test_data_raises_if_not_initialized(self):
-    #     with self.assertRaises(ValueError):
-    #         data.data()
-
-    # def test_valid(self):
-    #     self.assertEqual(data.valid(), False)
-    #     data.init()
-    #     data.data()
-    #     self.assertEqual(data.valid(), True)
-
-    # def test_clear(self):
-    #     data.init()
-    #     data.data()
-    #     self.assertEqual(data.valid(), True)
-    #     data.clear()
-    #     self.assertEqual(data.valid(), False)
-
-    # def test_does_not_refresh_if_not_force(self):
-    #     data.init()
-    #     p1 = data.data()["projects"]
-    #     self.assertEqual(len(p1), 4)
-    #     # merge the default cache with a new projects list containing only two projects
-    #     # self.override_default_cache(projects=["a", "b"])
-    #     p2 = data.data()["projects"]
-    #     self.assertEqual(p2, p1)
-
-    # def test_does_refresh_if_force_all(self):
-    #     data.init()
-    #     p1 = data.data()["projects"]
-    #     self.assertEqual(len(p1), 4)
-    #     # self.override_default_cache(projects=["a", "b"])
-    #     p2 = data.data(force=True)["projects"]
-    #     self.assertNotEqual(p2, p1)
-    #     self.assertEqual(len(p2), 2)
-
-    # def test_get_data_for_one_product(self):
-    #     data.init("c4d")
-    #     # inst = data.data()["instance_types"]
-    #     sw = data.data()["software"]
-    #     self.assertEqual(len(sw.supported_host_names()), 2)
-
-    # def test_several_products(self):
-    #     data.init("c4d", "maya")
-    #     sw = data.data()["software"]
-    #     self.assertEqual(len(sw.supported_host_names()), 5)
-
-    # def test_one_product_can_be_a_plugin(self):
-    #     data.init("redshift")
-    #     sw = data.data()["software"]
-    #     self.assertEqual(len(sw.supported_host_names()), 4)
-
-    # def test_several_products_ignore_plugins(self):
-    #     data.init("redshift", "c4d")
-    #     sw = data.data()["software"]
-    #     self.assertEqual(len(sw.supported_host_names()), 2)
-
-    # def test_linux_only_instance_types(self):
-    #     # self.override_default_cache(instance_types=LIN_INSTANCE_TYPES)
-    #     data.init()
-    #     h = data.data()["instance_types"]
-    #     self.assertEqual(len(h.instance_types.keys()), 4)
-
-    # def test_linux_only_packages_when_linux_only_instance_types(self):
-    #     # self.override_default_cache(instance_types=LIN_INSTANCE_TYPES)
-    #     data.init("c4d")
-    #     sw = data.data()["software"]
-    #     self.assertEqual(len(sw.supported_host_names()), 1)
-
-    # def test_platforms_method_only_linux(self):
-    #     self.override_default_cache(instance_types=LIN_INSTANCE_TYPES)/
-    #     data.init("c4d")
-    #     data.data()
-    #     self.assertEqual({"linux"}, data.platforms())
-
-    # def test_many_products(self):
-    #     self.override_default_cache(instance_types=LIN_INSTANCE_TYPES)
-    #     data.init("c4d", "maya")
-    #     sw = data.data()["software"]
-    #     self.assertEqual(len(sw.supported_host_names()), 4)
-
-    # def test_product_keyword_translates_to_single_arg(self):
-    #     data.init(product="c4d")
-    #     sw = data.data()["software"]
-    #     self.assertEqual(len(sw.supported_host_names()), 2)
-
-    # def test_product_keyword_all_translates_to_all_products(self):
-    #     data.init(product="all")
-    #     sw = data.data()["software"]
-    #     self.assertEqual(len(sw.supported_host_names()), 11)
-
-    # def test_data_raises_if_both_products_and_kwarg_given(self):
-    #     with self.assertRaises(ValueError):
-    #         data.init("maya", product="c4d")
-
-    # def test_data_raises_if_products_and_kwarg_given(self):
-    #     with self.assertRaises(ValueError):
-    #         data.init("maya", product="c4d")
-
-class TestDataSmoke(unittest.TestCase):
-    
-    def test_smoke(self):
-        self.assertTrue(True)
+
+class TestDataAllInstanceTypes(TestDataSingleton):
+    def setUp(self):
+        super(TestDataAllInstanceTypes, self).setUp()
+        instance_types_patcher = mock.patch(
+            "ciocore.api_client.request_instance_types", return_value=ALL_INSTANCE_TYPES
+        )
+        self.mock_instance_types = instance_types_patcher.start()
+        self.addCleanup(instance_types_patcher.stop)
+
+    def test_init_empty_projects_global(self):
+        data.init()
+        self.assertEqual(data.products(), [])
+
+    def test_init_stores_all_args(self):
+        data.init("a", "b", "c")
+        self.assertEqual(data.products(), ["a", "b", "c"])
+
+    def test_data_raises_if_not_initialized(self):
+        with self.assertRaises(ValueError):
+            data.data()
+
+    def test_valid(self):
+        self.assertEqual(data.valid(), False)
+        data.init()
+        data.data()
+        self.assertEqual(data.valid(), True)
+
+    def test_clear(self):
+        data.init()
+        data.data()
+        self.assertEqual(data.valid(), True)
+        data.clear()
+        self.assertEqual(data.valid(), False)
+
+    def test_does_not_refresh_if_not_force(self):
+        data.init()
+        p1 = data.data()["projects"]
+        self.assertEqual(len(p1), 4)
+        self.mock_projects.return_value = ["a", "b"]
+        p2 = data.data()["projects"]
+        self.assertEqual(p2, p1)
+
+    def test_does_refresh_if_force_all(self):
+        data.init()
+        p1 = data.data()["projects"]
+        self.assertEqual(len(p1), 4)
+        self.mock_projects.return_value = ["a", "b"]
+        p2 = data.data(force=True)["projects"]
+        self.assertNotEqual(p2, p1)
+        self.assertEqual(len(p2), 2)
+
+    def test_get_data_for_one_product(self):
+        data.init("c4d")
+        inst = data.data()["instance_types"]
+        sw = data.data()["software"]
+        self.assertEqual(len(sw.supported_host_names()), 2)
+
+    def test_auto_filter_instance_types_based_on_software_plaforms(self):
+        data.init("maya")
+        inst = data.data()["instance_types"]
+
+    def test_several_products(self):
+        data.init("c4d", "maya")
+        sw = data.data()["software"]
+        self.assertEqual(len(sw.supported_host_names()), 5)
+
+    def test_one_product_can_be_a_plugin(self):
+        data.init("redshift")
+        sw = data.data()["software"]
+        self.assertEqual(len(sw.supported_host_names()), 4)
+
+    def test_several_products_ignore_plugins(self):
+        data.init("redshift", "c4d")
+        sw = data.data()["software"]
+        self.assertEqual(len(sw.supported_host_names()), 2)
+
+
+class TestDataHomogenousInstanceTypes(TestDataSingleton):
+    def setUp(self):
+        """
+        The orchestrator provides only one platform.
+        """
+        super(TestDataHomogenousInstanceTypes, self).setUp()
+        instance_types_patcher = mock.patch(
+            "ciocore.api_client.request_instance_types", return_value=LIN_INSTANCE_TYPES
+        )
+        self.mock_instance_types = instance_types_patcher.start()
+        self.addCleanup(instance_types_patcher.stop)
+
+    def test_linux_only_instance_types(self):
+        data.init()
+        h = data.data()["instance_types"]
+        self.assertEqual(len(h.instance_types.keys()), 4)
+
+    def test_linux_only_packages_when_linux_only_instance_types(self):
+        data.init("c4d")
+        sw = data.data()["software"]
+        self.assertEqual(len(sw.supported_host_names()), 1)
+
+    def test_platforms_method_only_linux(self):
+        data.init("c4d")
+        data.data()
+        self.assertEqual({"linux"}, data.platforms())
+
+    def test_several_products(self):
+        data.init("c4d", "maya")
+        sw = data.data()["software"]
+        self.assertEqual(len(sw.supported_host_names()), 4)
+
+
+class TestBackwardsCompatibiliity(TestDataSingleton):
+    def setUp(self):
+        super(TestBackwardsCompatibiliity, self).setUp()
+        instance_types_patcher = mock.patch(
+            "ciocore.api_client.request_instance_types", return_value=ALL_INSTANCE_TYPES
+        )
+        self.mock_instance_types = instance_types_patcher.start()
+        self.addCleanup(instance_types_patcher.stop)
+
+    def test_product_keyword_translates_to_single_arg(self):
+        data.init(product="c4d")
+        sw = data.data()["software"]
+        self.assertEqual(len(sw.supported_host_names()), 2)
+
+    def test_product_keyword_all_translates_to_all_products(self):
+        data.init(product="all")
+        sw = data.data()["software"]
+        self.assertEqual(len(sw.supported_host_names()), 11)
+
+    def test_data_raises_if_both_products_and_kwarg_given(self):
+        with self.assertRaises(ValueError):
+            data.init("maya", product="c4d")
+
+    def test_data_raises_if_both_products_and_kwarg_given(self):
+        with self.assertRaises(ValueError):
+            data.init("maya", product="c4d")
```

## tests/test_hardware_set.py

```diff
@@ -1,30 +1,26 @@
 """ test data
 
    isort:skip_file
 """
 import unittest
+ 
 
 
 from ciocore.hardware_set import HardwareSet
 
 PROJECTS = [
     "Deadpool",
     "Harry Potter & the chamber of secrets",
     "Captain Corelli's Mandolin",
     "Gone with the Wind",
 ]
 
 from package_fixtures import *
-from instance_type_fixtures import (
-    CW_INSTANCE_TYPES,
-    AWS_INSTANCE_TYPES,
-    CW_INSTANCE_TYPES_WITH_GPUS,
-    ALL_INSTANCE_TYPES,
-)
+from instance_type_fixtures import CW_INSTANCE_TYPES, CW_INSTANCE_TYPES_WITH_GPUS, ALL_INSTANCE_TYPES
 
 
 class TestCategorizedInstanceTypes(unittest.TestCase):
     def setUp(self):
         self.hs = HardwareSet(CW_INSTANCE_TYPES)
 
     def test_number_of_categories(self):
@@ -32,108 +28,63 @@
 
     def test_categories_sorted_on_order(self):
         labels = [i["label"] for i in self.hs.get_model()]
         self.assertEqual(labels, ["low", "mid", "high", "extra"])
 
     def test_content_count(self):
         low_category_values = [c["value"] for c in self.hs.get_model()[0]["content"]]
-        self.assertEqual(low_category_values, ["cw-a-4-16", "cw-b-8-16"])
+        self.assertEqual(low_category_values, ["a-4-16", "b-8-16"])
 
     def test_in_several_categories(self):
         low_category_values = [c["value"] for c in self.hs.get_model()[0]["content"]]
         extra_category_values = [c["value"] for c in self.hs.get_model()[3]["content"]]
-        self.assertIn("cw-a-4-16", low_category_values)
-        self.assertIn("cw-a-4-16", extra_category_values)
-
-    def test_category_names(self):
-        names = self.hs.labels()
-        self.assertEqual(names, ["low", "mid", "high", "extra"])
-
+        self.assertIn("a-4-16", low_category_values)
+        self.assertIn("a-4-16", extra_category_values)
 
 class TestRecategorizeInstanceTypes(unittest.TestCase):
     def setUp(self):
         self.hs = HardwareSet(CW_INSTANCE_TYPES_WITH_GPUS)
-
+        
     def test_recategorize_cpu_gpu(self):
-        test_func = (
-            lambda x: [{"label": "GPU", "order": 2}]
-            if "gpu" in x and x["gpu"]
-            else [{"label": "CPU", "order": 1}]
-        )
+        test_func = lambda x: [{'label': 'GPU', 'order': 2}] if "gpu" in x and x["gpu"] else [{'label': 'CPU', 'order': 1}]
         self.hs.recategorize(test_func)
         self.assertEqual(self.hs.number_of_categories(), 2)
 
     def test_find_all_by_condition(self):
         test_func = lambda x: "gpu" in x and x["gpu"]
         result = self.hs.find_all(test_func)
         self.assertEqual(len(result), 2)
         print(self.hs)
-
+        
     def test_find_first_by_condition(self):
-        test_func = lambda x: x["memory"] < 32
+        test_func = lambda x: x[ "memory"] < 32
         result = self.hs.find_first(test_func)
         self.assertTrue(result["memory"] < 32)
-
+        
     def test_find_category(self):
         label = "mid"
         result = self.hs.find_category(label)
-        self.assertEqual(result["label"], label)
-        self.assertEqual(result["order"], 2)
-
-
-class TestFind(unittest.TestCase):
-    def setUp(self):
-        self.hs = HardwareSet(CW_INSTANCE_TYPES_WITH_GPUS)
-
-    def test_find_unspecified_category(self):
-        result = self.hs.find("cw-e-4-32")
-        self.assertIsNotNone(result)
-
-    def test_category_that_exists(self):
-        result = self.hs.find("cw-e-4-32", category="high")
-        self.assertIsNotNone(result)
-
-    def test_returns_none_if_not_in_category(self):
-        result = self.hs.find("cw-e-4-32", category="low")
-        self.assertIsNone(result)
-
-    def test_returns_none_if_non_existent_category(self):
-        result = self.hs.find("cw-e-4-32", category="foo")
-        self.assertIsNone(result)
-
-
+        self.assertEqual(result["label"] , label)
+        self.assertEqual(result["order"] , 2)
+        
 class TestUncategorizedInstanceTypes(unittest.TestCase):
     def setUp(self):
         self.hs = HardwareSet(ALL_INSTANCE_TYPES)
 
     def test_number_of_categories_uncategorized(self):
         self.assertEqual(self.hs.number_of_categories(), 1)
 
     def test_model_sorted_on_cores_mem(self):
         content = self.hs.get_model()[0]["content"]
         result = [c["label"] for c in content]
         self.assertEqual(
             result,
             [
-                "windows 4 core 26.0GB Mem",
-                "linux 4 core 27.0GB Mem",
-                "linux 8 core 30.0GB Mem",
-                "windows 32 core 208.0GB Mem",
-                "linux 32 core 208.0GB Mem",
-                "windows 64 core 416.0GB Mem",
-                "linux 64 core 416.0GB Mem",
+                'windows 4 core 26.0GB Mem',
+                'linux 4 core 27.0GB Mem',
+                'linux 8 core 30.0GB Mem',
+                'windows 32 core 208.0GB Mem',
+                'linux 32 core 208.0GB Mem',
+                'windows 64 core 416.0GB Mem',
+                'linux 64 core 416.0GB Mem'
             ],
         )
-
-
-class TestProvider(unittest.TestCase):
-    def test_gcp_provider(self):
-        hs = HardwareSet(ALL_INSTANCE_TYPES)
-        self.assertEqual(hs.provider, "gcp")
-
-    def test_cw_provider(self):
-        hs = HardwareSet(CW_INSTANCE_TYPES_WITH_GPUS)
-        self.assertEqual(hs.provider, "cw")
-
-    def test_aws_provider(self):
-        hs = HardwareSet(AWS_INSTANCE_TYPES)
-        self.assertEqual(hs.provider, "aws")
```

## Comparing `ciocore/downloader/legacy_downloader.py` & `ciocore/downloader.py`

 * *Files 0% similar despite different names*

```diff
@@ -489,14 +489,15 @@
 
         for job_id in job_ids:
             endpoint = self.endpoint_downloads_job % job_id
             for tid in task_ids:
                 downloads = _get_job_download(endpoint, self.api_client, job_id, tid)
                 if downloads:
                     for task_download in downloads.get("downloads", []):
+                        print("putting in queue: %s" % task_download)
                         self.pending_queue.put(task_download, block=True)
 
     @common.dec_catch_exception(raise_=True)
     def download_target(self, pending_queue, downloading_queue, task_download_state):
         """
         This function is called in a new thread (and many threads may be executing this at a single
         time). This function is responsible for downloading a single Download (essentially an entity
```

## Comparing `ciocore-8.0.0rc1.dist-info/METADATA` & `ciocore-8.1.0b1.dist-info/METADATA`

 * *Files 13% similar despite different names*

```diff
@@ -1,73 +1,54 @@
 Metadata-Version: 2.1
 Name: ciocore
-Version: 8.0.0rc1
+Version: 8.1.0b1
 Summary: Core functionality for Conductor's client tools
 Home-page: https://github.com/ConductorTechnologies/ciocore
 Author: conductor
 Author-email: info@conductortech.com
 Classifier: Operating System :: OS Independent
 Classifier: Programming Language :: Python
 Classifier: Topic :: Multimedia :: Graphics :: 3D Rendering
 Description-Content-Type: text/markdown
-Requires-Dist: requests[use_chardet_on_py3] ==2.28.1
 Requires-Dist: pyjwt ==1.7.1
+Requires-Dist: requests <=2.25.1,>=2.10.0
 Requires-Dist: future >=0.18.2
-Requires-Dist: cioseq <1.0.0,>=0.4.1
-Requires-Dist: Click <9.0.0,>=8.1.3
-Requires-Dist: markdown <4.0.0,>=3.5.2
-Requires-Dist: colorlog <7.0.0,>=6.8.2
 
-## Introduction
+# Conductor Core
 
-Conductor Core is a Python package that provides an API and a command-line interface for interacting with Conductor.
+Use the command-line uploader and downloader or develop your own tools using the Python API.
 
 ## Install
 
+**To install the latest version.**
 ```bash
-pip install ciocore
+pip install --upgrade ciocore
 ```
 
 Run the conductor command to confirm the package was installed.
 ```bash
 conductor --help
 ```
 
-Get help for a specific sub command
-```bash
-conductor downloader --help
-conductor uploader --help
-conductor software --help
-```
-
 Example API usage
 ```python
 from ciocore import api_client
 print(api_client.request_software_packages()[0])
 
 ```
 ## Contributing
 
 See [CONTRIBUTING](CONTRIBUTING.md)
 
 ## Changelog
 
-## Version:8.0.0-rc.1 -- 05 Apr 2024
-
-Major Updates in Version 8.0.0
-
-
-* The `conductor` commandline interface (CLI) has undergone a complete overhaul to improve its functionality and performance. It is now installed as a Python console script, which simplifies the installation process on all platforms and ensures that the tool is always accessible. In addition, this standardized installation allows for better integration into our Companion application and potentially customers' own proprietary tools.
-* The commandline downloader has been rewritten to improve its performance and reliability. In previous versions, the downloader would fetch all download URLs from the server before starting any downloads. This approach was inefficient and would lead to performance issues when handling jobs with large numbers of tasks. The new downloader fetches tasks in small batches, which allows the download to start almost immediately, regardless of the number of tasks. It provides several new commandline flags, such as filtering based on filenames, and better logging, giving users more control and visibility over the download process. In addition, the new downloader has been written as an event based system that can be seamlessly integrated into other pipeline tools and GUIs. Pipeline TDs need only to register handlers to any of the events that the downloader emits.
-* The `conductor` commandline now supports a new subcommand, `docs`, which opens API and commandline documentation in a local web browser. The documentation is generated using `mkdocs` and is included in the package. This command is useful as it ensures the **API** documentation is relevant to the version of the tool being used.
-* The `conductor` commandline now supports a new subcommand, `packages`, which displays all available software packages on Conductor's cloud. The command can output the document as text, markdown, or nicely formatted HTML. The data is pulled from the Conductor server and is always up-to-date.
-* The Python API now provides four new endpoints: `kill_tasks`, `kill_jobs`, `get_jobs`, and `get_log`.
-* The Python API now provides access to entries in the Dashboard's Extra Environment Variables section. This feature allows administrators to set environment variables that are passed to all jobs run on the cloud. It's useful for setting up access to license servers or other shared resources. The API is open to clients so that they may insert account-widse variables with the correct level of precedence. 
-* In addition, the environment variable schema has been extended to support a merge policy of `prepend`. This policy allows users to add new values to the beginning of the list of values for an environment variable and is useful for search paths.
+## Unreleased:
 
+* 8.1.0-beta.1
+  * remove entries from setup
 
 ## Version:7.0.2 -- 10 Nov 2023
 
 * Bug fix: Fixed an issue where a list was copied with a Python3 only construct.
 * Compatibility improvement: Tests can now be run in Python 2.7 as well.
 * Cross-version support: Ensured that all tests pass successfully in both Python 2.7 and 3.9.
 * Improvement: Rewrite instance type descriptions on the client. Ensure uniqueness.
```

