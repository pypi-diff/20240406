# Comparing `tmp/datalad-0.9.3.tar.gz` & `tmp/datalad-1.0.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "dist/datalad-0.9.3.tar", last modified: Fri Mar 16 22:27:17 2018, max compression
+gzip compressed data, was "datalad-1.0.0.tar", last modified: Sat Apr  6 16:46:28 2024, max compression
```

## Comparing `datalad-0.9.3.tar` & `datalad-1.0.0.tar`

### file list

```diff
@@ -1,484 +1,585 @@
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/
--rw-------   0 yoh      (47521) yoh      (47522)    10556 2018-03-04 16:37:40.000000 datalad-0.9.3/setup_support.py
--rw-------   0 yoh      (47521) yoh      (47522)      234 2018-03-04 16:37:39.000000 datalad-0.9.3/CONTRIBUTORS
--rw-------   0 yoh      (47521) yoh      (47522)    10538 2018-03-16 22:26:59.000000 datalad-0.9.3/.travis.yml
--rw-------   0 yoh      (47521) yoh      (47522)     1008 2018-03-04 16:37:39.000000 datalad-0.9.3/Makefile
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/docs/
--rw-------   0 yoh      (47521) yoh      (47522)     7442 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/Makefile
--rw-------   0 yoh      (47521) yoh      (47522)     2183 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/config-format.rst
--rw-------   0 yoh      (47521) yoh      (47522)    21350 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/design.rst
--rw-------   0 yoh      (47521) yoh      (47522)     5594 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/datalad-metadata.rst
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/docs/examples/
--rwx------   0 yoh      (47521) yoh      (47522)    12911 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/examples/nipype_workshop_dataset.sh
--rwx------   0 yoh      (47521) yoh      (47522)     2222 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/examples/make_pymvpa_collection.sh_broken
--rwx------   0 yoh      (47521) yoh      (47522)     1106 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/examples/make_large_bogus_collection.sh_broken
--rw-------   0 yoh      (47521) yoh      (47522)    10504 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/examples/3rdparty_analysis_workflow.sh
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/docs/casts/
--rw-------   0 yoh      (47521) yoh      (47522)     3262 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/casts/boxcom.sh
--rw-------   0 yoh      (47521) yoh      (47522)     1738 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/casts/seamless_nested_repos.sh
--rw-------   0 yoh      (47521) yoh      (47522)     1990 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/casts/simple_provenance_tracking.sh
--rw-------   0 yoh      (47521) yoh      (47522)     3120 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/casts/publish_on_github.sh
--rw-------   0 yoh      (47521) yoh      (47522)     2621 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/casts/basic_search.sh
--rw-------   0 yoh      (47521) yoh      (47522)     1743 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/casts/track_data_from_webpage.sh
--rwx------   0 yoh      (47521) yoh      (47522)     1128 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/casts/cmdline_basic_usage.sh
--rw-------   0 yoh      (47521) yoh      (47522)     4925 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/casts/reproducible_analysis.sh
--rw-------   0 yoh      (47521) yoh      (47522)     5459 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/casts/heudiconv_dicom_to_bids.sh
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/docs/source/
--rw-------   0 yoh      (47521) yoh      (47522)     4781 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/source/customization.rst
--rw-------   0 yoh      (47521) yoh      (47522)      738 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/source/index.rst
--rw-------   0 yoh      (47521) yoh      (47522)    22788 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/source/changelog.rst
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/docs/source/_extras/
--rw-------   0 yoh      (47521) yoh      (47522)     1105 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/source/_extras/schema.json
--rw-------   0 yoh      (47521) yoh      (47522)     1717 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/source/glossary.rst
--rw-------   0 yoh      (47521) yoh      (47522)     3060 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/source/gettingstarted.rst
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/docs/source/usecases/
--rw-------   0 yoh      (47521) yoh      (47522)      185 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/source/usecases/index.rst
--rw-------   0 yoh      (47521) yoh      (47522)     6446 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/source/metadata.rst
--rw-------   0 yoh      (47521) yoh      (47522)     1744 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/source/config.rst
--rw-------   0 yoh      (47521) yoh      (47522)     2568 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/source/modref.rst
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/docs/source/crawler/
--rw-------   0 yoh      (47521) yoh      (47522)      361 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/source/crawler/index.rst
--rw-------   0 yoh      (47521) yoh      (47522)     2806 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/source/crawler/basics.rst
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/docs/source/_templates/
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/docs/source/_templates/autosummary/
--rw-------   0 yoh      (47521) yoh      (47522)      436 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/source/_templates/autosummary/module.rst
--rw-------   0 yoh      (47521) yoh      (47522)      859 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/source/acknowledgements.rst
--rw-------   0 yoh      (47521) yoh      (47522)    10163 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/source/conf.py
--rw-------   0 yoh      (47521) yoh      (47522)     8801 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/source/basics.rst
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/docs/source/_static/
--rw-------   0 yoh      (47521) yoh      (47522)      958 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/source/_static/datalad_logo.png
--rw-------   0 yoh      (47521) yoh      (47522)     1040 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/source/faq.rst
--rw-------   0 yoh      (47521) yoh      (47522)     6547 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/source/related.rst
--rw-------   0 yoh      (47521) yoh      (47522)     1725 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/source/cmdline.rst
--rw-------   0 yoh      (47521) yoh      (47522)     5942 2018-03-04 16:37:40.000000 datalad-0.9.3/docs/source/background.rst
--rw-------   0 yoh      (47521) yoh      (47522)     9167 2018-03-16 22:27:17.000000 datalad-0.9.3/PKG-INFO
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/.github/
--rw-------   0 yoh      (47521) yoh      (47522)      163 2018-03-04 16:37:39.000000 datalad-0.9.3/.github/PULL_REQUEST_TEMPLATE.md
--rw-------   0 yoh      (47521) yoh      (47522)      430 2018-03-04 16:37:39.000000 datalad-0.9.3/.github/issue_template.md
--rw-------   0 yoh      (47521) yoh      (47522)      295 2018-03-04 16:37:39.000000 datalad-0.9.3/Gruntfile.js
--rw-------   0 yoh      (47521) yoh      (47522)     1404 2018-03-04 16:37:40.000000 datalad-0.9.3/tox.ini
--rw-------   0 yoh      (47521) yoh      (47522)      367 2018-03-04 16:37:40.000000 datalad-0.9.3/requirements.txt
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/tools/
--rwx------   0 yoh      (47521) yoh      (47522)     4562 2018-03-04 16:37:40.000000 datalad-0.9.3/tools/mimic_repo
--rw-------   0 yoh      (47521) yoh      (47522)      776 2018-03-04 16:37:40.000000 datalad-0.9.3/tools/cmdline-completion
--rwx------   0 yoh      (47521) yoh      (47522)      751 2018-03-04 16:37:40.000000 datalad-0.9.3/tools/eval_under_testloopfs
--rwx------   0 yoh      (47521) yoh      (47522)     2127 2018-03-04 16:37:40.000000 datalad-0.9.3/tools/git-web-submodules.sh
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/tools/testing/
--rwx------   0 yoh      (47521) yoh      (47522)     1931 2018-03-04 16:37:40.000000 datalad-0.9.3/tools/testing/test_README_in_docker
--rwx------   0 yoh      (47521) yoh      (47522)     4134 2018-03-04 16:37:40.000000 datalad-0.9.3/tools/testing/start_website_in_docker
--rwx------   0 yoh      (47521) yoh      (47522)      481 2018-03-04 16:37:40.000000 datalad-0.9.3/tools/testing/travis_ifdown_nonlo.sh
--rw-------   0 yoh      (47521) yoh      (47522)     1285 2018-03-04 16:37:40.000000 datalad-0.9.3/tools/testing/test_README_in_docker-Dockerfile.in
--rwx------   0 yoh      (47521) yoh      (47522)     1041 2018-03-04 16:37:40.000000 datalad-0.9.3/tools/testing/run_doc_examples
--rw-------   0 yoh      (47521) yoh      (47522)       33 2018-03-04 16:37:40.000000 datalad-0.9.3/tools/testing/.gitignore
--rwx------   0 yoh      (47521) yoh      (47522)     3457 2018-03-04 16:37:40.000000 datalad-0.9.3/tools/testing/make_test_repo
--rw-------   0 yoh      (47521) yoh      (47522)     2633 2018-03-04 16:37:40.000000 datalad-0.9.3/tools/testing/start_website_in_docker-Dockerfile.in
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/tools/testing/bad_internals/
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/tools/testing/bad_internals/_scrapy/
--rw-------   0 yoh      (47521) yoh      (47522)       57 2018-03-04 16:37:40.000000 datalad-0.9.3/tools/testing/bad_internals/_scrapy/scrapy.py
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/tools/testing/conf/
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/tools/testing/conf/etc/
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/tools/testing/conf/etc/apt/
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/tools/testing/conf/etc/apt/apt.conf.d/
--rw-------   0 yoh      (47521) yoh      (47522)      112 2018-03-04 16:37:40.000000 datalad-0.9.3/tools/testing/conf/etc/apt/apt.conf.d/99apt-cacher
--rw-------   0 yoh      (47521) yoh      (47522)      179 2018-03-04 16:37:40.000000 datalad-0.9.3/tools/testing/conf/apache-ssh-supervisor.conf
--rwx------   0 yoh      (47521) yoh      (47522)      938 2018-03-04 16:37:40.000000 datalad-0.9.3/tools/testing/make_test_repos
--rwx------   0 yoh      (47521) yoh      (47522)      593 2018-03-04 16:37:40.000000 datalad-0.9.3/tools/upgrade-annex-osx.sh
--rwx------   0 yoh      (47521) yoh      (47522)     1531 2018-03-04 16:37:40.000000 datalad-0.9.3/tools/dtime
--rwx------   0 yoh      (47521) yoh      (47522)     2117 2018-03-04 16:37:40.000000 datalad-0.9.3/tools/cast2narration
--rwx------   0 yoh      (47521) yoh      (47522)      359 2018-03-04 16:37:40.000000 datalad-0.9.3/tools/mkcontrib
--rwx------   0 yoh      (47521) yoh      (47522)      656 2018-03-04 16:37:40.000000 datalad-0.9.3/tools/cast2script
--rwx------   0 yoh      (47521) yoh      (47522)     3064 2018-03-04 16:37:40.000000 datalad-0.9.3/tools/cast_live
--rwx------   0 yoh      (47521) yoh      (47522)     2472 2018-03-04 16:37:40.000000 datalad-0.9.3/tools/copy_urls_from_datalad.py
--rw-------   0 yoh      (47521) yoh      (47522)     1715 2018-03-04 16:37:40.000000 datalad-0.9.3/tools/cast_bash.rc
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/tools/ci/
--rw-------   0 yoh      (47521) yoh      (47522)      589 2018-03-04 16:37:40.000000 datalad-0.9.3/tools/ci/gpg-90F7E9EB.pub
--rwx------   0 yoh      (47521) yoh      (47522)      513 2018-03-04 16:37:40.000000 datalad-0.9.3/tools/ci/install-annex-snapshot.sh
--rwx------   0 yoh      (47521) yoh      (47522)      714 2018-03-04 16:37:40.000000 datalad-0.9.3/tools/ci/prep-travis-devel-annex.sh
--rwx------   0 yoh      (47521) yoh      (47522)      581 2018-03-04 16:37:40.000000 datalad-0.9.3/tools/ci/prep-travis-forssh.sh
--rwx------   0 yoh      (47521) yoh      (47522)       97 2018-03-04 16:37:40.000000 datalad-0.9.3/tools/ci/prep-travis-forssh-sudo.sh
--rwx------   0 yoh      (47521) yoh      (47522)     4000 2018-03-04 16:37:40.000000 datalad-0.9.3/tools/cast2asciinema
--rwx------   0 yoh      (47521) yoh      (47522)     2797 2018-03-04 16:37:40.000000 datalad-0.9.3/tools/monitor-interrupts.py
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/tools/coverage-bin/
--rwx------   0 yoh      (47521) yoh      (47522)      806 2018-03-04 16:37:40.000000 datalad-0.9.3/tools/coverage-bin/git-annex-remote-datalad
--rwx------   0 yoh      (47521) yoh      (47522)      806 2018-03-04 16:37:40.000000 datalad-0.9.3/tools/coverage-bin/git-annex-remote-datalad-archives
--rwx------   0 yoh      (47521) yoh      (47522)      806 2018-03-04 16:37:40.000000 datalad-0.9.3/tools/coverage-bin/datalad
--rwx------   0 yoh      (47521) yoh      (47522)     3010 2018-03-04 16:37:40.000000 datalad-0.9.3/tools/mimic_merges
--rwx------   0 yoh      (47521) yoh      (47522)     3565 2018-03-04 16:37:40.000000 datalad-0.9.3/tools/urlinfo
--rw-------   0 yoh      (47521) yoh      (47522)    25104 2018-03-16 22:26:59.000000 datalad-0.9.3/CHANGELOG.md
--rw-------   0 yoh      (47521) yoh      (47522)     2114 2018-03-04 16:37:39.000000 datalad-0.9.3/COPYING
--rw-------   0 yoh      (47521) yoh      (47522)      377 2018-03-04 16:37:40.000000 datalad-0.9.3/requirements-devel.txt
--rw-------   0 yoh      (47521) yoh      (47522)       38 2018-03-16 22:27:17.000000 datalad-0.9.3/setup.cfg
--rw-------   0 yoh      (47521) yoh      (47522)      223 2018-03-04 16:37:39.000000 datalad-0.9.3/.gitignore
--rw-------   0 yoh      (47521) yoh      (47522)     7150 2018-03-04 16:37:39.000000 datalad-0.9.3/README.md
--rw-------   0 yoh      (47521) yoh      (47522)    18887 2018-03-16 22:26:59.000000 datalad-0.9.3/CONTRIBUTING.md
--rw-------   0 yoh      (47521) yoh      (47522)     6066 2018-03-04 16:37:39.000000 datalad-0.9.3/asv.conf.json
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/cfgs/
--rw-------   0 yoh      (47521) yoh      (47522)      441 2018-03-04 16:37:39.000000 datalad-0.9.3/cfgs/openfmri.cfg
--rw-------   0 yoh      (47521) yoh      (47522)     1179 2018-03-04 16:37:39.000000 datalad-0.9.3/cfgs/openfmri-ds000001.cfg
--rw-------   0 yoh      (47521) yoh      (47522)       90 2018-03-04 16:37:39.000000 datalad-0.9.3/cfgs/arjlover.cfg
--rw-------   0 yoh      (47521) yoh      (47522)     2409 2018-03-04 16:37:39.000000 datalad-0.9.3/cfgs/nihhelpers.py
--rw-------   0 yoh      (47521) yoh      (47522)     1150 2018-03-04 16:37:39.000000 datalad-0.9.3/cfgs/nih-videos.cfg
--rw-------   0 yoh      (47521) yoh      (47522)      979 2018-03-04 16:37:39.000000 datalad-0.9.3/cfgs/arjlover-fast.cfg
--rw-------   0 yoh      (47521) yoh      (47522)      451 2018-03-04 16:37:39.000000 datalad-0.9.3/cfgs/example-feat.cfg
--rw-------   0 yoh      (47521) yoh      (47522)     2015 2018-03-04 16:37:39.000000 datalad-0.9.3/cfgs/common.cfg
--rw-------   0 yoh      (47521) yoh      (47522)      810 2018-03-04 16:37:39.000000 datalad-0.9.3/cfgs/allen-human.cfg
--rw-------   0 yoh      (47521) yoh      (47522)      305 2018-03-04 16:37:39.000000 datalad-0.9.3/CODE_OF_CONDUCT.md
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/benchmarks/
--rw-------   0 yoh      (47521) yoh      (47522)     2216 2018-03-04 16:37:39.000000 datalad-0.9.3/benchmarks/core.py
--rw-------   0 yoh      (47521) yoh      (47522)        1 2018-03-04 16:37:39.000000 datalad-0.9.3/benchmarks/__init__.py
--rw-------   0 yoh      (47521) yoh      (47522)     4012 2018-03-04 16:37:39.000000 datalad-0.9.3/benchmarks/api.py
--rw-------   0 yoh      (47521) yoh      (47522)      696 2018-03-04 16:37:39.000000 datalad-0.9.3/benchmarks/common.py
--rw-------   0 yoh      (47521) yoh      (47522)      517 2018-03-04 16:37:39.000000 datalad-0.9.3/.mailmap
--rw-------   0 yoh      (47521) yoh      (47522)    10446 2018-03-04 16:37:40.000000 datalad-0.9.3/formatters.py
--rw-------   0 yoh      (47521) yoh      (47522)      125 2018-03-04 16:37:40.000000 datalad-0.9.3/readthedocs.yml
--rw-------   0 yoh      (47521) yoh      (47522)       75 2018-03-04 16:37:39.000000 datalad-0.9.3/.gitmodules
--rw-------   0 yoh      (47521) yoh      (47522)       45 2018-03-04 16:37:39.000000 datalad-0.9.3/.coveragerc
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad.egg-info/
--rw-------   0 yoh      (47521) yoh      (47522)        8 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad.egg-info/top_level.txt
--rw-------   0 yoh      (47521) yoh      (47522)     9167 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad.egg-info/PKG-INFO
--rw-------   0 yoh      (47521) yoh      (47522)     1607 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad.egg-info/requires.txt
--rw-------   0 yoh      (47521) yoh      (47522)    13684 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad.egg-info/SOURCES.txt
--rw-------   0 yoh      (47521) yoh      (47522)        1 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad.egg-info/dependency_links.txt
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/tests/
--rw-------   0 yoh      (47521) yoh      (47522)     2098 2018-03-16 22:26:59.000000 datalad-0.9.3/datalad/tests/test_base.py
--rw-------   0 yoh      (47521) yoh      (47522)     7686 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/tests/test_constraints.py
--rw-------   0 yoh      (47521) yoh      (47522)     2421 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/tests/test_testrepos.py
--rw-------   0 yoh      (47521) yoh      (47522)     4561 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/tests/test_interface.py
--rw-------   0 yoh      (47521) yoh      (47522)     1530 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/tests/test__main__.py
--rw-------   0 yoh      (47521) yoh      (47522)    27257 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/tests/test_utils.py
--rw-------   0 yoh      (47521) yoh      (47522)      252 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/tests/heavyoutput.py
--rw-------   0 yoh      (47521) yoh      (47522)     2961 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/tests/test_s3.py
--rw-------   0 yoh      (47521) yoh      (47522)     4013 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/tests/utils_testdatasets.py
--rw-------   0 yoh      (47521) yoh      (47522)     1442 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/tests/test_misc.py
--rw-------   0 yoh      (47521) yoh      (47522)     5302 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/tests/test_log.py
--rw-------   0 yoh      (47521) yoh      (47522)     7396 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/tests/test_auto.py
--rw-------   0 yoh      (47521) yoh      (47522)      811 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/tests/__init__.py
--rw-------   0 yoh      (47521) yoh      (47522)    19205 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/tests/test_tests_utils.py
--rw-------   0 yoh      (47521) yoh      (47522)     6891 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/tests/test_archives.py
--rw-------   0 yoh      (47521) yoh      (47522)    12049 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/tests/test_config.py
--rw-------   0 yoh      (47521) yoh      (47522)    11446 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/tests/test_cmd.py
--rw-------   0 yoh      (47521) yoh      (47522)    46631 2018-03-16 22:26:59.000000 datalad-0.9.3/datalad/tests/utils.py
--rw-------   0 yoh      (47521) yoh      (47522)     7897 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/tests/utils_testrepos.py
--rw-------   0 yoh      (47521) yoh      (47522)     1924 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/tests/test_utils_testrepos.py
--rw-------   0 yoh      (47521) yoh      (47522)     2714 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/tests/test_api.py
--rw-------   0 yoh      (47521) yoh      (47522)     6919 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/tests/test_protocols.py
--rw-------   0 yoh      (47521) yoh      (47522)     1323 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/tests/test_installed.py
--rw-------   0 yoh      (47521) yoh      (47522)     5168 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/tests/test_dochelpers.py
--rw-------   0 yoh      (47521) yoh      (47522)     1256 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/tests/test_strings.py
--rw-------   0 yoh      (47521) yoh      (47522)     3700 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/tests/test_direct_mode.py
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/interface/
--rw-------   0 yoh      (47521) yoh      (47522)    17663 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/interface/base.py
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/interface/tests/
--rw-------   0 yoh      (47521) yoh      (47522)    12370 2018-03-16 22:26:59.000000 datalad-0.9.3/datalad/interface/tests/test_save.py
--rw-------   0 yoh      (47521) yoh      (47522)    20655 2018-03-16 22:26:59.000000 datalad-0.9.3/datalad/interface/tests/test_add_archive_content.py
--rw-------   0 yoh      (47521) yoh      (47522)     4053 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/interface/tests/test_ls.py
--rw-------   0 yoh      (47521) yoh      (47522)    10639 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/interface/tests/test_utils.py
--rw-------   0 yoh      (47521) yoh      (47522)     3626 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/interface/tests/test_crawl_init.py
--rw-------   0 yoh      (47521) yoh      (47522)     6223 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/interface/tests/test_unlock.py
--rw-------   0 yoh      (47521) yoh      (47522)      450 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/interface/tests/__init__.py
--rw-------   0 yoh      (47521) yoh      (47522)     8010 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/interface/tests/test_diff.py
--rw-------   0 yoh      (47521) yoh      (47522)    13953 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/interface/tests/test_annotate_paths.py
--rw-------   0 yoh      (47521) yoh      (47522)     4040 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/interface/tests/test_docs.py
--rw-------   0 yoh      (47521) yoh      (47522)    11328 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/interface/tests/test_ls_webui.py
--rw-------   0 yoh      (47521) yoh      (47522)     1921 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/interface/tests/test_download_url.py
--rw-------   0 yoh      (47521) yoh      (47522)     3876 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/interface/tests/test_crawl.py
--rw-------   0 yoh      (47521) yoh      (47522)    17720 2018-03-16 22:26:59.000000 datalad-0.9.3/datalad/interface/tests/test_run.py
--rw-------   0 yoh      (47521) yoh      (47522)     2385 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/interface/tests/test_clean.py
--rw-------   0 yoh      (47521) yoh      (47522)     8670 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/interface/run.py
--rw-------   0 yoh      (47521) yoh      (47522)    24739 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/interface/add_archive_content.py
--rw-------   0 yoh      (47521) yoh      (47522)     4510 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/interface/clean.py
--rw-------   0 yoh      (47521) yoh      (47522)    13120 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/interface/results.py
--rw-------   0 yoh      (47521) yoh      (47522)     8048 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/interface/crawl.py
--rw-------   0 yoh      (47521) yoh      (47522)     3999 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/interface/download_url.py
--rw-------   0 yoh      (47521) yoh      (47522)    23863 2018-03-16 22:26:59.000000 datalad-0.9.3/datalad/interface/ls.py
--rw-------   0 yoh      (47521) yoh      (47522)     3398 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/interface/__init__.py
--rw-------   0 yoh      (47521) yoh      (47522)     6492 2018-03-16 22:26:59.000000 datalad-0.9.3/datalad/interface/unlock.py
--rw-------   0 yoh      (47521) yoh      (47522)    17190 2018-03-16 22:26:59.000000 datalad-0.9.3/datalad/interface/save.py
--rw-------   0 yoh      (47521) yoh      (47522)     1936 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/interface/test.py
--rw-------   0 yoh      (47521) yoh      (47522)    21736 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/interface/utils.py
--rw-------   0 yoh      (47521) yoh      (47522)    12546 2018-03-16 22:26:59.000000 datalad-0.9.3/datalad/interface/common_opts.py
--rw-------   0 yoh      (47521) yoh      (47522)    34420 2018-03-16 22:26:59.000000 datalad-0.9.3/datalad/interface/annotate_paths.py
--rw-------   0 yoh      (47521) yoh      (47522)    15880 2018-03-16 22:26:59.000000 datalad-0.9.3/datalad/interface/diff.py
--rw-------   0 yoh      (47521) yoh      (47522)     9291 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/interface/common_cfg.py
--rw-------   0 yoh      (47521) yoh      (47522)     3957 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/interface/crawl_init.py
--rw-------   0 yoh      (47521) yoh      (47522)    16245 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/interface/ls_webui.py
--rw-------   0 yoh      (47521) yoh      (47522)    12661 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/interface/rerun.py
--rw-------   0 yoh      (47521) yoh      (47522)     3132 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/__main__.py
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/resources/
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/resources/website/
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/resources/website/tests/
--rw-------   0 yoh      (47521) yoh      (47522)    13759 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/resources/website/tests/test.html
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/resources/website/tests/qunit/
--rw-------   0 yoh      (47521) yoh      (47522)   208178 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/resources/website/tests/qunit/sinon-1.17.5.js
--rw-------   0 yoh      (47521) yoh      (47522)   117821 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/resources/website/tests/qunit/qunit-2.0.1.js
--rw-------   0 yoh      (47521) yoh      (47522)     7456 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/resources/website/tests/qunit/qunit-2.0.1.css
--rw-------   0 yoh      (47521) yoh      (47522)     2443 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/resources/website/tests/qunit/sinon-qunit-1.0.0.js
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/resources/website/assets/
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/resources/website/assets/images/
--rw-------   0 yoh      (47521) yoh      (47522)    12537 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/resources/website/assets/images/logo_wide.svg
--rw-------   0 yoh      (47521) yoh      (47522)      158 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/resources/website/assets/images/sort_desc.png
--rw-------   0 yoh      (47521) yoh      (47522)      272 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/resources/website/assets/images/favicon-16x16.png
--rw-------   0 yoh      (47521) yoh      (47522)      199 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/resources/website/assets/images/sort_both.png
--rw-------   0 yoh      (47521) yoh      (47522)    15086 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/resources/website/assets/images/favicon.ico
--rw-------   0 yoh      (47521) yoh      (47522)    16891 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/resources/website/assets/images/favicon.svg
--rw-------   0 yoh      (47521) yoh      (47522)      158 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/resources/website/assets/images/sort_asc.png
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/resources/website/assets/js/
--rw-------   0 yoh      (47521) yoh      (47522)     8572 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/resources/website/assets/js/md5-2.3.0.js
--rw-------   0 yoh      (47521) yoh      (47522)    21039 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/resources/website/assets/js/main.js
--rw-------   0 yoh      (47521) yoh      (47522)   447304 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/resources/website/assets/js/jquery.dataTables-1.10.12.js
--rw-------   0 yoh      (47521) yoh      (47522)   257551 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/resources/website/assets/js/jquery-2.2.4.js
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/resources/website/assets/css/
--rw-------   0 yoh      (47521) yoh      (47522)    15423 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/resources/website/assets/css/jquery.dataTables-1.10.12.css
--rw-------   0 yoh      (47521) yoh      (47522)     3121 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/resources/website/index.html
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/customremotes/
--rw-------   0 yoh      (47521) yoh      (47522)    23561 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/customremotes/base.py
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/customremotes/tests/
--rw-------   0 yoh      (47521) yoh      (47522)     5199 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/customremotes/tests/test_base.py
--rw-------   0 yoh      (47521) yoh      (47522)     1112 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/customremotes/tests/__init__.py
--rw-------   0 yoh      (47521) yoh      (47522)    11004 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/customremotes/tests/test_archives.py
--rw-------   0 yoh      (47521) yoh      (47522)     4182 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/customremotes/tests/test_datalad.py
--rw-------   0 yoh      (47521) yoh      (47522)     4740 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/customremotes/main.py
--rw-------   0 yoh      (47521) yoh      (47522)      483 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/customremotes/__init__.py
--rw-------   0 yoh      (47521) yoh      (47522)     6906 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/customremotes/datalad.py
--rw-------   0 yoh      (47521) yoh      (47522)    17001 2018-03-16 22:26:59.000000 datalad-0.9.3/datalad/customremotes/archives.py
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/plugin/
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/plugin/tests/
--rw-------   0 yoh      (47521) yoh      (47522)      446 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/plugin/tests/__init__.py
--rw-------   0 yoh      (47521) yoh      (47522)     8700 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/plugin/tests/test_plugins.py
--rw-------   0 yoh      (47521) yoh      (47522)     3354 2018-03-16 22:26:59.000000 datalad-0.9.3/datalad/plugin/tests/test_tarball.py
--rw-------   0 yoh      (47521) yoh      (47522)     4256 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/plugin/no_annex.py
--rw-------   0 yoh      (47521) yoh      (47522)     2934 2018-03-16 22:26:59.000000 datalad-0.9.3/datalad/plugin/export_tarball.py
--rw-------   0 yoh      (47521) yoh      (47522)     3645 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/plugin/wtf.py
--rw-------   0 yoh      (47521) yoh      (47522)     9976 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/plugin/__init__.py
--rw-------   0 yoh      (47521) yoh      (47522)     2373 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/plugin/add_readme.py
--rw-------   0 yoh      (47521) yoh      (47522)    24506 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/config.py
--rw-------   0 yoh      (47521) yoh      (47522)    10578 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/auto.py
--rw-------   0 yoh      (47521) yoh      (47522)    10190 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/log.py
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/support/
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/support/tests/
--rw-------   0 yoh      (47521) yoh      (47522)      663 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/tests/test_cache.py
--rw-------   0 yoh      (47521) yoh      (47522)    18314 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/tests/test_network.py
--rw-------   0 yoh      (47521) yoh      (47522)    79763 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/tests/test_annexrepo.py
--rw-------   0 yoh      (47521) yoh      (47522)     1076 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/tests/test_vcr_.py
--rw-------   0 yoh      (47521) yoh      (47522)    42904 2018-03-16 22:26:59.000000 datalad-0.9.3/datalad/support/tests/test_gitrepo.py
--rw-------   0 yoh      (47521) yoh      (47522)     2025 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/tests/test_digests.py
--rw-------   0 yoh      (47521) yoh      (47522)     3184 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/tests/test_stats.py
--rw-------   0 yoh      (47521) yoh      (47522)      469 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/tests/__init__.py
--rw-------   0 yoh      (47521) yoh      (47522)     1508 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/tests/test_status.py
--rw-------   0 yoh      (47521) yoh      (47522)     3268 2018-03-16 22:26:59.000000 datalad-0.9.3/datalad/support/tests/test_locking.py
--rw-------   0 yoh      (47521) yoh      (47522)     6055 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/tests/test_versions.py
--rw-------   0 yoh      (47521) yoh      (47522)     1628 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/tests/utils.py
--rw-------   0 yoh      (47521) yoh      (47522)     1812 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/tests/test_sshrun.py
--rw-------   0 yoh      (47521) yoh      (47522)     1397 2018-03-16 22:26:59.000000 datalad-0.9.3/datalad/support/tests/test_json_py.py
--rw-------   0 yoh      (47521) yoh      (47522)     7912 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/tests/test_external_versions.py
--rw-------   0 yoh      (47521) yoh      (47522)     7611 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/tests/test_sshconnector.py
--rw-------   0 yoh      (47521) yoh      (47522)     6913 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/versions.py
--rw-------   0 yoh      (47521) yoh      (47522)     3247 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/keyring_.py
--rw-------   0 yoh      (47521) yoh      (47522)     5045 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/param.py
--rw-------   0 yoh      (47521) yoh      (47522)     3178 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/sshrun.py
--rw-------   0 yoh      (47521) yoh      (47522)     8584 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/external_versions.py
--rw-------   0 yoh      (47521) yoh      (47522)   143144 2018-03-16 22:26:59.000000 datalad-0.9.3/datalad/support/annexrepo.py
--rw-------   0 yoh      (47521) yoh      (47522)     7368 2018-03-16 22:26:59.000000 datalad-0.9.3/datalad/support/_lru_cache2.py
--rw-------   0 yoh      (47521) yoh      (47522)    14013 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/constraints.py
--rw-------   0 yoh      (47521) yoh      (47522)    82203 2018-03-16 22:26:59.000000 datalad-0.9.3/datalad/support/gitrepo.py
--rw-------   0 yoh      (47521) yoh      (47522)     2381 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/status.py
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/support/third/
--rw-------   0 yoh      (47521) yoh      (47522)    13859 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/third/noseclasses.py
--rw-------   0 yoh      (47521) yoh      (47522)        0 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/third/__init__.py
--rw-------   0 yoh      (47521) yoh      (47522)    15888 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/third/nosetester.py
--rw-------   0 yoh      (47521) yoh      (47522)     3872 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/third/nda_aws_token_generator.py
--rw-------   0 yoh      (47521) yoh      (47522)     9015 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/protocol.py
--rw-------   0 yoh      (47521) yoh      (47522)     2313 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/nda_.py
--rw-------   0 yoh      (47521) yoh      (47522)     3055 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/cookies.py
--rw-------   0 yoh      (47521) yoh      (47522)      502 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/__init__.py
--rw-------   0 yoh      (47521) yoh      (47522)    17412 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/sshconnector.py
--rw-------   0 yoh      (47521) yoh      (47522)     2105 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/strings.py
--rw-------   0 yoh      (47521) yoh      (47522)     9770 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/exceptions.py
--rw-------   0 yoh      (47521) yoh      (47522)    30840 2018-03-16 22:26:59.000000 datalad-0.9.3/datalad/support/network.py
--rw-------   0 yoh      (47521) yoh      (47522)     6905 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/repo.py
--rw-------   0 yoh      (47521) yoh      (47522)     3413 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/vcr_.py
--rw-------   0 yoh      (47521) yoh      (47522)     2470 2018-03-16 22:26:59.000000 datalad-0.9.3/datalad/support/json_py.py
--rw-------   0 yoh      (47521) yoh      (47522)    14697 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/s3.py
--rw-------   0 yoh      (47521) yoh      (47522)     6114 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/configparserinc.py
--rw-------   0 yoh      (47521) yoh      (47522)     1454 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/ansi_colors.py
--rw-------   0 yoh      (47521) yoh      (47522)     1672 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/digests.py
--rw-------   0 yoh      (47521) yoh      (47522)    19700 2018-03-16 22:26:59.000000 datalad-0.9.3/datalad/support/archives.py
--rw-------   0 yoh      (47521) yoh      (47522)     6187 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/support/stats.py
--rw-------   0 yoh      (47521) yoh      (47522)     1221 2018-03-16 22:26:59.000000 datalad-0.9.3/datalad/support/cache.py
--rw-------   0 yoh      (47521) yoh      (47522)     3204 2018-03-16 22:26:59.000000 datalad-0.9.3/datalad/support/locking.py
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/crawler/
--rw-------   0 yoh      (47521) yoh      (47522)      504 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/base.py
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/crawler/tests/
--rw-------   0 yoh      (47521) yoh      (47522)     9825 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/tests/test_pipeline.py
--rw-------   0 yoh      (47521) yoh      (47522)      566 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/tests/__init__.py
--rw-------   0 yoh      (47521) yoh      (47522)    21798 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/pipeline.py
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/crawler/dbs/
--rw-------   0 yoh      (47521) yoh      (47522)     7153 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/dbs/base.py
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/crawler/dbs/tests/
--rw-------   0 yoh      (47521) yoh      (47522)     5198 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/dbs/tests/test_files.py
--rw-------   0 yoh      (47521) yoh      (47522)      389 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/dbs/tests/__init__.py
--rw-------   0 yoh      (47521) yoh      (47522)     3141 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/dbs/versions.py
--rw-------   0 yoh      (47521) yoh      (47522)      461 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/dbs/__init__.py
--rw-------   0 yoh      (47521) yoh      (47522)     5217 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/dbs/files.py
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/crawler/oldconfig/
--rw-------   0 yoh      (47521) yoh      (47522)     6357 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/oldconfig/base.py
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/crawler/oldconfig/tests/
--rw-------   0 yoh      (47521) yoh      (47522)        0 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/oldconfig/tests/__init__.py
--rw-------   0 yoh      (47521) yoh      (47522)     2082 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/oldconfig/tests/test_config.py
--rw-------   0 yoh      (47521) yoh      (47522)      516 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/oldconfig/__init__.py
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/crawler/nodes/
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/crawler/nodes/tests/
--rw-------   0 yoh      (47521) yoh      (47522)    16493 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/nodes/tests/test_annex.py
--rw-------   0 yoh      (47521) yoh      (47522)     4261 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/nodes/tests/test_matches.py
--rw-------   0 yoh      (47521) yoh      (47522)     9340 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/nodes/tests/test_s3.py
--rw-------   0 yoh      (47521) yoh      (47522)    17177 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/nodes/tests/test_misc.py
--rw-------   0 yoh      (47521) yoh      (47522)      501 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/nodes/tests/__init__.py
--rw-------   0 yoh      (47521) yoh      (47522)     2973 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/nodes/tests/test_crawl_url.py
--rw-------   0 yoh      (47521) yoh      (47522)     6822 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/nodes/crawl_url.py
--rw-------   0 yoh      (47521) yoh      (47522)    67617 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/nodes/annex.py
--rw-------   0 yoh      (47521) yoh      (47522)      894 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/nodes/__init__.py
--rw-------   0 yoh      (47521) yoh      (47522)    18482 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/nodes/misc.py
--rw-------   0 yoh      (47521) yoh      (47522)    12951 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/nodes/s3.py
--rw-------   0 yoh      (47521) yoh      (47522)     7386 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/nodes/matches.py
--rw-------   0 yoh      (47521) yoh      (47522)      504 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/__init__.py
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/crawler/pipelines/
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/crawler/pipelines/tests/
--rw-------   0 yoh      (47521) yoh      (47522)     1065 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/pipelines/tests/test_nda.py
--rw-------   0 yoh      (47521) yoh      (47522)     2948 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/pipelines/tests/test_openfmri_collection.py
--rw-------   0 yoh      (47521) yoh      (47522)     3398 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/pipelines/tests/test_simple_s3.py
--rw-------   0 yoh      (47521) yoh      (47522)     3153 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/pipelines/tests/test_fcptable.py
--rw-------   0 yoh      (47521) yoh      (47522)      177 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/pipelines/tests/__init__.py
--rw-------   0 yoh      (47521) yoh      (47522)    24487 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/pipelines/tests/test_openfmri.py
--rw-------   0 yoh      (47521) yoh      (47522)     2889 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/pipelines/tests/test_simple_with_archives.py
--rw-------   0 yoh      (47521) yoh      (47522)      920 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/pipelines/tests/utils.py
--rw-------   0 yoh      (47521) yoh      (47522)     1847 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/pipelines/tests/test_crcns.py
--rw-------   0 yoh      (47521) yoh      (47522)    12480 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/pipelines/tests/test_balsa.py
--rw-------   0 yoh      (47521) yoh      (47522)    11087 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/pipelines/crcns.py
--rw-------   0 yoh      (47521) yoh      (47522)    14323 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/pipelines/openfmri.py
--rw-------   0 yoh      (47521) yoh      (47522)     1086 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/pipelines/template.py
--rw-------   0 yoh      (47521) yoh      (47522)     9920 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/pipelines/balsa.py
--rw-------   0 yoh      (47521) yoh      (47522)     4569 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/pipelines/simple_with_archives.py
--rw-------   0 yoh      (47521) yoh      (47522)      458 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/pipelines/__init__.py
--rw-------   0 yoh      (47521) yoh      (47522)     4163 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/pipelines/openfmri_s3.py
--rw-------   0 yoh      (47521) yoh      (47522)     3173 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/pipelines/abstractsonline.py
--rw-------   0 yoh      (47521) yoh      (47522)     6061 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/pipelines/simple_s3.py
--rw-------   0 yoh      (47521) yoh      (47522)     6072 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/pipelines/nda.py
--rw-------   0 yoh      (47521) yoh      (47522)     7165 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/crawler/pipelines/fcptable.py
--rw-------   0 yoh      (47521) yoh      (47522)     2393 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/consts.py
--rw-------   0 yoh      (47521) yoh      (47522)     7078 2018-03-16 22:26:59.000000 datalad-0.9.3/datalad/__init__.py
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/ui/
--rw-------   0 yoh      (47521) yoh      (47522)     1170 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/ui/base.py
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/ui/tests/
--rw-------   0 yoh      (47521) yoh      (47522)     2512 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/ui/tests/test_base.py
--rw-------   0 yoh      (47521) yoh      (47522)     3824 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/ui/tests/test_dialog.py
--rw-------   0 yoh      (47521) yoh      (47522)      442 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/ui/tests/__init__.py
--rw-------   0 yoh      (47521) yoh      (47522)    10191 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/ui/dialog.py
--rw-------   0 yoh      (47521) yoh      (47522)     2673 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/ui/__init__.py
--rw-------   0 yoh      (47521) yoh      (47522)     1633 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/ui/utils.py
--rw-------   0 yoh      (47521) yoh      (47522)     6123 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/ui/progressbars.py
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/downloaders/
--rw-------   0 yoh      (47521) yoh      (47522)    22303 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/downloaders/base.py
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/downloaders/tests/
--rw-------   0 yoh      (47521) yoh      (47522)      454 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/downloaders/tests/test_base.py
--rw-------   0 yoh      (47521) yoh      (47522)     4629 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/downloaders/tests/test_credentials.py
--rw-------   0 yoh      (47521) yoh      (47522)     4533 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/downloaders/tests/test_s3.py
--rw-------   0 yoh      (47521) yoh      (47522)        0 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/downloaders/tests/__init__.py
--rw-------   0 yoh      (47521) yoh      (47522)     1241 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/downloaders/tests/utils.py
--rw-------   0 yoh      (47521) yoh      (47522)     3508 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/downloaders/tests/test_providers.py
--rw-------   0 yoh      (47521) yoh      (47522)    18778 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/downloaders/tests/test_http.py
--rw-------   0 yoh      (47521) yoh      (47522)    20844 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/downloaders/http.py
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/downloaders/configs/
--rw-------   0 yoh      (47521) yoh      (47522)      350 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/downloaders/configs/openfmri.cfg
--rw-------   0 yoh      (47521) yoh      (47522)      883 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/downloaders/configs/nda.cfg
--rw-------   0 yoh      (47521) yoh      (47522)     1633 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/downloaders/configs/nitrc.cfg
--rw-------   0 yoh      (47521) yoh      (47522)      203 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/downloaders/configs/indi.cfg
--rw-------   0 yoh      (47521) yoh      (47522)     1801 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/downloaders/configs/crcns.cfg
--rw-------   0 yoh      (47521) yoh      (47522)      594 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/downloaders/configs/kaggle.cfg
--rw-------   0 yoh      (47521) yoh      (47522)      289 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/downloaders/configs/providers.cfg
--rw-------   0 yoh      (47521) yoh      (47522)     1874 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/downloaders/configs/hcp.cfg
--rw-------   0 yoh      (47521) yoh      (47522)      200 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/downloaders/configs/crawdad.cfg
--rw-------   0 yoh      (47521) yoh      (47522)    14362 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/downloaders/providers.py
--rw-------   0 yoh      (47521) yoh      (47522)      744 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/downloaders/__init__.py
--rw-------   0 yoh      (47521) yoh      (47522)     9280 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/downloaders/s3.py
--rw-------   0 yoh      (47521) yoh      (47522)    10938 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/downloaders/credentials.py
--rw-------   0 yoh      (47521) yoh      (47522)     3069 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/api.py
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/metadata/
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/metadata/tests/
--rw-------   0 yoh      (47521) yoh      (47522)    15618 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/metadata/tests/test_base.py
--rw-------   0 yoh      (47521) yoh      (47522)    10395 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/metadata/tests/test_manipulation.py
--rw-------   0 yoh      (47521) yoh      (47522)     6231 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/metadata/tests/test_search.py
--rw-------   0 yoh      (47521) yoh      (47522)        0 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/metadata/tests/__init__.py
--rw-------   0 yoh      (47521) yoh      (47522)     9176 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/metadata/aggregate.py
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/metadata/parsers/
--rw-------   0 yoh      (47521) yoh      (47522)     2359 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/metadata/parsers/base.py
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/metadata/parsers/tests/
--rw-------   0 yoh      (47521) yoh      (47522)     2935 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/metadata/parsers/tests/test_datacite_xml.py
--rw-------   0 yoh      (47521) yoh      (47522)     2866 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/metadata/parsers/tests/test_frictionless_datapackage.py
--rw-------   0 yoh      (47521) yoh      (47522)      412 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/metadata/parsers/tests/__init__.py
--rw-------   0 yoh      (47521) yoh      (47522)     3457 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/metadata/parsers/tests/test_rfc822.py
--rw-------   0 yoh      (47521) yoh      (47522)     1406 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/metadata/parsers/tests/test_aggregate.py
--rw-------   0 yoh      (47521) yoh      (47522)     3810 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/metadata/parsers/tests/test_bids.py
--rw-------   0 yoh      (47521) yoh      (47522)     3628 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/metadata/parsers/datalad_rfc822.py
--rw-------   0 yoh      (47521) yoh      (47522)     3755 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/metadata/parsers/aggregate.py
--rw-------   0 yoh      (47521) yoh      (47522)     2279 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/metadata/parsers/frictionless_datapackage.py
--rw-------   0 yoh      (47521) yoh      (47522)      619 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/metadata/parsers/__init__.py
--rw-------   0 yoh      (47521) yoh      (47522)     4027 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/metadata/parsers/datacite.py
--rw-------   0 yoh      (47521) yoh      (47522)     2750 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/metadata/parsers/bids.py
--rw-------   0 yoh      (47521) yoh      (47522)    11907 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/metadata/__init__.py
--rw-------   0 yoh      (47521) yoh      (47522)    20445 2018-03-16 22:26:59.000000 datalad-0.9.3/datalad/metadata/metadata.py
--rw-------   0 yoh      (47521) yoh      (47522)    17723 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/metadata/search.py
--rw-------   0 yoh      (47521) yoh      (47522)    42364 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/utils.py
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/distribution/
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/distribution/tests/
--rw-------   0 yoh      (47521) yoh      (47522)    22962 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/distribution/tests/test_create_sibling.py
--rw-------   0 yoh      (47521) yoh      (47522)    13503 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/distribution/tests/test_update.py
--rw-------   0 yoh      (47521) yoh      (47522)     3276 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/distribution/tests/test_create_github.py
--rw-------   0 yoh      (47521) yoh      (47522)     3143 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/distribution/tests/test_create_test_dataset.py
--rw-------   0 yoh      (47521) yoh      (47522)     2908 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/distribution/tests/test_utils.py
--rw-------   0 yoh      (47521) yoh      (47522)     1569 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/distribution/tests/test_dataset_binding.py
--rw-------   0 yoh      (47521) yoh      (47522)     6735 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/distribution/tests/test_subdataset.py
--rw-------   0 yoh      (47521) yoh      (47522)    27076 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/distribution/tests/test_publish.py
--rw-------   0 yoh      (47521) yoh      (47522)    10489 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/distribution/tests/test_dataset.py
--rw-------   0 yoh      (47521) yoh      (47522)    33997 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/distribution/tests/test_install.py
--rw-------   0 yoh      (47521) yoh      (47522)      452 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/distribution/tests/__init__.py
--rw-------   0 yoh      (47521) yoh      (47522)    19339 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/distribution/tests/test_get.py
--rw-------   0 yoh      (47521) yoh      (47522)     9059 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/distribution/tests/test_siblings.py
--rw-------   0 yoh      (47521) yoh      (47522)    20126 2018-03-16 22:26:59.000000 datalad-0.9.3/datalad/distribution/tests/test_uninstall.py
--rw-------   0 yoh      (47521) yoh      (47522)    11123 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/distribution/tests/test_create.py
--rw-------   0 yoh      (47521) yoh      (47522)     1277 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/distribution/tests/test_dataset_config.py
--rw-------   0 yoh      (47521) yoh      (47522)    12543 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/distribution/tests/test_clone.py
--rw-------   0 yoh      (47521) yoh      (47522)    13433 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/distribution/tests/test_add.py
--rw-------   0 yoh      (47521) yoh      (47522)    36866 2018-03-16 22:26:59.000000 datalad-0.9.3/datalad/distribution/publish.py
--rw-------   0 yoh      (47521) yoh      (47522)    34419 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/distribution/create_sibling.py
--rw-------   0 yoh      (47521) yoh      (47522)    17452 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/distribution/subdatasets.py
--rw-------   0 yoh      (47521) yoh      (47522)    33694 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/distribution/siblings.py
--rw-------   0 yoh      (47521) yoh      (47522)    14779 2018-03-16 22:26:59.000000 datalad-0.9.3/datalad/distribution/remove.py
--rw-------   0 yoh      (47521) yoh      (47522)     1032 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/distribution/add_sibling.py
--rw-------   0 yoh      (47521) yoh      (47522)     6562 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/distribution/create_test_dataset.py
--rw-------   0 yoh      (47521) yoh      (47522)    15482 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/distribution/create_sibling_github.py
--rw-------   0 yoh      (47521) yoh      (47522)    19940 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/distribution/install.py
--rw-------   0 yoh      (47521) yoh      (47522)     8188 2018-03-16 22:26:59.000000 datalad-0.9.3/datalad/distribution/drop.py
--rw-------   0 yoh      (47521) yoh      (47522)     8996 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/distribution/uninstall.py
--rw-------   0 yoh      (47521) yoh      (47522)    18620 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/distribution/dataset.py
--rw-------   0 yoh      (47521) yoh      (47522)    29372 2018-03-16 22:26:59.000000 datalad-0.9.3/datalad/distribution/get.py
--rw-------   0 yoh      (47521) yoh      (47522)      498 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/distribution/__init__.py
--rw-------   0 yoh      (47521) yoh      (47522)    11979 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/distribution/clone.py
--rw-------   0 yoh      (47521) yoh      (47522)     8479 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/distribution/update.py
--rw-------   0 yoh      (47521) yoh      (47522)    24570 2018-03-16 22:26:59.000000 datalad-0.9.3/datalad/distribution/add.py
--rw-------   0 yoh      (47521) yoh      (47522)     8057 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/distribution/utils.py
--rw-------   0 yoh      (47521) yoh      (47522)    17186 2018-03-16 22:26:59.000000 datalad-0.9.3/datalad/distribution/create.py
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/cmdline/
-drwx------   0 yoh      (47521) yoh      (47522)        0 2018-03-16 22:27:17.000000 datalad-0.9.3/datalad/cmdline/tests/
--rw-------   0 yoh      (47521) yoh      (47522)      448 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/cmdline/tests/__init__.py
--rw-------   0 yoh      (47521) yoh      (47522)     2744 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/cmdline/tests/test_helpers.py
--rw-------   0 yoh      (47521) yoh      (47522)     3101 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/cmdline/tests/test_formatters.py
--rw-------   0 yoh      (47521) yoh      (47522)     8463 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/cmdline/tests/test_main.py
--rw-------   0 yoh      (47521) yoh      (47522)     9146 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/cmdline/helpers.py
--rw-------   0 yoh      (47521) yoh      (47522)     2242 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/cmdline/common_args.py
--rw-------   0 yoh      (47521) yoh      (47522)    17916 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/cmdline/main.py
--rw-------   0 yoh      (47521) yoh      (47522)      433 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/cmdline/__init__.py
--rw-------   0 yoh      (47521) yoh      (47522)     2090 2018-03-16 22:26:59.000000 datalad-0.9.3/datalad/version.py
--rw-------   0 yoh      (47521) yoh      (47522)    10543 2018-03-04 16:37:40.000000 datalad-0.9.3/datalad/dochelpers.py
--rw-------   0 yoh      (47521) yoh      (47522)    26600 2018-03-04 16:37:39.000000 datalad-0.9.3/datalad/cmd.py
--rwx------   0 yoh      (47521) yoh      (47522)     5864 2018-03-16 22:26:59.000000 datalad-0.9.3/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.382451 datalad-1.0.0/
+-rw-r--r--   0 runner    (1001) docker     (127)      114 2024-04-06 16:45:49.000000 datalad-1.0.0/.coveragerc
+-rw-r--r--   0 runner    (1001) docker     (127)   305112 2024-04-06 16:46:20.000000 datalad-1.0.0/CHANGELOG.md
+-rw-r--r--   0 runner    (1001) docker     (127)    31594 2024-04-06 16:45:49.000000 datalad-1.0.0/CONTRIBUTING.md
+-rw-r--r--   0 runner    (1001) docker     (127)      574 2024-04-06 16:45:49.000000 datalad-1.0.0/CONTRIBUTORS
+-rw-r--r--   0 runner    (1001) docker     (127)     2104 2024-04-06 16:45:49.000000 datalad-1.0.0/COPYING
+-rw-r--r--   0 runner    (1001) docker     (127)      728 2024-04-06 16:45:49.000000 datalad-1.0.0/MANIFEST.in
+-rw-r--r--   0 runner    (1001) docker     (127)     2025 2024-04-06 16:45:49.000000 datalad-1.0.0/Makefile
+-rw-r--r--   0 runner    (1001) docker     (127)    37754 2024-04-06 16:46:28.382451 datalad-1.0.0/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)    27149 2024-04-06 16:45:49.000000 datalad-1.0.0/README.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.286450 datalad-1.0.0/_datalad_build_support/
+-rw-r--r--   0 runner    (1001) docker     (127)      529 2024-04-06 16:45:49.000000 datalad-1.0.0/_datalad_build_support/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10971 2024-04-06 16:45:49.000000 datalad-1.0.0/_datalad_build_support/formatters.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13311 2024-04-06 16:45:49.000000 datalad-1.0.0/_datalad_build_support/setup.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6048 2024-04-06 16:45:49.000000 datalad-1.0.0/asv.conf.json
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.286450 datalad-1.0.0/benchmarks/
+-rw-r--r--   0 runner    (1001) docker     (127)        1 2024-04-06 16:45:49.000000 datalad-1.0.0/benchmarks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3454 2024-04-06 16:45:49.000000 datalad-1.0.0/benchmarks/api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1460 2024-04-06 16:45:49.000000 datalad-1.0.0/benchmarks/cli.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5561 2024-04-06 16:45:49.000000 datalad-1.0.0/benchmarks/common.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1875 2024-04-06 16:45:49.000000 datalad-1.0.0/benchmarks/core.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.286450 datalad-1.0.0/benchmarks/plugins/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-06 16:45:49.000000 datalad-1.0.0/benchmarks/plugins/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2068 2024-04-06 16:45:49.000000 datalad-1.0.0/benchmarks/plugins/addurls.py
+-rw-r--r--   0 runner    (1001) docker     (127)      846 2024-04-06 16:45:49.000000 datalad-1.0.0/benchmarks/repo.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.286450 datalad-1.0.0/benchmarks/scripts/
+-rwxr-xr-x   0 runner    (1001) docker     (127)      223 2024-04-06 16:45:49.000000 datalad-1.0.0/benchmarks/scripts/heavyout
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.286450 datalad-1.0.0/benchmarks/support/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-06 16:45:49.000000 datalad-1.0.0/benchmarks/support/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1715 2024-04-06 16:45:49.000000 datalad-1.0.0/benchmarks/support/path.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4697 2024-04-06 16:45:49.000000 datalad-1.0.0/benchmarks/usecases.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.386451 datalad-1.0.0/datalad/
+-rw-r--r--   0 runner    (1001) docker     (127)     5607 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3824 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/__main__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      502 2024-04-06 16:46:28.386451 datalad-1.0.0/datalad/_version.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2895 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/api.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.294450 datalad-1.0.0/datalad/cli/
+-rw-r--r--   0 runner    (1001) docker     (127)      271 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/cli/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6527 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/cli/common_args.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5174 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/cli/exec.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10321 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/cli/helpers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4309 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/cli/interface.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8261 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/cli/main.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24331 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/cli/parser.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3164 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/cli/renderer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.294450 datalad-1.0.0/datalad/cli/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/cli/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5636 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/cli/tests/test_exec.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3114 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/cli/tests/test_formatters.py
+-rw-r--r--   0 runner    (1001) docker     (127)      971 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/cli/tests/test_helpers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2396 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/cli/tests/test_interface.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15683 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/cli/tests/test_main.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2591 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/cli/tests/test_parser.py
+-rw-r--r--   0 runner    (1001) docker     (127)      387 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/cli/tests/test_renderer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1199 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/cli/tests/test_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)      978 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/cli/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22299 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/cmd.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.294450 datalad-1.0.0/datalad/cmdline/
+-rw-r--r--   0 runner    (1001) docker     (127)      623 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/cmdline/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2121 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/cmdline/helpers.py
+-rw-r--r--   0 runner    (1001) docker     (127)      748 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/cmdline/main.py
+-rw-r--r--   0 runner    (1001) docker     (127)    44706 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/config.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10092 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/conftest.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2651 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/consts.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.294450 datalad-1.0.0/datalad/core/
+-rw-r--r--   0 runner    (1001) docker     (127)      805 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/core/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.294450 datalad-1.0.0/datalad/core/distributed/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/core/distributed/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    36141 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/core/distributed/clone.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5404 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/core/distributed/clone_ephemeral.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1840 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/core/distributed/clone_ria.py
+-rw-r--r--   0 runner    (1001) docker     (127)    36247 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/core/distributed/clone_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    36859 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/core/distributed/push.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.294450 datalad-1.0.0/datalad/core/distributed/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/core/distributed/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    66865 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/core/distributed/tests/test_clone.py
+-rw-r--r--   0 runner    (1001) docker     (127)    40767 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/core/distributed/tests/test_push.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.298451 datalad-1.0.0/datalad/core/local/
+-rw-r--r--   0 runner    (1001) docker     (127)      738 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/core/local/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22200 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/core/local/create.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16916 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/core/local/diff.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2098 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/core/local/repo.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7711 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/core/local/resulthooks.py
+-rw-r--r--   0 runner    (1001) docker     (127)    42741 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/core/local/run.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17029 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/core/local/save.py
+-rw-r--r--   0 runner    (1001) docker     (127)    27762 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/core/local/status.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.298451 datalad-1.0.0/datalad/core/local/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/core/local/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18286 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/core/local/tests/test_create.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20775 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/core/local/tests/test_diff.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3731 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/core/local/tests/test_resulthooks.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2068 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/core/local/tests/test_results.py
+-rw-r--r--   0 runner    (1001) docker     (127)    28900 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/core/local/tests/test_run.py
+-rw-r--r--   0 runner    (1001) docker     (127)    40272 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/core/local/tests/test_save.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16350 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/core/local/tests/test_status.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1634 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/coreapi.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.298451 datalad-1.0.0/datalad/customremotes/
+-rw-r--r--   0 runner    (1001) docker     (127)     4056 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/customremotes/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19135 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/customremotes/archives.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5780 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/customremotes/base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4453 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/customremotes/datalad.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3074 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/customremotes/main.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1878 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/customremotes/ria_remote.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8281 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/customremotes/ria_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.302451 datalad-1.0.0/datalad/customremotes/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)      387 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/customremotes/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10912 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/customremotes/tests/test_archives.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2818 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/customremotes/tests/test_base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3403 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/customremotes/tests/test_datalad.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1467 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/customremotes/tests/test_main.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5335 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/customremotes/tests/test_ria_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.302451 datalad-1.0.0/datalad/dataset/
+-rw-r--r--   0 runner    (1001) docker     (127)      466 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/dataset/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    28903 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/dataset/gitrepo.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11777 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/dataset/repo.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.302451 datalad-1.0.0/datalad/dataset/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)      432 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/dataset/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12261 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/dataset/tests/test_gitrepo.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.302451 datalad-1.0.0/datalad/distributed/
+-rw-r--r--   0 runner    (1001) docker     (127)      635 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distributed/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24668 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distributed/create_sibling_ghlike.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7221 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distributed/create_sibling_gin.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5523 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distributed/create_sibling_gitea.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12166 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distributed/create_sibling_github.py
+-rw-r--r--   0 runner    (1001) docker     (127)    27785 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distributed/create_sibling_gitlab.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4136 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distributed/create_sibling_gogs.py
+-rw-r--r--   0 runner    (1001) docker     (127)    31196 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distributed/create_sibling_ria.py
+-rw-r--r--   0 runner    (1001) docker     (127)    31971 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distributed/drop.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9709 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distributed/export_archive_ora.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15010 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distributed/export_to_figshare.py
+-rw-r--r--   0 runner    (1001) docker     (127)    64838 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distributed/ora_remote.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.306450 datalad-1.0.0/datalad/distributed/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distributed/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3020 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distributed/tests/ria_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6139 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distributed/tests/test_create_sibling_ghlike.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1348 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distributed/tests/test_create_sibling_gin.py
+-rw-r--r--   0 runner    (1001) docker     (127)      758 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distributed/tests/test_create_sibling_gitea.py
+-rw-r--r--   0 runner    (1001) docker     (127)      758 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distributed/tests/test_create_sibling_github.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17937 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distributed/tests/test_create_sibling_gitlab.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1069 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distributed/tests/test_create_sibling_gogs.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14789 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distributed/tests/test_create_sibling_ria.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22502 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distributed/tests/test_drop.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1363 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distributed/tests/test_export_to_figshare.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5863 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distributed/tests/test_ora_http.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2695 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distributed/tests/test_ria.py
+-rw-r--r--   0 runner    (1001) docker     (127)    27819 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distributed/tests/test_ria_basics.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14542 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distributed/tests/test_ria_git_remote.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.310451 datalad-1.0.0/datalad/distribution/
+-rw-r--r--   0 runner    (1001) docker     (127)      496 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distribution/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    42187 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distribution/create_sibling.py
+-rw-r--r--   0 runner    (1001) docker     (127)      714 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distribution/create_sibling_github.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6455 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distribution/create_test_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (127)    29636 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distribution/dataset.py
+-rw-r--r--   0 runner    (1001) docker     (127)      535 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distribution/drop.py
+-rw-r--r--   0 runner    (1001) docker     (127)    41294 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distribution/get.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18719 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distribution/install.py
+-rw-r--r--   0 runner    (1001) docker     (127)      529 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distribution/remove.py
+-rw-r--r--   0 runner    (1001) docker     (127)    35111 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distribution/siblings.py
+-rw-r--r--   0 runner    (1001) docker     (127)      544 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distribution/subdatasets.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.310451 datalad-1.0.0/datalad/distribution/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)      450 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distribution/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    36685 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distribution/tests/test_create_sibling.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2758 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distribution/tests/test_create_test_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21165 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distribution/tests/test_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1219 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distribution/tests/test_dataset_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2553 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distribution/tests/test_dataset_binding.py
+-rw-r--r--   0 runner    (1001) docker     (127)      954 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distribution/tests/test_drop.py
+-rw-r--r--   0 runner    (1001) docker     (127)    30776 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distribution/tests/test_get.py
+-rw-r--r--   0 runner    (1001) docker     (127)    39348 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distribution/tests/test_install.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21275 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distribution/tests/test_siblings.py
+-rw-r--r--   0 runner    (1001) docker     (127)      681 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distribution/tests/test_uninstall.py
+-rw-r--r--   0 runner    (1001) docker     (127)    44945 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distribution/tests/test_update.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2270 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distribution/tests/test_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.310451 datalad-1.0.0/datalad/distribution/tests/vcr_cassettes/
+-rw-r--r--   0 runner    (1001) docker     (127)    38684 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distribution/tests/vcr_cassettes/github_datalad_tester_org.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)    26085 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distribution/tests/vcr_cassettes/github_yarikoptic.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     5341 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distribution/uninstall.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24518 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distribution/update.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5157 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/distribution/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9648 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/dochelpers.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.314451 datalad-1.0.0/datalad/downloaders/
+-rw-r--r--   0 runner    (1001) docker     (127)     1058 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/downloaders/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    29242 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/downloaders/base.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.314451 datalad-1.0.0/datalad/downloaders/configs/
+-rw-r--r--   0 runner    (1001) docker     (127)      210 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/downloaders/configs/crawdad.cfg
+-rw-r--r--   0 runner    (1001) docker     (127)     1805 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/downloaders/configs/crcns.cfg
+-rw-r--r--   0 runner    (1001) docker     (127)      104 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/downloaders/configs/dockerio.cfg
+-rw-r--r--   0 runner    (1001) docker     (127)      279 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/downloaders/configs/figshare.cfg
+-rw-r--r--   0 runner    (1001) docker     (127)     1880 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/downloaders/configs/hcp.cfg
+-rw-r--r--   0 runner    (1001) docker     (127)      174 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/downloaders/configs/indi.cfg
+-rw-r--r--   0 runner    (1001) docker     (127)      595 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/downloaders/configs/kaggle.cfg
+-rw-r--r--   0 runner    (1001) docker     (127)      268 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/downloaders/configs/loris.cfg
+-rw-r--r--   0 runner    (1001) docker     (127)      883 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/downloaders/configs/nda.cfg
+-rw-r--r--   0 runner    (1001) docker     (127)     1636 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/downloaders/configs/nitrc.cfg
+-rw-r--r--   0 runner    (1001) docker     (127)      421 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/downloaders/configs/nsidc.cfg
+-rw-r--r--   0 runner    (1001) docker     (127)      351 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/downloaders/configs/openfmri.cfg
+-rw-r--r--   0 runner    (1001) docker     (127)      289 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/downloaders/configs/providers.cfg
+-rw-r--r--   0 runner    (1001) docker     (127)    19843 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/downloaders/credentials.py
+-rw-r--r--   0 runner    (1001) docker     (127)    28532 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/downloaders/http.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24331 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/downloaders/providers.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16270 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/downloaders/s3.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1634 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/downloaders/shub.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.318451 datalad-1.0.0/datalad/downloaders/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/downloaders/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      452 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/downloaders/tests/test_base.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11204 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/downloaders/tests/test_credentials.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1792 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/downloaders/tests/test_docker_registry.py
+-rw-r--r--   0 runner    (1001) docker     (127)    31188 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/downloaders/tests/test_http.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9287 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/downloaders/tests/test_providers.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12649 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/downloaders/tests/test_s3.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1988 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/downloaders/tests/test_shub.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1239 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/downloaders/tests/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.318451 datalad-1.0.0/datalad/interface/
+-rw-r--r--   0 runner    (1001) docker     (127)     3774 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/interface/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      819 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/interface/add_archive_content.py
+-rw-r--r--   0 runner    (1001) docker     (127)    34196 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/interface/base.py
+-rw-r--r--   0 runner    (1001) docker     (127)      753 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/interface/clean.py
+-rw-r--r--   0 runner    (1001) docker     (127)    30528 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/interface/common_cfg.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15858 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/interface/common_opts.py
+-rw-r--r--   0 runner    (1001) docker     (127)      786 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/interface/download_url.py
+-rw-r--r--   0 runner    (1001) docker     (127)      749 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/interface/rerun.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16776 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/interface/results.py
+-rw-r--r--   0 runner    (1001) docker     (127)      780 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/interface/run_procedure.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2719 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/interface/shell_completion.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.318451 datalad-1.0.0/datalad/interface/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)      448 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/interface/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2496 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/interface/tests/test_base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3884 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/interface/tests/test_docs.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1618 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/interface/tests/test_results.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1604 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/interface/tests/test_shell_completion.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19645 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/interface/tests/test_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)      758 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/interface/unlock.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17915 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/interface/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.322450 datalad-1.0.0/datalad/local/
+-rw-r--r--   0 runner    (1001) docker     (127)      613 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    30822 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/add_archive_content.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9681 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/add_readme.py
+-rw-r--r--   0 runner    (1001) docker     (127)    60394 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/addurls.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7404 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/check_dates.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9007 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/clean.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15074 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (127)    30262 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/copy_file.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12610 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/download_url.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7142 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/export_archive.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18739 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/foreach_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5371 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/gitcredential.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8307 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/gitcredential_datalad.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5469 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/no_annex.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11056 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/remove.py
+-rw-r--r--   0 runner    (1001) docker     (127)    27294 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/rerun.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21317 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/run_procedure.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17769 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/subdatasets.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.326451 datalad-1.0.0/datalad/local/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    25061 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/tests/test_add_archive_content.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2290 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/tests/test_add_readme.py
+-rw-r--r--   0 runner    (1001) docker     (127)    37452 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/tests/test_addurls.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3271 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/tests/test_check_dates.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4391 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/tests/test_clean.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4301 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/tests/test_configuration.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14233 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/tests/test_copy_file.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8929 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/tests/test_download_url.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4104 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/tests/test_export_archive.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5181 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/tests/test_foreach_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7218 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/tests/test_gitcredential.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2270 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/tests/test_no_annex.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10057 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/tests/test_remove.py
+-rw-r--r--   0 runner    (1001) docker     (127)    34225 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/tests/test_rerun.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22961 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/tests/test_rerun_merges.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15920 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/tests/test_run_procedure.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12436 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/tests/test_subdataset.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8135 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/tests/test_unlock.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6293 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/tests/test_wtf.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7557 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/unlock.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21259 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/local/wtf.py
+-rw-r--r--   0 runner    (1001) docker     (127)    25582 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/log.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.330451 datalad-1.0.0/datalad/plugin/
+-rw-r--r--   0 runner    (1001) docker     (127)      432 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/plugin/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      269 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/plugin/add_readme.py
+-rw-r--r--   0 runner    (1001) docker     (127)      260 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/plugin/addurls.py
+-rw-r--r--   0 runner    (1001) docker     (127)      272 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/plugin/check_dates.py
+-rw-r--r--   0 runner    (1001) docker     (127)      281 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/plugin/export_archive.py
+-rw-r--r--   0 runner    (1001) docker     (127)      305 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/plugin/export_to_figshare.py
+-rw-r--r--   0 runner    (1001) docker     (127)      263 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/plugin/no_annex.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.330451 datalad-1.0.0/datalad/plugin/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)      444 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/plugin/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      248 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/plugin/wtf.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.278450 datalad-1.0.0/datalad/resources/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.330451 datalad-1.0.0/datalad/resources/procedures/
+-rw-r--r--   0 runner    (1001) docker     (127)     1515 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/resources/procedures/cfg_noannex.py
+-rw-r--r--   0 runner    (1001) docker     (127)      738 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/resources/procedures/cfg_text2git.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2257 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/resources/procedures/cfg_yoda.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.330451 datalad-1.0.0/datalad/resources/procedures/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/resources/procedures/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1892 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/resources/procedures/tests/test_noannex.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.330451 datalad-1.0.0/datalad/runner/
+-rw-r--r--   0 runner    (1001) docker     (127)      733 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/runner/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1707 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/runner/coreprotocols.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3510 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/runner/exception.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12377 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/runner/gitrunner.py
+-rw-r--r--   0 runner    (1001) docker     (127)    30114 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/runner/nonasyncrunner.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8080 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/runner/protocol.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9995 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/runner/runner.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7567 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/runner/runnerthreads.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.334451 datalad-1.0.0/datalad/runner/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/runner/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3316 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/runner/tests/test_exception.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4276 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/runner/tests/test_generatormixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1767 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/runner/tests/test_gitrunner.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24452 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/runner/tests/test_nonasyncrunner.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3240 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/runner/tests/test_threadsafety.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6546 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/runner/tests/test_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14360 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/runner/tests/test_witless_runner.py
+-rw-r--r--   0 runner    (1001) docker     (127)      340 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/runner/tests/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5676 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/runner/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.338451 datalad-1.0.0/datalad/support/
+-rw-r--r--   0 runner    (1001) docker     (127)      500 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3639 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/annex_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)   159444 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/annexrepo.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2582 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/ansi_colors.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3020 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/archive_utils_7z.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7547 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/archive_utils_patool.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16198 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/archives.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1142 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/cache.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2037 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/collections.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14584 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/constraints.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4225 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/cookies.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2406 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/digests.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2058 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/due.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4815 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/due_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3306 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/entrypoints.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18826 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/exceptions.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4755 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/extensions.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14985 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/external_versions.py
+-rw-r--r--   0 runner    (1001) docker     (127)   156510 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/gitrepo.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9034 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/globbedpaths.py
+-rw-r--r--   0 runner    (1001) docker     (127)      650 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/itertools.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6224 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/json_py.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4762 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/keyring_.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6499 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/locking.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2311 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/nda_.py
+-rw-r--r--   0 runner    (1001) docker     (127)    40503 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/network.py
+-rw-r--r--   0 runner    (1001) docker     (127)    25153 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/parallel.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5304 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/param.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10699 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/path.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10635 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/repodates.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19803 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/s3.py
+-rw-r--r--   0 runner    (1001) docker     (127)    31564 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/sshconnector.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4451 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/sshrun.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6188 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/stats.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2379 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/status.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2294 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/strings.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.342451 datalad-1.0.0/datalad/support/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)      467 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    99030 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/tests/test_annexrepo.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4151 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/tests/test_ansi_colors.py
+-rw-r--r--   0 runner    (1001) docker     (127)      668 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/tests/test_cache.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4238 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/tests/test_captured_exception.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1975 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/tests/test_cookies.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2029 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/tests/test_digests.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2540 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/tests/test_due_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1761 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/tests/test_extensions.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10293 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/tests/test_external_versions.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15124 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/tests/test_fileinfo.py
+-rw-r--r--   0 runner    (1001) docker     (127)    55678 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/tests/test_gitrepo.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8142 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/tests/test_globbedpaths.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2521 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/tests/test_json_py.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7001 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/tests/test_locking.py
+-rw-r--r--   0 runner    (1001) docker     (127)    26036 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/tests/test_network.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11713 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/tests/test_parallel.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4877 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/tests/test_path.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6349 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/tests/test_repo_save.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3875 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/tests/test_repodates.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11667 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/tests/test_sshconnector.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3743 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/tests/test_sshrun.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3145 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/tests/test_stats.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1497 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/tests/test_status.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1068 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/tests/test_vcr_.py
+-rw-r--r--   0 runner    (1001) docker     (127)      445 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/tests/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.342451 datalad-1.0.0/datalad/support/third/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/third/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1457 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/third/loris_token_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3818 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/third/nda_aws_token_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4069 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/support/vcr_.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.346451 datalad-1.0.0/datalad/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)      809 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/tests/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.350451 datalad-1.0.0/datalad/tests/ca/
+-rw-r--r--   0 runner    (1001) docker     (127)     1322 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/tests/ca/README
+-rw-r--r--   0 runner    (1001) docker     (127)     1766 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/tests/ca/ca-key.pem
+-rw-r--r--   0 runner    (1001) docker     (127)     1151 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/tests/ca/ca-root.pem
+-rw-r--r--   0 runner    (1001) docker     (127)       41 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/tests/ca/ca-root.srl
+-rw-r--r--   0 runner    (1001) docker     (127)     2180 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/tests/ca/ca_bundle.pem
+-rw-r--r--   0 runner    (1001) docker     (127)     1679 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/tests/ca/certificate-key.pem
+-rw-r--r--   0 runner    (1001) docker     (127)     1029 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/tests/ca/certificate-pub.pem
+-rw-r--r--   0 runner    (1001) docker     (127)      907 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/tests/ca/certificate.csr
+-rw-r--r--   0 runner    (1001) docker     (127)      252 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/tests/heavyoutput.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1098 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/tests/test__main__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3365 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/tests/test_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9532 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/tests/test_archives.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3308 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/tests/test_base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7257 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/tests/test_cmd.py
+-rw-r--r--   0 runner    (1001) docker     (127)    31290 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/tests/test_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7374 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/tests/test_constraints.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3466 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/tests/test_direct_mode.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4004 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/tests/test_dochelpers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1625 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/tests/test_installed.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4162 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/tests/test_interface.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8223 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/tests/test_log.py
+-rw-r--r--   0 runner    (1001) docker     (127)      997 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/tests/test_misc.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4912 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/tests/test_s3.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1261 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/tests/test_strings.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21530 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/tests/test_tests_utils_pytest.py
+-rw-r--r--   0 runner    (1001) docker     (127)    46028 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/tests/test_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12336 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/tests/test_utils_cached_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1950 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/tests/test_utils_testrepos.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3142 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/tests/test_version.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9968 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/tests/utils_cached_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (127)    68204 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/tests/utils_pytest.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1512 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/tests/utils_testdatasets.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7262 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/tests/utils_testrepos.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1070 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/typing.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.350451 datalad-1.0.0/datalad/ui/
+-rw-r--r--   0 runner    (1001) docker     (127)     3671 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/ui/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1386 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/ui/base.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14079 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/ui/dialog.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12376 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/ui/progressbars.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.350451 datalad-1.0.0/datalad/ui/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)      440 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/ui/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2825 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/ui/tests/test_base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7484 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/ui/tests/test_dialog.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2373 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/ui/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    96337 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)      838 2024-04-06 16:45:49.000000 datalad-1.0.0/datalad/version.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.366451 datalad-1.0.0/datalad.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (127)    37754 2024-04-06 16:46:28.000000 datalad-1.0.0/datalad.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)    17010 2024-04-06 16:46:28.000000 datalad-1.0.0/datalad.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (127)        1 2024-04-06 16:46:28.000000 datalad-1.0.0/datalad.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (127)      388 2024-04-06 16:46:28.000000 datalad-1.0.0/datalad.egg-info/entry_points.txt
+-rw-r--r--   0 runner    (1001) docker     (127)     2733 2024-04-06 16:46:28.000000 datalad-1.0.0/datalad.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (127)        8 2024-04-06 16:46:28.000000 datalad-1.0.0/datalad.egg-info/top_level.txt
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.350451 datalad-1.0.0/docs/
+-rw-r--r--   0 runner    (1001) docker     (127)     7048 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/Makefile
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.350451 datalad-1.0.0/docs/casts/
+-rw-r--r--   0 runner    (1001) docker     (127)     2622 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/casts/basic_search.sh
+-rw-r--r--   0 runner    (1001) docker     (127)     3262 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/casts/boxcom.sh
+-rw-r--r--   0 runner    (1001) docker     (127)      580 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/casts/cmdline_basic_usage.sh
+-rw-r--r--   0 runner    (1001) docker     (127)     4283 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/casts/datalad_convenience_vs_git.sh
+-rw-r--r--   0 runner    (1001) docker     (127)     5460 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/casts/heudiconv_dicom_to_bids.sh
+-rw-r--r--   0 runner    (1001) docker     (127)     3282 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/casts/publish_on_github.sh
+-rw-r--r--   0 runner    (1001) docker     (127)     4927 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/casts/reproducible_analysis.sh
+-rw-r--r--   0 runner    (1001) docker     (127)     1669 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/casts/seamless_nested_repos.sh
+-rw-r--r--   0 runner    (1001) docker     (127)     3911 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/casts/simple_provenance_tracking.sh
+-rw-r--r--   0 runner    (1001) docker     (127)     1743 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/casts/track_data_from_webpage.sh
+-rw-r--r--   0 runner    (1001) docker     (127)     2183 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/config-format.rst
+-rw-r--r--   0 runner    (1001) docker     (127)    21354 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/design.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.354451 datalad-1.0.0/docs/source/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.354451 datalad-1.0.0/docs/source/_extras/
+-rw-r--r--   0 runner    (1001) docker     (127)       24 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/_extras/readme.txt
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.354451 datalad-1.0.0/docs/source/_static/
+-rw-r--r--   0 runner    (1001) docker     (127)      958 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/_static/datalad_logo.png
+-rw-r--r--   0 runner    (1001) docker     (127)    15086 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/_static/favicon.ico
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.278450 datalad-1.0.0/docs/source/_templates/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.354451 datalad-1.0.0/docs/source/_templates/autosummary/
+-rw-r--r--   0 runner    (1001) docker     (127)      436 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/_templates/autosummary/module.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     1114 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/acknowledgements.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     5942 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/background.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     9836 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/basics.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     3788 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/basics_cmdline.rst.in
+-rw-r--r--   0 runner    (1001) docker     (127)     5957 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/basics_nesteddatasets.rst.in
+-rw-r--r--   0 runner    (1001) docker     (127)   348581 2024-04-06 16:46:21.000000 datalad-1.0.0/docs/source/changelog.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     5435 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/cmdline.rst
+-rw-r--r--   0 runner    (1001) docker     (127)    12514 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/conf.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2128 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/config.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     3897 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/credentials.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     2610 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/customization.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.358451 datalad-1.0.0/docs/source/design/
+-rw-r--r--   0 runner    (1001) docker     (127)     2330 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/design/application_vs_library_mode.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     5234 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/design/batched_command.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     5342 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/design/cli.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     7104 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/design/credentials.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     2604 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/design/dataset_argument.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     3030 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/design/docstrings.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     5391 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/design/drop.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     3851 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/design/exception_handling.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     4247 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/design/file_url_handling.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     2644 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/design/github_actions.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      634 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/design/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      972 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/design/log_levels.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     3574 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/design/miscpatterns.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     2377 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/design/pos_vs_kw_parameters.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     7749 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/design/progress_reporting.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     5860 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/design/provenance_capture.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     2118 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/design/python_imports.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     7793 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/design/result_records.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     1799 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/design/standard_parameters.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     5756 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/design/testing.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     7496 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/design/threaded_runner.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     3481 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/design/url_substitution.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     5894 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/design/user_messaging.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     2457 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/glossary.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     2216 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     2652 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/modref.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     1774 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/publications.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     6545 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/source/related.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.358451 datalad-1.0.0/docs/utils/
+-rw-r--r--   0 runner    (1001) docker     (127)     6164 2024-04-06 16:45:49.000000 datalad-1.0.0/docs/utils/pygments_ansi_color.py
+-rw-r--r--   0 runner    (1001) docker     (127)      221 2024-04-06 16:45:49.000000 datalad-1.0.0/pyproject.toml
+-rw-r--r--   0 runner    (1001) docker     (127)      223 2024-04-06 16:45:49.000000 datalad-1.0.0/requirements-devel.txt
+-rw-r--r--   0 runner    (1001) docker     (127)      367 2024-04-06 16:45:49.000000 datalad-1.0.0/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (127)      259 2024-04-06 16:46:28.386451 datalad-1.0.0/setup.cfg
+-rwxr-xr-x   0 runner    (1001) docker     (127)     6122 2024-04-06 16:45:49.000000 datalad-1.0.0/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.362451 datalad-1.0.0/tools/
+-rw-r--r--   0 runner    (1001) docker     (127)      924 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/Dockerfile.fullmaster
+-rw-r--r--   0 runner    (1001) docker     (127)     1249 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/Singularity
+-rwxr-xr-x   0 runner    (1001) docker     (127)     1211 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/adhoc-httpd
+-rwxr-xr-x   0 runner    (1001) docker     (127)      932 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/bisect-git-annex
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.362451 datalad-1.0.0/tools/bisect-git-annex.scripts/
+-rwxr-xr-x   0 runner    (1001) docker     (127)      241 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/bisect-git-annex.scripts/bisect-git-annex-doublepasswd.sh
+-rwxr-xr-x   0 runner    (1001) docker     (127)      197 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/bisect-git-annex.scripts/bisect-git-annex-lock.sh
+-rwxr-xr-x   0 runner    (1001) docker     (127)      884 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/changelog-todo
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2177 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/checkpwd.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.366451 datalad-1.0.0/tools/ci/
+-rw-r--r--   0 runner    (1001) docker     (127)      448 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/ci/appveyor_enable_windevmode.ps1
+-rw-r--r--   0 runner    (1001) docker     (127)      101 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/ci/appveyor_env_setup.bat
+-rw-r--r--   0 runner    (1001) docker     (127)      336 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/ci/appveyor_install_git-annex.bat
+-rw-r--r--   0 runner    (1001) docker     (127)      735 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/ci/appveyor_ssh2localhost.bat
+-rwxr-xr-x   0 runner    (1001) docker     (127)      120 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/ci/appveyor_ssh2localhost.sh
+-rw-r--r--   0 runner    (1001) docker     (127)      203 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/ci/appveyor_ssh_config
+-rwxr-xr-x   0 runner    (1001) docker     (127)      826 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/ci/benchmark-travis-pr.sh
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2943 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/ci/bisect-python.sh
+-rwxr-xr-x   0 runner    (1001) docker     (127)      300 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/ci/debians_disable_outdated_ssl_cert
+-rwxr-xr-x   0 runner    (1001) docker     (127)      358 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/ci/deploy_datalad-rootca
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2481 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/ci/download-latest-artifact
+-rw-r--r--   0 runner    (1001) docker     (127)      589 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/ci/gpg-90F7E9EB.pub
+-rwxr-xr-x   0 runner    (1001) docker     (127)     8846 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/ci/install-annex.sh
+-rwxr-xr-x   0 runner    (1001) docker     (127)      379 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/ci/install-minimum-git.sh
+-rwxr-xr-x   0 runner    (1001) docker     (127)      531 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/ci/install-singularity.sh
+-rwxr-xr-x   0 runner    (1001) docker     (127)      220 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/ci/install-upstream-git.sh
+-rwxr-xr-x   0 runner    (1001) docker     (127)      715 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/ci/prep-travis-devel-annex.sh
+-rwxr-xr-x   0 runner    (1001) docker     (127)     1048 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/ci/prep-travis-forssh.sh
+-rw-r--r--   0 runner    (1001) docker     (127)       95 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/cmdline-completion
+-rwxr-xr-x   0 runner    (1001) docker     (127)      880 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/convert-git-annex-layout
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2352 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/copy_urls_from_datalad.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.366451 datalad-1.0.0/tools/coverage-bin/
+-rwxr-xr-x   0 runner    (1001) docker     (127)      858 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/coverage-bin/datalad
+-rwxr-xr-x   0 runner    (1001) docker     (127)      858 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/coverage-bin/git-annex-remote-datalad
+-rwxr-xr-x   0 runner    (1001) docker     (127)      858 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/coverage-bin/git-annex-remote-datalad-archives
+-rwxr-xr-x   0 runner    (1001) docker     (127)      858 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/coverage-bin/git-annex-remote-ora
+-rwxr-xr-x   0 runner    (1001) docker     (127)       65 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/coverage-bin/sitecustomize.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     1007 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/describegitannex
+-rwxr-xr-x   0 runner    (1001) docker     (127)     1791 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/downgrade-annex
+-rwxr-xr-x   0 runner    (1001) docker     (127)     1536 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/dtime
+-rwxr-xr-x   0 runner    (1001) docker     (127)      943 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/eval_under_nfs
+-rwxr-xr-x   0 runner    (1001) docker     (127)      801 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/eval_under_testloopfs
+-rwxr-xr-x   0 runner    (1001) docker     (127)      876 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/find-hanged-tests
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2127 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/git-web-submodules.sh
+-rwxr-xr-x   0 runner    (1001) docker     (127)      447 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/link_issues_CHANGELOG
+-rwxr-xr-x   0 runner    (1001) docker     (127)     3010 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/mimic_merges
+-rwxr-xr-x   0 runner    (1001) docker     (127)     4567 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/mimic_repo
+-rwxr-xr-x   0 runner    (1001) docker     (127)      419 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/mkcontrib
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2802 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/monitor-interrupts.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2298 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/profile_python
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.366451 datalad-1.0.0/tools/testing/
+-rw-r--r--   0 runner    (1001) docker     (127)       33 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/testing/.gitignore
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.282451 datalad-1.0.0/tools/testing/bad_internals/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.366451 datalad-1.0.0/tools/testing/bad_internals/_scrapy/
+-rw-r--r--   0 runner    (1001) docker     (127)       57 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/testing/bad_internals/_scrapy/scrapy.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.366451 datalad-1.0.0/tools/testing/conf/
+-rw-r--r--   0 runner    (1001) docker     (127)      179 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/testing/conf/apache-ssh-supervisor.conf
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.282451 datalad-1.0.0/tools/testing/conf/etc/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.282451 datalad-1.0.0/tools/testing/conf/etc/apt/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-06 16:46:28.366451 datalad-1.0.0/tools/testing/conf/etc/apt/apt.conf.d/
+-rw-r--r--   0 runner    (1001) docker     (127)      112 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/testing/conf/etc/apt/apt.conf.d/99apt-cacher
+-rwxr-xr-x   0 runner    (1001) docker     (127)     3461 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/testing/make_test_repo
+-rwxr-xr-x   0 runner    (1001) docker     (127)      938 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/testing/make_test_repos
+-rwxr-xr-x   0 runner    (1001) docker     (127)     4134 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/testing/start_website_in_docker
+-rw-r--r--   0 runner    (1001) docker     (127)     2636 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/testing/start_website_in_docker-Dockerfile.in
+-rwxr-xr-x   0 runner    (1001) docker     (127)     1932 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/testing/test_README_in_docker
+-rw-r--r--   0 runner    (1001) docker     (127)     1288 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/testing/test_README_in_docker-Dockerfile.in
+-rwxr-xr-x   0 runner    (1001) docker     (127)      481 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/testing/travis_ifdown_nonlo.sh
+-rwxr-xr-x   0 runner    (1001) docker     (127)      593 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/upgrade-annex-osx.sh
+-rwxr-xr-x   0 runner    (1001) docker     (127)     3570 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/urlinfo
+-rw-r--r--   0 runner    (1001) docker     (127)      813 2024-04-06 16:45:49.000000 datalad-1.0.0/tools/which.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3510 2024-04-06 16:45:49.000000 datalad-1.0.0/tox.ini
+-rw-r--r--   0 runner    (1001) docker     (127)    81087 2024-04-06 16:45:49.000000 datalad-1.0.0/versioneer.py
```

### filetype from file(1)

```diff
@@ -1 +1 @@
-POSIX tar archive (GNU)
+POSIX tar archive
```

### Comparing `datalad-0.9.3/setup_support.py` & `datalad-1.0.0/datalad/distributed/export_archive_ora.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,282 +1,275 @@
+# emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
-#   See COPYING file distributed along with the DataLad package for the
+#   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
+"""Export an archive of a local annex object store, suitable for an ORA remote"""
 
-
-import os
-import platform
-import sys
-from genericpath import exists
-from os import linesep, makedirs
-from os.path import dirname, join as opj
-
-from distutils.core import Command
-from distutils.errors import DistutilsOptionError
-import datetime
-import formatters as fmt
+__docformat__ = 'restructuredtext'
 
 
-def _path_rel2file(p):
-    return opj(dirname(__file__), p)
+import logging
+import os
+import os.path as op
+import shutil
+import subprocess
+from argparse import REMAINDER
+
+from datalad.utils import (
+    ensure_list,
+    rmtree,
+)
+from datalad.interface.base import (
+    Interface,
+    build_doc,
+    eval_results,
+)
+from datalad.interface.results import (
+    get_status_dict,
+)
+from datalad.support.param import Parameter
+from datalad.support.constraints import (
+    EnsureChoice,
+    EnsureNone,
+    EnsureStr,
+)
+from datalad.support.exceptions import CapturedException
+from datalad.distribution.dataset import (
+    EnsureDataset,
+    datasetmethod,
+    require_dataset,
+    resolve_path,
+)
+from datalad.log import log_progress
+
+lgr = logging.getLogger('datalad.customremotes.export_archive_ora')
+
+
+@build_doc
+class ExportArchiveORA(Interface):
+    """Export an archive of a local annex object store for the ORA remote.
+
+    Keys in the local annex object store are reorganized in a temporary
+    directory (using links to avoid storage duplication) to use the
+    'hashdirlower' setup used by git-annex for bare repositories and
+    the directory-type special remote. This alternative object store is
+    then moved into a 7zip archive that is suitable for use in a
+    ORA remote dataset store. Placing such an archive into::
 
+      <dataset location>/archives/archive.7z
 
-def get_version():
-    """Load version of datalad from version.py without entailing any imports
+    Enables the ORA special remote to locate and retrieve all keys contained
+    in the archive.
     """
-    # This might entail lots of imports which might not yet be available
-    # so let's do ad-hoc parsing of the version.py
-    with open(opj(dirname(__file__), 'datalad', 'version.py')) as f:
-        version_lines = list(filter(lambda x: x.startswith('__version__'), f))
-    assert (len(version_lines) == 1)
-    return version_lines[0].split('=')[1].strip(" '\"\t\n")
-
-
-class BuildManPage(Command):
-    # The BuildManPage code was originally distributed
-    # under the same License of Python
-    # Copyright (c) 2014 Oz Nahum Tiram  <nahumoz@gmail.com>
-
-    description = 'Generate man page from an ArgumentParser instance.'
-
-    user_options = [
-        ('manpath=', None, 'output path for manpages'),
-        ('rstpath=', None, 'output path for RST files'),
-        ('parser=', None, 'module path to an ArgumentParser instance'
-         '(e.g. mymod:func, where func is a method or function which return'
-         'a dict with one or more arparse.ArgumentParser instances.'),
-    ]
-
-    def initialize_options(self):
-        self.manpath = opj('build', 'man')
-        self.rstpath = opj('docs', 'source', 'generated', 'man')
-        self.parser = 'datalad.cmdline.main:setup_parser'
-
-    def finalize_options(self):
-        if self.manpath is None:
-            raise DistutilsOptionError('\'manpath\' option is required')
-        if self.rstpath is None:
-            raise DistutilsOptionError('\'rstpath\' option is required')
-        if self.parser is None:
-            raise DistutilsOptionError('\'parser\' option is required')
-        self.manpath = _path_rel2file(self.manpath)
-        self.rstpath = _path_rel2file(self.rstpath)
-        mod_name, func_name = self.parser.split(':')
-        fromlist = mod_name.split('.')
+    _params_ = dict(
+        dataset=Parameter(
+            args=("-d", "--dataset"),
+            doc="""specify the dataset to process.  If
+            no dataset is given, an attempt is made to identify the dataset
+            based on the current working directory""",
+            constraints=EnsureDataset() | EnsureNone()),
+        target=Parameter(
+            args=("target",),
+            metavar="TARGET",
+            doc="""if an existing directory, an 'archive.7z' is placed into
+            it, otherwise this is the path to the target archive""",
+            constraints=EnsureStr() | EnsureNone()),
+        remote=Parameter(
+            args=("--for",),
+            dest="remote",
+            metavar='LABEL',
+            doc="""name of the target sibling, wanted/preferred settings
+            will be used to filter the files added to the archives""",
+            constraints=EnsureStr() | EnsureNone()),
+        annex_wanted=Parameter(
+            args=("--annex-wanted",),
+            metavar="FILTERS",
+            doc="""git-annex-preferred-content expression for
+            git-annex find to filter files. Should start with
+            'or' or 'and' when used in combination with `--for`"""),
+        froms=Parameter(
+            args=("--from",),
+            dest="froms",
+            metavar="FROM",
+            nargs="+",
+            doc="""one or multiple tree-ish from which to select files"""),
+        opts=Parameter(
+            args=("opts",),
+            nargs=REMAINDER,
+            metavar="...",
+            doc="""list of options for 7z to replace the default '-mx0' to
+            generate an uncompressed archive"""),
+        missing_content=Parameter(
+            args=("--missing-content",),
+            doc="""By default, any discovered file with missing content will
+            result in an error and the export is aborted. Setting this to
+            'continue' will issue warnings instead of failing on error. The
+            value 'ignore' will only inform about problem at the 'debug' log
+            level. The latter two can be helpful when generating a TAR archive
+            from a dataset where some file content is not available
+            locally.""",
+            constraints=EnsureChoice("error", "continue", "ignore")),
+    )
+
+    @staticmethod
+    @datasetmethod(name='export_archive_ora')
+    @eval_results
+    def __call__(
+            target,
+            opts=None,
+            *,  # opts is positional but optional in CLI
+            dataset=None,
+            remote=None,
+            annex_wanted=None,
+            froms=None,
+            missing_content='error',):
+        # only non-bare repos have hashdirmixed, so require one
+        ds = require_dataset(
+            dataset, check_installed=True, purpose='export to ORA archive')
+        ds_repo = ds.repo
+
+        annex_objs = ds_repo.dot_git / 'annex' / 'objects'
+
+        archive = resolve_path(target, dataset)
+        if archive.is_dir():
+            archive = archive / 'archive.7z'
+        else:
+            archive.parent.mkdir(exist_ok=True, parents=True)
+
+        froms = ensure_list(froms)
+
+        if not opts:
+            # uncompressed by default
+            opts = ['-mx0']
+
+        res_kwargs = dict(
+            action="export-archive-ora",
+            logger=lgr,
+        )
+
+        if not annex_objs.is_dir():
+            yield get_status_dict(
+                ds=ds,
+                status='notneeded',
+                message='no annex keys present',
+                **res_kwargs,
+            )
+            return
+
+        exportdir = ds_repo.dot_git / 'datalad' / 'tmp' / 'ora_archive'
+        if exportdir.exists():
+            yield get_status_dict(
+                ds=ds,
+                status='error',
+                message=(
+                    'export directory already exists, please remove first: %s',
+                    str(exportdir)),
+                **res_kwargs,
+            )
+            return
+
+        def expr_to_opts(expr):
+            opts = []
+            expr = expr.replace('(', ' ( ').replace(')', ' ) ')
+            for sub_expr in expr.split(' '):
+                if len(sub_expr):
+                    if sub_expr in '()':
+                        opts.append(f"-{sub_expr}")
+                    else:
+                        opts.append(f"--{sub_expr}")
+            return opts
+
+        find_filters = []
+        if remote:
+            find_filters = ['-('] + expr_to_opts(ds_repo.get_preferred_content('wanted', remote)) + ['-)']
+        if annex_wanted:
+            find_filters.extend(expr_to_opts(annex_wanted))
+        # git-annex find results need to be uniqued with set, as git-annex find
+        # will return duplicates if multiple symlinks point to the same key.
+        #
+        # TODO: use --json which was already added, checked with 10.20230407+git131-gb90c2156a6
+        if froms:
+            keypaths = set([
+                annex_objs.joinpath(k) for treeish in froms for k in ds_repo.call_annex_items_([
+                'find', *find_filters, f"--branch={treeish}",
+                "--format=${hashdirmixed}${key}/${key}\\n"])
+                ])
+        else:
+            keypaths = set(annex_objs.joinpath(k) for k in ds_repo.call_annex_items_([
+                'find', *find_filters,
+                "--format=${hashdirmixed}${key}/${key}\\n"
+            ]))
+
+        log_progress(
+            lgr.info,
+            'oraarchiveexport',
+            'Start ORA archive export %s', ds,
+            total=len(keypaths),
+            label='ORA archive export',
+            unit=' Keys',
+        )
+
+        if missing_content == 'continue':
+            missing_file_lgr_func = lgr.warning
+        elif missing_content == 'ignore':
+            missing_file_lgr_func = lgr.debug
+
+        link_fx = os.link
+        for keypath in keypaths:
+            key = keypath.name
+            hashdir = op.join(keypath.parts[-4], keypath.parts[-3])
+            log_progress(
+                lgr.info,
+                'oraarchiveexport',
+                'Export key %s to %s', key, hashdir,
+                update=1,
+                increment=True)
+            keydir = exportdir / hashdir / key
+            keydir.mkdir(parents=True, exist_ok=True)
+            try:
+                link_fx(str(keypath), str(keydir / key))
+            except FileNotFoundError as e:
+                if missing_content == 'error':
+                    raise IOError('Key %s has no content available' % keypath)
+                missing_file_lgr_func(
+                    'Key %s has no content available',
+                    str(keypath))
+            except OSError:
+                lgr.warning(
+                    'No hard links supported at %s, will copy files instead',
+                    str(keypath))
+                # no hard links supported
+                # switch function after first error
+                link_fx = shutil.copyfile
+                link_fx(str(keypath), str(keydir / key))
+
+        log_progress(
+            lgr.info,
+            'oraarchiveexport',
+            'Finished RIA archive export from %s', ds
+        )
         try:
-            mod = __import__(mod_name, fromlist=fromlist)
-            self._parser = getattr(mod, func_name)(
-                formatter_class=fmt.ManPageFormatter,
-                return_subparsers=True)
-
-        except ImportError as err:
-            raise err
-
-        self.announce('Writing man page(s) to %s' % self.manpath)
-        self._today = datetime.date.today()
-
-    def run(self):
-
-        dist = self.distribution
-        #homepage = dist.get_url()
-        #appname = self._parser.prog
-        appname = 'datalad'
-
-        sections = {
-            'Authors': """{0} is developed by {1} <{2}>.""".format(
-                appname, dist.get_author(), dist.get_author_email()),
-        }
-
-        dist = self.distribution
-        for cls, opath, ext in ((fmt.ManPageFormatter, self.manpath, '1'),
-                                (fmt.RSTManPageFormatter, self.rstpath, 'rst')):
-            if not os.path.exists(opath):
-                os.makedirs(opath)
-            for cmdname in self._parser:
-                p = self._parser[cmdname]
-                cmdname = "{0}{1}".format(
-                    'datalad-' if cmdname != 'datalad' else '',
-                    cmdname)
-                format = cls(cmdname, ext_sections=sections, version=get_version())
-                formatted = format.format_man_page(p)
-                with open(opj(opath, '{0}.{1}'.format(
-                        cmdname,
-                        ext)),
-                        'w') as f:
-                    f.write(formatted)
-
-
-class BuildRSTExamplesFromScripts(Command):
-    description = 'Generate RST variants of example shell scripts.'
-
-    user_options = [
-        ('expath=', None, 'path to look for example scripts'),
-        ('rstpath=', None, 'output path for RST files'),
-    ]
-
-    def initialize_options(self):
-        self.expath = opj('docs', 'examples')
-        self.rstpath = opj('docs', 'source', 'generated', 'examples')
-
-    def finalize_options(self):
-        if self.expath is None:
-            raise DistutilsOptionError('\'expath\' option is required')
-        if self.rstpath is None:
-            raise DistutilsOptionError('\'rstpath\' option is required')
-        self.expath = _path_rel2file(self.expath)
-        self.rstpath = _path_rel2file(self.rstpath)
-        self.announce('Converting example scripts')
-
-    def run(self):
-        opath = self.rstpath
-        if not os.path.exists(opath):
-            os.makedirs(opath)
-
-        from glob import glob
-        for example in glob(opj(self.expath, '*.sh')):
-            exname = os.path.basename(example)[:-3]
-            with open(opj(opath, '{0}.rst'.format(exname)), 'w') as out:
-                fmt.cmdline_example_to_rst(
-                    open(example),
-                    out=out,
-                    ref='_example_{0}'.format(exname))
-
-
-class BuildConfigInfo(Command):
-    description = 'Generate RST documentation for all config items.'
-
-    user_options = [
-        ('rstpath=', None, 'output path for RST file'),
-    ]
-
-    def initialize_options(self):
-        self.rstpath = opj('docs', 'source', 'generated', 'cfginfo')
-
-    def finalize_options(self):
-        if self.rstpath is None:
-            raise DistutilsOptionError('\'rstpath\' option is required')
-        self.rstpath = _path_rel2file(self.rstpath)
-        self.announce('Generating configuration documentation')
-
-    def run(self):
-        opath = self.rstpath
-        if not os.path.exists(opath):
-            os.makedirs(opath)
-
-        from datalad.interface.common_cfg import definitions as cfgdefs
-        from datalad.dochelpers import _indent
-
-        categories = {
-            'global': {},
-            'local': {},
-            'dataset': {},
-            'misc': {}
-        }
-        for term, v in cfgdefs.items():
-            categories[v.get('destination', 'misc')][term] = v
-
-        for cat in categories:
-            with open(opj(opath, '{}.rst'.format(cat)), 'w') as rst:
-                rst.write('.. glossary::\n')
-                for term, v in sorted(categories[cat].items(), key=lambda x: x[0]):
-                    rst.write(_indent(term, '\n  '))
-                    qtype, docs = v.get('ui', (None, {}))
-                    desc_tmpl = '\n'
-                    if 'title' in docs:
-                        desc_tmpl += '{title}:\n'
-                    if 'text' in docs:
-                        desc_tmpl += '{text}\n'
-                    if 'default' in v:
-                        default = v['default']
-                        if hasattr(default, 'replace'):
-                            # protect against leaking specific home dirs
-                            v['default'] = default.replace(os.path.expanduser('~'), '~')
-                        desc_tmpl += 'Default: {default}\n'
-                    if 'type' in v:
-                        type_ = v['type']
-                        if hasattr(type_, 'long_description'):
-                            type_ = type_.long_description()
-                        else:
-                            type_ = type_.__name__
-                        desc_tmpl += '\n[{type}]\n'
-                        v['type'] = type_
-                    if desc_tmpl == '\n':
-                        # we need something to avoid joining terms
-                        desc_tmpl += 'undocumented\n'
-                    v.update(docs)
-                    rst.write(_indent(desc_tmpl.format(**v), '    '))
-
-
-def setup_entry_points(entry_points):
-    """Sneaky monkey patching could be fixed only via even sneakier monkey patching
-
-    It will never break, I promise!
-    """
-
-    def get_script_content(script_name, shebang="#!/usr/bin/env python"):
-        return linesep.join([
-            shebang,
-            "#",
-            "# Custom simplistic runner for DataLad. Assumes datalad module",
-            "# being available.  Generated by monkey patching monkey patched",
-            "# setuptools.",
-            "#",
-            "from %s import main" % entry_points[script_name],
-            "main()",
-            ""]).encode()
-
-    def patch_write_script(mod):
-        """Patches write_script of the module with our shim to provide
-        lightweight invocation script
-        """
-
-        orig_meth = getattr(mod, 'write_script')
-
-        def _provide_lean_script_contents(
-                self, script_name, contents, mode="t", *ignored):
-            # could be a script from another module -- let it be as is
-            if script_name in entry_points:
-                # keep shebang
-                contents = get_script_content(
-                    script_name,
-                    contents.splitlines()[0].decode())
-            return orig_meth(self, script_name, contents, mode=mode)
-
-        setattr(mod, 'write_script', _provide_lean_script_contents)
-
-    # We still need this one so that setuptools known about the scripts
-    # So we generate some bogus ones, and provide a list of them ;)
-    # pre-generate paths so we could give them to setuptools
-    scripts_build_dir = opj('build', 'scripts_generated')
-    scripts = [opj(scripts_build_dir, x) for x in entry_points]
-
-    if 'clean' not in sys.argv:
-        if not exists(scripts_build_dir):
-            makedirs(scripts_build_dir)
-        for s, mod in entry_points.items():
-            with open(opj(scripts_build_dir, s), 'wb') as f:
-                f.write(get_script_content(s))
-
-    platform_system = platform.system().lower()
-    setup_kwargs = {}
-
-    if platform_system == 'windows':
-        # TODO: investigate https://github.com/matthew-brett/myscripter,
-        # nibabel/nixext approach to support similar setup on Windows
-        setup_kwargs['entry_points'] = {
-            'console_scripts': ['%s=%s:main' % i for i in entry_points.items()]
-        }
-    else:
-        # Damn you sharktopus!
-        from setuptools.command.install_scripts import \
-            install_scripts as stinstall_scripts
-        from setuptools.command.easy_install import easy_install
-
-        patch_write_script(stinstall_scripts)
-        patch_write_script(easy_install)
-
-        setup_kwargs['scripts'] = scripts
-
-    return setup_kwargs
+            subprocess.run(
+                ['7z', 'u', str(archive), '.'] + opts,
+                cwd=str(exportdir),
+            )
+            yield get_status_dict(
+                path=str(archive),
+                type='file',
+                status='ok',
+                **res_kwargs)
+        except Exception as e:
+            ce = CapturedException(e)
+            yield get_status_dict(
+                path=str(archive),
+                type='file',
+                status='error',
+                message=('7z failed: %s', ce),
+                exception=ce,
+                **res_kwargs)
+            return
+        finally:
+            rmtree(str(exportdir))
```

### Comparing `datalad-0.9.3/docs/Makefile` & `datalad-1.0.0/docs/Makefile`

 * *Files 5% similar despite different names*

```diff
@@ -1,21 +1,16 @@
 # Makefile for Sphinx documentation
 #
 
 # You can set these variables from the command line.
 SPHINXOPTS    = -W
-SPHINXBUILD   = sphinx-build
+SPHINXBUILD   = DATALAD_SPHINX_RUN=1 sphinx-build
 PAPER         =
 BUILDDIR      = build
 
-# User-friendly check for sphinx-build
-ifeq ($(shell which $(SPHINXBUILD) >/dev/null 2>&1; echo $$?), 1)
-$(error The '$(SPHINXBUILD)' command was not found. Make sure you have Sphinx installed, then set the SPHINXBUILD environment variable to point to the full path of the '$(SPHINXBUILD)' executable. Alternatively you can add the directory with the executable to your PATH. If you don't have Sphinx installed, grab it from http://sphinx-doc.org/)
-endif
-
 # Internal variables.
 PAPEROPT_a4     = -D latex_paper_size=a4
 PAPEROPT_letter = -D latex_paper_size=letter
 ALLSPHINXOPTS   = -d $(BUILDDIR)/doctrees $(PAPEROPT_$(PAPER)) $(SPHINXOPTS) source
 # the i18n builder cannot share the environment and doctrees with the others
 I18NSPHINXOPTS  = $(PAPEROPT_$(PAPER)) $(SPHINXOPTS) source
 
@@ -45,15 +40,15 @@
 	@echo "  xml        to make Docutils-native XML files"
 	@echo "  pseudoxml  to make pseudoxml-XML files for display purposes"
 	@echo "  linkcheck  to check all external links for integrity"
 	@echo "  doctest    to run all doctests embedded in the documentation (if enabled)"
 	@echo "  coverage   to run coverage check of the documentation (if enabled)"
 
 clean:
-	rm -rf $(BUILDDIR)/* source/generated
+	rm -rf $(BUILDDIR)/* source/generated source/_extras/schema*.json source/build
 
 html:
 	$(SPHINXBUILD) -b html $(ALLSPHINXOPTS) $(BUILDDIR)/html
 	@echo
 	@echo "Build finished. The HTML pages are in $(BUILDDIR)/html."
 
 dirhtml:
```

### Comparing `datalad-0.9.3/docs/config-format.rst` & `datalad-1.0.0/docs/config-format.rst`

 * *Files identical despite different names*

### Comparing `datalad-0.9.3/docs/design.rst` & `datalad-1.0.0/docs/design.rst`

 * *Files 1% similar despite different names*

```diff
@@ -46,15 +46,15 @@
 a part of this provider -- only download from the web::
 
     [provider:incoming_http]
     type = web
     mode = (download|fast|relaxed)            # fast/relaxed/download
     filename = (url|request)                  # of cause also could be _e'valuated given the bs4 link get_video_filename(link, filename)
     recurse_(a|href) =                        # regexes to recurse
-    # mimicing scrapy
+    # mimicking scrapy
     start_urls = http://...                   #
     certificates =                            # if there are https -- we need to allow specifying those
     allowed_domains = example.com/buga/duga   # to limit recursion
                       sample.com
     excluded_hrefs =                          # do not even search for "download" URLs on given pages.  Should also allow to be a function/callback to decide based on request?
     include_(a|href) =                        # what to download
     exclude_(a|href) =                        # and not (even if matches)
@@ -345,15 +345,15 @@
    somehow?
 
 
 Move/Rename/Delete
 ~~~~~~~~~~~~~~~~~~
 
 Just move/rename/delete some files around e.g. for a custom view of
-the dataset (e.g. to conform openfmri layout). Key would simply be
+the dataset (e.g. to conform to OpenfMRI layout). Key would simply be
 reused ;)
 
 Q: should it be 'Within-branch' filter?
 
 
 Command
 ~~~~~~~
@@ -537,15 +537,15 @@
 -> URL(URI):  will be our first and main "target" but it could
               also be direct S3, etc.
 
 a URI should be associated with an "Hosting" (many-to-one), so we could
 e.g. provide authentication information per actual "Hosting" as the
 entity.  But now we are getting back to DataProvider, which is the
 Hosting, or actually also a part of it (since Hosting could serve
-multiple Providers, e.g. openfmri -> providers per each dataset?)
+multiple Providers, e.g. OpenfMRI -> providers per each dataset?)
 But also Provider might use/point to multiple Hostings (e.g. mirrors
 listed on nitp-2013).
 
 Hosting
 ~~~~~~~
 
 Each DataProvider would be a factory of File's.
```

### Comparing `datalad-0.9.3/docs/casts/boxcom.sh` & `datalad-1.0.0/docs/casts/boxcom.sh`

 * *Files identical despite different names*

### Comparing `datalad-0.9.3/docs/casts/seamless_nested_repos.sh` & `datalad-1.0.0/docs/casts/seamless_nested_repos.sh`

 * *Files 8% similar despite different names*

```diff
@@ -1,43 +1,43 @@
-say "DataLad provides seamless management of nested Git repositories..."
+say "DataLad makes a tree of nested Git repositories feel like a big monorepo..."
 
-say "Let's create a dataset"
+say "Let's create a root dataset"
 run "datalad create demo"
 run "cd demo"
-say "A DataLad dataset is just a Git repo with some initial configuration"
+say "Any DataLad dataset is just a Git repo with some initial configuration"
 run "git log --oneline"
 
-say "We can generate nested datasets, by telling DataLad to register a new dataset in a parent dataset"
+say "We can nest datasets, by telling DataLad to register a new dataset in a parent dataset"
 run "datalad create -d . sub1"
-say "A subdataset is nothing more than regular Git submodule"
+say "A subdataset is a regular Git submodule"
 run "git submodule"
 
-say "Of course subdatasets can be nested"
+say "Datasets can be nested arbitrarily deep"
 run "datalad create -d . sub1/justadir/sub2"
 
 say "Unlike Git, DataLad automatically takes care of committing all changes associated with the added subdataset up to the given parent dataset"
-run "git status"
+run "datalad status"
 
 say "Let's create some content in the deepest subdataset"
 run "mkdir sub1/justadir/sub2/anotherdir"
 run "touch sub1/justadir/sub2/anotherdir/afile"
 
-say "Git can only tell us that something underneath the top-most subdataset was modified"
+say "Git only reports changes within a repository, in the case the whole subdataset"
 run "git status"
 
-say "DataLad saves us from further investigation"
-run "datalad diff -r"
+say "DataLad considers the entire tree"
+run "datalad status -r"
 
 say "Like Git, it can report individual untracked files, but also across repository boundaries"
-run "datalad diff -r --report-untracked all"
+run "datalad status -r --untracked all"
 
 say "Adding this new content with Git or git-annex would be an exercise"
 run_expfail "git add sub1/justadir/sub2/anotherdir/afile"
 
-say "DataLad does not require users to determine the correct repository in the tree"
-run "datalad add -d . sub1/justadir/sub2/anotherdir/afile"
+say "Again, DataLad does not require users to determine the correct repository"
+run "datalad save -d . sub1/justadir/sub2/anotherdir/afile"
 
-say "Again, all associated changes in the entire dataset tree, up to the given parent dataset, were committed"
-run "git status"
+say "All associated changes in the entire dataset tree were committed"
+run "datalad status"
 
 say "DataLad's 'diff' is able to report the changes from these related commits throughout the repository tree"
-run "datalad diff --revision @~1 -r"
+run "datalad diff -r -f @~1"
```

### Comparing `datalad-0.9.3/docs/casts/publish_on_github.sh` & `datalad-1.0.0/docs/casts/publish_on_github.sh`

 * *Files 4% similar despite different names*

```diff
@@ -14,16 +14,16 @@
 run "datalad run dd if=/dev/urandom of=big.dat bs=1M count=600"
 
 say "Publishing this dataset in a way that allows anyone to simply install it from GitHub, AND get the big data file, requires two steps"
 say "1. Host the data file at some publicly accessible location"
 say "2. Configure DataLad to make sure that getting data from GitHub transparently requests from this other location instead"
 
 say "Here we use a personal webserver with SSH access, but, in principle, any hosting solution supported by git-annex is equally suitable"
-say "We create a remote sibling of our dataset under the name 'myserver' via SSH, and tell datalad to track it as a common data source that is available for any future installation of this dataset. Access to this location will happen via the given http:// URL"
-run "datalad create-sibling -s myserver demo.datalad.org:public_html/publish-demo --as-common-datasrc demo-server --target-url http://demo.datalad.org/publish-demo/.git"
+say "We create a remote sibling of our dataset under the name 'myserver' via SSH, and tell datalad to track it as a common data source that is available for any future installation of this dataset. Access to this location will happen via the given http:// URL, and --ui true tells to install DataLad web UI as on https://datasets.datalad.org. Note that /.git in the URL most likely to be necessary in your case."
+run "datalad create-sibling -s myserver demo.datalad.org:public_html/publish-demo --ui true --as-common-datasrc demo-server --target-url http://demo.datalad.org/publish-demo/.git"
 
 say "With this configuration in place, we can now create a repository on GitHub, and configure the remote sibling on the SSH server as a publication dependency"
 run "datalad create-sibling-github --github-organization datalad --publish-depends myserver --access-protocol ssh publish-demo"
 
 say "Let's quickly recap that the data file is just in our local dataset"
 run "git annex whereis"
```

### Comparing `datalad-0.9.3/docs/casts/basic_search.sh` & `datalad-1.0.0/docs/casts/basic_search.sh`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 full_title="Demo of basic datasets meta-data search using DataLad"
 #run "set -eu  # Fail early if any error happens"
 
 say "DataLad allows to aggregate dataset-level meta-data, i.e. data describing the dataset (description, authors, etc), from a variety of formats (see http://docs.datalad.org/en/latest/metadata.html for more info)."
 say 'In this example we will start from a point where someone who has not used datalad before decided to find datasets which related to "raiders" (after "Raiders of the Lost Ark" movie) and neuroimaging.'
-say 'As you will see below, upon the very first invocation of "datalad search" command, DataLad will need first to acquire aggregated meta-data for our collection of datasets available at http://datasets.datalad.org and for that it will install that top level super-dataset (a pure git repository) under ~/datalad:'
+say 'As you will see below, upon the very first invocation of "datalad search" command, DataLad will need first to acquire aggregated meta-data for our collection of datasets available at https://datasets.datalad.org and for that it will install that top level super-dataset (a pure git repository) under ~/datalad:'
 ( sleep 4; type yes; key Return; ) &
 run "datalad search raiders neuroimaging"
 
 say '"search" searches within current dataset (unless -d option is used), and if it is outside of any it would offer to search within the ~/datalad we have just installed'
 ( sleep 4; type yes; key Return; ) &
 run "datalad search raiders neuroimaging"
```

### Comparing `datalad-0.9.3/docs/casts/track_data_from_webpage.sh` & `datalad-1.0.0/docs/casts/track_data_from_webpage.sh`

 * *Files identical despite different names*

### Comparing `datalad-0.9.3/docs/casts/reproducible_analysis.sh` & `datalad-1.0.0/docs/casts/reproducible_analysis.sh`

 * *Files 1% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 say "It always starts with a dataset"
 run "datalad create demo"
 run "cd demo"
 
 say "For this demo we are using two public brain imaging datasets that were published on OpenFMRI.org, and are available from DataLad's datasets.datalad.org"
 run "datalad install -d . -s ///openfmri/ds000001 inputs/ds000001"
 
-say "BTW: '///' is just short for http://datasets.datalad.org"
+say "BTW: '///' is just short for https://datasets.datalad.org"
 
 run "datalad install -d . -s ///openfmri/ds000002 inputs/ds000002"
 
 say "Both datasets are now registered as subdatasets, and their precise versions are on record"
 run "datalad --output-format '{path}: {revision_descr}' subdatasets"
 
 say "However, very little data were actually downloaded (the full datasets are several gigabytes in size):"
@@ -37,15 +37,15 @@
   bet \\\$nifti \\\$subdir/anat -m
   count=\\\$((count + 1)) 
 done
 EOT"
 
 say "Note that this script uses the 'datalad get' command which automatically obtains the required files from their remote source -- we will see this in action shortly"
 say "We are saving this script in the dataset. This way we will know exactly which code was used for the analysis. Also, we track this code file with Git, so we can see more easily how it was edited over time."
-run "datalad add code -m \"Brain extraction script\" --to-git"
+run "datalad save code -m \"Brain extraction script\" --to-git"
 
 say "In addition, we will \"tag\" this state of the dataset. This is optional, but it can help to identify important milestones more easily"
 run "datalad save --version-tag setup_done"
 
 say "Now we can run our analysis code to produce results. However, instead of running it directly, we will run it with DataLad -- this will automatically create a record of exactly how this script was executed"
 say "For this demo we will just run it on the structural images of the first subject from each dataset. The uniform structure of the datasets makes this very easy. Of course we could run it on all subjects; we are simply saving some time for this demo."
 say "While the command runs, you should notice a few things:"
```

### Comparing `datalad-0.9.3/docs/casts/heudiconv_dicom_to_bids.sh` & `datalad-1.0.0/docs/casts/heudiconv_dicom_to_bids.sh`

 * *Files 1% similar despite different names*

```diff
@@ -40,15 +40,15 @@
 run "datalad create -d . derivatives/preprocess1"
 say "and do our thorough preprocessing (see http://bids-apps.neuroimaging.io for ready to use pipelines like mriqc and fmriprep), in our case a sample brain extraction:"
 run "source /etc/fsl/fsl.sh   # to enable FSL on NeuroDebian systems"
 run "mkdir -p derivatives/preprocess1/sub-phantom1sid1/ses-localizer/anat/  # create target output directory"
 run "bet {,derivatives/preprocess1/}sub-phantom1sid1/ses-localizer/anat/sub-phantom1sid1_ses-localizer_T1w.nii.gz"
 
 say "To keep control over the versions of all data we work with, we add results of pre-processing under DataLad version control"
-run "datalad add -m 'added initial preprocessing (well -- BETing output)' derivatives/preprocess1/*"
+run "datalad save -m 'added initial preprocessing (well -- BETing output)' derivatives/preprocess1/*"
 say "and then also adjust meta-data templates heudiconv pre-generated for us:"
 run "sed -i -e 's,First1 Last1,Data Lad,g' -e '/TODO/d' dataset_description.json"
 say "We save all so far accumulated changes"
 run "datalad save -m 'Finished initial preprocessing, specified PI in dataset description and removed TODOs.'"
 
 say "Whenever more data is acquired, heudiconv conversion could be ran again to complement previously acquired datasets with new data."
 run "cd"
```

### Comparing `datalad-0.9.3/docs/source/glossary.rst` & `datalad-1.0.0/docs/source/glossary.rst`

 * *Files 25% similar despite different names*

```diff
@@ -40,9 +40,23 @@
   annex
     Extension to a Git_ repository, provided and managed by git-annex_ as
     means to track and distribute large (and small) files without having to
     inject them directly into a Git_ repository (which would slow Git
     operations significantly and impair handling of such repositories in
     general).
 
+  CLI
+    A `Command Line Interface`_. Could be used interactively by executing
+    commands in a `shell`_, or as a programmable API for shell scripts.
+
+  DataLad extension
+    A Python package, developed outside of the core DataLad codebase, which
+    (when installed) typically either provides additional top level `datalad`
+    commands and/or additional metadata extractors.  Visit
+    `Handbook, Ch.2. DataLad’s extensions <http://handbook.datalad.org/en/latest/basics/101-144-intro_extensions.html>`_
+    for a representative list of extensions and instructions on how to install
+    them.
+
 .. _Git: https://git-scm.com
 .. _Git-annex: http://git-annex.branchable.com
+.. _`Command Line Interface`: https://en.wikipedia.org/wiki/Command-line_interface
+.. _shell: https://en.wikipedia.org/wiki/Shell_(computing)
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `datalad-0.9.3/docs/source/config.rst` & `datalad-1.0.0/docs/source/config.rst`

 * *Files 23% similar despite different names*

```diff
@@ -1,7 +1,9 @@
+.. _configuration:
+
 Configuration
 *************
 
 DataLad uses the same configuration mechanism and syntax as Git itself.
 Consequently, datalad can be configured using the :command:`git config`
 command. Both a *global* user configuration (typically at
 :file:`~/.gitconfig`), and a *local* repository-specific configuration
@@ -14,35 +16,42 @@
 machine always override configuration shipped with a dataset.
 
 All datalad-specific configuration variables are prefixed with ``datalad.``.
 
 It is possible to override or amend the configuration using environment
 variables. Any variable with a name that starts with ``DATALAD_`` will
 be available as the corresponding ``datalad.`` configuration variable,
-replacing any ``_`` in the name with a dot, and all letters converted
-to lower case. Values from environment variables take precedence over
+replacing any ``__`` (two underscores) with a hyphen, then any ``_``
+(single underscore) with a dot, and finally converting all letters to
+lower case. Values from environment variables take precedence over
 configuration file settings.
 
+In addition, the ``DATALAD_CONFIG_OVERRIDES_JSON`` environment variable can
+be set to a JSON record with configuration values.  This is
+particularly useful for options that aren't accessible through the
+naming scheme described above (e.g., an option name that includes an
+underscore).
+
 The following sections provide a (non-exhaustive) list of settings honored
 by datalad. They are categorized according to the scope they are typically
 associated with.
 
 
 Global user configuration
 =========================
 
-.. include:: generated/cfginfo/global.rst
+.. include:: generated/cfginfo/global.rst.in
 
 Local repository configuration
 ==============================
 
-.. include:: generated/cfginfo/local.rst
+.. include:: generated/cfginfo/local.rst.in
 
 Sticky dataset configuration
 =============================
 
-.. include:: generated/cfginfo/dataset.rst
+.. include:: generated/cfginfo/dataset.rst.in
 
 Miscellaneous configuration
 ===========================
 
-.. include:: generated/cfginfo/misc.rst
+.. include:: generated/cfginfo/misc.rst.in
```

### Comparing `datalad-0.9.3/docs/source/modref.rst` & `datalad-1.0.0/docs/source/modref.rst`

 * *Files 19% similar despite different names*

```diff
@@ -20,138 +20,136 @@
 ------------------
 
 .. currentmodule:: datalad
 .. autosummary::
    :toctree: generated
 
    api.Dataset
-   api.add
    api.create
    api.create_sibling
    api.create_sibling_github
+   api.create_sibling_gitlab
+   api.create_sibling_gogs
+   api.create_sibling_gitea
+   api.create_sibling_gin
+   api.create_sibling_ria
    api.drop
-   api.plugin
    api.get
    api.install
-   api.publish
+   api.push
    api.remove
    api.save
+   api.status
    api.update
-   api.uninstall
    api.unlock
 
-Meta data handling
-------------------
+
+Reproducible execution
+----------------------
 
 .. currentmodule:: datalad
 .. autosummary::
    :toctree: generated
 
-   api.search
-   api.aggregate_metadata
+   api.run
+   api.rerun
+   api.run_procedure
+
 
 Plumbing commands
 -----------------
 
 .. currentmodule:: datalad
 .. autosummary::
    :toctree: generated
 
-   api.annotate_paths
    api.clean
    api.clone
+   api.copy_file
    api.create_test_dataset
    api.diff
    api.download_url
-   api.ls
-   api.sshrun
+   api.foreach_dataset
    api.siblings
+   api.sshrun
    api.subdatasets
 
 Miscellaneous commands
 ----------------------
 
 .. currentmodule:: datalad
 .. autosummary::
    :toctree: generated
 
    api.add_archive_content
-   api.crawl
-   api.crawl_init
-   api.test
-
-Plugins
--------
-
-DataLad can be customized by plugins. The following plugins are shipped
-with DataLad.
-
-.. currentmodule:: datalad.plugin
-.. autosummary::
-   :toctree: generated
-
-   add_readme
-   export_tarball
-   no_annex
-   wtf
-
+   api.add_readme
+   api.addurls
+   api.check_dates
+   api.configuration
+   api.export_archive
+   api.export_archive_ora
+   api.export_to_figshare
+   api.no_annex
+   api.shell_completion
+   api.wtf
 
 Support functionality
 =====================
 
 .. currentmodule:: datalad
 .. autosummary::
    :toctree: generated
 
-   auto
    cmd
    consts
    log
    utils
    version
+   support.gitrepo
    support.annexrepo
    support.archives
-   support.configparserinc
-   customremotes.main
+   support.extensions
    customremotes.base
    customremotes.archives
+   runner.nonasyncrunner
+   runner.protocol
 
 Configuration management
 ========================
 
 .. currentmodule:: datalad
 .. autosummary::
    :toctree: generated
 
    config
 
-Crawler
-=======
+Test infrastructure
+===================
 
 .. currentmodule:: datalad
 .. autosummary::
    :toctree: generated
 
-   crawler.base
-   crawler.pipeline
+   tests.utils_pytest
+   tests.utils_testrepos
+   tests.heavyoutput
 
-Test infrastructure
-===================
+Command interface
+=================
 
 .. currentmodule:: datalad
 .. autosummary::
    :toctree: generated
 
-   tests.utils
-   tests.utils_testrepos
-   tests.heavyoutput
+   interface.base
 
 Command line interface infrastructure
 =====================================
 
 .. currentmodule:: datalad
 .. autosummary::
    :toctree: generated
 
-   cmdline.main
-   cmdline.helpers
-   cmdline.common_args
+   cli.exec
+   cli.main
+   cli.parser
+   cli.renderer
```

### Comparing `datalad-0.9.3/docs/source/acknowledgements.rst` & `datalad-1.0.0/docs/source/acknowledgements.rst`

 * *Files 20% similar despite different names*

```diff
@@ -2,18 +2,21 @@
 ***************
 
 DataLad development is being performed as part of a US-German collaboration in
 computational neuroscience (CRCNS) project "DataGit: converging catalogues,
 warehouses, and deployment logistics into a federated 'data distribution'"
 (Halchenko_/Hanke_), co-funded by the US National Science Foundation (`NSF
 1429999`_) and the German Federal Ministry of Education and Research (`BMBF
-01GQ1411`_).
+01GQ1411`_). Additional support is provided by the German federal state of
+Saxony-Anhalt and the European Regional Development
+Fund (ERDF), Project: `Center for Behavioral Brain Sciences`_, Imaging Platform
 
 DataLad is built atop the git-annex_ software that is being developed and
 maintained by `Joey Hess`_.
 
 .. _Halchenko: http://haxbylab.dartmouth.edu/ppl/yarik.html
 .. _Hanke: http://www.psychoinformatics.de
 .. _NSF 1429999: http://www.nsf.gov/awardsearch/showAward?AWD_ID=1429999
 .. _BMBF 01GQ1411: http://www.gesundheitsforschung-bmbf.de/de/2550.php
+.. _Center for Behavioral Brain Sciences: http://cbbs.eu/en/
 .. _git-annex: http://git-annex.branchable.com
 .. _Joey Hess: https://joeyh.name
```

### Comparing `datalad-0.9.3/docs/source/conf.py` & `datalad-1.0.0/docs/source/conf.py`

 * *Files 24% similar despite different names*

```diff
@@ -8,36 +8,64 @@
 #
 # Note that not all possible configuration values are present in this
 # autogenerated file.
 #
 # All configuration values have a default; values that are commented out
 # serve to show the default.
 
-import sys
+import datetime
 import os
-import shlex
-from os.path import join as opj, exists
+import sys
+from glob import glob
 from os import pardir
+from os.path import exists
+from os.path import join as opj
+
+
+def setup(sphinx):
+    sys.path.insert(0, os.path.abspath('utils'))  # travis
+    sys.path.insert(0, os.path.abspath(opj(pardir, 'utils')))  # RTD
+    # As of Sphinx v2.1, passing an instance is deprecated.
+    # TODO: Remove when minimum sphinx version is at least 2.1.
+    import sphinx as sphinx_mod
+    from pygments_ansi_color import AnsiColorLexer
+    sphinx_ver = int(sphinx_mod.__version__.split('.')[0])
+    if sphinx_ver < 3:  # Check against 3 rather than 2.1 for simplicity.
+        AnsiColorLexer = AnsiColorLexer()
+    sphinx.add_lexer("ansi-color", AnsiColorLexer)
+
 
 # If extensions (or modules to document with autodoc) are in another directory,
 # add these directories to sys.path here. If the directory is relative to the
 # documentation root, use os.path.abspath to make it absolute, like shown here.
 #sys.path.insert(0, os.path.abspath('.'))
 
 # generate missing pieces
 for setup_py_path in (opj(pardir, 'setup.py'),  # travis
                       opj(pardir, pardir, 'setup.py')):  # RTD
     if exists(setup_py_path):
         try:
-            for cmd in 'manpage', 'cfginfo', 'examples':
+            for cmd in 'manpage', 'cfginfo':
                 os.system('{} build_{}'.format(setup_py_path, cmd))
         except:
             # shut up and do your best
             pass
 
+# if docs are build for a development version of datalad installed with
+# `pip install -e`, add its location to the path
+if 'VIRTUAL_ENV' in os.environ:
+    site_packages_glob = os.sep.join([
+        os.environ['VIRTUAL_ENV'],
+        'lib', 'python*', 'site-packages', 'datalad.*'])
+    for site_package in glob(site_packages_glob):
+        if site_package.endswith('.egg-link'):
+            sys.path.insert(0, open(site_package).read().strip('\n.'))
+
+import datalad
+
 # -- General configuration ------------------------------------------------
 
 # If your documentation needs a minimal Sphinx version, state it here.
 #needs_sphinx = '1.0'
 
 # Add any Sphinx extension module names here, as strings. They can be
 # extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
@@ -50,14 +78,17 @@
     'sphinx.ext.todo',
     'sphinx.ext.coverage',
     'sphinx.ext.mathjax',
     'sphinx.ext.ifconfig',
     'sphinx.ext.inheritance_diagram',
     'sphinx.ext.viewcode',
     'sphinx.ext.napoleon',
+    # github.com/readthedocs/sphinx_rtd_theme/issues/1452 (rm next line when fixed)
+    'sphinxcontrib.jquery',
+    'sphinx_autodoc_typehints',
 ]
 
 # for the module reference
 autosummary_generate = True
 
 # Add any paths that contain templates here, relative to this directory.
 templates_path = ['_templates']
@@ -70,33 +101,34 @@
 # The encoding of source files.
 #source_encoding = 'utf-8-sig'
 
 # The master toctree document.
 master_doc = 'index'
 
 # General information about the project.
-project = u'datalad'
-copyright = u'2016, DataLad team'
+current_year = datetime.datetime.now().year
+project = u'DataLad'
+copyright = (u'2016-{}, DataLad team').format(current_year)
 author = u'DataLad team'
 
 # The version info for the project you're documenting, acts as replacement for
 # |version| and |release|, also used in various other places throughout the
 # built documents.
 #
 # The short X.Y version.
-version = '0.1'
+version = '.'.join(datalad.__version__.split('.')[:2])
 # The full version, including alpha/beta/rc tags.
-release = '0.1'
+release = datalad.__version__
 
 # The language for content autogenerated by Sphinx. Refer to documentation
 # for a list of supported languages.
 #
 # This is also used if you do content translation via gettext catalogs.
 # Usually you set "language" from the command line for these cases.
-language = None
+language = 'en'
 
 # There are two options for replacing |today|: either, you set today to some
 # non-false value, then it is used:
 #today = ''
 # Else, today_fmt is used as the format for a strftime call.
 #today_fmt = '%B %d, %Y'
 
@@ -157,15 +189,15 @@
 # The name of an image file (relative to this directory) to place at the top
 # of the sidebar.
 html_logo = '_static/datalad_logo.png'
 
 # The name of an image file (within the static path) to use as favicon of the
 # docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
 # pixels large.
-#html_favicon = None
+html_favicon = '_static/favicon.ico'
 
 # Add any paths that contain custom static files (such as style sheets) here,
 # relative to this directory. They are copied after the builtin static files,
 # so a file named "default.css" will overwrite the builtin "default.css".
 html_static_path = ['_static']
 
 # Add any extra paths that contain custom files (such as robots.txt or
@@ -310,8 +342,42 @@
 #texinfo_show_urls = 'footnote'
 
 # If true, do not generate a @detailmenu in the "Top" node's menu.
 #texinfo_no_detailmenu = False
 
 
 # Example configuration for intersphinx: refer to the Python standard library.
-intersphinx_mapping = {'https://docs.python.org/': None}
+intersphinx_mapping = {
+    'handbook': (
+        # use handbook matching "major" release
+        'http://handbook.datalad.org/en/{version}/'.format(
+            #version='.'.join(datalad.__version__.split('.', maxsplit=2)[:2]),
+            version='latest',
+        ),
+        None),
+}
+
+# sphinx-autodoc-typehints config:
+always_document_param_types = True
+
+
+import sphinx
+
+if sphinx.__version__ == '5.1.0':
+    # see https://github.com/sphinx-doc/sphinx/issues/10701
+    # hope is it would get fixed for the next release
+
+    # Although crash happens within NumpyDocstring, it is subclass of GoogleDocstring
+    # so we need to overload method there
+    from functools import wraps
+
+    from sphinx.ext.napoleon.docstring import GoogleDocstring
+
+
+    @wraps(GoogleDocstring._consume_inline_attribute)
+    def _consume_inline_attribute_safe(self):
+        try:
+            return self._consume_inline_attribute_safe()
+        except:
+            return "", []
+
+    GoogleDocstring._consume_inline_attribute = _consume_inline_attribute_safe
```

### Comparing `datalad-0.9.3/docs/source/basics.rst` & `datalad-1.0.0/docs/source/basics.rst`

 * *Files 11% similar despite different names*

```diff
@@ -18,75 +18,132 @@
 Datasets
 ========
 
 A DataLad :term:`dataset` is a Git repository that may or may not have a data
 :term:`annex` that is used to manage data referenced in a dataset. In practice,
 most DataLad datasets will come with an annex.
 
+Types of IDs used in datasets
+-----------------------------
+
+Four types of unique identifiers are used by DataLad to enable identification
+of different aspects of datasets and their components.
+
+Dataset ID
+  A UUID that identifies a dataset as a whole across its entire history and
+  flavors. This ID is stored in a dataset's own configuration file
+  (``<dataset root>/.datalad/config``) under the configuration key
+  ``datalad.dataset.id``.
+  As this configuration is stored in a file that is part of the Git history of
+  a dataset, this ID is identical for all "clones" of a dataset and across all
+  its versions. If the purpose or scope of a dataset changes enough to warrant
+  a new dataset ID, it can be changed by altering the dataset configuration
+  setting.
+Annex ID
+  A UUID assigned to an annex of each individual clone of a dataset repository.
+  Git-annex uses this UUID to track file content availability information. The
+  UUID is available under the configuration key ``annex.uuid`` and is stored
+  in the configuration file of a local clone (``<dataset root>/.git/config``).
+  A single dataset instance (i.e. clone) can only have a single annex UUID,
+  but a dataset with multiple clones will have multiple annex UUIDs.
+Commit ID
+  A Git hexsha or tag that identifies a version of a dataset. This ID uniquely
+  identifies the content and history of a dataset up to its present state. As
+  the dataset history also includes the dataset ID, a commit ID of a DataLad
+  dataset is unique to a particular dataset.
+Content ID
+  Git-annex key (typically a checksum) assigned to the content of a file in
+  a dataset's annex. The checksum reflects the content of a file, not its name.
+  Hence the content of multiple identical files in a single (or across)
+  dataset(s) will have the same checksum. Content IDs are managed by Git-annex
+  in a dedicated ``annex`` branch of the dataset's Git repository.
+
+
+Dataset nesting
+---------------
+
 Datasets can contain other datasets (:term:`subdataset`\s), which can in turn
 contain subdatasets, and so on. There is no limit to the depth of nesting
 datasets. Each dataset in such a hierarchy has its own annex and its own
 history. The parent or :term:`superdataset` only tracks the specific state of a
 subdataset, and information on where it can be obtained. This is a powerful yet
 lightweight mechanism for combining multiple individual datasets for a specific
 purpose, such as the combination of source code repositories with other
 resources for a tailored application. In many cases DataLad can work with a
-hierarchy of datasets just as if it were a single dataset.
+hierarchy of datasets just as if it were a single dataset. Here is a demo:
+
+.. include:: basics_nesteddatasets.rst.in
+   :start-after: Let's create a dataset
+   :end-before:  ___________________________
+
+
+Dataset collections
+-------------------
 
 A superdataset can also be seen as a curated collection of datasets, for example,
-for a certain data modality, a field of science, a certain author, or some
-all from one project (maybe the resource for a movie production). The lightweight
+for a certain data modality, a field of science, a certain author, or from
+one project (maybe the resource for a movie production). This lightweight
 coupling between super and subdatasets enables scenarios where individual datasets
 are maintained by a disjoint set of people, and the dataset collection itself can
 be curated by a completely independent entity. Any individual dataset can be
 part of any number of such collections.
 
-Benefitting from Git's support for workflows based on decentralized "clones" of
+Benefiting from Git's support for workflows based on decentralized "clones" of
 a repository, DataLad's datasets can be (re-)published to a new location
-without loosing the connection between the "original" and the new "copy". This
+without losing the connection between the "original" and the new "copy". This
 is extremely useful for collaborative work, but also in more mundane scenarios
-such as data backup, or temporary deployment fo a dataset on a compute cluster,
+such as data backup, or temporary deployment of a dataset on a compute cluster,
 or in the cloud.  Using git-annex, data can also get synchronized across
-different location of a dataset (:term:`sibling`\s in DataLad terminology).
+different locations of a dataset (:term:`sibling`\s in DataLad terminology).
 Using metadata tags, it is even possible to configure different levels of
 desired data redundancy across the network of dataset, or to prevent
 publication of sensitive data to publicly accessible repositories. Individual
 datasets in a hierarchy of (sub)datasets need not be stored at the same location.
 Continuing with an earlier example, it is possible to post a curated
-collection of datasets, as a superdataset, on Github, while the actual datasets
+collection of datasets, as a superdataset, on GitHub, while the actual datasets
 live on different servers all around the world.
 
+Basic command line usage
+========================
+
+.. include:: basics_cmdline.rst.in
+   :end-before:  ___________________________
+
+
 API principles
 ==============
 
 You can use DataLad's ``install`` command to download datasets. The command accepts
 URLs of different protocols (``http``, ``ssh``) as an argument. Nevertheless, the easiest way
-to obtain a first dataset is downloading the canonical :term:`superdataset` from
-http://datasets.datalad.org/ using a shortcut.
+to obtain a first dataset is downloading the default :term:`superdataset` from
+https://datasets.datalad.org/ using a shortcut.
 
-Downloading DataLad's canonical superdataset
+Downloading DataLad's default superdataset
 --------------------------------------------
 
-DataLad's canonical :term:`superdataset` provides an automated collection of datasets
-from various portals and sites (see :ref:`chap_crawler`). The argument ``///`` can be used 
-as a shortcut that points to the superdataset located at http://datasets.datalad.org/. 
+https://datasets.datalad.org provides a super-dataset consisting of datasets
+from various portals and sites.  Many of them were crawled, and periodically
+updated, using `datalad-crawler <https://github.com/datalad/datalad-crawler>`__
+extension.  The argument ``///`` can be used
+as a shortcut that points to the superdataset located at https://datasets.datalad.org/. 
 Here are three common examples in command line notation:
 
 ``datalad install ///``
-    installs the canonical superdataset (metadata without subdatasets) in a
+    installs this superdataset (metadata without subdatasets) in a
     `datasets.datalad.org/` subdirectory under the current directory
 ``datalad install -r ///openfmri``
     installs the openfmri superdataset into an `openfmri/` subdirectory.
     Additionally, the ``-r`` flag recursively downloads all metadata of datasets 
     available from http://openfmri.org as subdatasets into the `openfmri/` subdirectory
 ``datalad install -g -J3 -r ///labs/haxby``
     installs the superdataset of datasets released by the lab of Dr. James V. Haxby
     and all subdatasets' metadata. The ``-g`` flag indicates getting the actual data, too.
     It does so by using 3 parallel download processes (``-J3`` flag).
 
+
 Downloading datasets via http
 -----------------------------
 
 In most places where DataLad accepts URLs as arguments these URLs can be
 regular ``http`` or ``https`` protocol URLs. For example:
 
 ``datalad install https://github.com/psychoinformatics-de/studyforrest-data-phase2.git``
@@ -97,37 +154,14 @@
 
 ``datalad install ssh://me@localhost/path``
 
 Finally, DataLad supports SSH login style resource identifiers, such as ``me@localhost:/path``.
 
 ``datalad install me@localhost:/path``
 
-`--dataset` argument
---------------------
-
-All commands which operate with/on datasets (practically all commands) have a
-``dataset`` argument (``-d`` or ``--dataset`` in command line) which takes a
-path to the dataset that the command should operate on. If a dataset is
-identified this way then any relative path that is provided as an argument to
-the command will be interpreted as being relative to the topmost directory of that
-dataset.  If no dataset argument is provided, relative paths are considered to be
-relative to the current directory.
-
-There are also some useful pre-defined "shortcut" values for dataset arguments:
-
-``///``
-   refers to the "canonical" dataset located under `$HOME/datalad/`.
-   So running ``datalad install -d/// crcns`` will install the ``crcns`` subdataset
-   under ``$HOME/datalad/crcns``.  This is the same as running
-   ``datalad install $HOME/datalad/crcns``.
-``^``
-   topmost superdataset containing the dataset the current directory is part of.
-   For example, if you are in ``$HOME/datalad/openfmri/ds000001/sub-01`` and want
-   to search metadata of the entire superdataset you are under (in this case
-   ``///``), run ``datalad search -d^ [something to search]``.
 
 Commands `install` vs `get`
 ---------------------------
 
 The ``install`` and ``get`` commands might seem confusingly similar at first.
 Both of them could be used to install any number of subdatasets, and fetch
 content of the data files.  Differences lie primarily in their default
@@ -143,26 +177,25 @@
   default
 
 - **get** primarily operates at the level of **paths** (datasets, directories, and/or
   files). As a result it returns only what was installed (datasets) or fetched
   (files).  So result of rerunning the same ``get`` command should report that
   nothing new was installed or fetched.  It **fetches** data files by default.
 
-In how both commands operate on provided paths, it could be said that
-``install == get -n``, and ``install -g == get``.  But ``install`` also has ability to
+In how both commands operate on provided paths, it could be said that ``install
+== get -n``, and ``install -g == get``.  But ``install`` also has ability to
 install new datasets from remote locations given their URLs (e.g.,
-``http://datasets.datalad.org/`` for our super-dataset) and SSH targets (e.g.,
+``https://datasets.datalad.org/`` for our super-dataset) and SSH targets (e.g.,
 ``[login@]host:path``) if they are provided as the argument to its call or
-explicitly as ``--source`` option.  If ``datalad install --source URL DESTINATION`` (command
-line example) is used, then dataset from URL gets installed under PATH. In case of
-``datalad install URL`` invocation, PATH is taken from the last name within URL similar to
-how ``git clone`` does it.  If former specification allows to specify only a single
-URL and a PATH at a time, later one can take multiple remote locations from which
-datasets could be installed.
+explicitly as ``--source`` option.  If ``datalad install --source URL
+DESTINATION`` (command line example) is used, then dataset from URL gets
+installed under PATH. In case of ``datalad install URL`` invocation, PATH is
+taken from the last name within URL similar to how ``git clone`` does it.  If
+former specification allows to specify only a single URL and a PATH at a time,
+later one can take multiple remote locations from which datasets could be
+installed.
 
 So, as a rule of thumb -- if you want to install from external URL or fetch a
 sub-dataset without downloading data files stored under annex -- use ``install``.
 In Python API ``install`` is also to be used when you want to receive in output the
 corresponding Dataset object to operate on, and be able to use it even if you
-rerun the script.
-If you would like to fetch data (possibly while installing any necessary to be
-installed sub-dataset to get to the file) -- use ``get``.
+rerun the script. In all other cases, use ``get``.
```

### Comparing `datalad-0.9.3/docs/source/_static/datalad_logo.png` & `datalad-1.0.0/docs/source/_static/datalad_logo.png`

 * *Files identical despite different names*

### Comparing `datalad-0.9.3/docs/source/related.rst` & `datalad-1.0.0/docs/source/related.rst`

 * *Files 1% similar despite different names*

```diff
@@ -71,17 +71,17 @@
 are not necessarily needed by DataLad (git-annex assistant, encryption support,
 etc.) make it even more appealing for extended coverage of numerous scenarios.
 Moreover, neither of the alternative solutions has already reached a maturity,
 availability, and level of adoption that would be comparable to that of
 git-annex.
 
 .. _git-annex: http://git-annex.branchable.com
-.. _comparison: http://git-annex.branchable.com/not}
+.. _comparison: http://git-annex.branchable.com/not
 .. _git-media: https://github.com/schacon/git-media
-.. _git-fat: https://github.com/jedbrown/git-fat}
+.. _git-fat: https://github.com/jedbrown/git-fat
 
 .. _chap-git-annex-datalad-comparison:
 
 Git/Git-annex/DataLad
 =====================
 
 Although it is possible, and intended, to use DataLad without ever invoking git
```

### Comparing `datalad-0.9.3/docs/source/background.rst` & `datalad-1.0.0/docs/source/background.rst`

 * *Files identical despite different names*

### Comparing `datalad-0.9.3/tools/mimic_repo` & `datalad-1.0.0/tools/mimic_repo`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-#!/usr/bin/python
+#!/usr/bin/env python2
 #emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*- 
 #ex: set sts=4 ts=4 sw=4 noet:
 """
  Simple script to simulate an annex repository given a list of files.
 
  Not sure why I wrote it in Python, since in bash it would be more natural and shorter ;)
```

### Comparing `datalad-0.9.3/tools/git-web-submodules.sh` & `datalad-1.0.0/tools/git-web-submodules.sh`

 * *Files identical despite different names*

### Comparing `datalad-0.9.3/tools/testing/test_README_in_docker` & `datalad-1.0.0/tools/testing/test_README_in_docker`

 * *Ordering differences only*

 * *Files 0% similar despite different names*

```diff
@@ -53,8 +53,8 @@
 tox="$topgitdir/.tox"
 if [ -e "$tox" ]; then
     echo "I: removing existing tox under $tox"
     rm -r $tox
 fi
 
 echo "I: running tox within docker"
-docker run -it --rm=true -v $topgitdir:/home/$USER/datalad $tag tox --sitepackages
+docker run -it --rm=true -v $topgitdir:/home/$USER/datalad $tag tox --sitepackages
```

### Comparing `datalad-0.9.3/tools/testing/start_website_in_docker` & `datalad-1.0.0/tools/testing/start_website_in_docker`

 * *Files identical despite different names*

### Comparing `datalad-0.9.3/tools/testing/test_README_in_docker-Dockerfile.in` & `datalad-1.0.0/tools/testing/test_README_in_docker-Dockerfile.in`

 * *Files 8% similar despite different names*

```diff
@@ -26,16 +26,16 @@
 
 USER DL_USER
 
 WORKDIR /home/DL_USER
 # Prepare system for working with Git
 RUN git config --global user.email "DL_GIT_USER_EMAIL"
 RUN git config --global user.name "DL_GIT_USER_NAME"
-# RUN git clone git://github.com/datalad/datalad
+# RUN git clone https://github.com/datalad/datalad
 RUN mkdir datalad
 
 WORKDIR /home/DL_USER/datalad
 
 # run outside so we could re-enter etc
 # RUN tox
 # Fall back to root, so we have full control happen we want to reuse it?
-# USER root
+# USER root
```

### Comparing `datalad-0.9.3/tools/testing/make_test_repo` & `datalad-1.0.0/tools/testing/make_test_repo`

 * *Files 2% similar despite different names*

```diff
@@ -63,15 +63,15 @@
 }
 
 create_github_repo() {
 	_info "creating github repo"
 	cd $repodir
 	# it wouldn't delete or complain in exit code if exists already
 	hub create -d "$descr (otherwise userless)" $ghrepo
-	git remote | grep -q '^origin' || git remote add origin git://github.com/$ghrepo
+	git remote | grep -q '^origin' || git remote add origin https://github.com/$ghrepo
 	git push --set-upstream --all -f origin
 }
 
 flavor_basic() {
 	create_repo
 	create_info_file
 	echo "123" > test.dat
@@ -115,15 +115,15 @@
 fi
 
 register_repo_submodule() {
 	cd $reposdir
 	_info "registering git submodule"
 	# TODO: verify if not registered already
 	#git submodule ...
-	git submodule add --force git://github.com/$ghrepo ./$flavor/$name && \
+	git submodule add --force https://github.com/$ghrepo ./$flavor/$name && \
 		git commit -m "Added test $flavor/$name" -a && \
 		git push origin
 }
 
 # cd datalad/tests/testrepos
 # hub create -d "Super-submodule collating test repositories for datalad" datalad/testrepos
```

### Comparing `datalad-0.9.3/tools/testing/start_website_in_docker-Dockerfile.in` & `datalad-1.0.0/tools/testing/start_website_in_docker-Dockerfile.in`

 * *Files 0% similar despite different names*

```diff
@@ -35,15 +35,15 @@
 
 USER DL_USER
 
 WORKDIR /home/DL_USER
 # Prepare system for working with Git
 RUN git config --global user.email "DL_GIT_USER_EMAIL"
 RUN git config --global user.name "DL_GIT_USER_NAME"
-# RUN git clone git://github.com/datalad/datalad
+# RUN git clone https://github.com/datalad/datalad
 RUN mkdir datalad
 
 # WORKDIR /home/DL_USER/datalad
 
 #
 # Need to clone and install datalad system-wide but there is a problem that
 # we can't mount hostdir at build time:
@@ -66,8 +66,8 @@
 RUN mkdir -p /var/lock/apache2 /var/run/apache2 /var/run/sshd /var/log/supervisor
 RUN eatmydata apt-get install -y -q supervisor
 COPY conf/apache-ssh-supervisor.conf /etc/supervisor/conf.d/supervisord.conf
 
 # ENTRYPOINT ["/usr/sbin/apache2ctl", "-D", "FOREGROUND"]
 RUN apt-get clean
 EXPOSE 22 80 443
-CMD ["/usr/bin/supervisord"]
+CMD ["/usr/bin/supervisord"]
```

### Comparing `datalad-0.9.3/tools/testing/make_test_repos` & `datalad-1.0.0/tools/testing/make_test_repos`

 * *Files identical despite different names*

### Comparing `datalad-0.9.3/tools/upgrade-annex-osx.sh` & `datalad-1.0.0/tools/upgrade-annex-osx.sh`

 * *Files identical despite different names*

### Comparing `datalad-0.9.3/tools/dtime` & `datalad-1.0.0/tools/dtime`

 * *Files 7% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-#!/usr/bin/python
+#!/usr/bin/env python3
 #emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
 #ex: set sts=4 ts=4 sw=4 noet:
 """Little helper to annotate logfile with difference between timestamps in consecutive lines
 
 It prints time estimated from previous line on the previous line, with 0 always printed as well
 so it becomes possible to sort -n the output to see from what line it took longest to the next
 """
```

### Comparing `datalad-0.9.3/tools/copy_urls_from_datalad.py` & `datalad-1.0.0/tools/copy_urls_from_datalad.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-#!/usr/bin/env python
+#!/usr/bin/env python3
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
 # ex: set sts=4 ts=4 sw=4 noet:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
@@ -26,15 +26,15 @@
     return []
 
 if __name__ == '__main__':
     annex = AnnexRepo('.', create=False, init=False)
     # enable datalad special remote
     urls_to_register = defaultdict(list)  # key: urls
     try:
-        annex._annex_custom_command([], ["git", "annex", "enableremote", "datalad"])
+        annex.call_annex(["enableremote", "datalad"])
         # go through each and see where urls aren't yet under web
         # seems might have also --in=datalad to restrict
         w = annex.whereis([], options=['--all'], output='full')
         lgr.info("Got %d entries", len(w))
         for k, rec in tqdm(w.items()):
             datalad_urls = get_remote_urls(rec, 'datalad')
             web_urls = set(get_remote_urls(rec, 'web'))
@@ -50,14 +50,12 @@
         annex.remove_remote("datalad") # need to disable it first
     lgr.info(
         "Got %d entries which could get new urls",
         len(urls_to_register)
     )
     for k, urls in tqdm(urls_to_register.items()):
         for url in urls:
-            annex._annex_custom_command(
-                [],
-                ["git", "annex", "registerurl", '-c', 'annex.alwayscommit=false', k, url]
-            )
+            annex.call_annex([
+                "registerurl", '-c', 'annex.alwayscommit=false', k, url])
     # to cause annex to commit all the changes
-    annex._annex_custom_command([], ["git", "annex", "merge"])
-    annex.gc(allow_background=False)
+    annex.call_annex(["merge"])
+    annex.gc(allow_background=False)
```

### Comparing `datalad-0.9.3/tools/ci/gpg-90F7E9EB.pub` & `datalad-1.0.0/tools/ci/gpg-90F7E9EB.pub`

 * *Files identical despite different names*

### Comparing `datalad-0.9.3/tools/monitor-interrupts.py` & `datalad-1.0.0/tools/monitor-interrupts.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-#!/usr/bin/python
+#!/usr/bin/env python2
 #emacs: -*- mode: python-mode; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*- 
 #ex: set sts=4 ts=4 sw=4 noet:
 """
 
  COPYRIGHT: Yaroslav Halchenko 2015
 
  LICENSE: MIT
```

### Comparing `datalad-0.9.3/tools/coverage-bin/git-annex-remote-datalad` & `datalad-1.0.0/tools/coverage-bin/datalad`

 * *Files 12% similar despite different names*

```diff
@@ -6,18 +6,17 @@
 set -eu
 
 bin=$(basename $0)
 curbin=$(which "$bin")
 curdatalad=$(which datalad)
 curdir=$(dirname $curdatalad)
 
-
-COVERAGE=${COVERAGE:-python-coverage}
-COVERAGE_RUN="`which $COVERAGE` run"
-
+COVERAGE_RUN="-m coverage run"
+export COVERAGE_PROCESS_START=$PWD/../.coveragerc
+export PYTHONPATH="$PWD/../tools/coverage-bin/"
 export PATH=${PATH//$curdir:/}
 newdatalad=$(which datalad)
 newbin=$(which $bin)
 newpython=$(sed -ne '1s/#!//gp' $newdatalad)
 
 if [ $newdatalad = $curdatalad ]; then
    echo "E: binary remained the same: $newdatalad" >&2
```

### Comparing `datalad-0.9.3/tools/coverage-bin/git-annex-remote-datalad-archives` & `datalad-1.0.0/tools/coverage-bin/git-annex-remote-datalad`

 * *Files 12% similar despite different names*

```diff
@@ -6,18 +6,17 @@
 set -eu
 
 bin=$(basename $0)
 curbin=$(which "$bin")
 curdatalad=$(which datalad)
 curdir=$(dirname $curdatalad)
 
-
-COVERAGE=${COVERAGE:-python-coverage}
-COVERAGE_RUN="`which $COVERAGE` run"
-
+COVERAGE_RUN="-m coverage run"
+export COVERAGE_PROCESS_START=$PWD/../.coveragerc
+export PYTHONPATH="$PWD/../tools/coverage-bin/"
 export PATH=${PATH//$curdir:/}
 newdatalad=$(which datalad)
 newbin=$(which $bin)
 newpython=$(sed -ne '1s/#!//gp' $newdatalad)
 
 if [ $newdatalad = $curdatalad ]; then
    echo "E: binary remained the same: $newdatalad" >&2
```

### Comparing `datalad-0.9.3/tools/coverage-bin/datalad` & `datalad-1.0.0/tools/coverage-bin/git-annex-remote-datalad-archives`

 * *Files 12% similar despite different names*

```diff
@@ -6,18 +6,17 @@
 set -eu
 
 bin=$(basename $0)
 curbin=$(which "$bin")
 curdatalad=$(which datalad)
 curdir=$(dirname $curdatalad)
 
-
-COVERAGE=${COVERAGE:-python-coverage}
-COVERAGE_RUN="`which $COVERAGE` run"
-
+COVERAGE_RUN="-m coverage run"
+export COVERAGE_PROCESS_START=$PWD/../.coveragerc
+export PYTHONPATH="$PWD/../tools/coverage-bin/"
 export PATH=${PATH//$curdir:/}
 newdatalad=$(which datalad)
 newbin=$(which $bin)
 newpython=$(sed -ne '1s/#!//gp' $newdatalad)
 
 if [ $newdatalad = $curdatalad ]; then
    echo "E: binary remained the same: $newdatalad" >&2
```

### Comparing `datalad-0.9.3/tools/mimic_merges` & `datalad-1.0.0/tools/mimic_merges`

 * *Files identical despite different names*

### Comparing `datalad-0.9.3/tools/urlinfo` & `datalad-1.0.0/tools/urlinfo`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-#!/usr/bin/python
+#!/usr/bin/env python2
 #emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*- 
 #ex: set sts=4 ts=4 sw=4 noet:
 #------------------------- =+- Python script -+= -------------------------
 """
   Yaroslav Halchenko                                            Dartmouth
   web:     http://www.onerussian.com                              College
   e-mail:  yoh@onerussian.com                              ICQ#: 60653192
```

### Comparing `datalad-0.9.3/asv.conf.json` & `datalad-1.0.0/asv.conf.json`

 * *Files 2% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 
     // The URL or local path of the source code repository for the
     // project being benchmarked
     "repo": ".",
 
     // List of branches to benchmark. If not provided, defaults to "master"
     // (for git) or "default" (for mercurial).
-    "branches": ["master", "0.5.x", "0.9.x"], // for git
+    "branches": ["master", "maint"], // for git
     // "branches": ["default"],    // for mercurial
 
     // The DVCS being used.  If not set, it will be automatically
     // determined from "repo" by looking at the protocol in the URL
     // (if remote), or by looking for special directories, such as
     // ".git" (if local).
     // "dvcs": "git",
@@ -36,16 +36,16 @@
     //"install_timeout": 600,
 
     // the base URL to show a commit for the project.
     "show_commit_url": "http://github.com/datalad/datalad/commit/",
 
     // The Pythons you'd like to test against.  If not provided, defaults
     // to the current version of Python used to run `asv`.
-    // Some hosts might not have python3.5 yet, so 3.4 was added
-    "pythons": ["2.7", "3.4"],
+    // We are looking into the future now, so benchmarking 3.7
+    "pythons": ["3.7"],
 
     // The matrix of dependencies to test.  Each key is the name of a
     // package (in PyPI) and the values are version numbers.  An empty
     // list or empty string indicates to just test against the default
     // (latest) version. null indicates that the package is to not be
     // installed. If the package to be tested is only available from
     // PyPi, and the 'environment_type' is conda, then you can preface
```

### Comparing `datalad-0.9.3/benchmarks/core.py` & `datalad-1.0.0/benchmarks/core.py`

 * *Files 16% similar despite different names*

```diff
@@ -6,78 +6,65 @@
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 """Benchmarks for DataLad"""
 
 import os
 import sys
 import os.path as osp
-from os.path import join as opj
-import tarfile
-import timeit
 
 from subprocess import call
 
-from datalad.api import add
-from datalad.api import create
-from datalad.api import create_test_dataset
-from datalad.api import Dataset
-from datalad.api import install
-from datalad.api import ls
-from datalad.api import remove
-from datalad.api import uninstall
+from datalad.runner import (
+    Runner,
+    GitRunner,
+    StdOutErrCapture,
+)
 
-from datalad.utils import rmtree
-from datalad.utils import getpwd
 
 # Some tracking example -- may be we should track # of datasets.datalad.org
 #import gc
 #def track_num_objects():
 #    return len(gc.get_objects())
 #track_num_objects.unit = "objects"
 
 
 from .common import SuprocBenchmarks
 
+scripts_dir = osp.join(osp.dirname(__file__), 'scripts')
+heavyout_cmd = "{} 1000".format(osp.join(scripts_dir, 'heavyout'))
+
 
 class startup(SuprocBenchmarks):
     """
     Benchmarks for datalad commands startup
     """
 
     def setup(self):
         # we need to prepare/adjust PATH to point to installed datalad
         # We will base it on taking sys.executable
         python_path = osp.dirname(sys.executable)
         self.env = os.environ.copy()
         self.env['PATH'] = '%s:%s' % (python_path, self.env.get('PATH', ''))
 
-    def time_help_np(self):
-        call(["datalad", "--help-np"], env=self.env)
-        
     def time_import(self):
         call([sys.executable, "-c", "import datalad"])
 
     def time_import_api(self):
         call([sys.executable, "-c", "import datalad.api"])
 
 
-class runner(SuprocBenchmarks):
-    """Some rudimentary tests to see if there is no major slowdowns from Runner
+class witlessrunner(SuprocBenchmarks):
+    """Some rudimentary tests to see if there is no major slowdowns of Runner
     """
 
     def setup(self):
-        from datalad.cmd import Runner
         self.runner = Runner()
-        # older versions might not have it
-        try:
-            from datalad.cmd import GitRunner
-            self.git_runner = GitRunner()
-        except ImportError:
-            pass
+        self.git_runner = GitRunner()
 
     def time_echo(self):
-        self.runner.run("echo")
+        self.runner.run(["echo"])
 
     def time_echo_gitrunner(self):
-        self.git_runner.run("echo")
-
+        self.git_runner.run(["echo"])
 
+    def time_echo_gitrunner_fullcapture(self):
+        self.git_runner.run(["echo"], protocol=StdOutErrCapture)
```

### Comparing `datalad-0.9.3/formatters.py` & `datalad-1.0.0/_datalad_build_support/formatters.py`

 * *Files 8% similar despite different names*

```diff
@@ -3,15 +3,18 @@
 #   See COPYING file distributed along with the DataLad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 
 import argparse
 import datetime
+import os
 import re
+import time
+from textwrap import wrap
 
 
 class ManPageFormatter(argparse.HelpFormatter):
     # This code was originally distributed
     # under the same License of Python
     # Copyright (c) 2014 Oz Nahum Tiram  <nahumoz@gmail.com>
     def __init__(self,
@@ -20,24 +23,26 @@
                  max_help_position=4,
                  width=1000000,
                  section=1,
                  ext_sections=None,
                  authors=None,
                  version=None
                  ):
-
+        from datalad import cfg
         super(ManPageFormatter, self).__init__(
             prog,
             indent_increment=indent_increment,
             max_help_position=max_help_position,
             width=width)
 
         self._prog = prog
         self._section = 1
-        self._today = datetime.date.today().strftime('%Y\\-%m\\-%d')
+        self._today = datetime.datetime.utcfromtimestamp(
+            cfg.obtain('datalad.source.epoch')
+        ).strftime('%Y\\-%m\\-%d')
         self._ext_sections = ext_sections
         self._version = version
 
     def _get_formatter(self, **kwargs):
         return self.formatter_class(prog=self.prog, **kwargs)
 
     def _markup(self, txt):
@@ -65,28 +70,31 @@
 
         usage = usage.replace('%s ' % self._prog, '')
         usage = '.SH SYNOPSIS\n.nh\n.HP\n\\fB%s\\fR %s\n.hy\n' % (self._markup(self._prog),
                                                     usage)
         return usage
 
     def _mk_title(self, prog):
-        name_version = "\"{0} {1}\"".format(prog, self._version)
-        return '.TH {0} {1} {2} {3}\n'.format(prog, self._section,
-                                              self._today, name_version)
+        name_version = "{0} {1}".format(prog, self._version)
+        return '.TH "{0}" "{1}" "{2}" "{3}"\n'.format(
+            prog, self._section, self._today, name_version)
 
-    def _make_name(self, parser):
+    def _mk_name(self, prog, desc):
         """
-        this method is in consitent with others ... it relies on
+        this method is in consistent with others ... it relies on
         distribution
         """
-        return '.SH NAME\n%s \\- %s\n' % (parser.prog,
-                                          parser.description)
+        desc = desc.splitlines()[0] if desc else 'it is in the name'
+        # ensure starting lower case
+        desc = desc[0].lower() + desc[1:]
+        return '.SH NAME\n%s \\- %s\n' % (self._bold(prog), desc)
 
     def _mk_description(self, parser):
         desc = parser.description
+        desc = '\n'.join(desc.splitlines()[1:])
         if not desc:
             return ''
         desc = desc.replace('\n\n', '\n.PP\n')
         # sub-section headings
         desc = re.sub(r'^\*(.*)\*$', r'.SS \1', desc, flags=re.MULTILINE)
         # italic commands
         desc = re.sub(r'^  ([-a-z]*)$', r'.TP\n\\fI\1\\fR', desc, flags=re.MULTILINE)
@@ -109,14 +117,15 @@
             footer.append(part)
 
         return '\n'.join(footer)
 
     def format_man_page(self, parser):
         page = []
         page.append(self._mk_title(self._prog))
+        page.append(self._mk_name(self._prog, parser.description))
         page.append(self._mk_synopsis(parser))
         page.append(self._mk_description(parser))
         page.append(self._mk_options(parser))
         page.append(self._mk_footer(self._ext_sections))
 
         return ''.join(page)
 
@@ -140,15 +149,15 @@
         help = re.sub(r'([a-z]),([a-z])', '\\1, \\2', help)
         # get proper indentation for argument items
         help = re.sub(r'^  (\S.*)\n', '.TP\n\\1\n', help, flags=re.MULTILINE)
         # deindent body text, leave to troff viewer
         help = re.sub(r'^    (\S.*)\n', '\\1\n', help, flags=re.MULTILINE)
         return '.SH OPTIONS\n' + help
 
-    def _format_action_invocation(self, action):
+    def _format_action_invocation(self, action, doubledash='--'):
         if not action.option_strings:
             metavar, = self._metavar_formatter(action, action.dest)(1)
             return metavar
 
         else:
             parts = []
 
@@ -163,15 +172,15 @@
             else:
                 default = self._underline(action.dest.upper())
                 args_string = self._format_args(action, default)
                 for option_string in action.option_strings:
                     parts.append('%s %s' % (self._bold(option_string),
                                             args_string))
 
-            return ', '.join(parts)
+            return ', '.join(p.replace('--', doubledash) for p in parts)
 
 
 class RSTManPageFormatter(ManPageFormatter):
     def _get_formatter(self, **kwargs):
         return self.formatter_class(prog=self.prog, **kwargs)
 
     def _markup(self, txt):
@@ -187,26 +196,28 @@
     def _mk_synopsis(self, parser):
         self.add_usage(parser.usage, parser._actions,
                        parser._mutually_exclusive_groups, prefix='')
         usage = self._format_usage(None, parser._actions,
                                    parser._mutually_exclusive_groups, '')
 
         usage = usage.replace('%s ' % self._prog, '')
-        usage = 'Synopsis\n--------\n::\n\n  %s %s\n' \
+        usage = '\n'.join(wrap(
+            usage, break_on_hyphens=False, subsequent_indent=6*' '))
+        usage = 'Synopsis\n--------\n::\n\n  %s %s\n\n' \
                 % (self._markup(self._prog), usage)
         return usage
 
     def _mk_title(self, prog):
         # and an easy to use reference point
-        title = ".. _man_%s:\n\n" % prog
+        title = ".. _man_%s:\n\n" % prog.replace(' ', '-')
         title += "{0}".format(prog)
         title += '\n{0}\n\n'.format('=' * len(prog))
         return title
 
-    def _make_name(self, parser):
+    def _mk_name(self, prog, desc):
         return ''
 
     def _mk_description(self, parser):
         desc = parser.description
         if not desc:
             return ''
         return 'Description\n-----------\n%s\n' % self._markup(desc)
@@ -243,15 +254,15 @@
         # determine help from format above
         option_sec = formatter.format_help()
 
         return '\n\nOptions\n-------\n{0}'.format(option_sec)
 
     def _format_action(self, action):
         # determine the required width and the entry label
-        action_header = self._format_action_invocation(action)
+        action_header = self._format_action_invocation(action, doubledash='-\\-')
 
         if action.help:
             help_text = self._expand_help(action)
             help_lines = self._split_lines(help_text, 80)
             help = ' '.join(help_lines)
         else:
             help = ''
@@ -262,15 +273,15 @@
 
             '~' * len(action_header),
             help)
 
 
 def cmdline_example_to_rst(src, out=None, ref=None):
     if out is None:
-        from six.moves import StringIO
+        from io import StringIO
         out = StringIO()
 
     # place header
     out.write('.. AUTO-GENERATED FILE -- DO NOT EDIT!\n\n')
     if ref:
         # place cross-ref target
         out.write('.. {0}:\n\n'.format(ref))
```

### Comparing `datalad-0.9.3/datalad/tests/test_constraints.py` & `datalad-1.0.0/datalad/tests/test_constraints.py`

 * *Files 9% similar despite different names*

```diff
@@ -5,26 +5,20 @@
 #   See COPYING file distributed along with the DataLad package for the
 #   copyright and license terms.
 #
 ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 '''Unit tests for basic constraints functionality.'''
 
 
-import sys
-import os
-from os.path import isabs, abspath, join as opj, normpath
-from six import string_types, PY2
+from datalad.tests.utils_pytest import (
+    assert_equal,
+    assert_raises,
+)
 
 from ..support import constraints as ct
-from ..support.gitrepo import GitRepo
-from ..utils import getpwd, chpwd
-from ..tests.utils import assert_cwd_unchanged, with_testrepos, with_tempfile, \
-    on_windows
-
-from nose.tools import assert_equal, assert_raises, eq_, ok_
 
 
 def test_int():
     c = ct.EnsureInt()
     # this should always work
     assert_equal(c(7), 7)
     assert_equal(c(7.0), 7)
@@ -51,22 +45,22 @@
 
 
 def test_bool():
     c = ct.EnsureBool()
     # this should always work
     assert_equal(c(True), True)
     assert_equal(c(False), False)
-    # all that resuls in True
+    # all that results in True
     assert_equal(c('True'), True)
     assert_equal(c('true'), True)
     assert_equal(c('1'), True)
     assert_equal(c('yes'), True)
     assert_equal(c('on'), True)
     assert_equal(c('enable'), True)
-    # all that resuls in False
+    # all that results in False
     assert_equal(c('false'), False)
     assert_equal(c('False'), False)
     assert_equal(c('0'), False)
     assert_equal(c('no'), False)
     assert_equal(c('off'), False)
     assert_equal(c('disable'), False)
     # this should always fail
@@ -98,14 +92,16 @@
     assert_raises(ValueError, c, 'h')
 
 
 def test_none():
     c = ct.EnsureNone()
     # this should always work
     assert_equal(c(None), None)
+    # instance of NoneDeprecated is also None
+    assert_equal(c(ct.NoneDeprecated), None)
     # this should always fail
     assert_raises(ValueError, lambda: c('None'))
     assert_raises(ValueError, lambda: c([]))
 
 
 def test_callable():
     c = ct.EnsureCallable()
@@ -137,45 +133,43 @@
 
 
 def test_range():
     c = ct.EnsureRange(min=3, max=7)
     # this should always work
     assert_equal(c(3.0), 3.0)
 
-    # Python 3 raises an TypeError if incompatible types are compared,
-    # whereas Python 2 raises a ValueError
-    type_error = TypeError if sys.version_info[0] >= 3 else ValueError
-
     # this should always fail
     assert_raises(ValueError, lambda: c(2.9999999))
     assert_raises(ValueError, lambda: c(77))
-    assert_raises(type_error, lambda: c('fail'))
-    assert_raises(type_error, lambda: c((3, 4)))
+    assert_raises(TypeError, lambda: c('fail'))
+    assert_raises(TypeError, lambda: c((3, 4)))
     # since no type checks are performed
-    assert_raises(type_error, lambda: c('7'))
+    assert_raises(TypeError, lambda: c('7'))
 
     # Range doesn't have to be numeric
     c = ct.EnsureRange(min="e", max="qqq")
     assert_equal(c('e'), 'e')
     assert_equal(c('fa'), 'fa')
     assert_equal(c('qq'), 'qq')
     assert_raises(ValueError, c, 'a')
     assert_raises(ValueError, c, 'qqqa')
 
 
 def test_listof():
     c = ct.EnsureListOf(str)
     assert_equal(c(['a', 'b']), ['a', 'b'])
     assert_equal(c(['a1', 'b2']), ['a1', 'b2'])
+    assert_equal(c('a1 b2'), ['a1 b2'])
 
 
 def test_tupleof():
     c = ct.EnsureTupleOf(str)
     assert_equal(c(('a', 'b')), ('a', 'b'))
     assert_equal(c(('a1', 'b2')), ('a1', 'b2'))
+    assert_equal(c('a1 b2'), ('a1 b2',))
 
 
 def test_constraints():
     # this should always work
     c = ct.Constraints(ct.EnsureFloat())
     assert_equal(c(7.0), 7.0)
     c = ct.Constraints(ct.EnsureFloat(), ct.EnsureRange(min=4.0))
@@ -232,8 +226,8 @@
     assert_equal(c(7.0), 7.0)
     assert_equal(c(None), None)
     # this should always fail
     assert_raises(ValueError, lambda: c(77.0))
 
 def test_type_str():
     assert_equal(ct._type_str((str,)), 'str')
-    assert_equal(ct._type_str(string_types), 'basestring' if PY2 else 'str')
+    assert_equal(ct._type_str(str), 'str')
```

### Comparing `datalad-0.9.3/datalad/tests/test_testrepos.py` & `datalad-1.0.0/datalad/local/tests/test_no_annex.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,62 +1,69 @@
-# emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# emacs: -*- mode: python-mode; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
+# -*- coding: utf-8 -*-
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
+"""Test no_annex"""
 
-from datalad.tests.utils import known_failure_direct_mode
 
-import git
-import os
+from os.path import join as opj
 
-from datalad.tests.utils import integration
-from datalad.tests.utils import usecase
-from .utils import eq_, ok_, with_testrepos, with_tempfile
-from datalad.cmd import Runner
-from .utils import local_testrepo_flavors
-from .utils_testdatasets import make_studyforrest_mockup
-
-
-@with_testrepos('.*annex.*', flavors=['clone'])
-def test_having_annex(path):
-    ok_(os.path.exists(os.path.join(path, '.git')))
-    repo = git.Repo(path)
-    # might not necessarily be present upon initial submodule init
-    #branches = [r.name for r in repo.branches]
-    #ok_('git-annex' in branches, msg="Didn't find git-annex among %s" % branches)
-    # look for it among remote refs
-    refs = [_.name for _ in repo.remote().refs]
-    ok_('origin/git-annex' in refs, msg="Didn't find git-annex among refs %s"
-                                        % refs)
-
-@with_testrepos(flavors=['network'])
-def test_point_to_github(url):
-    ok_('github.com' in url)
-    ok_(url.startswith('git://github.com/datalad/testrepo--'))
-
-@with_testrepos
-@with_tempfile
-def test_clone(src, tempdir):
-    # Verify that all our repos are clonable
-    r = Runner()
-    output = r.run(["git" , "clone", src, tempdir], log_online=True)
-    #status, output = getstatusoutput("git clone %(src)s %(tempdir)s" % locals())
-    ok_(os.path.exists(os.path.join(tempdir, ".git")))
-    # TODO: requires network for sure! ;)
-    # TODO: figure out why fails on travis  -- demands init first! bleh
-    #  but since this is not a purpose of this test really -- screw it
-    #status1, output1 = getstatusoutput("cd %(tempdir)s && git annex get --from=web test-annex.dat"
-    #                                   % locals())
-    #eq_(status1, 0, msg="Status: %d  Output was: %r" % (status1, output1))
-    #ok_("get test-annex.dat" in output1)
+from datalad.api import (
+    create,
+    no_annex,
+)
+from datalad.tests.utils_pytest import (
+    SkipTest,
+    assert_repo_status,
+    create_tree,
+    eq_,
+    known_failure_githubci_win,
+    with_tempfile,
+)
+from datalad.utils import Path
 
 
-@usecase
+@known_failure_githubci_win
 @with_tempfile(mkdir=True)
-@known_failure_direct_mode  #FIXME
-def test_make_studyforrest_mockup(path):
-    # smoke test
-    make_studyforrest_mockup(path)
+def test_no_annex(path=None):
+    ds = create(path)
+    assert_repo_status(ds.path)
+    create_tree(
+        ds.path,
+        {'code': {
+            'inannex': 'content',
+            'notinannex': 'othercontent'},
+         'README': 'please'})
+    # add inannex pre configuration
+    ds.save(opj('code', 'inannex'))
+    no_annex(pattern=['code/**', 'README'], dataset=ds.path)
+
+    inannex = (ds.pathobj / 'code' / 'inannex')
+
+    # add inannex and README post configuration
+    ds.save([opj('code', 'notinannex'), 'README'])
+
+    repo = ds.repo
+    try:
+        assert_repo_status(ds.path)
+    except AssertionError:
+        # If on an adjusted branch and notinannex's mtime is as recent or newer
+        # than .git/index's, the clean filter runs on it when save() is called.
+        # This leads to a racy failure until after git-annex's 424bef6b6
+        # (smudge: check for known annexed inodes before checking
+        # annex.largefiles, 2021-05-03).
+        #
+        # https://git-annex.branchable.com/forum/one-off_unlocked_annex_files_that_go_against_large/
+        if repo.is_managed_branch() and repo.git_annex_version <= "8.20210428":
+            assert_repo_status(ds.path, modified=[inannex])
+            raise SkipTest("Known bug fixed in git-annex")
+        raise
+
+    # one is annex'ed, the other is not, despite no change in add call
+    # importantly, also .gitattribute is not annexed
+    eq_([opj('code', 'inannex')],
+        [str(Path(p)) for p in repo.get_annexed_files()])
```

### Comparing `datalad-0.9.3/datalad/tests/test_interface.py` & `datalad-1.0.0/datalad/tests/test_interface.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,28 +1,35 @@
 # emacs: -*- mode: python-mode; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 """Test command call wrapper
 """
 
 import re
-from nose.tools import assert_is, assert_equal, assert_raises, assert_true
 
-from ..support.param import Parameter
-from ..support import constraints as cnstr
-from ..interface.base import Interface, get_api_name, get_cmdline_command_name
-
-from ..utils import swallow_outputs
-from .utils import assert_re_in
-from .utils import assert_in
+from datalad.interface.base import (
+    Interface,
+    get_api_name,
+)
+from datalad.support import constraints as cnstr
+from datalad.support.param import Parameter
+from datalad.tests.utils_pytest import (
+    assert_equal,
+    assert_in,
+    assert_is,
+    assert_raises,
+    assert_re_in,
+    assert_true,
+    swallow_outputs,
+)
 
 
 class Demo(Interface):
     """I am a demo"""
     _params_ = dict(
 
         demoposarg=Parameter(
@@ -65,24 +72,25 @@
     p = Parameter(doc=doc, constraints=cnstr.EnsureInt() | cnstr.EnsureStr())
     autodoc = p.get_autodoc('testname')
     assert_true('int or str' in autodoc)
 
     with assert_raises(ValueError) as cmr:
         Parameter(unknown_arg=123)
     assert_in('Detected unknown argument(s) for the Parameter: unknown_arg',
-              str(cmr.exception))
+              str(cmr.value))
 
 
 def test_interface():
     di = Demo()
 
     import argparse
     parser = argparse.ArgumentParser()
 
-    di.setup_parser(parser)
+    from datalad.cli.parser import setup_parser_for_interface
+    setup_parser_for_interface(parser, di)
     with swallow_outputs() as cmo:
         assert_equal(parser.print_help(), None)
         assert(cmo.out)
         assert_equal(cmo.err, '')
     args = parser.parse_args(['42', '11', '1', '2', '--demoarg', '23'])
     assert_is(args.demoarg, 23)
     assert_equal(args.demoposarg, [42, 11])
@@ -118,23 +126,7 @@
         'module')
     assert_equal(
         get_api_name(("some.module",
                       "SomeClass",
                       "cmdline_override",
                       "api_override-dont-touch")),
         "api_override-dont-touch")
-    assert_equal(
-        get_cmdline_command_name(("some.module_something", "SomeClass")),
-        "module-something")
-    assert_equal(
-        get_cmdline_command_name((
-            "some.module_something",
-            "SomeClass",
-            "override")),
-        "override")
-    assert_equal(
-        get_cmdline_command_name((
-            "some.module_something",
-            "SomeClass",
-            "override",
-            "api_ignore")),
-        "override")
```

### Comparing `datalad-0.9.3/datalad/tests/test__main__.py` & `datalad-1.0.0/datalad/tests/test__main__.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,42 +1,36 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 
 import sys
+from io import StringIO
+from unittest.mock import patch
 
-from mock import patch
-from six.moves import StringIO
-from tempfile import NamedTemporaryFile
-from nose.tools import assert_raises, assert_equal
-
-from .. import __main__, __version__
-from ..auto import AutomagicIO
+from datalad.tests.utils_pytest import (
+    assert_equal,
+    assert_raises,
+)
+
+from .. import (
+    __main__,
+    __version__,
+)
 
 
 @patch('sys.stdout', new_callable=StringIO)
-def test_main_help(stdout):
+def test_main_help(stdout=None):
     assert_raises(SystemExit, __main__.main, ['__main__.py', '--help'])
     assert(
         stdout.getvalue().startswith(
         "Usage: %s -m datalad [OPTIONS] <file> [ARGS]\n" % sys.executable
     ))
 
 @patch('sys.stdout', new_callable=StringIO)
-def test_main_version(stdout):
+def test_main_version(stdout=None):
     assert_raises(SystemExit, __main__.main, ['__main__.py', '--version'])
     assert_equal(stdout.getvalue().rstrip(), "datalad %s" % __version__)
-
-@patch.object(AutomagicIO, 'activate')
-@patch('sys.stdout', new_callable=StringIO)
-def test_main_run_a_script(stdout, mock_activate):
-    f = NamedTemporaryFile()
-    f.write('print("Running the script")\n'.encode()); f.flush()
-    __main__.main(['__main__.py', f.name])
-    assert_equal(stdout.getvalue().rstrip(), "Running the script")
-    # And we have "activated"
-    mock_activate.assert_called_once_with()
```

### Comparing `datalad-0.9.3/datalad/tests/test_utils.py` & `datalad-1.0.0/datalad/tests/test_tests_utils_pytest.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,843 +1,691 @@
-# emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# -*- coding: utf-8 -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil; coding: utf-8 -*-
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""Test testing utilities
 
-"""
-
-from datalad.tests.utils import known_failure_v6
-from datalad.tests.utils import known_failure_direct_mode
-import inspect
+import base64
+import logging
 import os
-import shutil
+import platform
+import random
 import sys
-import logging
-from mock import patch
-from six import PY3
-from six import text_type
-import six.moves.builtins as __builtin__
-
-from operator import itemgetter
-from os.path import dirname, normpath, pardir, basename
-from os.path import isabs, expandvars, expanduser
-from collections import OrderedDict
-
-from ..dochelpers import exc_str
-from ..utils import updated
-from os.path import join as opj, abspath, exists
-from ..utils import rotree, swallow_outputs, swallow_logs, setup_exceptionhook, md5sum
-from ..utils import getpwd, chpwd
-from ..utils import get_path_prefix
-from ..utils import auto_repr
-from ..utils import find_files
-from ..utils import line_profile
-from ..utils import not_supported_on_windows
-from ..utils import file_basename
-from ..utils import expandpath, is_explicit_path
-from ..utils import knows_annex
-from ..utils import any_re_search
-from ..utils import unique
-from ..utils import get_func_kwargs_doc
-from ..utils import make_tempfile
-from ..utils import on_windows
-from ..utils import _path_
-from ..utils import get_timestamp_suffix
-from ..utils import get_trace
-from ..utils import get_dataset_root
-from ..utils import better_wraps
-from ..utils import path_startswith
-from ..utils import safe_print
-from ..utils import generate_chunks
-from ..utils import disable_logger
-
-from ..support.annexrepo import AnnexRepo
-
-from nose.tools import ok_, eq_, assert_false, assert_equal, assert_true
-from datalad.tests.utils import nok_
-
-from .utils import with_tempfile, assert_in, with_tree
-from .utils import SkipTest
-from .utils import assert_cwd_unchanged, skip_if_on_windows
-from .utils import assure_dict_from_str, assure_list_from_str
-from .utils import assure_unicode
-from .utils import assure_bool
-from .utils import assure_list
-from .utils import ok_generator
-from .utils import assert_not_in
-from .utils import assert_raises
-from .utils import ok_startswith
-from .utils import skip_if_no_module
-from .utils import probe_known_failure, skip_known_failure, known_failure, known_failure_v6, known_failure_direct_mode
-
-
-def test_get_func_kwargs_doc():
-    def some_func(arg1, kwarg1=None, kwarg2="bu"):
-        return
-    eq_(get_func_kwargs_doc(some_func), ['arg1', 'kwarg1', 'kwarg2'])
-
-
-def test_better_wraps():
-    from functools import wraps
-    from inspect import getargspec
-
-    def wraps_decorator(func):
-        @wraps(func)
-        def new_func(*args, **kwargs):
-            return func(*args, **kwargs)
-
-        return new_func
-
-    def better_decorator(func):
-        @better_wraps(func)
-        def new_func(*args, **kwargs):
-            return func(*args, **kwargs)
-
-        return new_func
-
-    @wraps_decorator
-    def function1(a, b, c):
-        return "function1"
-
-    @better_decorator
-    def function2(a, b, c):
-        return "function2"
-
-    eq_("function1", function1(1, 2, 3))
-    eq_(getargspec(function1)[0], [])
-    eq_("function2", function2(1, 2, 3))
-    eq_(getargspec(function2)[0], ['a', 'b', 'c'])
 
+try:
+    # optional direct dependency we might want to kick out
+    import bs4
+except ImportError:  # pragma: no cover
+    bs4 = None
+
+from glob import glob
+from os.path import (
+    basename,
+    exists,
+)
+from os.path import join as opj
+from unittest.mock import patch
+from urllib.parse import quote as url_quote
+from urllib.request import (
+    Request,
+    urlopen,
+)
+
+import pytest
+from _pytest.outcomes import (
+    Failed,
+    Skipped,
+)
+
+from datalad import cfg as dl_cfg
+from datalad.support import path as op
+from datalad.support.gitrepo import GitRepo
+from datalad.tests.utils_pytest import (
+    OBSCURE_FILENAMES,
+    OBSCURE_PREFIX,
+    assert_cwd_unchanged,
+    assert_dict_equal,
+    assert_false,
+    assert_in,
+    assert_not_in,
+    assert_raises,
+    assert_re_in,
+    assert_str_equal,
+    assert_true,
+    eq_,
+    get_most_obscure_supported_name,
+    ignore_nose_capturing_stdout,
+    known_failure_githubci_win,
+    known_failure_windows,
+    local_testrepo_flavors,
+    nok_startswith,
+    ok_,
+    ok_broken_symlink,
+    ok_file_has_content,
+    ok_file_under_git,
+    ok_generator,
+    ok_good_symlink,
+    ok_startswith,
+    ok_symlink,
+    on_windows,
+    patch_config,
+    probe_known_failure,
+    rmtemp,
+    run_under_dir,
+    serve_path_via_http,
+    skip_if,
+    skip_if_no_module,
+    skip_if_no_network,
+    skip_if_on_windows,
+    skip_ssh,
+    skip_wo_symlink_capability,
+    swallow_logs,
+    with_tempfile,
+    with_testsui,
+    with_tree,
+    without_http_proxy,
+)
+from datalad.utils import (
+    Path,
+    chpwd,
+    getpwd,
+)
 
-@with_tempfile(mkdir=True)
-def test_rotree(d):
-    d2 = opj(d, 'd1', 'd2')  # deep nested directory
-    f = opj(d2, 'f1')
-    os.makedirs(d2)
-    with open(f, 'w') as f_:
-        f_.write("LOAD")
-    with swallow_logs():
-        ar = AnnexRepo(d2)
-    rotree(d)
-    # we shouldn't be able to delete anything UNLESS in "crippled" situation:
-    # root, or filesystem is FAT etc
-    # Theoretically annex should declare FS as crippled when ran as root, but
-    # see http://git-annex.branchable.com/bugs/decides_that_FS_is_crippled_under_cowbuilder___40__symlinks_supported_etc__41__/#comment-60c3cbe2710d6865fb9b7d6e247cd7aa
-    # so explicit 'or'
-    if not (ar.is_crippled_fs() or (os.getuid() == 0)):
-        assert_raises(OSError, os.unlink, f)
-        assert_raises(OSError, shutil.rmtree, d)
-        # but file should still be accessible
-        with open(f) as f_:
-            eq_(f_.read(), "LOAD")
-    # make it RW
-    rotree(d, False)
-    os.unlink(f)
-    shutil.rmtree(d)
-
-
-def test_swallow_outputs():
-    with swallow_outputs() as cm:
-        eq_(cm.out, '')
-        sys.stdout.write("out normal")
-        sys.stderr.write("out error")
-        eq_(cm.out, 'out normal')
-        sys.stdout.write(" and more")
-        eq_(cm.out, 'out normal and more')  # incremental
-        eq_(cm.err, 'out error')
-        eq_(cm.err, 'out error')  # the same value if multiple times
-
+#
+# Test with_tempfile, especially nested invocations
+#
 
 @with_tempfile
-def test_swallow_logs(logfile):
-    lgr = logging.getLogger('datalad')
-    with swallow_logs(new_level=9) as cm:
-        eq_(cm.out, '')
-        lgr.log(8, "very heavy debug")
-        eq_(cm.out, '')  # not even visible at level 9
-        lgr.log(9, "debug1")
-        eq_(cm.out, '[Level 9] debug1\n')  # not even visible at level 9
-        lgr.info("info")
-        eq_(cm.out, '[Level 9] debug1\n[INFO] info\n')  # not even visible at level 9
-    with swallow_logs(new_level=9, file_=logfile) as cm:
-        eq_(cm.out, '')
-        lgr.info("next info")
-    from datalad.tests.utils import ok_file_has_content
-    ok_file_has_content(logfile, "[INFO] next info", strip=True)
-
-
-def test_swallow_logs_assert():
-    lgr = logging.getLogger('datalad.tests')
-    with swallow_logs(new_level=9) as cm:
-        # nothing was logged so should fail
-        assert_raises(AssertionError, cm.assert_logged)
-        lgr.info("something")
-        cm.assert_logged("something")
-        cm.assert_logged(level="INFO")
-        cm.assert_logged("something", level="INFO")
-
-        # even with regex = False should match above
-        cm.assert_logged("something", regex=False)
-        cm.assert_logged(level="INFO", regex=False)
-        cm.assert_logged("something", level="INFO", regex=False)
-
-        # different level
-        assert_raises(AssertionError,
-                      cm.assert_logged, "something", level="DEBUG")
-        assert_raises(AssertionError, cm.assert_logged, "else")
-
-        cm.assert_logged("some.hing", level="INFO")  # regex ;-)
-        # does match
-        assert_raises(AssertionError,
-                      cm.assert_logged, "ome.hing", level="INFO")
-        # but we can change it
-        cm.assert_logged("some.hing", level="INFO", match=False)
-    # and we can continue doing checks after we left the cm block
-    cm.assert_logged("some.hing", level="INFO", match=False)
-    # and we indeed logged something
-    cm.assert_logged(match=False)
-
-
-def test_disable_logger():
-
-    # get a logger hierarchy:
-    lgr_top = logging.getLogger('datalad')
-    lgr_middle = logging.getLogger('datalad.tests')
-    lgr_bottom = logging.getLogger('datalad.tests.utils')
-
-    with swallow_logs(new_level=logging.DEBUG) as cml:
-        with disable_logger():  # default: 'datalad':
-            lgr_top.debug("log sth at top level")
-            lgr_middle.debug("log sth at mid level")
-            lgr_bottom.debug("log sth at bottom level")
-        # nothing logged:
-        assert_raises(AssertionError, cml.assert_logged)
-
-    # again, but pass in the logger at mid level:
-    with swallow_logs(new_level=logging.DEBUG) as cml:
-        with disable_logger(lgr_middle):
-            lgr_top.debug("log sth at top level")
-            lgr_middle.debug("log sth at mid level")
-            lgr_bottom.debug("log sth at bottom level")
-        # top level unaffected:
-        cml.assert_logged("log sth at top level", level="DEBUG", regex=False)
-        # but both of the lower ones don't log anything:
-        assert_raises(AssertionError, cml.assert_logged, "log sth at mid level")
-        assert_raises(AssertionError, cml.assert_logged, "log sth at bottom level")
-
-
-def _check_setup_exceptionhook(interactive):
-    old_exceptionhook = sys.excepthook
-
-    post_mortem_tb = []
-
-    def our_post_mortem(tb):
-        post_mortem_tb.append(tb)
-
-    with patch('sys.excepthook'), \
-            patch('datalad.utils.is_interactive', lambda: interactive), \
-            patch('pdb.post_mortem', our_post_mortem):
-        setup_exceptionhook()
-        our_exceptionhook = sys.excepthook
-        ok_(old_exceptionhook != our_exceptionhook)
-        #out = sys.stdout
-        with swallow_logs() as cml, swallow_outputs() as cmo:
-            # we need to call our_exceptionhook explicitly b/c nose
-            # swallows all Exceptions and hook never gets executed
-            try:
-                raise RuntimeError
-            except Exception as e:  # RuntimeError:
-                type_, value_, tb_ = sys.exc_info()
-            our_exceptionhook(type_, value_, tb_)
-            if PY3:
-                # Happens under tox environment but not in manually crafted ones -- not yet sure
-                # what it is about but --dbg does work with python3 so lettting it skip for now
-                raise SkipTest("TODO: Not clear why in PY3 calls cleanup if we try to access the beast")
-            assert_in('Traceback (most recent call last)', cmo.err)
-            assert_in('in _check_setup_exceptionhook', cmo.err)
-            if interactive:
-                assert_equal(post_mortem_tb[0], tb_)
-            else:
-                assert_equal(post_mortem_tb, [])
-                # assert_in('We cannot setup exception hook', cml.out)
-
-    eq_(old_exceptionhook, sys.excepthook)
-
-
-def test_setup_exceptionhook():
-    for tval in [True, False]:
-        yield _check_setup_exceptionhook, tval
-
-
-def test_md5sum():
-    # just a smoke (encoding/decoding) test for md5sum
-    _ = md5sum(__file__)
-
-
-@with_tree([('1.tar.gz', (('1 f.txt', '1 f load'),))])
-def test_md5sum_archive(d):
-    # just a smoke (encoding/decoding) test for md5sum
-    _ = md5sum(opj(d, '1.tar.gz'))
-
-
-def test_updated():
-    d = {}
-    eq_(updated(d, {1: 2}), {1: 2})
-    eq_(d, {})
-
-    d = {'a': 'b'}
-    eq_(updated(d, ((0, 1), (2, 3))), {0: 1, 'a': 'b', 2: 3})
-    eq_(d, {'a': 'b'})
-
-    # and that it would maintain the type
-    d = OrderedDict(((99, 0), ('z', 0), ('a', 0)))
-    d_ = updated(d, {0: 1})
-    ok_(isinstance(d_, OrderedDict))
-    eq_(d_, OrderedDict(((99, 0), ('z', 0), ('a', 0), (0, 1))))
-
-
-def test_get_local_file_url_windows():
-    raise SkipTest("TODO")
-
-
-@assert_cwd_unchanged
-def test_getpwd_basic():
-    pwd = getpwd()
-    ok_(isabs(pwd))
-    eq_(os.getcwd(), abspath(pwd))
-
-    # that we do not chdir anywhere if None provided
-    with patch('os.chdir') as oschdir:
-        with chpwd(None):
-            eq_(getpwd(), pwd)
-        assert_false(oschdir.called)
+def _with_tempfile_decorated_dummy(path):
+    return path
 
 
-@skip_if_on_windows
-@with_tempfile(mkdir=True)
-@assert_cwd_unchanged
-def test_getpwd_symlink(tdir):
-    sdir = opj(tdir, 's1')
-    pwd_orig = getpwd()
-    os.symlink('.', sdir)
-    s1dir = opj(sdir, 's1')
-    s2dir = opj(sdir, 's2')
-    try:
-        chpwd(sdir)
-        pwd = getpwd()
-        eq_(pwd, sdir)
-        chpwd('s1')
-        eq_(getpwd(), s1dir)
-        chpwd('.')
-        eq_(getpwd(), s1dir)
-        chpwd('..')
-        eq_(getpwd(), sdir)
-    finally:
-        chpwd(pwd_orig)
-
-    # test context handler way of use
-    with chpwd(s1dir):
-        eq_(getpwd(), s1dir)
-    eq_(getpwd(), pwd_orig)
-
-    assert_false(exists(s2dir))
-    with assert_raises(OSError):
-        with chpwd(s2dir):
-            pass
-    with chpwd(s2dir, mkdir=True):
-        ok_(exists(s2dir))
-        eq_(getpwd(), s2dir)
-
-
-def test_auto_repr():
-
-    class withoutrepr:
-        def __init__(self):
-            self.a = "does not matter"
-
-    @auto_repr
-    class buga:
-        def __init__(self):
-            self.a = 1
-            self.b = list(range(100))
-            self.c = withoutrepr()
-            self._c = "protect me"
-
-        def some(self):
-            return "some"
-
-    assert_equal(repr(buga()), "buga(a=1, b=<<[0, 1, 2, 3, 4, 5, 6, ...>>, c=<withoutrepr>)")
-    assert_equal(buga().some(), "some")
-
-
-def test_assure_list_copy():
-    l = [1]
-    assert assure_list(l) is l
-    assert assure_list(l, copy=True) is not l
-
-
-def test_assure_list_from_str():
-    assert_equal(assure_list_from_str(''), None)
-    assert_equal(assure_list_from_str([]), None)
-    assert_equal(assure_list_from_str('somestring'), ['somestring'])
-    assert_equal(assure_list_from_str('some\nmultiline\nstring'), ['some', 'multiline', 'string'])
-    assert_equal(assure_list_from_str(['something']), ['something'])
-    assert_equal(assure_list_from_str(['a', 'listof', 'stuff']), ['a', 'listof', 'stuff'])
-
-
-def test_assure_dict_from_str():
-    assert_equal(assure_dict_from_str(''), None)
-    assert_equal(assure_dict_from_str({}), None)
-    assert_equal(assure_dict_from_str(
-            '__ac_name={user}\n__ac_password={password}\nsubmit=Log in\ncookies_enabled='), dict(
-             __ac_name='{user}', __ac_password='{password}', cookies_enabled='', submit='Log in'))
-    assert_equal(assure_dict_from_str(
-        dict(__ac_name='{user}', __ac_password='{password}', cookies_enabled='', submit='Log in')), dict(
-             __ac_name='{user}', __ac_password='{password}', cookies_enabled='', submit='Log in'))
-
-
-def test_assure_bool():
-    for values, t in [
-        (['True', 1, '1', 'yes', 'on'], True),
-        (['False', 0, '0', 'no', 'off'], False)
-    ]:
-        for v in values:
-            eq_(assure_bool(v), t)
-    assert_raises(ValueError, assure_bool, "unknown")
-
-
-def test_generate_chunks():
-    ok_generator(generate_chunks([1], 1))
-    eq_(list(generate_chunks([1], 1)), [[1]])
-    eq_(list(generate_chunks([1], 2)), [[1]])
-    eq_(list(generate_chunks([1, 2, 3], 2)), [[1, 2], [3]])
-    # type is preserved
-    eq_(list(generate_chunks((1, 2, 3), 2)), [(1, 2), (3,)])
-    # no hangers
-    eq_(list(generate_chunks((1, 2, 3, 4), 2)), [(1, 2), (3, 4)])
-    assert_raises(AssertionError, list, generate_chunks([1], 0))
-
-
-def test_any_re_search():
-    assert_true(any_re_search('a', 'a'))
-    assert_true(any_re_search('a', 'bab'))
-    assert_false(any_re_search('^a', 'bab'))
-    assert_true(any_re_search(['b', '.ab'], 'bab'))
-    assert_false(any_re_search(['^b', 'bab'], 'ab'))
-
-
-def test_find_files():
-    tests_dir = dirname(__file__)
-    proj_dir = normpath(opj(dirname(__file__), pardir))
-
-    ff = find_files('.*', proj_dir)
-    ok_generator(ff)
-    files = list(ff)
-    assert(len(files) > 10)  # we have more than 10 test files here
-    assert_in(opj(tests_dir, 'test_utils.py'), files)
-    # and no directories should be mentioned
-    assert_not_in(tests_dir, files)
-
-    ff2 = find_files('.*', proj_dir, dirs=True)
-    files2 = list(ff2)
-    assert_in(opj(tests_dir, 'test_utils.py'), files2)
-    assert_in(tests_dir, files2)
-
-    # now actually matching the path
-    ff3 = find_files('.*/test_.*\.py$', proj_dir, dirs=True)
-    files3 = list(ff3)
-    assert_in(opj(tests_dir, 'test_utils.py'), files3)
-    assert_not_in(tests_dir, files3)
-    for f in files3:
-        ok_startswith(basename(f), 'test_')
-
-from .utils import with_tree
-@with_tree(tree={
-    '.git': {
-        '1': '2'
-    },
-    'd1': {
-        '.git': 'possibly a link from submodule'
-    },
-    'git': 'just a file'
-})
-def test_find_files_exclude_vcs(repo):
-    ff = find_files('.*', repo, dirs=True)
-    files = list(ff)
-    assert_equal({basename(f) for f in files}, {'d1', 'git'})
-    assert_not_in(opj(repo, '.git'), files)
-
-    ff = find_files('.*', repo, dirs=True, exclude_vcs=False)
-    files = list(ff)
-    assert_equal({basename(f) for f in files}, {'d1', 'git', '.git', '1'})
-    assert_in(opj(repo, '.git'), files)
-
-
-def test_line_profile():
-    skip_if_no_module('line_profiler')
-
-    @line_profile
-    def f(j):
-        i = j + 1  # xyz
-        return i
-
-    with swallow_outputs() as cmo:
-        assert_equal(f(3), 4)
-        assert_equal(cmo.err, '')
-        assert_in('i = j + 1  # xyz', cmo.out)
-
-
-def test_not_supported_on_windows():
-    with patch('datalad.utils.on_windows', True):
-        assert_raises(NotImplementedError, not_supported_on_windows)
-        assert_raises(NotImplementedError, not_supported_on_windows, "msg")
-
-    with patch('datalad.utils.on_windows', False):
-        assert_equal(not_supported_on_windows(), None)
-        assert_equal(not_supported_on_windows("msg"), None)
-
-
-def test_file_basename():
-    eq_(file_basename('1'), '1')
-    eq_(file_basename('d1/1'), '1')
-    eq_(file_basename('/d1/1'), '1')
-    eq_(file_basename('1.'), '1.')
-    eq_(file_basename('1.tar.gz'), '1')
-    eq_(file_basename('1.Tar.gz'), '1')
-    eq_(file_basename('1._bak.gz'), '1')
-    eq_(file_basename('1.tar.gz', return_ext=True), ('1', 'tar.gz'))
-    eq_(file_basename('/tmp/1.tar.gz'), '1')
-    eq_(file_basename('/tmp/1.longish.gz'), '1.longish')
-    eq_(file_basename('1_R1.1.1.tar.gz'), '1_R1.1.1')
-    eq_(file_basename('ds202_R1.1.1.tgz'), 'ds202_R1.1.1')
-
-
-def test_expandpath():
-    eq_(expandpath("some", False), expanduser('some'))
-    eq_(expandpath("some", False), expandvars('some'))
-    assert_true(isabs(expandpath('some')))
-    # this may have to go because of platform issues
-    eq_(expandpath("$HOME"), expanduser('~'))
-
-
-def test_is_explicit_path():
-    # by default expanded paths are absolute, hence explicit
-    assert_true(is_explicit_path(expandpath('~')))
-    assert_false(is_explicit_path("here"))
+def test_with_tempfile_dir_via_env_variable():
+    target = os.path.join(os.path.expanduser("~"), "dataladtesttmpdir")
+    assert_false(os.path.exists(target), "directory %s already exists." % target)
 
+    with patch_config({'datalad.tests.temp.dir': target}):
+        filename = _with_tempfile_decorated_dummy()
+        ok_startswith(filename, target)
 
 @with_tempfile
 @with_tempfile
-def test_knows_annex(here, there):
-    from datalad.support.gitrepo import GitRepo
-    from datalad.support.annexrepo import AnnexRepo
-    GitRepo(path=here, create=True)
-    assert_false(knows_annex(here))
-    AnnexRepo(path=here, create=True)
-    assert_true(knows_annex(here))
-    GitRepo.clone(path=there, url=here, create=True)
-    assert_true(knows_annex(there))
+def test_nested_with_tempfile_basic(f1=None, f2=None):
+    ok_(f1 != f2)
+    ok_(not os.path.exists(f1))
+    ok_(not os.path.exists(f2))
+
+
+# And the most obscure case to test.  Generator for the test is
+# used as well to verify that every one of those functions adds new argument
+# to the end of incoming arguments.
+@with_tempfile(prefix="TEST", suffix='big')
+@with_tree((('f1.txt', 'load'),))
+@with_tempfile(suffix='.cfg')
+@with_tempfile(suffix='.cfg.old')
+def check_nested_with_tempfile_parametrized_surrounded(
+        param, f0=None, tree=None, f1=None, f2=None, repo=None):
+    eq_(param, "param1")
+    ok_(f0.endswith('big'), msg="got %s" % f0)
+    ok_(os.path.basename(f0).startswith('TEST'), msg="got %s" % f0)
+    ok_(os.path.exists(os.path.join(tree, 'f1.txt')))
+    ok_(f1 != f2)
+    ok_(f1.endswith('.cfg'), msg="got %s" % f1)
+    ok_(f2.endswith('.cfg.old'), msg="got %s" % f2)
 
 
-def test_make_tempfile():
-    # check if mkdir, content conflict caught
-    with assert_raises(ValueError):
-        with make_tempfile(content="blah", mkdir=True):  # pragma: no cover
-            pass
 
+def test_nested_with_tempfile_parametrized_surrounded():
+    check_nested_with_tempfile_parametrized_surrounded("param1")
+
+
+@with_tempfile(content="testtest")
+def test_with_tempfile_content(f=None):
+    ok_file_has_content(f, "testtest")
+    ok_file_has_content(f, "test*", re_=True)
+
+
+def test_with_tempfile_content_raises_on_mkdir():
+
+    @with_tempfile(content="test", mkdir=True)
+    def t():  # pragma: no cover
+        raise AssertionError("must not be run")
 
-def test_unique():
-    eq_(unique(range(3)), [0, 1, 2])
-    eq_(unique((1, 0, 1, 3, 2, 0, 1)), [1, 0, 3, 2])
-    eq_(unique([]), [])
-    eq_(unique([(1, 2), (1,), (1, 2), (0, 3)]), [(1, 2), (1,), (0, 3)])
-
-    # with a key now
-    eq_(unique([(1, 2), (1,), (1, 2), (0, 3)], key=itemgetter(0)), [(1, 2), (0, 3)])
-    eq_(unique([(1, 2), (1, 3), (1, 2), (0, 3)], key=itemgetter(1)), [(1, 2), (1, 3)])
+    with assert_raises(ValueError):
+        # after this commit, it will check when invoking, not when decorating
+        t()
 
 
-def test_path_():
-    eq_(_path_('a'), 'a')
+def test_get_resolved_values():
+    from datalad.tests.utils_pytest import _get_resolved_flavors
+    flavors = ['networkish', 'local']
+    eq_(([] if dl_cfg.get('datalad.tests.nonetwork') else ['networkish'])
+        + ['local'],
+        _get_resolved_flavors(flavors))
+
+    with patch_config({'datalad.tests.nonetwork': '1'}):
+        eq_(_get_resolved_flavors(flavors), ['local'])
+
+def test_with_tempfile_mkdir():
+    dnames = []  # just to store the name within the decorated function
+
+    @with_tempfile(mkdir=True)
+    def check_mkdir(d1):
+        ok_(os.path.exists(d1))
+        ok_(os.path.isdir(d1))
+        dnames.append(d1)
+        eq_(glob(os.path.join(d1, '*')), [])
+        # Create a file to assure we can remove later the temporary load
+        with open(os.path.join(d1, "test.dat"), "w") as f:
+            f.write("TEST LOAD")
+
+    check_mkdir()
+    if not dl_cfg.get('datalad.tests.temp.keep'):
+        ok_(not os.path.exists(dnames[0]))  # got removed
+
+
+@with_tempfile()
+def test_with_tempfile_default_prefix(d1=None):
+    d = basename(d1)
+    short = 'datalad_temp_'
+    full = short + \
+           'test_with_tempfile_default_prefix'
     if on_windows:
-        eq_(_path_('a/b'), r'a\b')
+        ok_startswith(d, short)
+        nok_startswith(d, full)
     else:
-        p = 'a/b/c'
-        assert(_path_(p) is p)  # nothing is done to it whatsoever
-        eq_(_path_(p, 'd'), 'a/b/c/d')
+        ok_startswith(d, full)
 
 
-def test_get_timestamp_suffix():
-    # we need to patch temporarily TZ
-    import time
-    try:
-        with patch.dict('os.environ', {'TZ': 'GMT'}):
-            time.tzset()
-            assert_equal(get_timestamp_suffix(0), '-1970-01-01T00:00:00+0000')  # skynet DOB
-            assert_equal(get_timestamp_suffix(0, prefix="+"), '+1970-01-01T00:00:00+0000')
-            # yoh found no way to mock things out and didn't want to provide
-            # explicit call to anything to get current time with the timezone, so disabling
-            # this test for now besides that it should return smth sensible ;)
-            #with patch.object(time, 'localtime', lambda: 1):
-            #    assert_equal(get_timestamp_suffix(), '-1970-01-01T00:00:01+0000')  # skynet is 1 sec old
-            assert(get_timestamp_suffix().startswith('-'))
-    finally:
-        time.tzset()
-
-
-def test_memoized_generator():
-    called = [0]
-
-    def g1(n):
-        """a generator"""
-        called[0] += 1
-        for i in range(n):
-            yield i
-
-    from ..utils import saved_generator
-    ok_generator(g1(3))
-    g1_, g2_ = saved_generator(g1(3))
-    ok_generator(g1_)
-    ok_generator(g2_)
-    target = list(g1(3))
-    eq_(called[0], 1)
-    eq_(target, list(g1_))
-    eq_(called[0], 2)
-    eq_(target, list(g2_))
-    eq_(called[0], 2)  # no new call to make a generator
-    # but we can't (ab)use 2nd time
-    eq_([], list(g2_))
-
-
-def test_assure_unicode():
-    ok_(isinstance(assure_unicode("m"), text_type))
-    ok_(isinstance(assure_unicode('grandchild_äöü東'), text_type))
-    ok_(isinstance(assure_unicode(u'grandchild_äöü東'), text_type))
-    eq_(assure_unicode('grandchild_äöü東'), u'grandchild_äöü東')
+@with_tempfile(prefix="nodatalad_")
+def test_with_tempfile_specified_prefix(d1=None):
+    ok_startswith(basename(d1), 'nodatalad_')
+    ok_('test_with_tempfile_specified_prefix' not in d1)
 
 
-@with_tempfile(mkdir=True)
-def test_path_prefix(tdir):
-    eq_(get_path_prefix('/d1/d2', '/d1/d2'), '')
-    # so we are under /d1/d2 so path prefix is ..
-    eq_(get_path_prefix('/d1/d2', '/d1/d2/d3'), '..')
-    eq_(get_path_prefix('/d1/d2/d3', '/d1/d2'), 'd3')
-    # but if outside -- full path
-    eq_(get_path_prefix('/d1/d2', '/d1/d20/d3'), '/d1/d2')
-    with chpwd(tdir):
-        eq_(get_path_prefix('.'), '')
-        eq_(get_path_prefix('d1'), 'd1')
-        eq_(get_path_prefix('d1', 'd2'), opj(tdir, 'd1'))
-        eq_(get_path_prefix('..'), '..')
-
-
-def test_get_trace():
-    assert_raises(ValueError, get_trace, [], 'bumm', 'doesntmatter')
-    eq_(get_trace([('A', 'B')], 'A', 'A'), None)
-    eq_(get_trace([('A', 'B')], 'A', 'B'), [])
-    eq_(get_trace([('A', 'B')], 'A', 'C'), None)
-    eq_(get_trace([('A', 'B'),
-                   ('B', 'C')], 'A', 'C'), ['B'])
-    # order of edges doesn't matter
-    eq_(get_trace([
-        ('B', 'C'),
-        ('A', 'B')
-        ], 'A', 'C'), ['B'])
-    # mixed rubbish
-    eq_(get_trace([
-        (1, 3),
-        ('B', 'C'),
-        (None, ('schwak', 7)),
-        ('A', 'B'),
-        ], 'A', 'C'), ['B'])
-    # long
-    eq_(get_trace([
-        ('B', 'C'),
-        ('A', 'B'),
-        ('distract', 'me'),
-        ('C', 'D'),
-        ('D', 'E'),
-        ], 'A', 'E'), ['B', 'C', 'D'])
+def test_get_most_obscure_supported_name():
+    n = get_most_obscure_supported_name()
+    ok_startswith(n, OBSCURE_PREFIX)
+    ok_(len(OBSCURE_FILENAMES) > 1)
+    # from more complex to simpler ones
+    ok_(len(OBSCURE_FILENAMES[0]) > len(OBSCURE_FILENAMES[-1]))
+    print(repr(n))
 
 
-@with_tempfile(mkdir=True)
-def test_get_dataset_root(path):
-    eq_(get_dataset_root('/nonexistent'), None)
-    with chpwd(path):
-        repo = AnnexRepo(os.curdir, create=True)
-        subdir = opj('some', 'deep')
-        fname = opj(subdir, 'dummy')
-        os.makedirs(subdir)
-        with open(fname, 'w') as f:
-            f.write('some')
-        repo.add(fname)
-        # we can find this repo
-        eq_(get_dataset_root(os.curdir), os.curdir)
-        # and we get the type of path that we fed in
-        eq_(get_dataset_root(abspath(os.curdir)), abspath(os.curdir))
-        # subdirs are no issue
-        eq_(get_dataset_root(subdir), os.curdir)
-        # even more subdirs are no issue
-        eq_(get_dataset_root(opj(subdir, subdir)), os.curdir)
-        # non-dir paths are no issue
-        eq_(get_dataset_root(fname), os.curdir)
-
-
-def test_path_startswith():
-    ok_(path_startswith('/a/b', '/a'))
-    ok_(path_startswith('/a/b', '/'))
-    ok_(path_startswith('/aaa/b/c', '/aaa'))
-    nok_(path_startswith('/aaa/b/c', '/aa'))
-    nok_(path_startswith('/a/b', '/a/c'))
-    nok_(path_startswith('/a/b/c', '/a/c'))
-
-
-def test_safe_print():
-    """Just to test that we are getting two attempts to print"""
-
-    called = [0]
-    def _print(s):
-        assert_equal(s, "bua")
-        called[0] += 1
-        if called[0] == 1:
-            raise UnicodeEncodeError('crap', u"", 0, 1, 'whatever')
-
-    with patch.object(__builtin__, 'print', _print):
-        safe_print("bua")
-    assert_equal(called[0], 2)
+def test_keeptemp_via_env_variable():
 
+    if dl_cfg.get('datalad.tests.temp.keep'):  # pragma: no cover
+        pytest.skip("We have env variable set to preserve tempfiles")
 
-def test_probe_known_failure():
+    files = []
 
-    # Note: we can't test the switch "datalad.tests.knownfailures.probe"
-    # directly, since it was evaluated in the decorator already. So we need
-    # to have different assertions in this test based on config and have it
-    # tested across builds, which use different settings for that switch.
-
-    @probe_known_failure
-    def not_failing():
-        pass
-
-    @probe_known_failure
-    def failing():
-        raise AssertionError("Failed")
-
-    from datalad import cfg
-    switch = cfg.obtain("datalad.tests.knownfailures.probe")
-
-    if switch:
-        # if probing is enabled the failing is considered to be expected and
-        # therefore the decorated function doesn't actually fail:
-        failing()
-        # in opposition a function that doesn't fail raises an AssertionError:
-        assert_raises(AssertionError, not_failing)
-    else:
-        # if probing is disabled it should just fail/pass as is:
-        assert_raises(AssertionError, failing)
-        not_failing()
+    @with_tempfile()
+    def check(f):
+        open(f, 'w').write("LOAD")
+        files.append(f)
 
+    with patch.dict('os.environ', {}):
+        check()
 
-def test_skip_known_failure():
+    with patch.dict('os.environ', {'DATALAD_TESTS_TEMP_KEEP': '1'}):
+        check()
 
-    # Note: we can't test the switch "datalad.tests.knownfailures.skip"
-    # directly, since it was evaluated in the decorator already. So we need
-    # to have different assertions in this test based on config and have it
-    # tested across builds, which use different settings for that switch.
+    eq_(len(files), 2)
+    ok_(not exists(files[0]), msg="File %s still exists" % files[0])
+    ok_(    exists(files[1]), msg="File %s not exists" % files[1])
 
-    @skip_known_failure
-    def failing():
-        raise AssertionError("Failed")
+    rmtemp(files[-1])
 
-    from datalad import cfg
-    switch = cfg.obtain("datalad.tests.knownfailures.skip")
 
-    if switch:
-        # if skipping is enabled, we shouldn't see the exception:
-        failing()
-    else:
-        # if it's disabled, failing() is executed and therefore exception
-        # is raised:
-        assert_raises(AssertionError, failing)
+@skip_wo_symlink_capability
+@with_tempfile
+def test_ok_symlink_helpers(tmpfile=None):
 
+    assert_raises(AssertionError, ok_symlink, tmpfile)
+    assert_raises(AssertionError, ok_good_symlink, tmpfile)
+    assert_raises(AssertionError, ok_broken_symlink, tmpfile)
+
+    tmpfile_symlink = tmpfile + '_symlink'
+    Path(tmpfile_symlink).symlink_to(Path(tmpfile))
+
+    # broken symlink
+    ok_symlink(tmpfile_symlink)
+    ok_broken_symlink(tmpfile_symlink)
+    assert_raises(AssertionError, ok_good_symlink, tmpfile_symlink)
+
+    with open(tmpfile, 'w') as tf:
+        tf.write('test text')
+    
+    # tmpfile is still not a symlink here
+    assert_raises(AssertionError, ok_symlink, tmpfile)
+    assert_raises(AssertionError, ok_good_symlink, tmpfile)
+    assert_raises(AssertionError, ok_broken_symlink, tmpfile)
+
+    ok_symlink(tmpfile_symlink)
+    ok_good_symlink(tmpfile_symlink)
+    assert_raises(AssertionError, ok_broken_symlink, tmpfile_symlink)
+
+
+def test_ok_startswith():
+    ok_startswith('abc', 'abc')
+    ok_startswith('abc', 'a')
+    ok_startswith('abc', '')
+    ok_startswith(' abc', ' ')
+    ok_startswith('abc\r\n', 'a')  # no effect from \r\n etc
+    assert_raises(AssertionError, ok_startswith, 'abc', 'b')
+    assert_raises(AssertionError, ok_startswith, 'abc', 'abcd')
+
+
+def test_nok_startswith():
+    nok_startswith('abc', 'bc')
+    nok_startswith('abc', 'c')
+    assert_raises(AssertionError, nok_startswith, 'abc', 'a')
+    assert_raises(AssertionError, nok_startswith, 'abc', 'abc')
+
+def test_ok_generator():
+    def func(a, b=1):
+        return a+b
+    def gen(a, b=1):  # pragma: no cover
+        yield a+b
+    # not sure how to determine if xrange is a generator
+    assert_raises(AssertionError, ok_generator, range(2))
+    assert_raises(AssertionError, ok_generator, gen)
+    ok_generator(gen(1))
+    assert_raises(AssertionError, ok_generator, func)
+    assert_raises(AssertionError, ok_generator, func(1))
+
+
+@pytest.mark.parametrize("func", [os.chdir, chpwd])
+def test_assert_Xwd_unchanged(func):
+    orig_cwd = os.getcwd()
+    orig_pwd = getpwd()
+
+    @assert_cwd_unchanged
+    def do_chdir():
+        func(os.pardir)
+
+    with assert_raises(AssertionError) as cm:
+        do_chdir()
+
+    eq_(orig_cwd, os.getcwd(),
+        "assert_cwd_unchanged didn't return us back to cwd %s" % orig_cwd)
+    eq_(orig_pwd, getpwd(),
+        "assert_cwd_unchanged didn't return us back to pwd %s" % orig_pwd)
+
+@pytest.mark.parametrize("func", [os.chdir, chpwd])
+def test_assert_Xwd_unchanged_ok_chdir(func):
+    # Test that we are not masking out other "more important" exceptions
+
+    orig_cwd = os.getcwd()
+    orig_pwd = getpwd()
+
+    @assert_cwd_unchanged(ok_to_chdir=True)
+    def do_chdir_value_error():
+        func(os.pardir)
+        return "a value"
+
+    with swallow_logs() as cml:
+        eq_(do_chdir_value_error(), "a value")
+        eq_(orig_cwd, os.getcwd(),
+            "assert_cwd_unchanged didn't return us back to cwd %s" % orig_cwd)
+        eq_(orig_pwd, getpwd(),
+            "assert_cwd_unchanged didn't return us back to cwd %s" % orig_pwd)
+        assert_not_in("Mitigating and changing back", cml.out)
+
+
+def test_assert_cwd_unchanged_not_masking_exceptions():
+    # Test that we are not masking out other "more important" exceptions
+
+    orig_cwd = os.getcwd()
+
+    @assert_cwd_unchanged
+    def do_chdir_value_error():
+        os.chdir(os.pardir)
+        raise ValueError("error exception")
+
+    with swallow_logs(new_level=logging.WARN) as cml:
+        with assert_raises(ValueError) as cm:
+            do_chdir_value_error()
+        # retrospect exception
+        eq_(orig_cwd, os.getcwd(),
+            "assert_cwd_unchanged didn't return us back to %s" % orig_cwd)
+        assert_in("Mitigating and changing back", cml.out)
+
+    # and again but allowing to chdir
+    @assert_cwd_unchanged(ok_to_chdir=True)
+    def do_chdir_value_error():
+        os.chdir(os.pardir)
+        raise ValueError("error exception")
+
+    with swallow_logs(new_level=logging.WARN) as cml:
+        assert_raises(ValueError, do_chdir_value_error)
+        eq_(orig_cwd, os.getcwd(),
+            "assert_cwd_unchanged didn't return us back to %s" % orig_cwd)
+        assert_not_in("Mitigating and changing back", cml.out)
 
-def test_known_failure():
 
-    @known_failure
-    def failing():
-        raise AssertionError("Failed")
+@with_tempfile(mkdir=True)
+def _test_serve_path_via_http(test_fpath, use_ssl, auth, tmp_dir):  # pragma: no cover
+    tmp_dir = Path(tmp_dir)
+    test_fpath = Path(test_fpath)
+    # First verify that filesystem layer can encode this filename
+    # verify first that we could encode file name in this environment
+    try:
+        filesysencoding = sys.getfilesystemencoding()
+        test_fpath_encoded = str(test_fpath.as_posix()).encode(filesysencoding)
+    except UnicodeEncodeError:  # pragma: no cover
+        pytest.skip("Environment doesn't support unicode filenames")
+    if test_fpath_encoded.decode(filesysencoding) != test_fpath.as_posix():  # pragma: no cover
+        pytest.skip("Can't convert back/forth using %s encoding"
+                       % filesysencoding)
+
+    test_fpath_full = tmp_dir / test_fpath
+    test_fpath_full.parent.mkdir(parents=True, exist_ok=True)
+    test_fpath_full.write_text(
+        f'some txt and a randint {random.randint(1, 10)}')
+
+    @serve_path_via_http(tmp_dir, use_ssl=use_ssl, auth=auth)
+    def test_path_and_url(path, url):
+        def _urlopen(url, auth=None):
+            req = Request(url)
+            if auth:
+                req.add_header(
+                    "Authorization",
+                    b"Basic " + base64.standard_b64encode(
+                        '{0}:{1}'.format(*auth).encode('utf-8')))
+            return urlopen(req)
+
+        # @serve_ should remove http_proxy from the os.environ if was present
+        if not on_windows:
+            assert_false('http_proxy' in os.environ)
+        # get the "dir-view"
+        dirurl = url + test_fpath.parent.as_posix()
+        u = _urlopen(dirurl, auth)
+        assert_true(u.getcode() == 200)
+        html = u.read()
+        # get the actual content
+        file_html = _urlopen(
+            url + url_quote(test_fpath.as_posix()), auth).read().decode()
+        # verify we got the right one
+        eq_(file_html, test_fpath_full.read_text())
+
+        if bs4 is None:
+            return
+
+        # MIH is not sure what this part below is supposed to do
+        # possibly some kind of internal consistency test
+        soup = bs4.BeautifulSoup(html, "html.parser")
+        href_links = [txt.get('href') for txt in soup.find_all('a')]
+        assert_true(len(href_links) == 1)
+        parsed_url = f"{dirurl}/{href_links[0]}"
+        u = _urlopen(parsed_url, auth)
+        html = u.read().decode()
+        eq_(html, file_html)
+
+    test_path_and_url()
+
+
+@pytest.mark.parametrize("test_fpath", [
+    'test1.txt',
+    Path('test_dir', 'test2.txt'),
+    Path('test_dir', 'd2', 'd3', 'test3.txt'),
+    'file with space test4',
+    u'Джэйсон',
+    get_most_obscure_supported_name(),
+])
+@pytest.mark.parametrize("use_ssl,auth", [
+    (False, None),
+    (True, None),
+    (False, ('ernie', 'bert')),
+])
+def test_serve_path_via_http(test_fpath, use_ssl, auth):
+    _test_serve_path_via_http(test_fpath, use_ssl, auth)
+
+
+def test_serve_path_via_http_local_proxy():
+    # just with the last one check that we did remove proxy setting
+    with patch.dict('os.environ', {'http_proxy': 'http://127.0.0.1:9/'}):
+        _test_serve_path_via_http(get_most_obscure_supported_name(), False, None)
+
+
+@known_failure_githubci_win
+def test_without_http_proxy():
+
+    @without_http_proxy
+    def check(a, kw=False):
+        assert_false('http_proxy' in os.environ)
+        assert_false('https_proxy' in os.environ)
+        assert_in(kw, [False, 'custom'])
+
+    check(1)
+
+    with patch.dict('os.environ', {'http_proxy': 'http://127.0.0.1:9/'}):
+        check(1)
+        check(1, "custom")
+        with assert_raises(AssertionError):
+            check(1, "wrong")
+
+    with patch.dict('os.environ', {'https_proxy': 'http://127.0.0.1:9/'}):
+        check(1)
+    with patch.dict('os.environ', {'http_proxy': 'http://127.0.0.1:9/',
+                                   'https_proxy': 'http://127.0.0.1:9/'}):
+        check(1)
+
+
+def test_assert_re_in():
+    assert_re_in(".*", "")
+    assert_re_in(".*", ["any"])
+
+    # should do match not search
+    assert_re_in("ab", "abc")
+    assert_raises(AssertionError, assert_re_in, "ab", "cab")
+    assert_raises(AssertionError, assert_re_in, "ab$", "abc")
+
+    # Sufficient to have one entry matching
+    assert_re_in("ab", ["", "abc", "laskdjf"])
+    assert_raises(AssertionError, assert_re_in, "ab$", ["ddd", ""])
+
+    # Tuples should be ok too
+    assert_re_in("ab", ("", "abc", "laskdjf"))
+    assert_raises(AssertionError, assert_re_in, "ab$", ("ddd", ""))
+
+    # shouldn't "match" the empty list
+    assert_raises(AssertionError, assert_re_in, "", [])
+
+
+def test_skip_if_no_network():
+    cleaned_env = os.environ.copy()
+    cleaned_env.pop('DATALAD_TESTS_NONETWORK', None)
+    # we need to run under cleaned env to make sure we actually test in both conditions
+    with patch('os.environ', cleaned_env):
+        @skip_if_no_network
+        def somefunc(a1):
+            return a1
+        #ok_(hasattr(somefunc, "network"))
+        with patch_config({'datalad.tests.nonetwork': '1'}):
+            assert_raises(Skipped, somefunc, 1)
+        with patch.dict('os.environ', {}):
+            eq_(somefunc(1), 1)
+        # and now if used as a function, not a decorator
+        with patch_config({'datalad.tests.nonetwork': '1'}):
+            assert_raises(Skipped, skip_if_no_network)
+        with patch.dict('os.environ', {}):
+            eq_(skip_if_no_network(), None)
+
+
+def test_skip_if_no_module():
+
+    def testish():
+        skip_if_no_module("nonexistingforsuremodule")
+        raise ValueError
+    assert_raises(Skipped, testish)
+
+    def testish2():
+        skip_if_no_module("datalad")
+        return "magic"
+    eq_(testish2(), "magic")
+
+
+def test_skip_if():
+
+    with assert_raises(Skipped):
+        @skip_if(True)
+        def f():  # pragma: no cover
+            raise AssertionError("must have not been ran")
+        f()
+
+    @skip_if(False)
+    def f():
+        return "magical"
+    eq_(f(), 'magical')
 
-    from datalad import cfg
 
-    skip = cfg.obtain("datalad.tests.knownfailures.skip")
-    probe = cfg.obtain("datalad.tests.knownfailures.probe")
+@assert_cwd_unchanged
+@with_tempfile(mkdir=True)
+def test_run_under_dir(d=None):
+    orig_pwd = getpwd()
+    orig_cwd = os.getcwd()
+
+    @run_under_dir(d)
+    def f(arg, kwarg=None):
+        eq_(arg, 1)
+        eq_(kwarg, 2)
+        eq_(getpwd(), d)
+
+    f(1, 2)
+    eq_(getpwd(), orig_pwd)
+    eq_(os.getcwd(), orig_cwd)
+
+    # and if fails
+    assert_raises(AssertionError, f, 1, 3)
+    eq_(getpwd(), orig_pwd)
+    eq_(os.getcwd(), orig_cwd)
+
+
+def test_assert_dict_equal():
+    assert_dict_equal({}, {})
+    assert_dict_equal({"a": 3}, {"a": 3})
+    assert_raises(AssertionError, assert_dict_equal, {1: 3}, {1: 4})
+    assert_raises(AssertionError, assert_dict_equal, {1: 3}, {2: 4})
+    assert_raises(AssertionError, assert_dict_equal, {1: 3}, {2: 4, 1: 3})
+    assert_raises(AssertionError, assert_dict_equal, {1: 3}, {2: 4, 1: 'a'})
+    try:
+        import numpy as np
+    except:  # pragma: no cover
+        pytest.skip("need numpy for this tiny one")
+    # one is scalar another one array
+    assert_raises(AssertionError, assert_dict_equal, {1: 0}, {1: np.arange(1)})
+    assert_raises(AssertionError, assert_dict_equal, {1: 0}, {1: np.arange(3)})
+
+
+def test_assert_str_equal():
+    assert_str_equal("a", "a")
+    assert_str_equal("a\n", "a\n")
+    assert_str_equal("a\nb", "a\nb")
+    assert_raises(AssertionError, assert_str_equal, "a", "a\n")
+    assert_raises(AssertionError, assert_str_equal, "a", "b")
+    assert_raises(AssertionError, assert_str_equal, "ab", "b")
 
-    if skip:
-        # skipping takes precedence over probing
-        failing()
-    elif probe:
-        # if we probe a known failure it's okay to fail:
-        failing()
-    else:
-        # not skipping and not probing results in the original failure:
-        assert_raises(AssertionError, failing)
 
+def test_testsui():
+    # just one for now to test conflicting arguments
+    with assert_raises(ValueError):
+        @with_testsui(responses='some', interactive=False)
+        def some_func():   # pragma: no cover
+            pass
 
-def test_known_failure_v6():
+    from datalad.ui import ui
 
-    @known_failure_v6
-    def failing():
-        raise AssertionError("Failed")
-
-    from datalad import cfg
-
-    v6 = cfg.obtain("datalad.repo.version") == 6
-    skip = cfg.obtain("datalad.tests.knownfailures.skip")
-    probe = cfg.obtain("datalad.tests.knownfailures.probe")
-
-    if v6:
-        if skip:
-            # skipping takes precedence over probing
-            failing()
-        elif probe:
-            # if we probe a known failure it's okay to fail:
-            failing()
-        else:
-            # not skipping and not probing results in the original failure:
-            assert_raises(AssertionError, failing)
+    @with_testsui(responses=['yes', "maybe so"])
+    def func2(x):
+        assert x == 1
+        eq_(ui.yesno("title"), True)
+        eq_(ui.question("title2"), "maybe so")
+        assert_raises(AssertionError, ui.question, "asking more than we know")
+        return x*2
+    eq_(func2(1), 2)
+
+    @with_testsui(interactive=False)
+    def func3(x):
+        assert_false(ui.is_interactive)
+        return x*3
+    eq_(func3(2), 6)
+
+
+def test_setup():
+    # just verify that we monkey patched consts correctly
+    from datalad.consts import DATASETS_TOPURL
+    eq_(DATASETS_TOPURL, 'https://datasets-tests.datalad.org/')
+    from datalad.tests.utils_pytest import get_datasets_topdir
+    eq_(get_datasets_topdir(), 'datasets-tests.datalad.org')
+
+
+def test_skip_ssh():
+    with patch_config({'datalad.tests.ssh': False}):
+        with assert_raises(Skipped):
+            skip_ssh(lambda: False)()
 
-    else:
-        # behaves as if it wasn't decorated at all, no matter what
-        assert_raises(AssertionError, failing)
 
+def test_probe_known_failure():
+    # should raise assert error if function no longer fails
+    with patch_config({'datalad.tests.knownfailures.probe': True}):
+        with assert_raises(Failed):
+            probe_known_failure(lambda: True)()
+
+    with patch_config({'datalad.tests.knownfailures.probe': False}):
+        ok_(probe_known_failure(lambda: True))
+
+
+def test_ignore_nose_capturing_stdout():
+    # Just test the logic, not really a situation under overwritten stdout
+    def raise_exc():
+        raise AttributeError('nose causes a message which includes words '
+                             'StringIO and fileno')
+    with assert_raises(AttributeError):
+        ignore_nose_capturing_stdout(raise_exc)()
+
+
+@skip_wo_symlink_capability
+@with_tree(tree={'ingit': '', 'staged': 'staged', 'notingit': ''})
+def test_ok_file_under_git_symlinks(path=None):
+    # Test that works correctly under symlinked path
+    orepo = GitRepo(path)
+    orepo.add('ingit')
+    orepo.commit('msg')
+    orepo.add('staged')
+    lpath = path + "-symlink"  # will also be removed AFAIK by our tempfile handling
+    Path(lpath).symlink_to(Path(path))
+    ok_symlink(lpath)
+    ok_file_under_git(op.join(path, 'ingit'))
+    ok_file_under_git(op.join(lpath, 'ingit'))
+    ok_file_under_git(op.join(lpath, 'staged'))
+    with assert_raises(AssertionError):
+        ok_file_under_git(op.join(lpath, 'notingit'))
+    with assert_raises(AssertionError):
+        ok_file_under_git(op.join(lpath, 'nonexisting'))
+
+
+def test_assert_raises():
+    # rudimentary test of assert_raises shim prompted by suspicion in
+    # https://github.com/datalad/datalad/issues/6846#issuecomment-1363878497
+    def raise_ValueError():
+        raise ValueError("exc ValueError")
+
+    def raise_TypeError():
+        raise TypeError("exc TypeError")
 
-def test_known_failure_direct_mode():
+    # Test both forms of use
+    with assert_raises(ValueError):
+        raise_ValueError()
+    assert_raises(ValueError, raise_ValueError)
 
-    @known_failure_direct_mode
-    def failing():
-        raise AssertionError("Failed")
-
-    from datalad import cfg
-
-    direct = cfg.obtain("datalad.repo.direct")
-    skip = cfg.obtain("datalad.tests.knownfailures.skip")
-    probe = cfg.obtain("datalad.tests.knownfailures.probe")
-
-    if direct:
-        if skip:
-            # skipping takes precedence over probing
-            failing()
-        elif probe:
-            # if we probe a known failure it's okay to fail:
-            failing()
-        else:
-            # not skipping and not probing results in the original failure:
-            assert_raises(AssertionError, failing)
+    # can we specify multiple in a tuple?
+    with assert_raises((ValueError, TypeError)):
+        raise_ValueError()
+    with assert_raises((ValueError, TypeError)):
+        raise_TypeError()
 
-    else:
-        # behaves as if it wasn't decorated at all, no matter what
-        assert_raises(AssertionError, failing)
+    assert_raises((ValueError, TypeError), raise_TypeError)
+    assert_raises((ValueError, TypeError), raise_ValueError)
```

### Comparing `datalad-0.9.3/datalad/tests/test_s3.py` & `datalad-1.0.0/datalad/tests/test_s3.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,39 +1,71 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 """Test S3 supporting functionality
 
 """
 
-
-from ..support.s3 import get_versioned_url
-from .utils import use_cassette
-from .utils import ok_startswith
-
-from nose.tools import eq_, assert_raises
-from datalad.tests.utils import skip_if_no_network
-from ..downloaders.tests.utils import get_test_providers
+from datalad.downloaders.tests.utils import get_test_providers
+from datalad.support.network import URL
+from datalad.support.s3 import (
+    add_version_to_url,
+    get_versioned_url,
+)
+from datalad.tests.utils_pytest import (
+    assert_raises,
+    eq_,
+    ok_startswith,
+    skip_if_no_network,
+    use_cassette,
+)
+
+
+def test_add_version_to_url():
+    base_url = "http://ex.com/f.txt"
+    base_url_query = "http://ex.com/f.txt?k=v"
+    for replace in True, False:
+        eq_(add_version_to_url(URL(base_url), "new.id", replace=replace),
+            base_url + "?versionId=new.id")
+
+        eq_(add_version_to_url(URL(base_url_query),
+                               "new.id", replace=replace),
+            base_url_query + "&versionId=new.id")
+
+        expected = "new.id" if replace else "orig.id"
+        eq_(add_version_to_url(URL(base_url + "?versionId=orig.id"),
+                               "new.id",
+                               replace=replace),
+            base_url + "?versionId=" + expected)
+
+        eq_(add_version_to_url(URL(base_url_query + "&versionId=orig.id"),
+                               "new.id",
+                               replace=replace),
+            base_url_query + "&versionId=" + expected)
 
 
 @skip_if_no_network
 @use_cassette('s3_test_version_url')
-def test_version_url():
+def test_get_versioned_url():
     get_test_providers('s3://openfmri/tarballs')  # to verify having credentials to access openfmri via S3
     for url_pref in ('http://openfmri.s3.amazonaws.com', 'https://s3.amazonaws.com/openfmri'):
         eq_(get_versioned_url(url_pref + "/tarballs/ds001_raw.tgz"),
             url_pref + "/tarballs/ds001_raw.tgz?versionId=null")
 
-        eq_(get_versioned_url("http://openfmri.s3.amazonaws.com/tarballs/ds001_raw.tgz?param=1"),
-            "http://openfmri.s3.amazonaws.com/tarballs/ds001_raw.tgz?param=1&versionId=null")
+        eq_(get_versioned_url(url_pref + "/tarballs/ds001_raw.tgz?param=1"),
+            url_pref + "/tarballs/ds001_raw.tgz?param=1&versionId=null")
+
+        # We don't duplicate the version if it already exists.
+        eq_(get_versioned_url(url_pref + "/tarballs/ds001_raw.tgz?versionId=null"),
+            url_pref + "/tarballs/ds001_raw.tgz?versionId=null")
 
     # something is wrong there
     #print(get_versioned_url("http://openfmri.s3.amazonaws.com/ds001/demographics.txt"))
 
     eq_(get_versioned_url("someurl"), "someurl")  # should just return original one
     assert_raises(RuntimeError, get_versioned_url, "someurl", guarantee_versioned=True)
 
@@ -47,21 +79,38 @@
     urls = get_versioned_url("http://datalad-test0-versioned.s3.amazonaws.com/2versions-removed-recreated.txt",
                              return_all=True, verify=True)
     eq_(len(set(urls)), len(urls))  # all unique
     for url in urls:
         # so we didn't grab other files along with the same prefix
         ok_startswith(url, 'http://datalad-test0-versioned.s3.amazonaws.com/2versions-removed-recreated.txt?versionId=')
 
+    # Update a versioned URL with a newer version tag.
+    url_3ver = "http://datalad-test0-versioned.s3.amazonaws.com/3versions-allversioned.txt"
+    url_3ver_input = url_3ver + "?versionId=b.qCuh7Sg58VIYj8TVHzbRS97EvejzEl"
+    eq_(get_versioned_url(url_3ver_input), url_3ver_input)
+    eq_(get_versioned_url(url_3ver_input, update=True),
+        url_3ver + "?versionId=Kvuind11HZh._dCPaDAb0OY9dRrQoTMn")
+
+
+@skip_if_no_network
+@use_cassette('s3_test_version_url_anon')
+def test_get_versioned_url_anon():
+    # The one without any authenticator, was crashing.
+    # Also it triggered another bug about having . in the bucket name
+    url_on = "http://dandiarchive.s3.amazonaws.com/ros3test.nwb"
+    url_on_versioned = get_versioned_url(url_on)
+    ok_startswith(url_on_versioned, url_on + "?versionId=")
+
 
 @skip_if_no_network
 @use_cassette('s3_test_version_url_deleted')
 def test_version_url_deleted():
     get_test_providers('s3://datalad-test0-versioned/', reload=True)  # to verify having credentials to access
     # openfmri via S3
     # it existed and then was removed
     fpath = "1version-removed.txt"
     url = "http://datalad-test0-versioned.s3.amazonaws.com/%s" % fpath
     turl = "http://datalad-test0-versioned.s3.amazonaws.com/%s" \
            "?versionId=eZ5Hgwo8azfBv3QT7aW9dmm2sbLUY.QP" % fpath
     eq_(get_versioned_url(url), turl)
     # too heavy for verification!
-    #eq_(get_versioned_url(url, verify=True), turl)
+    #eq_(get_versioned_url(url, verify=True), turl)
```

### Comparing `datalad-0.9.3/datalad/tests/utils_testdatasets.py` & `datalad-1.0.0/benchmarks/usecases.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,77 +1,98 @@
-# emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
 # ex: set sts=4 ts=4 sw=4 noet:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
+"""Benchmarks for some use cases, typically at datalad.api level"""
 
-
+import sys
+import tempfile
+from datalad.utils import get_tempfile_kwargs
+import os.path as osp
 from os.path import join as opj
-from datalad.api import create
-from datalad.tests.utils import create_tree
 
+from datalad.api import create
 
-def make_studyforrest_mockup(path):
-    """Generate a dataset structure mimicking aspects of studyforrest.org
+from datalad.utils import (
+    create_tree,
+    rmtree,
+)
 
-    Under the given path there are two directories:
+from .common import SuprocBenchmarks
 
-    public - to be published datasets
-    private - never to be published datasets
 
-    The 'public' directory itself is a superdataset, the 'private' directory
-    is just a directory that contains standalone datasets in subdirectories.
+class study_forrest(SuprocBenchmarks):
+    """
+    Benchmarks for Study Forrest use cases
     """
-    public = create(opj(path, 'public'), description="umbrella dataset")
-    # the following tries to capture the evolution of the project
-    phase1 = public.create('phase1',
-                           description='old-style, no connection to RAW')
-    structural = public.create('structural', description='anatomy')
-    tnt = public.create('tnt', description='image templates')
-    tnt.clone(source=phase1.path, path=opj('src', 'phase1'), reckless=True)
-    tnt.clone(source=structural.path, path=opj('src', 'structural'), reckless=True)
-    aligned = public.create('aligned', description='aligned image data')
-    aligned.clone(source=phase1.path, path=opj('src', 'phase1'), reckless=True)
-    aligned.clone(source=tnt.path, path=opj('src', 'tnt'), reckless=True)
-    # new acquisition
-    labet = create(opj(path, 'private', 'labet'), description="raw data ET")
-    phase2_dicoms = create(opj(path, 'private', 'p2dicoms'), description="raw data P2MRI")
-    phase2 = public.create('phase2',
-                           description='new-style, RAW connection')
-    phase2.clone(source=labet.path, path=opj('src', 'labet'), reckless=True)
-    phase2.clone(source=phase2_dicoms.path, path=opj('src', 'dicoms'), reckless=True)
-    # add to derivatives
-    tnt.clone(source=phase2.path, path=opj('src', 'phase2'), reckless=True)
-    aligned.clone(source=phase2.path, path=opj('src', 'phase2'), reckless=True)
-    # never to be published media files
-    media = create(opj(path, 'private', 'media'), description="raw data ET")
-    # assuming all annotations are in one dataset (in reality this is also
-    # a superdatasets with about 10 subdatasets
-    annot = public.create('annotations', description='stimulus annotation')
-    annot.clone(source=media.path, path=opj('src', 'media'), reckless=True)
-    # a few typical analysis datasets
-    # (just doing 3, actual status quo is just shy of 10)
-    # and also the real goal -> meta analysis
-    metaanalysis = public.create('metaanalysis', description="analysis of analyses")
-    for i in range(1, 3):
-        ana = public.create('analysis{}'.format(i),
-                            description='analysis{}'.format(i))
-        ana.clone(source=annot.path, path=opj('src', 'annot'), reckless=True)
-        ana.clone(source=aligned.path, path=opj('src', 'aligned'), reckless=True)
-        ana.clone(source=tnt.path, path=opj('src', 'tnt'), reckless=True)
-        # link to metaanalysis
-        metaanalysis.clone(source=ana.path, path=opj('src', 'ana{}'.format(i)),
-                           reckless=True)
-        # simulate change in an input (but not raw) dataset
-        create_tree(
-            aligned.path,
-            {'modification{}.txt'.format(i): 'unique{}'.format(i)})
-        aligned.add('.')
-    # finally aggregate data
-    aggregate = public.create('aggregate', description='aggregate data')
-    aggregate.clone(source=aligned.path, path=opj('src', 'aligned'), reckless=True)
-    # the toplevel dataset is intentionally left dirty, to reflect the
-    # most likely condition for the joint dataset to be in at any given
-    # point in time
+
+    timeout = 180  # especially with profiling might take longer than default 60s
+
+    def setup(self):
+        self.path = tempfile.mkdtemp(**get_tempfile_kwargs({}, prefix='bm_forrest'))
+
+    def teardown(self):
+        if osp.exists(self.path):
+            rmtree(self.path)
+
+    def time_make_studyforrest_mockup(self):
+        path = self.path
+        # Carries a copy of the
+        # datalad.tests.utils_testdatasets.py:make_studyforrest_mockup
+        # as of 0.12.0rc2-76-g6ba6d53b
+        # A copy is made so we do not reflect in the benchmark results changes
+        # to that helper's code.  This copy only tests on 2 not 3 analyses
+        # subds
+        public = create(opj(path, 'public'), description="umbrella dataset")
+        # the following tries to capture the evolution of the project
+        phase1 = public.create('phase1',
+                               description='old-style, no connection to RAW')
+        structural = public.create('structural', description='anatomy')
+        tnt = public.create('tnt', description='image templates')
+        tnt.clone(source=phase1.path, path=opj('src', 'phase1'), reckless=True)
+        tnt.clone(source=structural.path, path=opj('src', 'structural'), reckless=True)
+        aligned = public.create('aligned', description='aligned image data')
+        aligned.clone(source=phase1.path, path=opj('src', 'phase1'), reckless=True)
+        aligned.clone(source=tnt.path, path=opj('src', 'tnt'), reckless=True)
+        # new acquisition
+        labet = create(opj(path, 'private', 'labet'), description="raw data ET")
+        phase2_dicoms = create(opj(path, 'private', 'p2dicoms'), description="raw data P2MRI")
+        phase2 = public.create('phase2',
+                               description='new-style, RAW connection')
+        phase2.clone(source=labet.path, path=opj('src', 'labet'), reckless=True)
+        phase2.clone(source=phase2_dicoms.path, path=opj('src', 'dicoms'), reckless=True)
+        # add to derivatives
+        tnt.clone(source=phase2.path, path=opj('src', 'phase2'), reckless=True)
+        aligned.clone(source=phase2.path, path=opj('src', 'phase2'), reckless=True)
+        # never to be published media files
+        media = create(opj(path, 'private', 'media'), description="raw data ET")
+        # assuming all annotations are in one dataset (in reality this is also
+        # a superdatasets with about 10 subdatasets
+        annot = public.create('annotations', description='stimulus annotation')
+        annot.clone(source=media.path, path=opj('src', 'media'), reckless=True)
+        # a few typical analysis datasets
+        # (just doing 2, actual status quo is just shy of 10)
+        # and also the real goal -> meta analysis
+        metaanalysis = public.create('metaanalysis', description="analysis of analyses")
+        for i in range(1, 2):
+            ana = public.create('analysis{}'.format(i),
+                                description='analysis{}'.format(i))
+            ana.clone(source=annot.path, path=opj('src', 'annot'), reckless=True)
+            ana.clone(source=aligned.path, path=opj('src', 'aligned'), reckless=True)
+            ana.clone(source=tnt.path, path=opj('src', 'tnt'), reckless=True)
+            # link to metaanalysis
+            metaanalysis.clone(source=ana.path, path=opj('src', 'ana{}'.format(i)),
+                               reckless=True)
+            # simulate change in an input (but not raw) dataset
+            create_tree(
+                aligned.path,
+                {'modification{}.txt'.format(i): 'unique{}'.format(i)})
+            aligned.save('.')
+        # finally aggregate data
+        aggregate = public.create('aggregate', description='aggregate data')
+        aggregate.clone(source=aligned.path, path=opj('src', 'aligned'), reckless=True)
+        # the toplevel dataset is intentionally left dirty, to reflect the
+        # most likely condition for the joint dataset to be in at any given
+        # point in time
```

### Comparing `datalad-0.9.3/datalad/tests/__init__.py` & `datalad-1.0.0/datalad/tests/__init__.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
```

### Comparing `datalad-0.9.3/datalad/tests/test_cmd.py` & `datalad-1.0.0/datalad/runner/tests/test_witless_runner.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,325 +1,469 @@
 # emacs: -*- mode: python-mode; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil; coding: utf-8 -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""Test command call wrapper
+"""Test WitlessRunner
 """
 
-from mock import patch
+from __future__ import annotations
+
+import logging
 import os
-from os.path import dirname, join as opj
+import signal
 import sys
-import logging
-import shlex
+import unittest.mock
+from threading import (
+    Lock,
+    Thread,
+)
+from time import (
+    sleep,
+    time,
+)
+from typing import Any
+
+import pytest
+
+from datalad.tests.utils_pytest import (
+    OBSCURE_FILENAME,
+    SkipTest,
+    assert_cwd_unchanged,
+    assert_in,
+    assert_raises,
+    eq_,
+    integration,
+    ok_,
+    ok_file_has_content,
+    skip_if_on_windows,
+    swallow_logs,
+    with_tempfile,
+)
+from datalad.utils import (
+    CMD_MAX_ARG,
+    Path,
+    on_windows,
+)
+
+from .. import (
+    CommandError,
+    KillOutput,
+    Protocol,
+    Runner,
+    StdOutCapture,
+    StdOutErrCapture,
+)
+from .utils import py2cmd
 
-from .utils import ok_, eq_, assert_is, assert_equal, assert_false, \
-    assert_true, assert_greater, assert_raises, assert_in, SkipTest
+result_counter = 0
 
-from ..cmd import Runner, link_file_load
-from ..cmd import GitRunner
-from ..support.exceptions import CommandError
-from ..support.protocol import DryRunProtocol
-from .utils import with_tempfile, assert_cwd_unchanged, \
-    ignore_nose_capturing_stdout, swallow_outputs, swallow_logs, \
-    on_linux, on_osx, on_windows, with_testrepos
-from .utils import lgr
-from ..utils import assure_unicode
 
-from .utils import local_testrepo_flavors
-
-
-@ignore_nose_capturing_stdout
 @assert_cwd_unchanged
 @with_tempfile
-def test_runner_dry(tempfile):
+def test_runner(tempfile: str = "") -> None:
+    runner = Runner()
+    content = 'Testing real run' if on_windows else 'Testing äöü東 real run'
+    cmd = 'echo %s > %s' % (content, tempfile)
+    res = runner.run(cmd)
+    assert isinstance(res, dict)
+    # no capture of any kind, by default
+    ok_(not res['stdout'])
+    ok_(not res['stderr'])
+    ok_file_has_content(tempfile, content, strip=True)
+    os.unlink(tempfile)
 
-    dry = DryRunProtocol()
-    runner = Runner(protocol=dry)
 
-    # test dry command call
-    cmd = 'echo Testing dry run > %s' % tempfile
-    with swallow_logs(new_level=9) as cml:
-        ret = runner.run(cmd)
-        cml.assert_logged("{DryRunProtocol} Running: %s" % cmd, regex=False)
-    assert_equal(("DRY", "DRY"), ret,
-                 "Output of dry run (%s): %s" % (cmd, ret))
-    assert_equal(shlex.split(cmd, posix=not on_windows), dry[0]['command'])
-    assert_false(os.path.exists(tempfile))
-
-    # test dry python function call
-    output = runner.call(os.path.join, 'foo', 'bar')
-    assert_is(None, output, "Dry call of: os.path.join, 'foo', 'bar' "
-                            "returned: %s" % output)
-    assert_in('join', dry[1]['command'][0])
-    assert_equal("args=('foo', 'bar')", dry[1]['command'][1])
+def test_runner_stderr_capture() -> None:
+    runner = Runner()
+    test_msg = "stderr-Message"
+    res = runner.run(py2cmd(
+        'import sys; print(%r, file=sys.stderr)' % test_msg),
+        protocol=StdOutErrCapture,
+    )
+    assert isinstance(res, dict)
+    eq_(res['stderr'].rstrip(), test_msg)
+    ok_(not res['stdout'])
 
 
-@ignore_nose_capturing_stdout
-@assert_cwd_unchanged
-@with_tempfile
-def test_runner(tempfile):
+def test_runner_stdout_capture() -> None:
+    runner = Runner()
+    test_msg = "stdout-Message"
+    res = runner.run(py2cmd(
+        'import sys; print(%r, file=sys.stdout)' % test_msg),
+        protocol=StdOutErrCapture,
+    )
+    assert isinstance(res, dict)
+    eq_(res['stdout'].rstrip(), test_msg)
+    ok_(not res['stderr'])
+
 
-    # test non-dry command call
+def test_runner_failure() -> None:
     runner = Runner()
-    cmd = 'echo Testing real run > %r' % tempfile
-    ret = runner.run(cmd)
-    assert_true(os.path.exists(tempfile),
-                "Run of: %s resulted with non-existing file %s" %
-                (cmd, tempfile))
-
-    # test non-dry python function call
-    output = runner.call(os.path.join, 'foo', 'bar')
-    assert_equal(os.path.join('foo', 'bar'), output,
-                 "Call of: os.path.join, 'foo', 'bar' returned %s" % output)
-
-
-@ignore_nose_capturing_stdout
-def test_runner_instance_callable_dry():
-
-    cmd_ = ['echo', 'Testing', '__call__', 'with', 'string']
-    for cmd in [cmd_, ' '.join(cmd_)]:
-        dry = DryRunProtocol()
-        runner = Runner(protocol=dry)
-        ret = runner(cmd)
-        # (stdout, stderr) is returned.  But in dry -- ("DRY","DRY")
-        eq_(ret, ("DRY", "DRY"))
-        assert_equal(cmd_, dry[0]['command'],
-                     "Dry run of Runner.__call__ didn't record command: %s.\n"
-                     "Buffer: %s" % (cmd, dry))
-
-    ret = runner(os.path.join, 'foo', 'bar')
-    eq_(ret, None)
-
-    assert_in('join', dry[1]['command'][0],
-              "Dry run of Runner.__call__ didn't record function join()."
-              "Buffer: %s" % dry)
-    assert_equal("args=('foo', 'bar')", dry[1]['command'][1],
-                 "Dry run of Runner.__call__ didn't record function join()."
-                 "Buffer: %s" % dry)
-
-
-@ignore_nose_capturing_stdout
-def test_runner_instance_callable_wet():
-
-    runner = Runner()
-    cmd = [sys.executable, "-c", "print('Testing')"]
-
-    out = runner(cmd)
-    eq_(out[0].rstrip(), ('Testing'))
-    eq_(out[1], '')
-
-    ret = runner(os.path.join, 'foo', 'bar')
-    eq_(ret, os.path.join('foo', 'bar'))
-
-
-@ignore_nose_capturing_stdout
-def test_runner_log_stderr():
-
-    runner = Runner(log_outputs=True)
-    cmd = 'echo stderr-Message should be logged >&2'
-    with swallow_outputs() as cmo:
-        with swallow_logs(new_level=9) as cml:
-            ret = runner.run(cmd, log_stderr=True, expect_stderr=True)
-            cml.assert_logged("Running: %s" % cmd, level='Level 9', regex=False)
-            if not on_windows:
-                # we can just count on sanity
-                cml.assert_logged("stderr| stderr-"
-                                  "Message should be logged", regex=False)
-            else:
-                # echo outputs quoted lines for some reason, so relax check
-                ok_("stdout-Message should be logged" in cml.lines[1])
-
-    cmd = 'echo stderr-Message should not be logged >&2'
-    with swallow_outputs() as cmo:
-        with swallow_logs(new_level=9) as cml:
-            ret = runner.run(cmd, log_stderr=False)
-            eq_(cmo.err.rstrip(), "stderr-Message should not be logged")
-            assert_raises(AssertionError, cml.assert_logged,
-                          "stderr| stderr-Message should not be logged")
-
-
-@ignore_nose_capturing_stdout
-def test_runner_log_stdout():
-    # TODO: no idea of how to check correct logging via any kind of
-    # assertion yet.
-
-    runner = Runner(log_outputs=True)
-    cmd_ = ['echo', 'stdout-Message should be logged']
-    for cmd in [cmd_, ' '.join(cmd_)]:
-        # should be identical runs, either as a string or as a list
-        kw = {}
-        # on Windows it can't find echo if ran outside the shell
-        if on_windows and isinstance(cmd, list):
-            kw['shell'] = True
-        with swallow_logs(9) as cm:
-            ret = runner.run(cmd, log_stdout=True, **kw)
-            cm.assert_logged("Running: %s" % cmd, level='Level 9', regex=False)
-            if not on_windows:
-                # we can just count on sanity
-                cm.assert_logged("stdout| stdout-"
-                                 "Message should be logged", regex=False)
-            else:
-                # echo outputs quoted lines for some reason, so relax check
-                ok_("stdout-Message should be logged" in cm.lines[1])
-
-    cmd = 'echo stdout-Message should not be logged'
-    with swallow_outputs() as cmo:
-        with swallow_logs(new_level=11) as cml:
-            ret = runner.run(cmd, log_stdout=False)
-            eq_(cmo.out, "stdout-Message should not be logged\n")
-            eq_(cml.out, "")
-
-
-@ignore_nose_capturing_stdout
-def check_runner_heavy_output(log_online):
-    # TODO: again, no automatic detection of this resulting in being
-    # stucked yet.
-
-    runner = Runner()
-    cmd = '%s %s' % (sys.executable, opj(dirname(__file__), "heavyoutput.py"))
-
-    with swallow_outputs() as cm, swallow_logs():
-        ret = runner.run(cmd,
-                         log_online=log_online,
-                         log_stderr=False, log_stdout=False,
-                         expect_stderr=True)
-        eq_(cm.err, cm.out)  # they are identical in that script
-        eq_(cm.out[:10], "0 [0, 1, 2")
-        eq_(cm.out[-15:], "997, 998, 999]\n")
-
-    # for some reason swallow_logs is not effective, so we just skip altogether
-    # if too heavy debug output
-    if lgr.getEffectiveLevel() <= logging.DEBUG:
-        raise SkipTest("Skipping due to too heavy impact on logs complicating debugging")
-
-    #do it again with capturing:
-    with swallow_logs():
-        ret = runner.run(cmd,
-                         log_online=True, log_stderr=True, log_stdout=True,
-                         expect_stderr=True)
-
-    if log_online:
-        # halting case of datalad add and other batch commands #2116
-        logged = []
-        with swallow_logs():
-            def process_stdout(l):
-                assert l
-                logged.append(l)
-            ret = runner.run(
-                cmd,
-                log_online=log_online,
-                log_stdout=process_stdout,
-                log_stderr='offline',
-                expect_stderr=True
-            )
-        assert_equal(len(logged), 100)
-        assert_greater(len(ret[1]), 1000)  # stderr all here
-        assert not ret[0], "all messages went into `logged`"
+    with assert_raises(CommandError) as cme:
+        runner.run(
+            py2cmd('import sys; sys.exit(53)')
+        )
+    eq_(53, cme.value.code)
+
+    # but we bubble up FileNotFoundError if executable does not exist at all
+    with assert_raises(FileNotFoundError) as cme:
+        runner.run(['dne1l2k3j4'])  # be damned the one who makes such a command
 
 
-def test_runner_heavy_output():
-    for log_online in [False, True]:
-        yield check_runner_heavy_output, log_online
+@with_tempfile(mkdir=True)
+def test_runner_fix_PWD(path: str = "") -> None:
+    env = os.environ.copy()
+    env['PWD'] = orig_cwd = os.getcwd()
+    runner = Runner(cwd=path, env=env)
+    res = runner.run(
+        py2cmd('import os; print(os.environ["PWD"])'),
+        protocol=StdOutCapture,
+    )
+    assert isinstance(res, dict)
+    eq_(res['stdout'].strip(), path)  # was fixed up to point to point to cwd's path
+    eq_(env['PWD'], orig_cwd)  # no side-effect
 
 
-@with_tempfile
-def test_link_file_load(tempfile):
-    tempfile2 = tempfile + '_'
+@with_tempfile(mkdir=True)
+def test_runner_cwd_encoding(path: str = "") -> None:
+    env = os.environ.copy()
+    # Add PWD to env so that runner will temporarily adjust it to point to cwd.
+    env['PWD'] = os.getcwd()
+    cwd = Path(path) / OBSCURE_FILENAME
+    cwd.mkdir()
+    # Running doesn't fail if cwd or env has unicode value.
+    Runner(cwd=cwd, env=env).run(
+        py2cmd(
+            'from pathlib import Path; (Path.cwd() / "foo").write_text("t")'))
+    (cwd / 'foo').exists()
+
+
+@with_tempfile(mkdir=True)
+def test_runner_stdin(path: str = "") -> None:
+    runner = Runner()
+    fakestdin = Path(path) / 'io'
+    # go for difficult content
+    fakestdin.write_text(OBSCURE_FILENAME)
+
+    res = runner.run(
+        py2cmd('import fileinput; print(fileinput.input().readline())'),
+        stdin=fakestdin.open(),
+        protocol=StdOutCapture,
+    )
+    assert isinstance(res, dict)
+    assert_in(OBSCURE_FILENAME, res['stdout'])
+
+    # we can do the same without a tempfile, too
+    res = runner.run(
+        py2cmd('import fileinput; print(fileinput.input().readline())'),
+        stdin=OBSCURE_FILENAME.encode('utf-8'),
+        protocol=StdOutCapture,
+    )
+    assert isinstance(res, dict)
+    assert_in(OBSCURE_FILENAME, res['stdout'])
+
+
+@pytest.mark.fail_slow(3)
+def test_runner_stdin_no_capture() -> None:
+    # Ensure that stdin writing alone progresses
+    runner = Runner()
+    runner.run(
+        py2cmd('import sys; print(sys.stdin.read()[-10:])'),
+        stdin=('ABCDEFGHIJKLMNOPQRSTUVWXYZ-' * 2 + '\n').encode('utf-8'),
+        protocol=None
+    )
 
-    with open(tempfile, 'w') as f:
-        f.write("LOAD")
 
-    link_file_load(tempfile, tempfile2)  # this should work in general
+@pytest.mark.fail_slow(3)
+def test_runner_no_stdin_no_capture() -> None:
+    # Ensure a runner without stdin data and output capture progresses
+    runner = Runner()
+    runner.run(
+        (["cmd.exe", "/c"] if on_windows else []) + ["echo", "a", "b", "c"],
+        stdin=None,
+        protocol=None
+    )
 
-    ok_(os.path.exists(tempfile2))
-
-    with open(tempfile2, 'r') as f:
-        assert_equal(f.read(), "LOAD")
-
-    def inode(fname):
-        with open(fname) as fd:
-            return os.fstat(fd.fileno()).st_ino
-
-    def stats(fname, times=True):
-        """Return stats on the file which should have been preserved"""
-        with open(fname) as fd:
-            st = os.fstat(fd.fileno())
-            stats = (st.st_mode, st.st_uid, st.st_gid, st.st_size)
-            if times:
-                return stats + (st.st_atime, st.st_mtime)
-            else:
-                return stats
-            # despite copystat mtime is not copied. TODO
-            #        st.st_mtime)
-
-    if on_linux or on_osx:
-        # above call should result in the hardlink
-        assert_equal(inode(tempfile), inode(tempfile2))
-        assert_equal(stats(tempfile), stats(tempfile2))
-
-        # and if we mock absence of .link
-        def raise_AttributeError(*args):
-            raise AttributeError("TEST")
-
-        with patch('os.link', raise_AttributeError):
-            with swallow_logs(logging.WARNING) as cm:
-                link_file_load(tempfile, tempfile2)  # should still work
-                ok_("failed (TEST), copying file" in cm.out)
-
-    # should be a copy (either originally for windows, or after mocked call)
-    ok_(inode(tempfile) != inode(tempfile2))
-    with open(tempfile2, 'r') as f:
-        assert_equal(f.read(), "LOAD")
-    assert_equal(stats(tempfile, times=False), stats(tempfile2, times=False))
-    os.unlink(tempfile2)  # TODO: next two with_tempfile
 
+@pytest.mark.fail_slow(3)
+def test_runner_empty_stdin() -> None:
+    # Ensure a runner without stdin data and output capture progresses
+    runner = Runner()
+    runner.run(
+        py2cmd('import sys; print(sys.stdin.read())'),
+        stdin=b"",
+        protocol=None
+    )
 
-@with_tempfile(mkdir=True)
-def test_runner_failure(dir_):
-    from ..support.annexrepo import AnnexRepo
-    repo = AnnexRepo(dir_, create=True)
-    runner = Runner()
-    failing_cmd = ['git-annex', 'add', 'notexistent.dat']
-
-    with assert_raises(CommandError) as cme, \
-         swallow_logs() as cml:
-        runner.run(failing_cmd, cwd=dir_)
-        assert_in('notexistent.dat not found', cml.out)
-    assert_equal(1, cme.exception.code)
 
+def test_runner_parametrized_protocol() -> None:
+    runner = Runner()
 
+    # protocol returns a given value whatever it receives
+    class ProtocolInt(StdOutCapture):
+        def __init__(self, value: bytes) -> None:
+            self.value = value
+            super().__init__()
+
+        def pipe_data_received(self, fd: int, data: bytes) -> None:
+            super().pipe_data_received(fd, self.value)
+
+    res = runner.run(
+        py2cmd('print(1, end="")'),
+        protocol=ProtocolInt,
+        # value passed to protocol constructor
+        value=b'5',
+    )
+    assert isinstance(res, dict)
+    eq_(res['stdout'], '5')
+
+
+@integration  # ~3 sec
 @with_tempfile(mkdir=True)
-def test_git_path(dir_):
-    from ..support.gitrepo import GitRepo
-    # As soon as we use any GitRepo we should get _GIT_PATH set in the Runner
-    repo = GitRepo(dir_, create=True)
-    assert GitRunner._GIT_PATH is not None
+@with_tempfile()
+def test_asyncio_loop_noninterference1(path1: str = "", path2: str = "") -> None:
+    if on_windows and sys.version_info < (3, 8):
+        raise SkipTest(
+            "get_event_loop() raises "
+            "RuntimeError: There is no current event loop in thread 'MainThread'.")
+    # minimalistic use case provided by Dorota
+    import datalad.api as dl
+    src = dl.create(path1)  # type: ignore[attr-defined]
+    reproducer = src.pathobj/ "reproducer.py"
+    reproducer.write_text(f"""\
+import asyncio
+asyncio.get_event_loop()
+import datalad.api as datalad
+ds = datalad.clone(path=r'{path2}', source=r"{path1}")
+loop = asyncio.get_event_loop()
+assert loop
+# simulate outside process closing the loop
+loop.close()
+# and us still doing ok
+ds.status()
+""")
+    Runner().run([sys.executable, str(reproducer)])  # if Error -- the test failed
+
+
+@with_tempfile
+def test_asyncio_forked(temp_: str = "") -> None:
+    # temp will be used to communicate from child either it succeeded or not
+    temp = Path(temp_)
+    runner = Runner()
+    try:
+        pid = os.fork()
+    except BaseException as exc:
+        # .fork availability is "Unix", and there are cases where it is "not supported"
+        # so we will just skip if no forking is possible
+        raise SkipTest(f"Cannot fork: {exc}")
+    # if does not fail (in original or in a fork) -- we are good
+    if sys.version_info < (3, 8) and pid != 0:
+        # for some reason it is crucial to sleep a little (but 0.001 is not enough)
+        # in the master process with older pythons or it takes forever to make the child run
+        sleep(0.1)
+    try:
+        runner.run([sys.executable, '--version'], protocol=StdOutCapture)
+        if pid == 0:
+            temp.write_text("I rule")
+    except:
+        if pid == 0:
+            temp.write_text("I suck")
+    if pid != 0:
+       # parent: look after the child
+       t0 = time()
+       try:
+           while not temp.exists() or temp.stat().st_size < 6:
+               if time() - t0 > 5:
+                   raise AssertionError("Child process did not create a file we expected!")
+       finally:
+           # kill the child
+           os.kill(pid, signal.SIGTERM)
+       # see if it was a good one
+       eq_(temp.read_text(), "I rule")
+    else:
+       # sleep enough so parent just kills me the kid before I continue doing bad deeds
+       sleep(10)
+
+
+def test_done_deprecation() -> None:
+    with unittest.mock.patch("datalad.cmd.warnings.warn") as warn_mock:
+        _ = Protocol("done")
+        warn_mock.assert_called_once()
+
+    with unittest.mock.patch("datalad.cmd.warnings.warn") as warn_mock:
+        _ = Protocol()
+        warn_mock.assert_not_called()
+
+
+def test_faulty_poll_detection() -> None:
+    popen_mock = unittest.mock.MagicMock(**{"pid": 666, "poll.return_value": None})
+    protocol = Protocol()
+    protocol.process = popen_mock
+    assert_raises(CommandError, protocol._prepare_result)
+
+
+def test_kill_output() -> None:
+    runner = Runner()
+    res = runner.run(
+        py2cmd('import sys; sys.stdout.write("aaaa\\n"); sys.stderr.write("bbbb\\n")'),
+        protocol=KillOutput)
+    assert isinstance(res, dict)
+    eq_(res['stdout'], '')
+    eq_(res['stderr'], '')
+
+
+@skip_if_on_windows  # no "hint" on windows since no ulimit command there
+def test_too_long() -> None:
+    with swallow_logs(new_level=logging.ERROR) as cml:
+        with assert_raises(OSError):  # we still raise an exception if we exceed too much
+            Runner().run(
+                [sys.executable, '-c', 'import sys; print(len(sys.argv))'] + [str(i) for i in range(CMD_MAX_ARG)],
+                protocol=StdOutCapture
+            )
+        cml.assert_logged('.*use.*ulimit.*')
+
+
+def test_path_to_str_conversion() -> None:
+    # Regression test to ensure that Path-objects are converted into strings
+    # before they are put into the environment variable `$PWD`
+    runner = Runner()
+    test_path = Path("a/b/c")
+    adjusted_env = runner._get_adjusted_env(
+        cwd=test_path,
+        env=dict(some_key="value")
+    )
+    assert adjusted_env is not None
+    assert str(test_path) == adjusted_env['PWD']
+
+
+def test_env_copying() -> None:
+    # Regression test to ensure environments are only copied
+    # if `copy=True` is given to `Runner._get_adjusted_env.`
+    # Test also for path adjustments, if not-`None` `pwd`-value
+    # is given to `Runner._get_adjusted_env`.
+    runner = Runner()
+    for original_env in (None, dict(some_key='value')):
+        for cwd in (None, Path('a/b/c')):
+            for do_copy in (True, False):
+                adjusted_env = runner._get_adjusted_env(
+                    cwd=cwd,
+                    env=original_env,
+                    copy=do_copy
+                )
+                if original_env is None:
+                    assert adjusted_env is None
+                else:
+                    assert adjusted_env is not None
+                    if do_copy is True:
+                        assert adjusted_env is not original_env
+                    else:
+                        assert adjusted_env is original_env
+                    if cwd is None:
+                        assert 'PWD' not in adjusted_env
+                    else:
+                        assert 'PWD' in adjusted_env
 
 
 @with_tempfile(mkdir=True)
-def test_runner_stdin(path):
+def test_environment(temp_dir_path: str = "") -> None:
+    # Ensure that the subprocess sees a string in `$PWD`, even if a Path-object
+    # is provided to `cwd`.
+    cmd = py2cmd("import os; print(os.environ['PWD'])")
+    cwd = Path(temp_dir_path)
+    env = dict(SYSTEMROOT=os.environ.get('SYSTEMROOT', ''))
     runner = Runner()
-    with open(opj(path, "test_input.txt"), "w") as f:
-        f.write("whatever")
+    results = runner.run(cmd=cmd, protocol=StdOutCapture, cwd=cwd, env=env)
+    assert isinstance(results, dict)
+    output = results['stdout'].splitlines()[0]
+    assert output == temp_dir_path
+
+    runner = Runner(cwd=cwd, env=env)
+    results = runner.run(cmd=cmd, protocol=StdOutCapture)
+    assert isinstance(results, dict)
+    output = results['stdout'].splitlines()[0]
+    assert output == temp_dir_path
+
+
+def test_argument_priority() -> None:
+    class X:
+        def __init__(self, *args: Any, **kwargs: Any) -> None:
+            self.args = args
+            self.kwargs = kwargs
+
+        def run(self) -> dict:
+            return dict(
+                code=0,
+                args=self.args,
+                kwargs=self.kwargs,
+            )
 
-    with swallow_outputs() as cmo, open(opj(path, "test_input.txt"), "r") as fake_input:
-        runner.run(['cat'], log_stdout=False, stdin=fake_input)
-        assert_in("whatever", cmo.out)
+    test_path_1 = "a/b/c"
+    test_env_1 = dict(source="constructor")
+    test_path_2 = "d/e/f"
+    test_env_2 = dict(source="run-method")
+
+    with unittest.mock.patch('datalad.runner.runner.ThreadedRunner') as tr_mock:
+
+        tr_mock.side_effect = X
+        runner = Runner(cwd=test_path_1, env=test_env_1)
+
+        result = runner.run("first-command")
+        assert isinstance(result, dict)
+        assert result['kwargs']['cwd'] == test_path_1
+        assert result['kwargs']['env'] == {
+            **test_env_1,
+            'PWD': test_path_1
+        }
+
+        result = runner.run("second-command", cwd=test_path_2, env=test_env_2)
+        assert isinstance(result, dict)
+        assert result['kwargs']['cwd'] == test_path_2
+        assert result['kwargs']['env'] == {
+            **test_env_2,
+            'PWD': test_path_2
+        }
 
 
-def test_process_remaining_output():
+def test_concurrent_execution() -> None:
     runner = Runner()
-    out = u"""\
-s
-п
-"""
-    out_bytes = out.encode('utf-8')
-    target = u"sп".encode('utf-8')
-    args = ['stdout', None, False, False]
-    #  probably #2185
-    eq_(runner._process_remaining_output(None, out_bytes, *args), target)
-    eq_(runner._process_remaining_output(None, out, *args), target)
+    caller_threads = []
+
+    result_list: list[str] = []
+    result_list_lock = Lock()
+
+    def target(count: int, r_list: list[str], r_list_lock: Lock) -> None:
+        result = runner.run(
+            py2cmd(
+                "import time;"
+                "import sys;"
+                "time.sleep(1);"
+                "print('end', sys.argv[1])",
+                str(count)
+            ),
+            protocol=StdOutCapture,
+        )
+        assert isinstance(result, dict)
+        output = result["stdout"].strip()
+        assert output == f"end {str(count)}"
+        with r_list_lock:
+            r_list.append(output)
+
+    for c in range(100):
+        caller_thread = Thread(
+            target=target,
+            kwargs=dict(
+                count=c,
+                r_list=result_list,
+                r_list_lock=result_list_lock,
+            ))
+        caller_thread.start()
+        caller_threads.append(caller_thread)
+
+    while caller_threads:
+        t = caller_threads.pop()
+        t.join()
+
+    assert len(result_list) == 100
```

### Comparing `datalad-0.9.3/datalad/tests/utils.py` & `datalad-1.0.0/datalad/tests/test_utils.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,1433 +1,1435 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# -*- coding: utf-8 -*-
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""Miscellaneous utilities to assist with testing"""
+"""Test testing utilities
+
+"""
 
-import glob
 import inspect
+import logging
+import os
+import os.path as op
 import shutil
 import stat
-from json import dumps
-import os
-import re
-import tempfile
-import platform
-import multiprocessing
-import logging
-import random
-import socket
-from six import PY2, text_type, iteritems
-from six import binary_type
-from fnmatch import fnmatch
+import sys
 import time
-from mock import patch
-
-from six.moves.SimpleHTTPServer import SimpleHTTPRequestHandler
-from six.moves.BaseHTTPServer import HTTPServer
-from six import reraise
-from six.moves import map
-
 from functools import wraps
-from os.path import exists, realpath, join as opj, pardir, split as pathsplit, curdir
-from os.path import relpath
-
-from nose.tools import \
-    assert_equal, assert_not_equal, assert_raises, assert_greater, assert_true, assert_false, \
-    assert_in, assert_not_in, assert_in as in_, assert_is, \
-    raises, ok_, eq_, make_decorator
-
-from nose.tools import assert_set_equal
-from nose.tools import assert_is_instance
-from nose import SkipTest
-
-from ..cmd import Runner
-from ..utils import *
-from ..support.exceptions import CommandNotAvailableError
-from ..support.vcr_ import *
-from ..support.keyring_ import MemoryKeyring
-from ..support.network import RI
-from ..dochelpers import exc_str, borrowkwargs
-from ..cmdline.helpers import get_repo_instance
-from ..consts import (
-    ARCHIVES_TEMP_DIR,
+from operator import itemgetter
+from os.path import (
+    abspath,
+    basename,
+    dirname,
+    exists,
+    expanduser,
+    expandvars,
+    isabs,
 )
-from . import _TEMP_PATHS_GENERATED
-
-# temp paths used by clones
-_TEMP_PATHS_CLONES = set()
-
-
-# additional shortcuts
-neq_ = assert_not_equal
-nok_ = assert_false
-
-lgr = logging.getLogger("datalad.tests.utils")
-
-
-def skip_if_no_module(module):
-    try:
-        imp = __import__(module)
-    except Exception as exc:
-        raise SkipTest("Module %s fails to load: %s" % (module, exc_str(exc)))
-
-
-def skip_if_scrapy_without_selector():
-    """A little helper to skip some tests which require recent scrapy"""
-    try:
-        import scrapy
-        from scrapy.selector import Selector
-    except ImportError:
-        from nose import SkipTest
-        raise SkipTest(
-            "scrapy misses Selector (too old? version: %s)"
-            % getattr(scrapy, '__version__'))
+from os.path import join as opj
+from os.path import (
+    normpath,
+    pardir,
+)
+from unittest.mock import patch
 
+import pytest
 
-def skip_if_url_is_not_available(url, regex=None):
-    # verify that dataset is available
-    from datalad.downloaders.providers import Providers
-    from datalad.downloaders.base import DownloadError
-    providers = Providers.from_config_files()
-    try:
-        content = providers.fetch(url)
-        if regex and re.search(regex, content):
-            raise SkipTest("%s matched %r -- skipping the test" % (url, regex))
-    except DownloadError:
-        raise SkipTest("%s failed to download" % url)
+from datalad import cfg as dl_cfg
+from datalad.support.annexrepo import AnnexRepo
+from datalad.utils import (
+    CMD_MAX_ARG,
+    Path,
+    _path_,
+    any_re_search,
+    auto_repr,
+    better_wraps,
+    chpwd,
+    create_tree,
+    disable_logger,
+    dlabspath,
+    ensure_write_permission,
+    expandpath,
+    file_basename,
+    find_files,
+    generate_chunks,
+    get_dataset_root,
+    get_open_files,
+    get_path_prefix,
+    get_sig_param_names,
+    get_timestamp_suffix,
+    get_trace,
+    getargspec,
+    getpwd,
+    import_module_from_file,
+    import_modules,
+    is_explicit_path,
+    is_interactive,
+    join_cmdline,
+    knows_annex,
+    line_profile,
+    make_tempfile,
+    map_items,
+    md5sum,
+    never_fail,
+    not_supported_on_windows,
+    obtain_write_permission,
+    on_windows,
+    partition,
+    path_is_subpath,
+    path_startswith,
+    rotree,
+    split_cmdline,
+    swallow_logs,
+    swallow_outputs,
+    todo_interface_for_extensions,
+    unique,
+    unlink,
+    updated,
+)
 
+from .utils_pytest import (
+    OBSCURE_FILENAME,
+    SkipTest,
+    assert_cwd_unchanged,
+    assert_equal,
+    assert_false,
+    assert_greater,
+    assert_in,
+    assert_not_in,
+    assert_raises,
+    assert_true,
+    ensure_bool,
+    ensure_dict_from_str,
+    ensure_iter,
+    ensure_list,
+    ensure_list_from_str,
+    ensure_unicode,
+    eq_,
+    has_symlink_capability,
+    known_failure,
+    nok_,
+    ok_,
+    ok_file_has_content,
+    ok_generator,
+    ok_startswith,
+    on_travis,
+    probe_known_failure,
+    skip_if,
+    skip_if_no_module,
+    skip_if_on_windows,
+    skip_if_root,
+    skip_known_failure,
+    skip_wo_symlink_capability,
+    with_tempfile,
+    with_tree,
+)
 
-# TODO: eventually we might want to make use of attr module
-class File(object):
-    """Helper for a file entry in the create_tree/@with_tree
 
-    It allows to define additional settings for entries
-    """
-    def __init__(self, name, executable=False):
-        """
+def test_better_wraps():
+    def wraps_decorator(func):
+        @wraps(func)
+        def  _wrap_wraps_decorator(*args, **kwargs):
+            return func(*args, **kwargs)
 
-        Parameters
-        ----------
-        name : str
-          Name of the file
-        executable: bool, optional
-          Make it executable
-        """
-        self.name = name
-        self.executable = executable
+        return  _wrap_wraps_decorator
 
-    def __str__(self):
-        return self.name
+    def better_decorator(func):
+        @better_wraps(func)
+        def  _wrap_better_decorator(*args, **kwargs):
+            return func(*args, **kwargs)
 
+        return  _wrap_better_decorator
 
-def create_tree_archive(path, name, load, overwrite=False, archives_leading_dir=True):
-    """Given an archive `name`, create under `path` with specified `load` tree
-    """
-    from ..support.archives import compress_files
-    dirname = file_basename(name)
-    full_dirname = opj(path, dirname)
-    os.makedirs(full_dirname)
-    create_tree(full_dirname, load, archives_leading_dir=archives_leading_dir)
-    # create archive
-    if archives_leading_dir:
-        compress_files([dirname], name, path=path, overwrite=overwrite)
-    else:
-        compress_files(list(map(basename, glob.glob(opj(full_dirname, '*')))),
-                       opj(pardir, name),
-                       path=opj(path, dirname),
-                       overwrite=overwrite)
-    # remove original tree
-    shutil.rmtree(full_dirname)
+    @wraps_decorator
+    def function1(a, b, c):
+        return "function1"
 
+    @better_decorator
+    def function2(a, b, c):
+        return "function2"
 
-def create_tree(path, tree, archives_leading_dir=True):
-    """Given a list of tuples (name, load) create such a tree
+    eq_("function1", function1(1, 2, 3))
+    # getargspec shim now can handle @wraps'ed functions just fine
+    eq_(getargspec(function1)[0], ['a', 'b', 'c'])
+    eq_("function2", function2(1, 2, 3))
+    eq_(getargspec(function2)[0], ['a', 'b', 'c'])
 
-    if load is a tuple itself -- that would create either a subtree or an archive
-    with that content and place it into the tree if name ends with .tar.gz
-    """
-    lgr.log(5, "Creating a tree under %s", path)
-    if not exists(path):
-        os.makedirs(path)
-
-    if isinstance(tree, dict):
-        tree = tree.items()
-
-    for file_, load in tree:
-        if isinstance(file_, File):
-            executable = file_.executable
-            name = file_.name
-        else:
-            executable = False
-            name = file_
-        full_name = opj(path, name)
-        if isinstance(load, (tuple, list, dict)):
-            if name.endswith('.tar.gz') or name.endswith('.tar') or name.endswith('.zip'):
-                create_tree_archive(path, name, load, archives_leading_dir=archives_leading_dir)
-            else:
-                create_tree(full_name, load, archives_leading_dir=archives_leading_dir)
-        else:
-            #encoding = sys.getfilesystemencoding()
-            #if isinstance(full_name, text_type):
-            #    import pydb; pydb.debugger()
-            with open(full_name, 'w') as f:
-                if PY2 and isinstance(load, text_type):
-                    load = load.encode('utf-8')
-                f.write(load)
-        if executable:
-            os.chmod(full_name, os.stat(full_name).st_mode | stat.S_IEXEC)
 
-#
-# Addition "checkers"
-#
+# TODO?: make again parametric on eq_argspec invocations?
+@pytest.mark.filterwarnings(r"ignore: inspect.getargspec\(\) is deprecated")
+def test_getargspec():
 
-import git
-import os
-from os.path import exists, join
-from datalad.support.gitrepo import GitRepo
-from datalad.support.annexrepo import AnnexRepo, FileNotInAnnexError
-from ..utils import chpwd, getpwd
-
-
-def ok_clean_git(path, annex=None, head_modified=[], index_modified=[],
-                 untracked=[], ignore_submodules=False):
-    """Verify that under given path there is a clean git repository
-
-    it exists, .git exists, nothing is uncommitted/dirty/staged
-
-    Note
-    ----
-    Parameters head_modified and index_modified currently work
-    in pure git or indirect mode annex only and are ignored otherwise!
-    Implementation is yet to do!
-
-    Parameters
-    ----------
-    path: str or Repo
-      in case of a str: path to the repository's base dir;
-      Note, that passing a Repo instance prevents detecting annex. This might be
-      useful in case of a non-initialized annex, a GitRepo is pointing to.
-    annex: bool or None
-      explicitly set to True or False to indicate, that an annex is (not)
-      expected; set to None to autodetect, whether there is an annex.
-      Default: None.
-    ignore_submodules: bool
-      if True, submodules are not inspected
-    """
-    # TODO: See 'Note' in docstring
+    def eq_argspec(f, expected, has_kwonlyargs=False):
+        """A helper to centralize testing of getargspec on original and wrapped function
 
-    if isinstance(path, AnnexRepo):
-        if annex is None:
-            annex = True
-        # if `annex` was set to False, but we find an annex => fail
-        assert_is(annex, True)
-        r = path
-    elif isinstance(path, GitRepo):
-        if annex is None:
-            annex = False
-        # explicitly given GitRepo instance doesn't make sense with 'annex' True
-        assert_is(annex, False)
-        r = path
-    else:
-        # 'path' is an actual path
-        try:
-            r = AnnexRepo(path, init=False, create=False)
-            if annex is None:
-                annex = True
-            # if `annex` was set to False, but we find an annex => fail
-            assert_is(annex, True)
-        except Exception:
-            # Instantiation failed => no annex
-            try:
-                r = GitRepo(path, init=False, create=False)
-            except Exception:
-                raise AssertionError("Couldn't find an annex or a git "
-                                     "repository at {}.".format(path))
-            if annex is None:
-                annex = False
-            # explicitly given GitRepo instance doesn't make sense with
-            # 'annex' True
-            assert_is(annex, False)
-
-    eq_(sorted(r.untracked_files), sorted(untracked))
-
-    if annex and r.is_direct_mode():
-        if head_modified or index_modified:
-            lgr.warning("head_modified and index_modified are not quite valid "
-                        "concepts in direct mode! Looking for any change "
-                        "(staged or not) instead.")
-            status = r.get_status(untracked=False, submodules=not ignore_submodules)
-            modified = []
-            for s in status:
-                modified.extend(status[s])
-            eq_(sorted(head_modified + index_modified),
-                sorted(f for f in modified))
+        has_kwonlyargs is to instruct if function has kwonly args so we do not try to compare
+        to inspect.get*spec functions, which would barf ValueError if attempted to run on a
+        function with kwonlys. And also we pass it as include_kwonlyargs to our getargspec
+        """
+        # so we know that our expected is correct
+        if not has_kwonlyargs:
+            # if False - we test function with kwonlys - inspect.getargspec would barf
+            if sys.version_info < (3, 11):
+                eq_(inspect.getargspec(f), expected)
+            # and getfullargspec[:4] wouldn't provide a full picture
+            eq_(inspect.getfullargspec(f)[:4], expected)
         else:
-            ok_(not r.is_dirty(untracked_files=not untracked,
-                               submodules=not ignore_submodules))
-    else:
-        repo = r.repo
-
-        if repo.index.entries.keys():
-            ok_(repo.head.is_valid())
+            if sys.version_info < (3, 11):
+                assert_raises(ValueError, inspect.getargspec, f)
+            inspect.getfullargspec(f)  # doesn't barf
+        eq_(getargspec(f, include_kwonlyargs=has_kwonlyargs), expected)
+
+        # and lets try on a wrapped one -- only ours can do the right thing
+        def decorator(f):
+            @wraps(f)
+            def wrapper(*args, **kwargs):  # pragma: no cover
+                return f(*args, **kwargs)
+            return wrapper
+        fw = decorator(f)
+        if has_kwonlyargs:
+            # We barf ValueError similarly to inspect.getargspec, unless explicitly requested
+            # to include kwonlyargs
+            assert_raises(ValueError, getargspec, fw)
+        eq_(getargspec(fw, include_kwonlyargs=has_kwonlyargs), expected)
+
+    def f0():  # pragma: no cover
+        pass
+
+    eq_argspec(f0, ([], None, None, None))
+
+    def f1(a1, kw1=None, kw0=1):  # pragma: no cover
+        pass
+
+    eq_argspec(f1, (['a1', 'kw1', 'kw0'], None, None, (None, 1)))
+
+    # Having *a already makes keyword args to be kwonlyargs, in that
+    # inspect.get*spec would barf
+    def f1_args(a1, *a, kw1=None, kw0=1, **kw):  # pragma: no cover
+        pass
+
+    eq_argspec(f1_args, (['a1', 'kw1', 'kw0'], 'a', 'kw', (None, 1)), True)
+
+    def f1_star(a1, *, kw1=None, kw0=1):  # pragma: no cover
+        pass
+
+    assert_raises(ValueError, getargspec, f1_star)
+    eq_argspec(f1_star, (['a1', 'kw1', 'kw0'], None, None, (None, 1)), True)
+
+
+def test_get_sig_param_names():
+    def f(a1, kw1=None, *args, kw2=None, **kwargs):
+        pass  # pragma: no cover
+
+    # note: `a1` could be used either positionally or via keyword, so is listed in kw_any
+    assert_equal(get_sig_param_names(f, ('kw_only', 'kw_any')), (['kw2'], ['a1', 'kw1', 'kw2']))
+    assert_equal(get_sig_param_names(f, ('any',)), (['a1', 'kw1', 'kw2'],))
+    assert_equal(get_sig_param_names(f, tuple()), ())
+    assert_raises(ValueError, get_sig_param_names, f, ('mumba',))
 
-            if not head_modified and not index_modified:
-                # get string representations of diffs with index to ease
-                # troubleshooting
-                head_diffs = [str(d) for d in repo.index.diff(repo.head.commit)]
-                index_diffs = [str(d) for d in repo.index.diff(None)]
-                eq_(head_diffs, [])
-                eq_(index_diffs, [])
-            else:
-                if head_modified:
-                    # we did ask for interrogating changes
-                    head_modified_ = [d.a_path for d in repo.index.diff(repo.head.commit)]
-                    eq_(head_modified_, head_modified)
-                if index_modified:
-                    index_modified_ = [d.a_path for d in repo.index.diff(None)]
-                    eq_(index_modified_, index_modified)
-
-
-def ok_file_under_git(path, filename=None, annexed=False):
-    """Test if file is present and under git/annex control
-
-    If relative path provided, then test from current directory
-    """
-    annex, file_repo_path, filename, path, repo = _prep_file_under_git(path, filename)
-    assert_in(file_repo_path, repo.get_indexed_files())  # file is known to Git
-
-    if annex:
-        try:
-            # operates on relative to curdir path
-            repo.get_file_key(file_repo_path)
-            in_annex = True
-        except FileNotInAnnexError as e:
-            in_annex = False
-    else:
-        in_annex = False
-
-    assert(annexed == in_annex)
 
+@with_tempfile(mkdir=True)
+def test_rotree(d=None):
+    d2 = opj(d, 'd1', 'd2')  # deep nested directory
+    f = opj(d2, 'f1')
+    os.makedirs(d2)
+    with open(f, 'w') as f_:
+        f_.write("LOAD")
+    with swallow_logs():
+        ar = AnnexRepo(d2)
+    rotree(d)
+    # we shouldn't be able to delete anything UNLESS in "crippled" situation:
+    # root, or filesystem is FAT etc
+    # Theoretically annex should declare FS as crippled when ran as root, but
+    # see http://git-annex.branchable.com/bugs/decides_that_FS_is_crippled_
+    # under_cowbuilder___40__symlinks_supported_etc__41__/#comment-60c3cbe2710d6865fb9b7d6e247cd7aa
+    # so explicit 'or'
+    if not (ar.is_crippled_fs() or (os.getuid() == 0)):
+        assert_raises(OSError, os.unlink, f)          # OK to use os.unlink
+        assert_raises(OSError, unlink, f)   # and even with waiting and trying!
+        assert_raises(OSError, shutil.rmtree, d)
+        # but file should still be accessible
+        with open(f) as f_:
+            eq_(f_.read(), "LOAD")
+    # make it RW
+    rotree(d, False)
+    unlink(f)
+    shutil.rmtree(d)
+
+
+def test_swallow_outputs():
+    with swallow_outputs() as cm:
+        eq_(cm.out, '')
+        sys.stdout.write("out normal")
+        sys.stderr.write("out error")
+        eq_(cm.out, 'out normal')
+        sys.stdout.write(" and more")
+        eq_(cm.out, 'out normal and more')  # incremental
+        eq_(cm.err, 'out error')
+        eq_(cm.err, 'out error')  # the same value if multiple times
+
+
+@with_tempfile
+def test_swallow_logs(logfile=None):
+    lgr = logging.getLogger('datalad')
+    with swallow_logs(new_level=9) as cm:
+        eq_(cm.out, '')
+        lgr.log(8, "very heavy debug")
+        eq_(cm.out, '')  # not even visible at level 9
+        lgr.log(9, "debug1")
+        eq_(cm.out, '[Level 9] debug1\n')  # not even visible at level 9
+        lgr.info("info")
+        # not even visible at level 9
+        eq_(cm.out, '[Level 9] debug1\n[INFO] info\n')
+    with swallow_logs(new_level=9, file_=logfile) as cm:
+        eq_(cm.out, '')
+        lgr.info("next info")
+    from datalad.tests.utils_pytest import ok_file_has_content
+    ok_file_has_content(logfile, "[INFO] next info", strip=True)
+
+
+def test_swallow_logs_assert():
+    lgr = logging.getLogger('datalad.tests')
+    with swallow_logs(new_level=9) as cm:
+        # nothing was logged so should fail
+        assert_raises(AssertionError, cm.assert_logged)
+        lgr.info("something")
+        cm.assert_logged("something")
+        cm.assert_logged(level="INFO")
+        cm.assert_logged("something", level="INFO")
+
+        # even with regex = False should match above
+        cm.assert_logged("something", regex=False)
+        cm.assert_logged(level="INFO", regex=False)
+        cm.assert_logged("something", level="INFO", regex=False)
+
+        # different level
+        assert_raises(AssertionError,
+                      cm.assert_logged, "something", level="DEBUG")
+        assert_raises(AssertionError, cm.assert_logged, "else")
+
+        cm.assert_logged("some.hing", level="INFO")  # regex ;-)
+        # does match
+        assert_raises(AssertionError,
+                      cm.assert_logged, "ome.hing", level="INFO")
+        # but we can change it
+        cm.assert_logged("some.hing", level="INFO", match=False)
+    # and we can continue doing checks after we left the cm block
+    cm.assert_logged("some.hing", level="INFO", match=False)
+    # and we indeed logged something
+    cm.assert_logged(match=False)
+
+
+def test_disable_logger():
+
+    # get a logger hierarchy:
+    lgr_top = logging.getLogger('datalad')
+    lgr_middle = logging.getLogger('datalad.tests')
+    lgr_bottom = logging.getLogger('datalad.tests.utils_pytest')
+
+    with swallow_logs(new_level=logging.DEBUG) as cml:
+        with disable_logger():  # default: 'datalad':
+            lgr_top.debug("log sth at top level")
+            lgr_middle.debug("log sth at mid level")
+            lgr_bottom.debug("log sth at bottom level")
+        # nothing logged:
+        assert_raises(AssertionError, cml.assert_logged)
+
+    # again, but pass in the logger at mid level:
+    with swallow_logs(new_level=logging.DEBUG) as cml:
+        with disable_logger(lgr_middle):
+            lgr_top.debug("log sth at top level")
+            lgr_middle.debug("log sth at mid level")
+            lgr_bottom.debug("log sth at bottom level")
+        # top level unaffected:
+        cml.assert_logged("log sth at top level", level="DEBUG", regex=False)
+        # but both of the lower ones don't log anything:
+        assert_raises(AssertionError, cml.assert_logged, "log sth at mid level")
+        assert_raises(AssertionError, cml.assert_logged, "log sth at bottom level")
+
+
+def test_md5sum():
+    # just a smoke (encoding/decoding) test for md5sum
+    _ = md5sum(__file__)
+
+
+@with_tree([('1.tar.gz', (('1 f.txt', '1 f load'),))])
+def test_md5sum_archive(d=None):
+    # just a smoke (encoding/decoding) test for md5sum
+    _ = md5sum(opj(d, '1.tar.gz'))
+
+
+def test_updated():
+    d = {}
+    eq_(updated(d, {1: 2}), {1: 2})
+    eq_(d, {})
+
+    d = {'a': 'b'}
+    eq_(updated(d, ((0, 1), (2, 3))), {0: 1, 'a': 'b', 2: 3})
+    eq_(d, {'a': 'b'})
+
+    # and that it would maintain the type
+    d = dict(((99, 0), ('z', 0), ('a', 0)))
+    d_ = updated(d, {0: 1})
+    ok_(isinstance(d_, dict))
+    eq_(d_, dict(((99, 0), ('z', 0), ('a', 0), (0, 1))))
+
+
+def test_get_local_file_url_windows():
+    raise SkipTest("TODO")
+
+
+@assert_cwd_unchanged
+def test_getpwd_basic():
+    pwd = getpwd()
+    ok_(isabs(pwd))
+    eq_(os.getcwd(), abspath(pwd))
+
+    # that we do not chdir anywhere if None provided
+    with patch('os.chdir') as oschdir:
+        with chpwd(None):
+            eq_(getpwd(), pwd)
+        assert_false(oschdir.called)
 
-def put_file_under_git(path, filename=None, content=None, annexed=False):
-    """Place file under git/annex and return used Repo
-    """
-    annex, file_repo_path, filename, path, repo = _prep_file_under_git(path, filename)
-    if content is None:
-        content = ""
-    with open(opj(repo.path, file_repo_path), 'w') as f_:
-        f_.write(content)
-
-    if annexed:
-        if not isinstance(repo, AnnexRepo):
-            repo = AnnexRepo(repo.path)
-        repo.add(file_repo_path, commit=True, _datalad_msg=True)
-    else:
-        repo.add(file_repo_path, git=True, _datalad_msg=True)
-    ok_file_under_git(repo.path, file_repo_path, annexed)
-    return repo
 
+@with_tempfile(mkdir=True)
+@assert_cwd_unchanged(ok_to_chdir=True)
+def test_getpwd_change_mode(tdir=None):
+    from datalad import utils
+    if utils._pwd_mode != 'PWD':
+        raise SkipTest("Makes sense to be tested only in PWD mode, "
+                       "but we seems to be beyond that already")
+    # The evil plain chdir call
+    os.chdir(tdir)
+    # Just testing the logic of switching to cwd mode and issuing a warning
+    with swallow_logs(new_level=logging.DEBUG) as cml:
+        pwd = getpwd()
+        eq_(pwd, str(Path(pwd).resolve()))  # might have symlinks, thus realpath
+    assert_in("symlinks in the paths will be resolved", cml.out)
+    eq_(utils._pwd_mode, 'cwd')
 
-def _prep_file_under_git(path, filename):
-    """Get instance of the repository for the given filename
 
-    Helper to be used by few functions
-    """
-    if filename is None:
-        # path provides the path and the name
-        path, filename = pathsplit(path)
+@skip_wo_symlink_capability
+@skip_if_on_windows
+@with_tempfile(mkdir=True)
+@assert_cwd_unchanged
+def test_getpwd_symlink(tdir=None):
+    sdir = opj(tdir, 's1')
+    pwd_orig = getpwd()
+    Path(sdir).symlink_to(Path('.'))
+    s1dir = opj(sdir, 's1')
+    s2dir = opj(sdir, 's2')
     try:
-        # if succeeds when must not (not `annexed`) -- fail
-        repo = get_repo_instance(path, class_=AnnexRepo)
-        annex = True
-    except RuntimeError as e:
-        # TODO: make a dedicated Exception
-        if "No annex repository found in" in str(e):
-            repo = get_repo_instance(path, class_=GitRepo)
-            annex = False
-        else:
-            raise
-
-    # path to the file within the repository
-    file_repo_dir = os.path.relpath(path, repo.path)
-    file_repo_path = filename if file_repo_dir == curdir else opj(file_repo_dir, filename)
-    return annex, file_repo_path, filename, path, repo
-
-
-#
-# Helpers to test symlinks
-#
-
-def ok_symlink(path):
-    """Checks whether path is either a working or broken symlink"""
-    link_path = os.path.islink(path)
-    if not link_path:
-        raise AssertionError("Path {} seems not to be a symlink".format(path))
-
-
-def ok_good_symlink(path):
-    ok_symlink(path)
-    rpath = realpath(path)
-    ok_(exists(rpath),
-        msg="Path {} seems to be missing.  Symlink {} is broken".format(
-                rpath, path))
-
-
-def ok_broken_symlink(path):
-    ok_symlink(path)
-    rpath = realpath(path)
-    assert_false(exists(rpath),
-            msg="Path {} seems to be present.  Symlink {} is not broken".format(
-                    rpath, path))
-
-
-def ok_startswith(s, prefix):
-    ok_(s.startswith(prefix),
-        msg="String %r doesn't start with %r" % (s, prefix))
-
-
-def ok_endswith(s, suffix):
-    ok_(s.endswith(suffix),
-        msg="String %r doesn't end with %r" % (s, suffix))
-
-
-def nok_startswith(s, prefix):
-    assert_false(s.startswith(prefix),
-        msg="String %r starts with %r" % (s, prefix))
-
-
-def ok_git_config_not_empty(ar):
-    """Helper to verify that nothing rewritten the config file"""
-    # TODO: we don't support bare -- do we?
-    assert_true(os.stat(opj(ar.path, '.git', 'config')).st_size)
-
-
-def ok_annex_get(ar, files, network=True):
-    """Helper to run .get decorated checking for correct operation
-
-    get passes through stderr from the ar to the user, which pollutes
-    screen while running tests
-
-    Note: Currently not true anymore, since usage of --json disables
-    progressbars
-    """
-    ok_git_config_not_empty(ar) # we should be working in already inited repo etc
-    with swallow_outputs() as cmo:
-        ar.get(files)
-    # verify that load was fetched
-    ok_git_config_not_empty(ar) # whatever we do shouldn't destroy the config file
-    has_content = ar.file_has_content(files)
-    if isinstance(has_content, bool):
-        ok_(has_content)
-    else:
-        ok_(all(has_content))
-
-
-def ok_generator(gen):
-    assert_true(inspect.isgenerator(gen), msg="%s is not a generator" % gen)
-
-
-assert_is_generator = ok_generator  # just an alias
-
-
-def ok_archives_caches(repopath, n=1, persistent=None):
-    """Given a path to repository verify number of archives
-
-    Parameters
-    ----------
-    repopath : str
-      Path to the repository
-    n : int, optional
-      Number of archives directories to expect
-    persistent: bool or None, optional
-      If None -- both persistent and not count.
-    """
-    # looking into subdirectories
-    glob_ptn = opj(repopath,
-                   ARCHIVES_TEMP_DIR + {None: '*', True: '', False: '-*'}[persistent],
-                   '*')
-    dirs = glob.glob(glob_ptn)
-    n2 = n * 2  # per each directory we should have a .stamp file
-    assert_equal(len(dirs), n2,
-                 msg="Found following dirs when needed %d of them: %s" % (n2, dirs))
-
-
-def ok_exists(path):
-    assert exists(path), 'path %s does not exist' % path
-
-
-def ok_file_has_content(path, content, strip=False, re_=False, **kwargs):
-    """Verify that file exists and has expected content"""
-    ok_exists(path)
-    with open(path, 'r') as f:
-        content_ = f.read()
+        chpwd(sdir)
+        pwd = getpwd()
+        eq_(pwd, sdir)
+        chpwd('s1')
+        eq_(getpwd(), s1dir)
+        chpwd('.')
+        eq_(getpwd(), s1dir)
+        chpwd('..')
+        eq_(getpwd(), sdir)
+    finally:
+        chpwd(pwd_orig)
+
+    # test context handler way of use
+    with chpwd(s1dir):
+        eq_(getpwd(), s1dir)
+    eq_(getpwd(), pwd_orig)
+
+    assert_false(exists(s2dir))
+    with assert_raises(OSError):
+        with chpwd(s2dir):
+            pass
+    with chpwd(s2dir, mkdir=True):
+        ok_(exists(s2dir))
+        eq_(getpwd(), s2dir)
 
-        if strip:
-            content_ = content_.strip()
 
-        if re_:
-            assert_re_in(content, content_, **kwargs)
-        else:
-            assert_equal(content, content_, **kwargs)
+@with_tempfile(mkdir=True)
+def test_chpwd_obscure_name(topdir=None):
+    path = op.join(topdir, OBSCURE_FILENAME)
+    os.mkdir(path)
+    # Just check that call doesn't fail.
+    with chpwd(path):
+        pass
+
+
+def test_auto_repr():
+
+    class WithoutReprClass:
+        def __init__(self):
+            self.a = "does not matter"
+
+    @auto_repr
+    class buga:
+        def __init__(self):
+            self.a = 1
+            self.b = list(range(20))
+            self.c = WithoutReprClass()
+            self._c = "protect me"
+
+        def some(self):
+            return "some"
+
+    @auto_repr(short=False)
+    class buga_long(object):
+        def __init__(self):
+            self.a = 1
+            self.b = list(range(20))
 
+        def some(self):
+            return "some"
 
-#
-# Decorators
-#
+    assert_equal(
+        repr(buga()),
+        "buga(a=1, b=<<[0, 1, 2, 3, 4++52 chars++ 19]>>, c=<WithoutReprClass>)"
+    )
+    assert_equal(buga().some(), "some")
 
+    assert_equal(
+        repr(buga_long()),
+        f"buga_long(a=1, b=[{', '.join(map(str, range(20)))}])"
+    )
+    assert_equal(buga_long().some(), "some")
 
-@optional_args
-def with_tree(t, tree=None, archives_leading_dir=True, delete=True, **tkwargs):
 
-    @wraps(t)
-    def newfunc(*arg, **kw):
-        tkwargs_ = get_tempfile_kwargs(tkwargs, prefix="tree", wrapped=t)
-        d = tempfile.mkdtemp(**tkwargs_)
-        create_tree(d, tree, archives_leading_dir=archives_leading_dir)
-        try:
-            return t(*(arg + (d,)), **kw)
-        finally:
-            if delete:
-                rmtemp(d)
-    return newfunc
+def test_todo_interface_for_extensions():
 
+    @todo_interface_for_extensions
+    def f(i, j):
+        return i*j
 
-lgr = logging.getLogger('datalad.tests')
+    assert_equal(f(2, 3), 6)
 
 
-class SilentHTTPHandler(SimpleHTTPRequestHandler):
-    """A little adapter to silence the handler
-    """
-    def __init__(self, *args, **kwargs):
-        self._silent = lgr.getEffectiveLevel() > logging.DEBUG
-        SimpleHTTPRequestHandler.__init__(self, *args, **kwargs)
-
-    def log_message(self, format, *args):
-        if self._silent:
-            return
-        lgr.debug("HTTP: " + format % args)
-
-
-def _multiproc_serve_path_via_http(hostname, path_to_serve_from, queue): # pragma: no cover
-    chpwd(path_to_serve_from)
-    httpd = HTTPServer((hostname, 0), SilentHTTPHandler)
-    queue.put(httpd.server_port)
-    httpd.serve_forever()
-
-
-@optional_args
-def serve_path_via_http(tfunc, *targs):
-    """Decorator which serves content of a directory via http url
-    """
+def test_assure_iter():
+    s = {1}
+    assert ensure_iter(None, set) == set()
+    assert ensure_iter(1, set) == s
+    assert ensure_iter(1, list) == [1]
+    assert ensure_iter(s, set) is s
+    assert ensure_iter(s, set, copy=True) is not s
 
-    @wraps(tfunc)
-    def newfunc(*args, **kwargs):
 
-        if targs:
-            # if a path is passed into serve_path_via_http, then it's in targs
-            assert len(targs) == 1
-            path = targs[0]
+def test_assure_list_copy():
+    l = [1]
+    assert ensure_list(l) is l
+    assert ensure_list(l, copy=True) is not l
 
-        elif len(args) > 1:
-            args, path = args[:-1], args[-1]
-        else:
-            args, path = (), args[0]
 
-        # There is a problem with Haskell on wheezy trying to
-        # fetch via IPv6 whenever there is a ::1 localhost entry in
-        # /etc/hosts.  Apparently fixing that docker image reliably
-        # is not that straightforward, although see
-        # http://jasonincode.com/customizing-hosts-file-in-docker/
-        # so we just force to use 127.0.0.1 while on wheezy
-        #hostname = '127.0.0.1' if on_debian_wheezy else 'localhost'
-        hostname = '127.0.0.1'
-
-        queue = multiprocessing.Queue()
-        multi_proc = multiprocessing.Process(
-            target=_multiproc_serve_path_via_http,
-            args=(hostname, path, queue))
-        multi_proc.start()
-        port = queue.get(timeout=300)
-        url = 'http://{}:{}/'.format(hostname, port)
-        lgr.debug("HTTP: serving {} under {}".format(path, url))
-
-        try:
-            # Such tests don't require real network so if http_proxy settings were
-            # provided, we remove them from the env for the duration of this run
-            env = os.environ.copy()
-            env.pop('http_proxy', None)
-            with patch.dict('os.environ', env, clear=True):
-                return tfunc(*(args + (path, url)), **kwargs)
-        finally:
-            lgr.debug("HTTP: stopping server under %s" % path)
-            multi_proc.terminate()
-
-    return newfunc
-
-
-@optional_args
-def with_memory_keyring(t):
-    """Decorator to use non-persistant MemoryKeyring instance
-    """
-    @wraps(t)
-    def newfunc(*args, **kwargs):
-        keyring = MemoryKeyring()
-        with patch("datalad.downloaders.credentials.keyring_", keyring):
-            return t(*(args + (keyring,)), **kwargs)
-
-    return newfunc
+@pytest.mark.parametrize(
+    "value,result",
+    [
+        ('', None),
+        ([], None),
+        ('somestring', ['somestring']),
+        ('some\nmultiline\nstring', ['some', 'multiline', 'string']),
+        (['something'], ['something']),
+        (['a', 'listof', 'stuff'], ['a', 'listof', 'stuff']),
+    ]
+)
+def test_assure_list_from_str(value, result):
+    assert ensure_list_from_str(value) == result
 
 
-@optional_args
-def without_http_proxy(tfunc):
-    """Decorator to remove http*_proxy env variables for the duration of the test
-    """
+def test_assure_dict_from_str():
+    assert_equal(ensure_dict_from_str(''), None)
+    assert_equal(ensure_dict_from_str({}), None)
+    target_dict = dict(
+        __ac_name='{user}', __ac_password='{password}',
+        cookies_enabled='', submit='Log in'
+    )
+    string = '__ac_name={user}\n__ac_password={password}\nsubmit=Log ' \
+               'in\ncookies_enabled='
+    assert_equal(ensure_dict_from_str(string), target_dict)
+    assert_equal(ensure_dict_from_str(
+        target_dict),
+        target_dict)
+
+
+def test_assure_bool():
+    for values, t in [
+        (['True', 1, '1', 'yes', 'on'], True),
+        (['False', 0, '0', 'no', 'off'], False)
+    ]:
+        for v in values:
+            eq_(ensure_bool(v), t)
+    assert_raises(ValueError, ensure_bool, "unknown")
+
+
+def test_generate_chunks():
+    ok_generator(generate_chunks([1], 1))
+    eq_(list(generate_chunks([1], 1)), [[1]])
+    eq_(list(generate_chunks([1], 2)), [[1]])
+    eq_(list(generate_chunks([1, 2, 3], 2)), [[1, 2], [3]])
+    # type is preserved
+    eq_(list(generate_chunks((1, 2, 3), 2)), [(1, 2), (3,)])
+    # no hangers
+    eq_(list(generate_chunks((1, 2, 3, 4), 2)), [(1, 2), (3, 4)])
+    assert_raises(AssertionError, list, generate_chunks([1], 0))
+
+
+def test_any_re_search():
+    assert_true(any_re_search('a', 'a'))
+    assert_true(any_re_search('a', 'bab'))
+    assert_false(any_re_search('^a', 'bab'))
+    assert_true(any_re_search(['b', '.ab'], 'bab'))
+    assert_false(any_re_search(['^b', 'bab'], 'ab'))
+
+
+def test_find_files():
+    tests_dir = dirname(__file__)
+    proj_dir = normpath(opj(dirname(__file__), pardir))
+
+    ff = find_files('.*', proj_dir)
+    ok_generator(ff)
+    files = list(ff)
+    assert(len(files) > 10)  # we have more than 10 test files here
+    assert_in(opj(tests_dir, 'test_utils.py'), files)
+    # and no directories should be mentioned
+    assert_not_in(tests_dir, files)
+
+    ff2 = find_files('.*', proj_dir, dirs=True)
+    files2 = list(ff2)
+    assert_in(opj(tests_dir, 'test_utils.py'), files2)
+    assert_in(tests_dir, files2)
+
+    # now actually matching the path
+    ff3 = find_files(
+        r'.*\\test_.*\.py$' if on_windows else r'.*/test_.*\.py$',
+        proj_dir, dirs=True)
+    files3 = list(ff3)
+    assert_in(opj(tests_dir, 'test_utils.py'), files3)
+    assert_not_in(tests_dir, files3)
+    for f in files3:
+        ok_startswith(basename(f), 'test_')
+
+
+@with_tree(tree={
+    '.git': {
+        '1': '2'
+    },
+    'd1': {
+        '.git': 'possibly a link from submodule'
+    },
+    'git': 'just a file'
+})
+def test_find_files_exclude_vcs(repo=None):
+    ff = find_files('.*', repo, dirs=True)
+    files = list(ff)
+    assert_equal({basename(f) for f in files}, {'d1', 'git'})
+    assert_not_in(opj(repo, '.git'), files)
+
+    ff = find_files('.*', repo, dirs=True, exclude_vcs=False)
+    files = list(ff)
+    assert_equal({basename(f) for f in files}, {'d1', 'git', '.git', '1'})
+    assert_in(opj(repo, '.git'), files)
+
+
+def test_not_supported_on_windows():
+    with patch('datalad.utils.on_windows', True):
+        assert_raises(NotImplementedError, not_supported_on_windows)
+        assert_raises(NotImplementedError, not_supported_on_windows, "msg")
+
+    with patch('datalad.utils.on_windows', False):
+        assert_equal(not_supported_on_windows(), None)
+        assert_equal(not_supported_on_windows("msg"), None)
+
+
+def test_file_basename():
+    eq_(file_basename('1'), '1')
+    eq_(file_basename('d1/1'), '1')
+    eq_(file_basename('/d1/1'), '1')
+    eq_(file_basename('1.'), '1.')
+    eq_(file_basename('1.tar.gz'), '1')
+    eq_(file_basename('1.Tar.gz'), '1')
+    eq_(file_basename('1._bak.gz'), '1')
+    eq_(file_basename('1.tar.gz', return_ext=True), ('1', 'tar.gz'))
+    eq_(file_basename('/tmp/1.tar.gz'), '1')
+    eq_(file_basename('/tmp/1.longish.gz'), '1.longish')
+    eq_(file_basename('1_R1.1.1.tar.gz'), '1_R1.1.1')
+    eq_(file_basename('ds202_R1.1.1.tgz'), 'ds202_R1.1.1')
+
+
+def test_expandpath():
+    eq_(expandpath("some", False), expanduser('some'))
+    eq_(expandpath("some", False), expandvars('some'))
+    assert_true(isabs(expandpath('some')))
+    # this may have to go because of platform issues
+    if not on_windows:
+        # expanduser is not influenced by our HOME setting adjustments
+        # for the tests on windows
+        eq_(expandpath("$HOME"), expanduser('~'))
+
+
+def test_is_explicit_path():
+    # by default expanded paths are absolute, hence explicit
+    assert_true(is_explicit_path(expandpath('~')))
+    assert_false(is_explicit_path("here"))
+
+
+@with_tempfile
+@with_tempfile
+def test_knows_annex(here=None, there=None):
+    from datalad.support.annexrepo import AnnexRepo
+    from datalad.support.gitrepo import GitRepo
+    GitRepo(path=here, create=True)
+    assert_false(knows_annex(here))
+    AnnexRepo(path=here, create=True)
+    assert_true(knows_annex(here))
+    GitRepo.clone(path=there, url=here, create=True)
+    assert_true(knows_annex(there))
+
+
+def test_make_tempfile():
+    # check if mkdir, content conflict caught
+    with assert_raises(ValueError):
+        with make_tempfile(content="blah", mkdir=True):  # pragma: no cover
+            pass
 
-    @wraps(tfunc)
-    def newfunc(*args, **kwargs):
-        # Such tests don't require real network so if http_proxy settings were
-        # provided, we remove them from the env for the duration of this run
-        env = os.environ.copy()
-        env.pop('http_proxy', None)
-        env.pop('https_proxy', None)
-        with patch.dict('os.environ', env, clear=True):
-            return tfunc(*args, **kwargs)
-
-    return newfunc
-
-
-@borrowkwargs(methodname=make_tempfile)
-@optional_args
-def with_tempfile(t, **tkwargs):
-    """Decorator function to provide a temporary file name and remove it at the end
-
-    Parameters
-    ----------
-
-    To change the used directory without providing keyword argument 'dir' set
-    DATALAD_TESTS_TEMP_DIR.
-
-    Examples
-    --------
-
-    ::
-
-        @with_tempfile
-        def test_write(tfile):
-            open(tfile, 'w').write('silly test')
-    """
 
-    @wraps(t)
-    def newfunc(*arg, **kw):
-        with make_tempfile(wrapped=t, **tkwargs) as filename:
-            return t(*(arg + (filename,)), **kw)
-
-    return newfunc
-
-
-def _get_resolved_flavors(flavors):
-    #flavors_ = (['local', 'clone'] + (['local-url'] if not on_windows else [])) \
-    #           if flavors == 'auto' else flavors
-    flavors_ = (['local', 'clone', 'local-url', 'network'] if not on_windows
-                else ['network', 'network-clone']) \
-               if flavors == 'auto' else flavors
-
-    if not isinstance(flavors_, list):
-        flavors_ = [flavors_]
-
-    if os.environ.get('DATALAD_TESTS_NONETWORK'):
-        flavors_ = [x for x in flavors_ if not x.startswith('network')]
-    return flavors_
-
-def _get_repo_url(path):
-    """Return ultimate URL for this repo"""
-
-    if path.startswith('http') or path.startswith('git'):
-        # We were given a URL, so let's just return it
-        return path
-
-    if not exists(opj(path, '.git')):
-        # do the dummiest check so we know it is not git.Repo's fault
-        raise AssertionError("Path %s does not point to a git repository "
-                             "-- missing .git" % path)
-    repo = git.Repo(path)
-    if len(repo.remotes) == 1:
-        remote = repo.remotes[0]
+def test_unique():
+    eq_(unique(range(3)), [0, 1, 2])
+    eq_(unique(range(3), reverse=True), [0, 1, 2])
+    eq_(unique((1, 0, 1, 3, 2, 0, 1)), [1, 0, 3, 2])
+    eq_(unique((1, 0, 1, 3, 2, 0, 1), reverse=True), [3, 2, 0, 1])
+    eq_(unique([]), [])
+    eq_(unique([], reverse=True), [])
+    eq_(unique([(1, 2), (1,), (1, 2), (0, 3)]), [(1, 2), (1,), (0, 3)])
+    eq_(unique([(1, 2), (1,), (1, 2), (0, 3)], reverse=True),
+        [(1,), (1, 2), (0, 3)])
+
+    # with a key now
+    eq_(unique([(1, 2), (1,), (1, 2), (0, 3)],
+               key=itemgetter(0)), [(1, 2), (0, 3)])
+    eq_(unique([(1, 2), (1,), (1, 2), (0, 3)],
+               key=itemgetter(0), reverse=True), [(1, 2), (0, 3)])
+
+    eq_(unique([(1, 2), (1, 3), (1, 2), (0, 3)],
+               key=itemgetter(1)), [(1, 2), (1, 3)])
+    eq_(unique([(1, 2), (1, 3), (1, 2), (0, 3)],
+               key=itemgetter(1), reverse=True), [(1, 2), (0, 3)])
+
+
+def test_partition():
+    def fn(*args, **kwargs):
+        left, right = partition(*args, **kwargs)
+        return list(left), list(right)
+
+    eq_(fn([False, True, False]),
+        ([False, False], [True]))
+
+    eq_(fn([1, 5, 4, 10], lambda x: x > 4),
+        ([1, 4], [5, 10]))
+
+    eq_(fn([1, 5, 4, 10], lambda x: x < 0),
+        ([1, 5, 4, 10], []))
+
+
+def test_path_():
+    eq_(_path_('a'), 'a')
+    if on_windows:
+        eq_(_path_('a/b'), r'a\b')
     else:
-        remote = repo.remotes.origin
-    return remote.config_reader.get('url')
+        p = 'a/b/c'
+        assert(_path_(p) is p)  # nothing is done to it whatsoever
+        eq_(_path_(p, 'd'), 'a/b/c/d')
+
+
+def test_get_timestamp_suffix():
+    # we need to patch temporarily TZ
+    with patch.dict('os.environ', {'TZ': 'GMT'}):
+        # figure out how GMT time zone suffix is represented
+        # could be +0 or -0, depending on platform
+        # just use whatever it is, not the subject of this test
+        tz_suffix = time.strftime('%z', time.gmtime(0))
+        # skynet DOB
+        target_ts = '1970-01-01T00:00:00' + tz_suffix
+        assert_equal(get_timestamp_suffix(0), '-' + target_ts)
+        assert_equal(get_timestamp_suffix(0, prefix="+"),
+                     '+' + target_ts)
+        # yoh found no way to mock things out and didn't want to provide
+        # explicit call to anything to get current time with the timezone,
+        # so disabling this test for now besides that it should return smth
+        # sensible ;)
+        #with patch.object(time, 'localtime', lambda: 1):
+        #    assert_equal(get_timestamp_suffix(),
+        #  '-1970-01-01T00:00:01+0000')  # skynet is 1 sec old
+        assert(get_timestamp_suffix().startswith('-'))
+
+
+def test_memoized_generator():
+    called = [0]
+
+    def g1(n):
+        """a generator"""
+        called[0] += 1
+        for i in range(n):
+            yield i
+
+    from ..utils import saved_generator
+    ok_generator(g1(3))
+    g1_, g2_ = saved_generator(g1(3))
+    ok_generator(g1_)
+    ok_generator(g2_)
+    target = list(g1(3))
+    eq_(called[0], 1)
+    eq_(target, list(g1_))
+    eq_(called[0], 2)
+    eq_(target, list(g2_))
+    eq_(called[0], 2)  # no new call to make a generator
+    # but we can't (ab)use 2nd time
+    eq_([], list(g2_))
+
+
+def test_assure_unicode():
+    ok_(isinstance(ensure_unicode("m"), str))
+    ok_(isinstance(ensure_unicode('grandchild_äöü東'), str))
+    ok_(isinstance(ensure_unicode(u'grandchild_äöü東'), str))
+    eq_(ensure_unicode('grandchild_äöü東'), u'grandchild_äöü東')
+    # now, non-utf8
+    # Decoding could be deduced with high confidence when the string is
+    # really encoded in that codepage
+    mom_koi8r = u"мама".encode('koi8-r')
+    eq_(ensure_unicode(mom_koi8r), u"мама")
+    eq_(ensure_unicode(mom_koi8r, confidence=0.9), u"мама")
+    mom_iso8859 = u'mamá'.encode('iso-8859-1')
+    eq_(ensure_unicode(mom_iso8859), u'mamá')
+    eq_(ensure_unicode(mom_iso8859, confidence=0.5), u'mamá')
+    # but when we mix, it does still guess something allowing to decode:
+    mixedin = mom_koi8r + u'東'.encode('iso2022_jp') + u'東'.encode('utf-8')
+    ok_(isinstance(ensure_unicode(mixedin), str))
+    # but should fail if we request high confidence result:
+    with assert_raises(ValueError):
+        ensure_unicode(mixedin, confidence=0.9)
+    # For other, non string values, actually just returns original value
+    # TODO: RF to actually "assure" or fail??  For now hardcoding that assumption
+    assert ensure_unicode(1) == 1
+
+
+def test_pathlib_unicode():
+    eq_(str(Path("a")), u"a")
+    eq_(str(Path(u"β")), u"β")
 
 
-def clone_url(url):
-    # delay import of our code until needed for certain
-    from ..cmd import Runner
-    runner = Runner()
-    tdir = tempfile.mkdtemp(**get_tempfile_kwargs({}, prefix='clone_url'))
-    _ = runner(["git", "clone", url, tdir], expect_stderr=True)
-    if GitRepo(tdir).is_with_annex():
-        AnnexRepo(tdir, init=True)
-    _TEMP_PATHS_CLONES.add(tdir)
-    return tdir
-
-
-if not on_windows:
-    local_testrepo_flavors = ['local'] # 'local-url'
-else:
-    local_testrepo_flavors = ['network-clone']
-
-from .utils_testrepos import BasicAnnexTestRepo, BasicGitTestRepo, \
-    SubmoduleDataset, NestedDataset, InnerSubmodule
-
-_TESTREPOS = None
-
-def _get_testrepos_uris(regex, flavors):
-    global _TESTREPOS
-    # we should instantiate those whenever test repos actually asked for
-    # TODO: just absorb all this lazy construction within some class
-    if not _TESTREPOS:
-        _basic_annex_test_repo = BasicAnnexTestRepo()
-        _basic_git_test_repo = BasicGitTestRepo()
-        _submodule_annex_test_repo = SubmoduleDataset()
-        _nested_submodule_annex_test_repo = NestedDataset()
-        _inner_submodule_annex_test_repo = InnerSubmodule()
-        _TESTREPOS = {'basic_annex':
-                        {'network': 'git://github.com/datalad/testrepo--basic--r1',
-                         'local': _basic_annex_test_repo.path,
-                         'local-url': _basic_annex_test_repo.url},
-                      'basic_git':
-                        {'local': _basic_git_test_repo.path,
-                         'local-url': _basic_git_test_repo.url},
-                      'submodule_annex':
-                        {'local': _submodule_annex_test_repo.path,
-                         'local-url': _submodule_annex_test_repo.url},
-                      'nested_submodule_annex':
-                        {'local': _nested_submodule_annex_test_repo.path,
-                         'local-url': _nested_submodule_annex_test_repo.url},
-                      # TODO: append 'annex' to the name:
-                      # Currently doesn't work with some annex tests, despite
-                      # working manually. So, figure out how the tests' setup
-                      # messes things up with this one.
-                      'inner_submodule':
-                        {'local': _inner_submodule_annex_test_repo.path,
-                         'local-url': _inner_submodule_annex_test_repo.url}
-                      }
-        # assure that now we do have those test repos created -- delayed
-        # their creation until actually used
-        if not on_windows:
-            _basic_annex_test_repo.create()
-            _basic_git_test_repo.create()
-            _submodule_annex_test_repo.create()
-            _nested_submodule_annex_test_repo.create()
-            _inner_submodule_annex_test_repo.create()
-    uris = []
-    for name, spec in iteritems(_TESTREPOS):
-        if not re.match(regex, name):
-            continue
-        uris += [spec[x] for x in set(spec.keys()).intersection(flavors)]
-
-        # additional flavors which might have not been
-        if 'clone' in flavors and 'clone' not in spec:
-            uris.append(clone_url(spec['local']))
-
-        if 'network-clone' in flavors and 'network-clone' not in spec:
-            uris.append(clone_url(spec['network']))
-
-    return uris
-
-
-@optional_args
-def with_testrepos(t, regex='.*', flavors='auto', skip=False, count=None):
-    """Decorator to provide a local/remote test repository
-
-    All tests under datalad/tests/testrepos are stored in two-level hierarchy,
-    where top-level name describes nature/identifier of the test repository,
-    and there could be multiple instances (e.g. generated differently) of the
-    same "content"
-
-    Parameters
-    ----------
-    regex : string, optional
-      Regex to select which test repos to use
-    flavors : {'auto', 'local', 'local-url', 'clone', 'network', 'network-clone'} or list of thereof, optional
-      What URIs to provide.  E.g. 'local' would just provide path to the
-      repository, while 'network' would provide url of the remote location
-      available on Internet containing the test repository.  'clone' would
-      clone repository first to a temporary location. 'network-clone' would
-      first clone from the network location. 'auto' would include the list of
-      appropriate ones (e.g., no 'network*' flavors if network tests are
-      "forbidden").
-    count: int, optional
-      If specified, only up to that number of repositories to test with
-
-    Examples
-    --------
-
-    >>> from datalad.tests.utils import with_testrepos
-    >>> @with_testrepos('basic_annex')
-    ... def test_write(repo):
-    ...    assert(os.path.exists(os.path.join(repo, '.git', 'annex')))
+@with_tempfile(mkdir=True)
+def test_path_prefix(path=None):
+    eq_(get_path_prefix(_p('/d1/d2'), _p('/d1/d2')), _p(''))
+    # so we are under /d1/d2 so path prefix is ..
+    eq_(get_path_prefix(_p('/d1/d2'), _p('/d1/d2/d3')), _p('..'))
+    eq_(get_path_prefix(_p('/d1/d2/d3'), _p('/d1/d2')), _p('d3'))
+    # but if outside -- full path
+    eq_(get_path_prefix(_p('/d1/d2'), _p('/d1/d20/d3')), _p('/d1/d2'))
+    with chpwd(path):
+        eq_(get_path_prefix('.'), '')
+        eq_(get_path_prefix('d1'), 'd1')
+        eq_(get_path_prefix('d1', 'd2'), opj(path, 'd1'))
+        eq_(get_path_prefix('..'), '..')
+
+
+def test_get_trace():
+    assert_raises(ValueError, get_trace, [], 'boom', 'does_not_matter')
+    eq_(get_trace([('A', 'B')], 'A', 'A'), None)
+    eq_(get_trace([('A', 'B')], 'A', 'B'), [])
+    eq_(get_trace([('A', 'B')], 'A', 'C'), None)
+    eq_(get_trace([('A', 'B'),
+                   ('B', 'C')], 'A', 'C'), ['B'])
+    # order of edges doesn't matter
+    eq_(get_trace([
+        ('B', 'C'),
+        ('A', 'B')
+        ], 'A', 'C'), ['B'])
+    # mixed rubbish
+    eq_(get_trace([
+        (1, 3),
+        ('B', 'C'),
+        (None, ('schwak', 7)),
+        ('A', 'B'),
+        ], 'A', 'C'), ['B'])
+    # long
+    eq_(get_trace([
+        ('B', 'C'),
+        ('A', 'B'),
+        ('distract', 'me'),
+        ('C', 'D'),
+        ('D', 'E'),
+        ], 'A', 'E'), ['B', 'C', 'D'])
 
-    """
 
-    @wraps(t)
-    def newfunc(*arg, **kw):
-        # TODO: would need to either avoid this "decorator" approach for
-        # parametric tests or again aggregate failures like sweepargs does
-        flavors_ = _get_resolved_flavors(flavors)
-
-        testrepos_uris = _get_testrepos_uris(regex, flavors_)
-        # we should always have at least one repo to test on, unless explicitly only
-        # network was requested by we are running without networked tests
-        if not (os.environ.get('DATALAD_TESTS_NONETWORK') and flavors == ['network']):
-            assert(testrepos_uris)
+@with_tempfile(mkdir=True)
+def test_get_dataset_root(path=None):
+    eq_(get_dataset_root('/nonexistent'), None)
+    with chpwd(path):
+        repo = AnnexRepo(os.curdir, create=True)
+        subdir = opj('some', 'deep')
+        fname = opj(subdir, 'dummy')
+        os.makedirs(subdir)
+        with open(fname, 'w') as f:
+            f.write('some')
+        repo.add(fname)
+        # we can find this repo
+        eq_(get_dataset_root(os.curdir), os.curdir)
+        # and we get the type of path that we fed in
+        eq_(get_dataset_root(abspath(os.curdir)), abspath(os.curdir))
+        # subdirs are no issue
+        eq_(get_dataset_root(subdir), os.curdir)
+        # even more subdirs are no issue
+        eq_(get_dataset_root(opj(subdir, subdir)), os.curdir)
+        # non-dir paths are no issue
+        eq_(get_dataset_root(fname), os.curdir)
+
+
+def _p(p: str) -> str:
+    """A helper to code paths as POSIX paths in tests  below. Would prepend fake drive
+       C: to absolute paths on Windows"""
+    if on_windows:
+        pm = p.replace('/', os.sep)
+        if p.startswith('/'):
+            return f"C:{pm}"
         else:
-            if not testrepos_uris:
-                raise SkipTest("No non-networked repos to test on")
+            return pm
+    return p
 
-        ntested = 0
-        for uri in testrepos_uris:
-            if count and ntested >= count:
-                break
-            ntested += 1
-            if __debug__:
-                lgr.debug('Running %s on %s' % (t.__name__, uri))
-            try:
-                t(*(arg + (uri,)), **kw)
-            finally:
-                if uri in _TEMP_PATHS_CLONES:
-                    _TEMP_PATHS_CLONES.discard(uri)
-                    rmtemp(uri)
-                pass  # might need to provide additional handling so, handle
-    return newfunc
-with_testrepos.__test__ = False
-
-
-@optional_args
-def with_fake_cookies_db(func, cookies={}):
-    """mock original cookies db with a fake one for the duration of the test
-    """
-    from ..support.cookies import cookies_db
 
-    @wraps(func)
-    def newfunc(*args, **kwargs):
-        try:
-            orig_cookies_db = cookies_db._cookies_db
-            cookies_db._cookies_db = cookies.copy()
-            return func(*args, **kwargs)
-        finally:
-            cookies_db._cookies_db = orig_cookies_db
-    return newfunc
+def test_path_startswith():
+    ok_(path_startswith(_p('/a/b'), _p('/a')))
+    ok_(path_startswith(_p('/a/b'), _p('/a/b')))
+    ok_(path_startswith(_p('/a/b'), _p('/a/b/')))
+    ok_(path_startswith(_p('/a/b/'), _p('/a/b')))
+    ok_(path_startswith(_p('/a/b'), _p('/')))
+    ok_(path_startswith(_p('/aaa/b/c'), _p('/aaa')))
+    nok_(path_startswith(_p('/aaa/b/c'), _p('/aa')))
+    nok_(path_startswith(_p('/a/b'), _p('/a/c')))
+    nok_(path_startswith(_p('/a/b/c'), _p('/a/c')))
+    # must not mix relative and abs
+    assert_raises(ValueError, path_startswith, _p('a/b'), _p('/a'))
+    assert_raises(ValueError, path_startswith, _p('/a/b'), _p('a'))
+
+
+def test_path_is_subpath():
+    ok_(path_is_subpath(_p('/a/b'), _p('/a')))
+    ok_(path_is_subpath(_p('/a/b/c'), _p('/a')))
+    nok_(path_is_subpath(_p('/a/b'), _p('/a/b')))
+    nok_(path_is_subpath(_p('/a/b'), _p('/a/b/')))
+    nok_(path_is_subpath(_p('/a/b/'), _p('/a/b')))
+    ok_(path_is_subpath(_p('/a/b'), _p('/')))
+    ok_(path_is_subpath(_p('/aaa/b/c'), _p('/aaa')))
+    nok_(path_is_subpath(_p('/aaa/b/c'), _p('/aa')))
+    nok_(path_is_subpath(_p('/a/b'), _p('/a/c')))
+    nok_(path_is_subpath(_p('/a/b/c'), _p('/a/c')))
+    # must not mix relative and abs
+    assert_raises(ValueError, path_is_subpath, _p('a/b'), _p('/a'))
+    assert_raises(ValueError, path_is_subpath, _p('/a/b'), _p('a'))
+
+
+def test_probe_known_failure():
+
+    # Note: we can't test the switch "datalad.tests.knownfailures.probe"
+    # directly, since it was evaluated in the decorator already. So we need
+    # to have different assertions in this test based on config and have it
+    # tested across builds, which use different settings for that switch.
 
+    @probe_known_failure
+    def not_failing():
+        pass
 
-def skip_if_no_network(func=None):
-    """Skip test completely in NONETWORK settings
-
-    If not used as a decorator, and just a function, could be used at the module level
-    """
+    @probe_known_failure
+    def failing():
+        raise AssertionError("Failed")
 
-    def check_and_raise():
-        if os.environ.get('DATALAD_TESTS_NONETWORK'):
-            raise SkipTest("Skipping since no network settings")
+    switch = dl_cfg.obtain("datalad.tests.knownfailures.probe")
 
-    if func:
-        @wraps(func)
-        def newfunc(*args, **kwargs):
-            check_and_raise()
-            return func(*args, **kwargs)
-        # right away tag the test as a networked test
-        tags = getattr(newfunc, 'tags', [])
-        newfunc.tags = tags + ['network']
-        return newfunc
+    if switch:
+        # if probing is enabled the failing is considered to be expected and
+        # therefore the decorated function doesn't actually fail:
+        failing()
+        # in opposition a function that doesn't fail raises an AssertionError:
+        assert_raises(AssertionError, not_failing)
     else:
-        check_and_raise()
-
-
-def skip_if_on_windows(func):
-    """Skip test completely under Windows
-    """
-    @wraps(func)
-    def newfunc(*args, **kwargs):
-        if on_windows:
-            raise SkipTest("Skipping on Windows")
-        return func(*args, **kwargs)
-    return newfunc
-
-
-@optional_args
-def skip_if(func, cond=True, msg=None):
-    """Skip test for specific condition
-    """
-    @wraps(func)
-    def newfunc(*args, **kwargs):
-        if cond:
-            raise SkipTest(msg if msg else "condition was True")
-        return func(*args, **kwargs)
-    return newfunc
-
-
-def skip_ssh(func):
-    """Skips SSH tests if on windows or if environment variable
-    DATALAD_TESTS_SSH was not set
-    """
-    @wraps(func)
-    def newfunc(*args, **kwargs):
-        if on_windows:
-            raise SkipTest("SSH currently not available on windows.")
-        from datalad import cfg
-        test_ssh = cfg.get("datalad.tests.ssh", '')
-        if test_ssh in ('', '0', 'false', 'no'):
-            raise SkipTest("Run this test by setting DATALAD_TESTS_SSH")
-        return func(*args, **kwargs)
-    return newfunc
-
-
-# ### ###
-# START known failure decorators
-# ### ###
-
-def probe_known_failure(func):
-    """Test decorator allowing the test to pass when it fails and vice versa
-
-    Setting config datalad.tests.knownfailures.probe to True tests, whether or
-    not the test is still failing. If it's not, an AssertionError is raised in
-    order to indicate that the reason for failure seems to be gone.
-    """
-
-    @wraps(func)
-    def newfunc(*args, **kwargs):
-        from datalad import cfg
-        if cfg.obtain("datalad.tests.knownfailures.probe"):
-            assert_raises(Exception, func, *args, **kwargs)  # marked as known failure
-            # Note: Since assert_raises lacks a `msg` argument, a comment
-            # in the same line is helpful to determine what's going on whenever
-            # this assertion fails and we see a trace back. Otherwise that line
-            # wouldn't be very telling.
-        else:
-            return func(*args, **kwargs)
-    return newfunc
-
-
-def skip_known_failure(func):
-    """Test decorator allowing to skip a test that is known to fail
-
-    Setting config datalad.tests.knownfailures.skip to a bool enables/disables
-    skipping.
-    """
-    from datalad import cfg
-
-    @skip_if(cond=cfg.obtain("datalad.tests.knownfailures.skip"),
-             msg="Skip test known to fail")
-    @wraps(func)
-    def newfunc(*args, **kwargs):
-        return func(*args, **kwargs)
-    return newfunc
-
-
-def known_failure(func):
-    """Test decorator marking a test as known to fail
-
-    This combines `probe_known_failure` and `skip_known_failure` giving the
-    skipping precedence over the probing.
-    """
+        # if probing is disabled it should just fail/pass as is:
+        assert_raises(AssertionError, failing)
+        not_failing()
+
+
+def test_skip_if():
+
+    def dummy():
+        raise AssertionError
+
+    assert_raises(AssertionError, dummy)
+    # if cond is False, call the decorated function:
+    assert_raises(AssertionError, skip_if(cond=False, method='raise')(dummy))
+    # raises SkipTest if cond is True
+    assert_raises(SkipTest, skip_if(cond=True, method='raise')(dummy))
+    # but with method 'pass', there is neither SkipTest nor AssertionError.
+    # Instead the function call is just skipped:
+    skip_if(cond=True, method='pass')(dummy)
+    # But if condition is False, the original function is still called:
+    assert_raises(AssertionError, skip_if(cond=False, method='pass')(dummy))
+
+
+def test_skip_known_failure():
+
+    # Note: we can't test the switch "datalad.tests.knownfailures.skip"
+    # directly, since it was evaluated in the decorator already. So we need
+    # to have different assertions in this test based on config and have it
+    # tested across builds, which use different settings for that switch.
 
     @skip_known_failure
-    @probe_known_failure
-    @wraps(func)
-    def newfunc(*args, **kwargs):
-        return func(*args, **kwargs)
-    return newfunc
-
-
-def known_failure_v6(func):
-    """Test decorator marking a test as known to fail in a v6 test run
-
-    If datalad.repo.version is set to 6 behaves like `known_failure`. Otherwise
-    the original (undecorated) function is returned.
-    """
-
-    from datalad import cfg
-
-    version = cfg.obtain("datalad.repo.version")
-    if version and version == 6:
-
-        @known_failure
-        @wraps(func)
-        def v6_func(*args, **kwargs):
-            return func(*args, **kwargs)
-
-        return v6_func
-
-    return func
-
-
-def known_failure_direct_mode(func):
-    """Test decorator marking a test as known to fail in a direct mode test run
-
-    If datalad.repo.direct is set to True behaves like `known_failure`.
-    Otherwise the original (undecorated) function is returned.
-    """
-
-    from datalad import cfg
-
-    direct = cfg.obtain("datalad.repo.direct")
-    if direct:
-
-        @known_failure
-        @wraps(func)
-        def dm_func(*args, **kwargs):
-            return func(*args, **kwargs)
-
-        return dm_func
+    def failing():
+        raise AssertionError("Failed")
 
-    return func
+    switch = dl_cfg.obtain("datalad.tests.knownfailures.skip")
 
-
-# ### ###
-# END known failure decorators
-# ### ###
-
-
-def skip_v6(func):
-    """Skips tests if datalad is configured to use v6 mode
-    (DATALAD_REPO_VERSION=6)
-    """
-
-    from datalad import cfg
-    version = cfg.obtain("datalad.repo.version")
-
-    @skip_if(version == 6, msg="Skip test in v6 test run")
-    @wraps(func)
-    def newfunc(*args, **kwargs):
-        return func(*args, **kwargs)
-    return newfunc
-
-
-def skip_direct_mode(func):
-    """Skips tests if datalad is configured to use direct mode
-    (set DATALAD_REPO_DIRECT)
-    """
-
-    from datalad import cfg
-
-    @skip_if(cfg.obtain("datalad.repo.direct"),
-             msg="Skip test in direct mode test run")
-    @wraps(func)
-    def newfunc(*args, **kwargs):
-        return func(*args, **kwargs)
-    return newfunc
-
-
-@optional_args
-def assert_cwd_unchanged(func, ok_to_chdir=False):
-    """Decorator to test whether the current working directory remains unchanged
-
-    Parameters
-    ----------
-    ok_to_chdir: bool, optional
-      If True, allow to chdir, so this decorator would not then raise exception
-      if chdir'ed but only return to original directory
-    """
-
-    @wraps(func)
-    def newfunc(*args, **kwargs):
-        cwd_before = os.getcwd()
-        pwd_before = getpwd()
-        exc_info = None
-        try:
-            func(*args, **kwargs)
-        except:
-            exc_info = sys.exc_info()
-        finally:
-            try:
-                cwd_after = os.getcwd()
-            except OSError as e:
-                lgr.warning("Failed to getcwd: %s" % e)
-                cwd_after = None
-
-        if cwd_after != cwd_before:
-            chpwd(pwd_before)
-            if not ok_to_chdir:
-                lgr.warning(
-                    "%s changed cwd to %s. Mitigating and changing back to %s"
-                    % (func, cwd_after, pwd_before))
-                # If there was already exception raised, we better re-raise
-                # that one since it must be more important, so not masking it
-                # here with our assertion
-                if exc_info is None:
-                    assert_equal(cwd_before, cwd_after,
-                                 "CWD changed from %s to %s" % (cwd_before, cwd_after))
-
-        if exc_info is not None:
-            reraise(*exc_info)
-
-    return newfunc
-
-
-@optional_args
-def run_under_dir(func, newdir='.'):
-    """Decorator to run tests under another directory
-
-    It is somewhat ugly since we can't really chdir
-    back to a directory which had a symlink in its path.
-    So using this decorator has potential to move entire
-    testing run under the dereferenced directory name -- sideeffect.
-
-    The only way would be to instruct testing framework (i.e. nose
-    in our case ATM) to run a test by creating a new process with
-    a new cwd
-    """
-
-    @wraps(func)
-    def newfunc(*args, **kwargs):
-        pwd_before = getpwd()
-        try:
-            chpwd(newdir)
-            func(*args, **kwargs)
-        finally:
-            chpwd(pwd_before)
+    if switch:
+        # if skipping is enabled, we shouldn't see the exception:
+        failing()
+    else:
+        # if it's disabled, failing() is executed and therefore exception
+        # is raised:
+        assert_raises(AssertionError, failing)
+
+
+def test_known_failure():
+
+    @known_failure
+    def failing():
+        raise AssertionError("Failed")
+
+    skip = dl_cfg.obtain("datalad.tests.knownfailures.skip")
+    probe = dl_cfg.obtain("datalad.tests.knownfailures.probe")
+
+    if skip:
+        # skipping takes precedence over probing
+        failing()
+    elif probe:
+        # if we probe a known failure it's okay to fail:
+        failing()
+    else:
+        # not skipping and not probing results in the original failure:
+        assert_raises(AssertionError, failing)
 
 
-    return newfunc
+from datalad.utils import read_csv_lines
 
 
-def assert_re_in(regex, c, flags=0, match=True, msg=None):
-    """Assert that container (list, str, etc) contains entry matching the regex
-    """
-    if not isinstance(c, (list, tuple)):
-        c = [c]
-    for e in c:
-        if (re.match if match else re.search)(regex, e, flags=flags):
-            return
-    raise AssertionError(
-        msg or "Not a single entry matched %r in %r" % (regex, c)
+def test_known_failure_direct_mode():
+    # Decorator is deprecated now and that is what we check
+    from .utils_pytest import known_failure_direct_mode
+
+    x = []
+    with swallow_logs(new_level=logging.WARNING) as cml:
+        @known_failure_direct_mode
+        def failing():
+            x.append('ok')
+            raise AssertionError("Failed")
+
+        assert_raises(AssertionError, failing)  # nothing is swallowed
+        eq_(x, ['ok'])  # everything runs
+        assert_in("Direct mode support is deprecated", cml.out)
+
+
+@with_tempfile(content="h1 h2\nv1 2\nv2 3")
+def test_read_csv_lines_basic(infile=None):
+    # Just a basic test, next one with unicode
+    gen = read_csv_lines(infile)
+    ok_generator(gen)
+    eq_(
+        list(gen),
+        [
+            {u'h1': u'v1', u'h2': u'2'},
+            {u'h1': u'v2', u'h2': u'3'},
+        ]
     )
 
 
-def assert_dict_equal(d1, d2):
-    msgs = []
-    if set(d1).difference(d2):
-        msgs.append(" keys in the first dict but not in the second: %s"
-                    % list(set(d1).difference(d2)))
-    if set(d2).difference(d1):
-        msgs.append(" keys in the second dict but not in the first: %s"
-                    % list(set(d2).difference(d1)))
-    for k in set(d1).intersection(d2):
-        same = True
-        try:
-            same = type(d1[k]) == type(d2[k]) and bool(d1[k] == d2[k])
-        except:  # if comparison or conversion to bool (e.g. with numpy arrays) fails
-            same = False
-
-        if not same:
-            msgs.append(" [%r] differs: %r != %r" % (k, d1[k], d2[k]))
-
-        if len(msgs) > 10:
-            msgs.append("and more")
-            break
-    if msgs:
-        raise AssertionError("dicts differ:\n%s" % "\n".join(msgs))
-    # do generic comparison just in case we screwed up to detect difference correctly above
-    eq_(d1, d2)
-
-
-def assert_status(label, results):
-    """Verify that each status dict in the results has a given status label
-
-    `label` can be a sequence, in which case status must be one of the items
-    in this sequence.
-    """
-    label = assure_list(label)
-    results = assure_list(results)
-    for i, r in enumerate(results):
-        try:
-            assert_in('status', r)
-            assert_in(r['status'], label)
-        except AssertionError:
-            raise AssertionError('Test {}/{}: expected status {} not found in:\n{}'.format(
-                i + 1,
-                len(results),
-                label,
-                dumps(results, indent=1, default=lambda x: "<not serializable>")))
-
-
-def assert_message(message, results):
-    """Verify that each status dict in the results has a message
+@with_tempfile(content=u"h1\th2\nv1\tдата".encode('utf-8'))
+def test_read_csv_lines_tsv_unicode(infile=None):
+    # Just a basic test, next one with unicode
+    gen = read_csv_lines(infile)
+    ok_generator(gen)
+    eq_(
+        list(gen),
+        [
+            {u'h1': u'v1', u'h2': u'дата'},
+        ]
+    )
 
-    This only tests the message template string, and not a formatted message
-    with args expanded.
-    """
-    for r in assure_list(results):
-        assert_in('message', r)
-        m = r['message'][0] if isinstance(r['message'], tuple) else r['message']
-        assert_equal(m, message)
-
-
-def assert_result_count(results, n, **kwargs):
-    """Verify specific number of results (matching criteria, if any)"""
-    count = 0
-    results = assure_list(results)
-    for r in results:
-        if not len(kwargs):
-            count += 1
-        elif all(k in r and r[k] == v for k, v in kwargs.items()):
-            count += 1
-    if not n == count:
-        raise AssertionError(
-            'Got {} instead of {} expected results matching {}. Inspected {} record(s):\n{}'.format(
-                count,
-                n,
-                kwargs,
-                len(results),
-                dumps(results, indent=1, default=lambda x: "<not serializable>")))
-
-
-def assert_in_results(results, **kwargs):
-    """Verify that the particular combination of keys and values is found in
-    one of the results"""
-    found = False
-    for r in assure_list(results):
-        if all(k in r and r[k] == v for k, v in kwargs.items()):
-            found = True
-    assert found, "Found no desired result (%s) among %s" % (repr(kwargs), repr(results))
-
-
-def assert_not_in_results(results, **kwargs):
-    """Verify that the particular combination of keys and values is not in any
-    of the results"""
-    for r in assure_list(results):
-        assert any(k not in r or r[k] != v for k, v in kwargs.items())
-
-
-def assert_result_values_equal(results, prop, values):
-    """Verify that the values of all results for a given key in the status dicts
-    match the given sequence"""
-    assert_equal(
-        [r[prop] for r in results],
-        values)
 
+@with_tempfile(content=u"h1\nv1\nv2")
+def test_read_csv_lines_one_column(infile=None):
+    # Just a basic test, next one with unicode
+    eq_(
+        list(read_csv_lines(infile)),
+        [
+            {u'h1': u'v1'},
+            {u'h1': u'v2'},
+        ]
+    )
 
-def assert_result_values_cond(results, prop, cond):
-    """Verify that the values of all results for a given key in the status dicts
-    fullfill condition `cond`.
-
-    Parameters
-    ----------
-    results:
-    prop: str
-    cond: callable
-    """
-    for r in assure_list(results):
-        ok_(cond(r[prop]),
-            msg="r[{prop}]: {value}".format(prop=prop, value=r[prop]))
 
+def _get_testm_tree(ind):
+    """Generate a fake package with submodules
 
-def ignore_nose_capturing_stdout(func):
-    """Decorator workaround for nose's behaviour with redirecting sys.stdout
-
-    Needed for tests involving the runner and nose redirecting stdout.
-    Counter-intuitively, that means it needed for nosetests without '-s'.
-    See issue reported here:
-    https://code.google.com/p/python-nose/issues/detail?id=243&can=1&sort=-id&colspec=ID%20Type%20Status%20Priority%20Stars%20Milestone%20Owner%20Summary
-    """
+    We need to increment index for different tests since otherwise e.g.
+    import_modules fails to import submodule if first import_module_from_file
+    imports that one
+    """
+    return {
+        'dltestm%d' % ind: {
+            '__init__.py': '',
+            'dlsub1': {'__init__.py': 'var = 1'},
+            'dlsub2.py': 'var = 2'}
+    }
 
-    @make_decorator(func)
-    def newfunc(*args, **kwargs):
-        try:
-            func(*args, **kwargs)
-        except AttributeError as e:
-            # Use args instead of .message which is PY2 specific
-            message = e.args[0] if e.args else ""
-            if message.find('StringIO') > -1 and message.find('fileno') > -1:
-                raise SkipTest("Triggered nose defect in masking out real stdout")
-            else:
-                raise
-    return newfunc
-
-
-def skip_httpretty_on_problematic_pythons(func):
-    """As discovered some httpretty bug causes a side-effect
-    on other tests on some Pythons.  So we skip the test if such
-    problematic combination detected
-
-    References
-    https://travis-ci.org/datalad/datalad/jobs/94464988
-    http://stackoverflow.com/a/29603206/1265472
-    """
+@with_tree(tree=_get_testm_tree(1))
+def test_import_modules(topdir=None):
+    try:
+        sys.path.append(topdir)
+        mods = import_modules(['dlsub1', 'bogus'], 'dltestm1')
+    finally:
+        sys.path.pop(sys.path.index(topdir))
+    eq_(len(mods), 1)
+    eq_(mods[0].__name__, 'dltestm1.dlsub1')
+
+
+@with_tree(tree=_get_testm_tree(2))
+def test_import_module_from_file(topdir=None):
+    with assert_raises(AssertionError):
+        # we support only submodule files ending with .py ATM. TODO
+        import_module_from_file(op.join(topdir, 'dltestm2', 'dlsub1'))
+
+    dlsub2_path = op.join(topdir, 'dltestm2', 'dlsub2.py')
+    mod = import_module_from_file(dlsub2_path)
+    eq_(mod.__name__, 'dlsub2')  # we are not asking to import as submod of the dltestm1
+    assert_in('dlsub2', sys.modules)
 
-    @make_decorator(func)
-    def newfunc(*args, **kwargs):
-        if sys.version_info[:3] == (3, 4, 2):
-            raise SkipTest("Known to cause trouble due to httpretty bug on this Python")
-        return func(*args, **kwargs)
-    return newfunc
+    try:
+        sys.path.append(topdir)
+        import dltestm2
+        mod = import_module_from_file(dlsub2_path, pkg=dltestm2)
+        eq_(mod.__name__, 'dltestm2.dlsub2')
+        assert_in('dltestm2.dlsub2', sys.modules)
+    finally:
+        sys.path.pop(sys.path.index(topdir))
+
+
+def test_import_modules_fail():
+    # test that we log failures correctly
+    failures = []
+    import_modules(['bogus'], 'datalad', 'Fail {package}.{module}', failures.append)
+    eq_(len(failures), 1)
+    ok_startswith(failures[0], "Fail datalad.bogus: No module")
+
+
+# Should be the last one since as discovered in NICEMAN might screw up coverage
+def test_line_profile():
+    skip_if_no_module('line_profiler')
+
+    @line_profile
+    def f(j):
+        i = j + 1  # xyz
+        return i
 
+    with swallow_outputs() as cmo:
+        assert_equal(f(3), 4)
+        assert_equal(cmo.err, '')
+        assert_in('i = j + 1  # xyz', cmo.out)
 
-@optional_args
-def with_batch_direct(t):
-    """Helper to run parametric test with possible combinations of batch and direct
-    """
-    @wraps(t)
-    def newfunc():
-        for batch in (False, True):
-            for direct in (False, True) if not on_windows else (True,):
-                yield t, batch, direct
-
-    return newfunc
-
-
-def dump_graph(graph, flatten=False):
-    if flatten:
-        from datalad.metadata import flatten_metadata_graph
-        graph = flatten_metadata_graph(graph)
-    return dumps(
-        graph,
-        indent=1,
-        default=lambda x: 'non-serializable object skipped')
-
-
-# List of most obscure filenames which might or not be supported by different
-# filesystems across different OSs.  Start with the most obscure
-OBSCURE_PREFIX = os.getenv('DATALAD_TESTS_OBSCURE_PREFIX', '')
-OBSCURE_FILENAMES = (
-    " \"';a&b/&cd `| ",  # shouldn't be supported anywhere I guess due to /
-    " \"';a&b&cd `| ",
-    " \"';abcd `| ",
-    " \"';abcd | ",
-    " \"';abcd ",
-    " ;abcd ",
-    " ;abcd",
-    " ab cd ",
-    " ab cd",
-    "a",
-    " abc d.dat ",
-    "abc d.dat ",  # they all should at least support spaces and dots
-)
 
 @with_tempfile(mkdir=True)
-def get_most_obscure_supported_name(tdir):
-    """Return the most obscure filename that the filesystem would support under TEMPDIR
-
-    TODO: we might want to use it as a function where we would provide tdir
-    """
-    for filename in OBSCURE_FILENAMES:
-        filename = OBSCURE_PREFIX + filename
-        if on_windows and filename.rstrip() != filename:
-            continue
-        try:
-            with open(opj(tdir, filename), 'w') as f:
-                f.write("TEST LOAD")
-            return filename  # it will get removed as a part of wiping up the directory
-        except:
-            lgr.debug("Filename %r is not supported on %s under %s",
-                      filename, platform.system(), tdir)
-            pass
-    raise RuntimeError("Could not create any of the files under %s among %s"
-                       % (tdir, OBSCURE_FILENAMES))
+def test_dlabspath(path=None):
+    if not has_symlink_capability():
+        raise SkipTest
+    # initially ran into on OSX https://github.com/datalad/datalad/issues/2406
+    opath = opj(path, "origin")
+    os.makedirs(opath)
+    lpath = opj(path, "linked")
+    os.symlink('origin', lpath)
+    for d in opath, lpath:
+        # regardless under which directory, all results should not resolve
+        # anything
+        eq_(d, dlabspath(d))
+        # in the root of ds
+        with chpwd(d):
+            eq_(dlabspath("path"), opj(d, "path"))
+            eq_(dlabspath("./path"), opj(d, "./path"))  # we do not normpath by default
+            eq_(dlabspath("./path", norm=True), opj(d, "path"))
+
+
+@with_tree({'1': 'content', 'd': {'2': 'more'}})
+def test_get_open_files(p=None):
+    pobj = Path(p)
+    skip_if_no_module('psutil')
+    eq_(get_open_files(p), {})
+    f1 = pobj / '1'
+    subd = pobj / 'd'
+    with f1.open() as f:
+        # since lsof does not care about PWD env var etc, paths
+        # will not contain symlinks, we better realpath them
+        # all before comparison
+        eq_(get_open_files(p, log_open=40)[str(f1.resolve())].pid,
+            os.getpid())
+
+    assert not get_open_files(str(subd))
+
+    if on_windows:
+        # the remainder of the test assume a certain performance.
+        # however, on windows get_open_files() can be very slow
+        # (e.g. the first invocation in this test (above) can easily
+        # take 30-50s). It is not worth slowing the tests to
+        # accommodate this issue, given we have tested proper functioning
+        # in principle already above).
+        return
+
+    # if we start a process within that directory, should get informed
+    from subprocess import (
+        PIPE,
+        Popen,
+    )
+    from time import time
+    t0 = time()
+    proc = Popen([sys.executable, '-c',
+                  r'import sys; sys.stdout.write("OK\n"); sys.stdout.flush();'
+                  r'import time; time.sleep(10)'],
+                 stdout=PIPE,
+                 cwd=str(subd))
+    # Assure that it started and we read the OK
+    eq_(ensure_unicode(proc.stdout.readline().strip()), u"OK")
+    assert time() - t0 < 5 # that we were not stuck waiting for process to finish
+    eq_(get_open_files(p)[str(subd.resolve())].pid, proc.pid)
+    eq_(get_open_files(subd)[str(subd.resolve())].pid, proc.pid)
+    proc.terminate()
+    assert_equal(get_open_files(str(subd)), {})
+
+
+def test_map_items():
+    def add10(x):
+        return x + 10
+    eq_(map_items(add10, {2: 3}), {12: 13})
+
+    class Custom(object):
+        """For testing with custom items possibly of varying length etc"""
+        def __init__(self, items):
+            self._items = list(items)
+
+        def items(self):
+            return self._items
+
+    c = Custom([(1,), (2, 3), (4, 5, 6)])
+    c_mapped = map_items(add10, c)
+    assert type(c) is type(c_mapped)
+    eq_(c_mapped.items(), [(11,), (12, 13), (14, 15, 16)])
+
+
+def test_CMD_MAX_ARG():
+    # 100 is arbitrarily large small integer ;)
+    # if fails -- we are unlikely to be able to work on this system
+    # and something went really wrong!
+    assert_greater(CMD_MAX_ARG, 100)
 
 
-@optional_args
-def with_testsui(t, responses=None, interactive=True):
-    """Switch main UI to be 'tests' UI and possibly provide answers to be used"""
-
-    @wraps(t)
-    def newfunc(*args, **kwargs):
-        from datalad.ui import ui
-        old_backend = ui.backend
-        try:
-            ui.set_backend('tests' if interactive else 'tests-noninteractive')
-            if responses:
-                ui.add_responses(responses)
-            ret = t(*args, **kwargs)
-            if responses:
-                responses_left = ui.get_responses()
-                assert not len(responses_left), "Some responses were left not used: %s" % str(responses_left)
-            return ret
-        finally:
-            ui.set_backend(old_backend)
-
-    if not interactive and responses is not None:
-        raise ValueError("Non-interactive UI cannot provide responses")
-
-    return newfunc
-
-with_testsui.__test__ = False
-
-
-def assert_no_errors_logged(func, skip_re=None):
-    """Decorator around function to assert that no errors logged during its execution"""
-    @wraps(func)
-    def new_func(*args, **kwargs):
-        with swallow_logs(new_level=logging.ERROR) as cml:
-            out = func(*args, **kwargs)
-            if cml.out:
-                if not (skip_re and re.search(skip_re, cml.out)):
-                    raise AssertionError(
-                        "Expected no errors to be logged, but log output is %s"
-                        % cml.out
-                    )
-        return out
-
-    return new_func
-
-
-def get_mtimes_and_digests(target_path):
-    """Return digests (md5) and mtimes for all the files under target_path"""
-    from datalad.utils import find_files
-    from datalad.support.digests import Digester
-    digester = Digester(['md5'])
-
-    # bother only with existing ones for this test, i.e. skip annexed files without content
-    target_files = [
-        f for f in find_files('.*', topdir=target_path, exclude_vcs=False, exclude_datalad=False)
-        if exists(f)
-    ]
-    # let's leave only relative paths for easier analysis
-    target_files_ = [relpath(f, target_path) for f in target_files]
+@with_tempfile(mkdir=True)
+def test_create_tree(path=None):
+    content = u"мама мыла раму"
+    create_tree(path, dict([
+        ('1', content),
+        ('sd', dict(
+            [
+            # right away an obscure case where we have both 1 and 1.gz
+                ('1', content*2),
+                ('1.gz', content*3),
+                ('1.xz', content*4),
+                ('1.lzma', content*5),
+            ]
+        )),
+    ]))
+    ok_file_has_content(op.join(path, '1'), content)
+    ok_file_has_content(op.join(path, 'sd', '1'), content*2)
+    ok_file_has_content(op.join(path, 'sd', '1.gz'), content*3, decompress=True)
+    ok_file_has_content(op.join(path, 'sd', '1.xz'), content*4, decompress=True)
+    ok_file_has_content(op.join(path, 'sd', '1.lzma'), content*5, decompress=True)
+
+
+def test_never_fail():
+
+    @never_fail
+    def iamok(arg):
+        return arg
+    eq_(iamok(1), 1)
+
+    @never_fail
+    def ifail(arg):
+        raise ValueError
+    eq_(ifail(1), None)
+
+    with patch.dict('os.environ', {'DATALAD_ALLOW_FAIL': '1'}):
+        # decision to create failing or not failing function
+        # is done at the time of decoration
+        @never_fail
+        def ifail2(arg):
+            raise ValueError
+
+        assert_raises(ValueError, ifail2, 1)
+
+
+@pytest.mark.xfail(reason="TODO: for some reason fails on Travis")
+@with_tempfile
+def test_is_interactive(fout=None):
+    # must not fail if one of the streams is no longer open:
+    # https://github.com/datalad/datalad/issues/3267
+    from datalad.cmd import (
+        KillOutput,
+        NoCapture,
+        StdOutErrCapture,
+        WitlessRunner,
+    )
+    from datalad.support.annexrepo import (
+        AnnexInitOutput,
+        AnnexJsonProtocol,
+    )
+    from datalad.support.gitrepo import GitProgress
 
-    digests = {frel: digester(f) for f, frel in zip(target_files, target_files_)}
-    mtimes = {frel: os.stat(f).st_mtime for f, frel in zip(target_files, target_files_)}
-    return digests, mtimes
+    bools = ["False", "True"]
 
+    def get_interactive(py_pre="", **run_kwargs):
+        out = WitlessRunner().run(
+            [sys.executable,
+             "-c",
+             py_pre +
+             'from datalad.utils import is_interactive; '
+             'f = open(%r, "w"); '
+             'f.write(str(is_interactive())); '
+             'f.close()'
+             % fout
+             ],
+            **run_kwargs
+        )
+        with open(fout) as f:
+            out = f.read()
+        assert_in(out, bools)
+        return bool(bools.index(out))
+
+    # verify that NoCapture can make fully interactive execution
+    # happen, also test the core protocols
+    # (we can only be interactive in a runner, if the test execution
+    # itself happens in an interactive environment)
+    for proto, interactive in ((NoCapture,
+                                # It is unclear why (on travis only) a child
+                                # process can report to be interactive
+                                # whenever the parent process is not.
+                                # Maintain this test exception until
+                                # someone can provide insight. The point of
+                                # this test is to ensure that NoCapture
+                                # in an interactive parent also keeps the
+                                # child interactive, so this oddity is not
+                                # relevant.
+                                True if on_travis else is_interactive()),
+                               (KillOutput, False),
+                               (StdOutErrCapture, False),
+                               (GitProgress, False),
+                               (AnnexInitOutput, False),
+                               (AnnexJsonProtocol, False)):
+        eq_(get_interactive(protocol=proto),
+            interactive,
+            msg='{} -> {}'.format(str(proto), interactive))
+    # and it must not crash if smth is closed
+    for o in ('stderr', 'stdin', 'stdout'):
+        eq_(get_interactive("import sys; sys.%s.close(); " % o), False)
+
+
+def test_splitjoin_cmdline():
+    # Do full round trip on a number of tricky samples
+    for args in (
+        ['cmd', '-o1', 'simple'],
+        ['c o', r'\m', ''],
+        ['c o', ' '],
+    ):
+        cmdline = join_cmdline(args)
+        assert isinstance(cmdline, str)
+        eq_(split_cmdline(cmdline), args)
+    # assure that there is no needless quoting
+    if on_windows:
+        # in quote_cmdlinearg we always quote on Windows
+        eq_(join_cmdline(['abc', 'def']), '"abc" "def"')
+    else:
+        eq_(join_cmdline(['abc', 'def']), 'abc def')
 
-def get_datasets_topdir():
-    """Delayed parsing so it could be monkey patched etc"""
-    from datalad.consts import DATASETS_TOPURL
-    return RI(DATASETS_TOPURL).hostname
 
-#
-# Context Managers
-#
+@skip_if_root
+@with_tempfile
+def test_obtain_write_permission(path=None):
+    path = Path(path)
 
+    # there's nothing at path yet:
+    assert_raises(FileNotFoundError, obtain_write_permission, path)
 
-#
-# Test tags
-#
-# To be explicit, and not "loose" some tests due to typos, decided to make
-# explicit decorators for common types
-
-from nose.plugins.attrib import attr
+    # Revoke write permission
+    path.write_text("something")
+    path.chmod(path.stat().st_mode & ~stat.S_IWRITE)
+    assert_raises(PermissionError, path.write_text, "different thing")
 
+    # Obtain and try again:
+    obtain_write_permission(path)
+    path.write_text("different thing")
 
-def integration(f):
-    """Mark test as an "integration" test which generally is not needed to be run
-    
-    Generally tend to be slower
-    """
-    return attr('integration')(f)
+    # Already having permission is no issue:
+    obtain_write_permission(path)
+    path.write_text("yet another thing")
 
 
-def slow(f):
-    """Mark test as a slow, although not necessarily integration or usecase test
-    """
-    return attr('slow')(f)
+@skip_if_root
+@with_tempfile(mkdir=True)
+def test_ensure_write_permission(path=None):
 
+    # This is testing the usecase of write protected directories needed for
+    # messing with an annex object tree (as done by the ORA special remote).
+    # However, that doesn't work on Windows since we can't revoke write
+    # permissions for the owner of a directory (at least on VFAT - may be
+    # true for NTFS as well - don't know).
+    # Hence, on windows/crippledFS only test on a file.
+
+    dir_ = Path(path)
+    if not on_windows and has_symlink_capability:
+        # set up write-protected dir containing a file
+        file_ = dir_ / "somefile"
+        file_.write_text("whatever")
+        dir_.chmod(dir_.stat().st_mode & ~stat.S_IWRITE)
+        assert_raises(PermissionError, file_.unlink)
+
+        # contextmanager lets us do it and restores permissions afterwards:
+        mode_before = dir_.stat().st_mode
+        with ensure_write_permission(dir_):
+            file_.unlink()
+
+        mode_after = dir_.stat().st_mode
+        assert_equal(mode_before, mode_after)
+        assert_raises(PermissionError, file_.write_text, "new file can't be "
+                                                         "written")
+
+        assert_raises(FileNotFoundError, ensure_write_permission(dir_ /
+                      "non" / "existent").__enter__)
+
+        # deletion within context doesn't let mode restoration fail:
+        with ensure_write_permission(dir_):
+            dir_.rmdir()
+
+        dir_.mkdir()  # recreate, since next block is executed unconditionally
+
+    # set up write-protected file:
+    file2 = dir_ / "protected.txt"
+    file2.write_text("unchangeable")
+    file2.chmod(file2.stat().st_mode & ~stat.S_IWRITE)
+    assert_raises(PermissionError, file2.write_text, "modification")
+
+    # within context we can:
+    with ensure_write_permission(file2):
+        file2.write_text("modification")
 
-def usecase(f):
-    """Mark test as a usecase user ran into and which (typically) caused bug report
-    to be filed/troubleshooted
-    """
-    return attr('usecase')(f)
+    # mode is restored afterwards:
+    assert_raises(PermissionError, file2.write_text, "modification2")
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `datalad-0.9.3/datalad/tests/utils_testrepos.py` & `datalad-1.0.0/datalad/tests/utils_testrepos.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,79 +1,79 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 
 import os
 import tempfile
+from abc import (
+    ABCMeta,
+    abstractmethod,
+)
+from os.path import (
+    exists,
+)
+from os.path import join as opj
 
-from abc import ABCMeta, abstractmethod
-from six import add_metaclass
-from os.path import dirname, join as opj, exists, pardir
+from datalad import cfg as dl_cfg
+from datalad.customremotes.base import init_datalad_remote
 
-from ..support.gitrepo import GitRepo
+from .. import __version__
 from ..support.annexrepo import AnnexRepo
-from ..cmd import Runner
-from ..support.network import get_local_file_url
 from ..support.external_versions import external_versions
-from ..utils import swallow_outputs
-from ..utils import swallow_logs
-
-from ..version import __version__
+from ..support.gitrepo import GitRepo
+from ..support.network import get_local_file_url
+from ..utils import (
+    swallow_logs,
+    swallow_outputs,
+)
 from . import _TEMP_PATHS_GENERATED
-from .utils import get_tempfile_kwargs
-from datalad.customremotes.base import init_datalad_remote
+from .utils_pytest import get_tempfile_kwargs
 
+# eventually become a URL to a local file served via http
+# that can be used for http/url-based testing
+remote_file_url = None
 
-# we need a local file, that is supposed to be treated as a remote file via
-# file-scheme URL
-remote_file_fd, remote_file_path = \
-    tempfile.mkstemp(**get_tempfile_kwargs({}, prefix='testrepo'))
-# to be removed upon teardown
-_TEMP_PATHS_GENERATED.append(remote_file_path)
-with open(remote_file_path, "w") as f:
-    f.write("content to be annex-addurl'd")
-# OS-level descriptor needs to be closed!
-os.close(remote_file_fd)
 
-
-@add_metaclass(ABCMeta)
-class TestRepo(object):
+class TestRepo(object, metaclass=ABCMeta):
 
     REPO_CLASS = None  # Assign to the class to be used in the subclass
 
     def __init__(self, path=None, puke_if_exists=True):
         if not path:
-            path = tempfile.mktemp(**get_tempfile_kwargs({}, prefix='testrepo'))
+            path = \
+                tempfile.mktemp(**get_tempfile_kwargs(
+                    {'dir': dl_cfg.get("datalad.tests.temp.dir")},
+                    prefix='testrepo'))
             # to be removed upon teardown
             _TEMP_PATHS_GENERATED.append(path)
         if puke_if_exists and exists(path):
             raise RuntimeError("Directory %s for test repo already exist" % path)
         # swallow logs so we don't print all those about crippled FS etc
         with swallow_logs():
             self.repo = self.REPO_CLASS(path)
-            # For additional testing of our datalad remote to not interfer
+            # For additional testing of our datalad remote to not interfere
             # and manage to handle all http urls and requests:
             if self.REPO_CLASS is AnnexRepo and \
                     os.environ.get('DATALAD_TESTS_DATALADREMOTE'):
                 init_datalad_remote(self.repo, 'datalad', autoenable=True)
 
         self._created = False
 
     @property
     def path(self):
         return self.repo.path
 
     @property
     def url(self):
-        return get_local_file_url(self.path)
+        return get_local_file_url(self.path, compatibility='git')
 
     def create_file(self, name, content, add=True, annex=False):
         filename = opj(self.path, name)
         with open(filename, 'wb') as f:
             f.write(content.encode())
         if add:
             if annex:
@@ -99,28 +99,26 @@
 
 class BasicAnnexTestRepo(TestRepo):
     """Creates a basic test git-annex repository"""
 
     REPO_CLASS = AnnexRepo
 
     def populate(self):
+        global remote_file_url
+        if not remote_file_url:
+            # we need a local file, that is server via a URL
+            from datalad.conftest import test_http_server
+            remote_file_name = 'testrepo-annex.dat'
+            with open(opj(test_http_server.path, remote_file_name), "w") as f:
+                f.write("content to be annex-addurl'd")
+            remote_file_url = '{}/{}'.format(test_http_server.url, remote_file_name)
         self.create_info_file()
         self.create_file('test.dat', '123\n', annex=False)
         self.repo.commit("Adding a basic INFO file and rudimentary load file for annex testing")
-        # even this doesn't work on bloody Windows
-        from .utils import on_windows
-        fileurl = get_local_file_url(remote_file_path)
-        # Note:
-        # The line above used to be conditional:
-        # if not on_windows \
-        # else "https://raw.githubusercontent.com/datalad/testrepo--basic--r1/master/test.dat"
-        # This self-reference-ish construction (pointing to 'test.dat'
-        # and therefore have the same content in git and annex) is outdated and
-        # causes trouble especially in annex V6 repos.
-        self.repo.add_url_to_file("test-annex.dat", fileurl)
+        self.repo.add_url_to_file("test-annex.dat", remote_file_url)
         self.repo.commit("Adding a rudimentary git-annex load file")
         self.repo.drop("test-annex.dat")  # since available from URL
 
     def create_info_file(self):
         annex_version = external_versions['cmd:annex']
         git_version = external_versions['cmd:git']
         self.create_file('INFO.txt',
@@ -157,50 +155,45 @@
 
     def populate(self):
 
         super(SubmoduleDataset, self).populate()
         # add submodules
         annex = BasicAnnexTestRepo()
         annex.create()
-        kw = dict(cwd=self.path, expect_stderr=True)
-        self.repo._git_custom_command(
-            '', ['git', 'submodule', 'add', annex.url, 'subm 1'], **kw)
-        self.repo._git_custom_command(
-            '', ['git', 'submodule', 'add', annex.url, '2'], **kw)
-        self.repo._git_custom_command(
-            '', ['git', 'commit', '-m', 'Added subm 1 and 2.'], **kw)
-        self.repo._git_custom_command(
-            '', ['git', 'submodule', 'update', '--init', '--recursive'], **kw)
+        kw = dict(expect_stderr=True)
+        self.repo.call_git(
+            ['submodule', 'add', annex.url, 'subm 1'], **kw)
+        self.repo.call_git(
+            ['submodule', 'add', annex.url, '2'], **kw)
+        self.repo.commit('Added subm 1 and 2.')
+        self.repo.call_git(
+            ['submodule', 'update', '--init', '--recursive'], **kw)
         # init annex in subdatasets
         for s in ('subm 1', '2'):
             AnnexRepo(opj(self.path, s), init=True)
 
 
 class NestedDataset(BasicAnnexTestRepo):
 
     def populate(self):
         super(NestedDataset, self).populate()
         ds = SubmoduleDataset()
         ds.create()
         kw = dict(expect_stderr=True)
-        self.repo._git_custom_command(
-            '', ['git', 'submodule', 'add', ds.url, 'sub dataset1'],
-            cwd=self.path, **kw)
-        self.repo._git_custom_command(
-            '', ['git', 'submodule', 'add', ds.url, 'sub sub dataset1'],
-            cwd=opj(self.path, 'sub dataset1'), **kw)
-        self.repo._git_custom_command(
-            '', ['git', 'commit', '-m', 'Added sub dataset.'],
-            cwd=opj(self.path, 'sub dataset1'), **kw)
-        self.repo._git_custom_command(
-            '', ['git', 'commit', '-a', '-m', 'Added subdatasets.'],
-            cwd=self.path, **kw)
-        self.repo._git_custom_command(
-            '', ['git', 'submodule', 'update', '--init', '--recursive'],
-            cwd=self.path, **kw)
+        self.repo.call_git(
+            ['submodule', 'add', ds.url, 'sub dataset1'], **kw)
+        self.repo.call_git(
+            ['-C', opj(self.path, 'sub dataset1'),
+             'submodule', 'add', ds.url, 'sub sub dataset1'],
+            **kw)
+        GitRepo(opj(self.path, 'sub dataset1')).commit('Added sub dataset.')
+        self.repo.commit('Added subdatasets.', options=["-a"])
+        self.repo.call_git(
+            ['submodule', 'update', '--init', '--recursive'],
+            **kw)
         # init all annexes
         for s in ('', 'sub dataset1', opj('sub dataset1', 'sub sub dataset1')):
             AnnexRepo(opj(self.path, s), init=True)
 
 
 class InnerSubmodule(object):
 
@@ -209,11 +202,11 @@
 
     @property
     def path(self):
         return opj(self._ds.path, 'sub dataset1', 'subm 1')
 
     @property
     def url(self):
-        return get_local_file_url(self.path)
+        return get_local_file_url(self.path, compatibility='git')
 
     def create(self):
         self._ds.create()
```

### Comparing `datalad-0.9.3/datalad/tests/test_api.py` & `datalad-1.0.0/datalad/customremotes/main.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,72 +1,90 @@
-# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
-# vi: set ft=python sts=4 ts=4 sw=4 et:
+# emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
+# ex: set sts=4 ts=4 sw=4 noet:
 ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
-#   See COPYING file distributed along with the DataLad package for the
+#   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-'''Unit tests for Python API functionality.'''
+"""CLI entrypoint for special remotes"""
 
-import re
-from inspect import getargspec
+__docformat__ = 'restructuredtext'
 
-from nose.tools import assert_true, assert_false
-from nose import SkipTest
-from nose.tools import eq_
-
-from datalad.tests.utils import assert_in
-
-
-def test_basic_setup():
-    # the import alone will verify that all default values match their
-    # constraints
-    from datalad import api
-    # random pick of something that should be there
-    assert_true(hasattr(api, 'install'))
-    assert_true(hasattr(api, 'test'))
-    assert_true(hasattr(api, 'crawl'))
-    # make sure all helper utilities do not pollute the namespace
-    # and we end up only with __...__ attributes
-    assert_false(list(filter(lambda s: s.startswith('_') and not re.match('__.*__', s), dir(api))))
-
-    assert_in('Parameters', api.Dataset.install.__doc__)
-    assert_in('Parameters', api.Dataset.create.__doc__)
-
-
-def _test_consistent_order_of_args(intf, spec_posargs):
-    f = getattr(intf, '__call__')
-    args, varargs, varkw, defaults = getargspec(f)
-    # now verify that those spec_posargs are first among args
-    if not spec_posargs:
-        raise SkipTest("no positional args") # print intf, "skipped"
-#    else:
-#        print intf, spec_posargs
-    if intf.__name__ == 'Save':
-        # it makes sense there to have most command argument first
-        # -- the message. But we don't enforce it on cmdline so it is
-        # optional
-        spec_posargs.add('message')
-    eq_(set(args[:len(spec_posargs)]), spec_posargs)
-
-
-def test_consistent_order_of_args():
-    from datalad.interface.base import get_interface_groups
-
-    from importlib import import_module
-
-    for grp_name, grp_descr, interfaces in get_interface_groups():
-        for intfspec in interfaces:
-            # turn the interface spec into an instance
-            mod = import_module(intfspec[0], package='datalad')
-            intf = getattr(mod, intfspec[1])
-            spec = getattr(intf, '_params_', dict())
-
-            # figure out which of the specs are "positional"
-            spec_posargs = {
-                name
-                for name, param in spec.items()
-                if param.cmd_args and not param.cmd_args[0].startswith('-')
-            }
-            # we have information about positional args
-            yield _test_consistent_order_of_args, intf, spec_posargs
+
+import argparse
+
+from datalad.cli.parser import (
+    parser_add_common_opt,
+    parser_add_version_opt,
+)
+from datalad.cli.utils import setup_exceptionhook
+from datalad.ui import ui
+
+import logging
+lgr = logging.getLogger('datalad.customremotes')
+
+
+def setup_parser(remote_name, description):
+    # setup cmdline args parser
+    # main parser
+    parser = argparse.ArgumentParser(
+        description= \
+        f"git-annex-remote-{remote_name} is a git-annex custom special " \
+        f"remote to {description}",
+        epilog='"DataLad\'s git-annex very special remote"',
+        formatter_class=argparse.RawDescriptionHelpFormatter,
+        add_help=True,
+    )
+    # common options
+    parser_add_common_opt(parser, 'log_level')
+    parser_add_version_opt(parser, 'datalad', include_name=True)
+    if __debug__:
+        parser.add_argument(
+            '--dbg', action='store_true', dest='common_debug',
+            help="Catch exceptions and fall into debugger upon exception")
+    return parser
+
+
+def _main(args, cls):
+    """Unprotected portion"""
+    assert(cls is not None)
+    from annexremote import Master
+    master = Master()
+    remote = cls(master)
+    master.LinkRemote(remote)
+    master.Listen()
+    # cleanup
+    if hasattr(remote, 'stop'):
+        remote.stop()
+
+
+def main(args=None, cls=None, remote_name=None, description=None):
+    import sys
+    parser = setup_parser(remote_name, description)
+    # parse cmd args
+    args = parser.parse_args(args)
+
+    # stdin/stdout will be used for interactions with annex
+    ui.set_backend('annex')
+
+    if args.common_debug:
+        # So we could see/stop clearly at the point of failure
+        setup_exceptionhook()
+        _main(args, cls)
+    else:
+        # Otherwise - guard and only log the summary. Postmortem is not
+        # as convenient if being caught in this ultimate except
+        try:
+            _main(args, cls)
+        except Exception as exc:
+            lgr.debug('%s (%s) - passing ERROR to git-annex and exiting',
+                      str(exc), exc.__class__.__name__)
+            # `SpecialRemote` classes are supposed to catch everything and
+            # turn it into a `RemoteError` resulting in an ERROR message to
+            # annex. If we end up here, something went wrong outside of the
+            # `master.Listen()` call in `_main`.
+            # In any case, exiting the special remote process should be
+            # accompanied by such an ERROR message to annex rather than a log
+            # message.
+            print("ERROR %s (%s)" % (str(exc), exc.__class__.__name__))
+            sys.exit(1)
```

### Comparing `datalad-0.9.3/datalad/tests/test_dochelpers.py` & `datalad-1.0.0/datalad/support/tests/test_path.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,172 +1,149 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""Tests for dochelpers (largely copied from PyMVPA, the same copyright)
-"""
 
 import os
-from mock import patch
+from pathlib import PurePosixPath
 
-from ..dochelpers import single_or_plural, borrowdoc, borrowkwargs
-from ..dochelpers import exc_str
-
-from .utils import assert_equal, assert_true, assert_raises
-from .utils import assert_re_in
-
-
-def test_basic():
-    assert_equal(single_or_plural('a', 'b', 1), 'a')
-    assert_equal(single_or_plural('a', 'b', 0), 'b')
-    assert_equal(single_or_plural('a', 'b', 123), 'b')
-    assert_equal(single_or_plural('a', 'b', 123, include_count=True), '123 b')
-
-
-def test_borrow_doc():
-
-    class A(object):
-        def met1(self):
-            """met1doc"""
-            pass  # pragma: no cover
-        def met2(self):
-            """met2doc"""
-            pass  # pragma: no cover
-
-    class B(object):
-        @borrowdoc(A)
-        def met1(self):
-            pass  # pragma: no cover
-        @borrowdoc(A, 'met1')
-        def met2(self):
-            pass  # pragma: no cover
-
-    assert_equal(B.met1.__doc__, A.met1.__doc__)
-    assert_equal(B.met2.__doc__, A.met1.__doc__)
-
-
-def test_borrow_kwargs():
-
-    class A(object):
-        def met1(self, kp1=None, kp2=1):
-            """met1 doc
-
-            Parameters
-            ----------
-            kp1 : None or int
-              keyword parameter 1
-            kp2 : int, optional
-              something
-            """
-            pass  # pragma: no cover
-
-        def met2(self):
-            """met2doc"""
-            pass  # pragma: no cover
-
-    class B(object):
-
-        @borrowkwargs(A)
-        def met1(self, bu, **kwargs):
-            """B.met1 doc
-
-            Parameters
-            ----------
-            bu
-              description
-            **kwargs
-              Same as in A.met1
-
-            Some postamble
-            """
-            pass  # pragma: no cover
-
-        @borrowkwargs(A, 'met1')
-        def met_nodoc(self, **kwargs):
-            pass  # pragma: no cover
-
-        @borrowkwargs(methodname=A.met1)
-        def met_anothermet(self, **kwargs):
-            pass  # pragma: no cover
-
-        @borrowkwargs(A, 'met1')
-        def met_nodockwargs(self, bogus=None, **kwargs):
-            """B.met_nodockwargs
-
-            Parameters
-            ----------
-            bogus
-              something
-            """
-            pass  # pragma: no cover
-
-        if True:
-            # Just so we get different indentation level
-            @borrowkwargs(A, 'met1', ['kp1'])
-            def met_excludes(self, boguse=None, **kwargs):
-                """B.met_excludes
-
-                Parameters
-                ----------
-                boguse
-                  something
-                """
-                pass  # pragma: no cover
-
-    assert_true('B.met1 doc' in B.met1.__doc__)
-    for m in (B.met1,
-              B.met_nodoc,
-              B.met_anothermet,
-              B.met_nodockwargs,
-              B.met_excludes):
-        docstring = m.__doc__
-        assert_true('Parameters' in docstring)
-        assert_true(not '*kwargs' in docstring,
-            msg="We shouldn't carry kwargs in docstring now,"
-                "Got %r for %s" % (docstring, m))
-        assert_true('kp2 ' in docstring)
-        assert_true((('kp1 ' in docstring)
-                             ^ (m == B.met_excludes)))
-        # indentation should have been squashed properly
-        assert_true(not '   ' in docstring)
-
-    # some additional checks to see if we are not loosing anything
-    assert_true('Some postamble' in B.met1.__doc__)
-    assert_true('B.met_nodockwargs' in B.met_nodockwargs.__doc__)
-    assert_true('boguse' in B.met_excludes.__doc__)
-
-def test_exc_str():
-    try:
-        raise Exception("my bad")
-    except Exception as e:
-        estr = exc_str(e)
-    assert_re_in("my bad \[test_dochelpers.py:test_exc_str:...\]", estr)
-
-    def f():
-        def f2():
-            raise Exception("my bad again")
-        f2()
-    try:
-        f()
-    except Exception as e:
-        # default one:
-        estr2 = exc_str(e, 2)
-        estr1 = exc_str(e, 1)
-        # and we can control it via environ by default
-        with patch.dict('os.environ', {'DATALAD_EXC_STR_TBLIMIT': '3'}):
-            estr3 = exc_str(e)
-        with patch.dict('os.environ', {}, clear=True):
-            estr_ = exc_str()
-
-    assert_re_in("my bad again \[test_dochelpers.py:test_exc_str:...,test_dochelpers.py:f:...,test_dochelpers.py:f2:...\]", estr3)
-    assert_re_in("my bad again \[test_dochelpers.py:f:...,test_dochelpers.py:f2:...\]", estr2)
-    assert_re_in("my bad again \[test_dochelpers.py:f2:...\]", estr1)
-    assert_equal(estr_, estr1)
-
-    try:
-        raise NotImplementedError
-    except Exception as e:
-        assert_re_in("NotImplementedError\(\) \[test_dochelpers.py:test_exc_str:...\]", exc_str(e))
+from ...tests.utils_pytest import (
+    SkipTest,
+    assert_raises,
+    eq_,
+    with_tempfile,
+)
+from ...utils import (
+    chpwd,
+    on_windows,
+    rmtree,
+)
+from ..path import (
+    abspath,
+    curdir,
+    get_parent_paths,
+    get_filtered_paths_,
+    robust_abspath,
+    split_ext,
+)
+
+import pytest
+
+
+@with_tempfile(mkdir=True)
+def test_robust_abspath(tdir=None):
+    with chpwd(tdir):
+        eq_(robust_abspath(curdir), tdir)
+        try:
+            if os.environ.get('DATALAD_ASSERT_NO_OPEN_FILES'):
+                raise Exception("cannot test under such pressure")
+            rmtree(tdir)
+        except Exception as exc:
+            # probably windows or above exception
+            raise SkipTest(
+                "Cannot test in current environment") from exc
+
+        assert_raises(OSError, abspath, curdir)
+        eq_(robust_abspath(curdir), tdir)
+
+
+def test_split_ext():
+    eq_(split_ext("file"), ("file", ""))
+
+    eq_(split_ext("file.py"), ("file", ".py"))
+    eq_(split_ext("file.tar.gz"), ("file", ".tar.gz"))
+    eq_(split_ext("file.toolong.gz"), ("file.toolong", ".gz"))
+
+    eq_(split_ext("file.a.b.c.d"), ("file", ".a.b.c.d"))
+    eq_(split_ext("file.a.b.cccc.d"), ("file", ".a.b.cccc.d"))
+    eq_(split_ext("file.a.b.ccccc.d"), ("file.a.b.ccccc", ".d"))
+
+    eq_(split_ext("file.a.b..c"), ("file", ".a.b..c"))
+
+
+@pytest.mark.parametrize("sep", [None, '/', '\\'])
+def test_get_parent_paths(sep):
+    if sep is None:
+        gpp = get_parent_paths
+    else:
+        from functools import partial
+        gpp = partial(get_parent_paths, sep=sep)
+
+    # sanity/border checks
+    eq_(gpp([], []), [])
+    eq_(gpp([], ['a']), [])
+    eq_(gpp(['a'], ['a']), ['a'])
+
+    # Helper to provide testing across different seps and platforms while
+    # specifying only POSIX paths here in the test
+    def _p(path):
+        if sep is None:
+            return path
+        else:
+            return path.replace('/', sep)
+    _pp = lambda paths: list(map(_p, paths))
+
+    # no absolute paths anywhere
+    if on_windows:
+        assert_raises(ValueError, gpp, 'C:\\a', ['a'])
+        assert_raises(ValueError, gpp, ['a'], 'C:\\a')
+    elif sep != '\\':  # \ does not make it absolute
+        assert_raises(ValueError, gpp, _p('/a'), ['a'])
+        assert_raises(ValueError, gpp, ['a'], [_p('/a')])
+    assert_raises(ValueError, gpp, [_p('a//a')], ['a'])
+    # dups the actual code but there is no other way AFAIK
+    asep = {'/': '\\', None: '\\', '\\': '/'}[sep]
+    assert_raises(ValueError, gpp, [f'a{asep}a'], ['a'])
+
+    paths = _pp(['a', 'a/b', 'a/b/file', 'c', 'd/sub/123'])
+
+    eq_(gpp(paths, []), paths)
+    eq_(gpp(paths, [], True), [])
+
+    # actually a tricky one!  we should check in descending lengths etc
+    eq_(gpp(paths, paths), paths)
+    # every path is also its own parent
+    eq_(gpp(paths, paths, True), paths)
+
+    # subdatasets not for every path -- multiple paths hitting the same parent,
+    # and we will be getting only a single entry
+    # to mimic how git ls-tree operates
+    eq_(gpp(paths, ['a']), ['a', 'c', _p('d/sub/123')])
+    eq_(gpp(paths, ['a'], True), ['a'])
+
+    # and we get the deepest parent
+    eq_(gpp(_pp(['a/b/file', 'a/b/file2']), _pp(['a', 'a/b'])), _pp(['a/b']))
+
+
+def test_get_filtered_paths_():
+    # just to avoid typing all the same
+    def gfp(*args, **kwargs):
+        return list(get_filtered_paths_(*args, **kwargs))
+
+    assert gfp(['a', 'b'], ['a', 'c']) == ['a']
+    assert gfp(['a', 'b'], ['b']) == ['b']
+    assert gfp(['a', 'b'], ['c']) == []
+
+    assert gfp(['a', 'b'], ['a/b', 'c']) == []  # a is not subpath of a/b
+    assert gfp(['a', 'b'], ['a/b', 'c'], include_within_path=True) == ['a']  # a is not subpath of a/b
+
+    # all paths returned due to '.', and order is sorted
+    paths = ['a', 'b', '1/2/3', 'abc']
+    paths_sorted = sorted(paths)
+    assert gfp(paths, ['.']) == paths_sorted
+    assert gfp(paths, paths_sorted) == paths_sorted
+    assert gfp(paths, paths_sorted, include_within_path=True) == paths_sorted
+    # we can take a mix of str and Path
+    assert gfp([PurePosixPath(paths[0])] + paths[1:], ['.']) == paths_sorted
+
+
+    # nothing within empty "filter_paths" matches -- so no paths yielded
+    assert gfp(paths, []) == []
+
+    assert_raises(ValueError, gfp, ['/a'], [])
+    assert_raises(ValueError, gfp, [PurePosixPath('/a')], [])
+    assert_raises(ValueError, gfp, ['a'], ['/a'])
+    assert_raises(ValueError, gfp, ['../a'], ['a'])
```

### Comparing `datalad-0.9.3/datalad/tests/test_strings.py` & `datalad-1.0.0/datalad/tests/test_strings.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 
-from .utils import *
-
 from ..support.strings import apply_replacement_rules
+from .utils_pytest import *
+
 
 def test_apply_replacement_rules():
     # replacement rule should be at least 3 char long
     assert_raises(ValueError, apply_replacement_rules, '/', 'some')
     assert_raises(ValueError, apply_replacement_rules, ['/a/b', '/'], 'some')
     # and pattern should have the separator only twice
     assert_raises(ValueError, apply_replacement_rules, '/ab', 'some')
```

### Comparing `datalad-0.9.3/datalad/tests/test_direct_mode.py` & `datalad-1.0.0/datalad/tests/test_direct_mode.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,105 +1,93 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 """Test direct mode mechanic
 
 """
 
+from unittest.mock import patch
 
-
-import logging
-
-# Please do ignore possible unused marking.
-# This is used via Dataset class:
-import datalad.api
-from datalad import cfg
-
-from nose.tools import ok_
-from mock import patch
-
+from datalad.support import path as op
 from datalad.support.annexrepo import AnnexRepo
-from datalad.utils import swallow_logs
-from datalad.distribution.dataset import Dataset
-
-from .utils import with_tempfile
-from .utils import skip_if_no_network
-from .utils import with_testrepos
-from .utils import on_windows
-from .utils import SkipTest
-
-
-if on_windows:
-    raise SkipTest("Can't test direct mode switch, "
-                   "if direct mode is forced by OS anyway.")
-
-repo_version = cfg.get("datalad.repo.version", None)
-if repo_version and int(repo_version) >= 6:
-    raise SkipTest("Can't test direct mode switch, "
-                   "if repository version 6 or later is enforced.")
-
-
-@with_tempfile
-@with_tempfile
-@with_tempfile
-@with_tempfile
-def test_direct_cfg(path1, path2, path3, path4):
-    with patch.dict('os.environ', {'DATALAD_REPO_DIRECT': 'True'}):
-        # create annex repo in direct mode:
-        with swallow_logs(new_level=logging.DEBUG) as cml:
-            ar = AnnexRepo(path1, create=True)
-            cml.assert_logged("Switching to direct mode",
-                              regex=False, level='DEBUG')
-            ok_(ar.is_direct_mode())
-
-        # but don't if repo version is 6 (actually, 6 or above):
-        with swallow_logs(new_level=logging.WARNING) as cml:
-            ar = AnnexRepo(path2, create=True, version=6)
-            ok_(not ar.is_direct_mode())
-            cml.assert_logged("direct mode not available", regex=False,
-                              level='WARNING')
-
-        # explicit parameter `direct` has priority:
-        ar = AnnexRepo(path3, create=True, direct=False)
-        if not ar.is_crippled_fs():  # otherwise forced direct mode
-            ok_(not ar.is_direct_mode())
-
-        # don't touch existing repo:
-        ar = AnnexRepo(path2, create=True)
-        if not ar.is_crippled_fs():  # otherwise forced direct mode
-            ok_(not ar.is_direct_mode())
-
-    # make sure, value is relevant:
-    with patch.dict('os.environ', {'DATALAD_REPO_DIRECT': '0'}):
-        # don't use direct mode
-        ar = AnnexRepo(path4, create=True)
-        if not ar.is_crippled_fs():  # otherwise forced direct mode
-            ok_(not ar.is_direct_mode())
+from datalad.support.exceptions import (
+    CommandNotAvailableError,
+    DirectModeNoLongerSupportedError,
+)
+from datalad.tests.utils_pytest import (
+    SkipTest,
+    assert_in,
+    assert_raises,
+    with_tempfile,
+)
+
+# if on_windows:
+#     raise SkipTest("Can't test direct mode switch, "
+#                    "if direct mode is forced by OS anyway.")
+#
+# repo_version = cfg.get("datalad.repo.version", None)
+# if repo_version and int(repo_version) >= 6:
+#     raise SkipTest("Can't test direct mode switch, "
+#                    "if repository version 6 or later is enforced.")
+
+
+# originally lifted from AnnexRepo, kept here to simulate a repo
+# that is still in direct mode
+def _set_direct_mode(self, enable_direct_mode=True):
+    """Switch to direct or indirect mode
+
+    WARNING!  To be used only for internal development purposes.
+              We no longer support direct mode and thus setting it in a
+              repository would render it unusable for DataLad
+
+    Parameters
+    ----------
+    enable_direct_mode: bool
+        True means switch to direct mode,
+        False switches to indirect mode
+
+    Raises
+    ------
+    CommandNotAvailableError
+        in case you try to switch to indirect mode on a crippled filesystem
+    """
+    if self.is_crippled_fs() and not enable_direct_mode:
+        # TODO: ?? DIRECT - should we call git annex upgrade?
+        raise CommandNotAvailableError(
+            cmd="git-annex indirect",
+            msg="Can't switch to indirect mode on that filesystem.")
+
+    self.call_annex(['direct' if enable_direct_mode else 'indirect']),
+    self.config.reload()
+
+    # For paranoid we will just re-request
+    self._direct_mode = None
+    assert(self.is_direct_mode() == enable_direct_mode)
+
+    # All further workarounds were stripped - no direct mode is supported
 
 
 @with_tempfile
-def test_direct_create(path):
-    with patch.dict('os.environ', {'DATALAD_REPO_DIRECT': 'True'}):
-        ds = Dataset(path).create()
-        if not ds.repo.is_crippled_fs():  # otherwise forced direct mode
-            ok_(ds.repo.is_direct_mode())
-
-
-# Note/TODO: Currently flavor 'network' only, since creation of local testrepos
-# fails otherwise ATM. (git submodule add without needed git options to work in
-# direct mode)
-@skip_if_no_network
-@with_testrepos('basic_annex', flavors=['network'])
 @with_tempfile
-def test_direct_install(url, path):
-
-    with patch.dict('os.environ', {'DATALAD_REPO_DIRECT': 'True'}):
-        ds = datalad.api.install(
-            path=path, source=url,
-            result_xfm='datasets', return_type='item-or-list')
-        if not ds.repo.is_crippled_fs():  # otherwise forced direct mode
-            ok_(ds.repo.is_direct_mode(), "Not in direct mode: %s" % ds)
+def test_direct_cfg(path1=None, path2=None):
+    # and if repo already exists and we have env var - we fail too
+    # Adding backend so we get some commit into the repo
+    ar = AnnexRepo(path1, create=True, backend='MD5E')
+    del ar;  AnnexRepo._unique_instances.clear()  # fight flyweight
+    for path in (path1, path2):
+        with patch.dict('os.environ', {'DATALAD_REPO_DIRECT': 'True'}):
+            # try to create annex repo in direct mode as see how it fails
+            with assert_raises(DirectModeNoLongerSupportedError) as cme:
+                AnnexRepo(path, create=True)
+            assert_in("no longer supported by DataLad", str(cme.value)) # we have generic part
+            assert_in("datalad.repo.direct configuration", str(cme.value)) # situation specific part
+    # assert not op.exists(path2)   # that we didn't create it - we do!
+    #   fixing for that would be too cumbersome since we first call GitRepo.__init__
+    #   with create
+    ar = AnnexRepo(path1)
+    # check if we somehow didn't reset the flag
+    assert not ar.is_direct_mode()
```

### Comparing `datalad-0.9.3/datalad/interface/tests/test_add_archive_content.py` & `datalad-1.0.0/datalad/local/tests/test_add_archive_content.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,57 +1,85 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 """Tests for add-archive-content command
 
 """
 
 __docformat__ = 'restructuredtext'
 
-from datalad.tests.utils import known_failure_v6
-
-
-import logging
 import os
-from os import unlink
-from os.path import exists, join as opj, pardir, basename, lexists
+import tempfile
 from glob import glob
+from os import unlink
+from os.path import (
+    basename,
+    exists,
+)
+from os.path import join as opj
+from os.path import (
+    lexists,
+    pardir,
+)
+from pathlib import Path
 
-from ...tests.utils import ok_, eq_, assert_cwd_unchanged, assert_raises, \
-    with_tempfile, assert_in
-from ...tests.utils import assert_equal, assert_not_equal
-from ...tests.utils import assert_false
-from ...tests.utils import assert_true
-from ...tests.utils import ok_archives_caches
-from ...tests.utils import SkipTest
-from ...tests.utils import assert_re_in
-from datalad.tests.utils import assert_result_values_cond
-
-from ...support.annexrepo import AnnexRepo
-from ...support.exceptions import FileNotInRepositoryError
-from ...support.exceptions import CommandError
-from ...tests.utils import with_tree, serve_path_via_http, ok_file_under_git, swallow_outputs
-from ...tests.utils import swallow_logs
-from ...tests.utils import integration
-from ...utils import chpwd, getpwd, rmtemp
-from ...utils import find_files
-from ...utils import rmtree
-from datalad.log import lgr
-from ...api import add_archive_content, clean
+from datalad.api import (
+    Dataset,
+    add_archive_content,
+    clean,
+)
+from datalad.consts import (
+    ARCHIVES_SPECIAL_REMOTE,
+    DATALAD_SPECIAL_REMOTES_UUIDS,
+)
+from datalad.support.exceptions import (
+    CommandError,
+    NoDatasetFound,
+)
 from datalad.support.external_versions import external_versions
-from datalad.consts import DATALAD_SPECIAL_REMOTES_UUIDS
-from datalad.consts import ARCHIVES_SPECIAL_REMOTE
-
-from datalad.tests.utils import create_tree
-from datalad.tests.utils import ok_clean_git
+from datalad.tests.utils_pytest import (
+    assert_cwd_unchanged,
+    assert_equal,
+    assert_false,
+    assert_in,
+    assert_in_results,
+    assert_not_in,
+    assert_raises,
+    assert_repo_status,
+    assert_result_values_cond,
+    assert_true,
+    create_tree,
+    eq_,
+    integration,
+    known_failure_windows,
+    ok_,
+    ok_archives_caches,
+    ok_file_has_content,
+    ok_file_under_git,
+    serve_path_via_http,
+    skip_if,
+    skip_if_adjusted_branch,
+    swallow_outputs,
+    with_tempfile,
+    with_tree,
+    xfail_buggy_annex_info,
+)
+from datalad.utils import (
+    chpwd,
+    find_files,
+    get_tempfile_kwargs,
+    getpwd,
+    on_windows,
+    rmtemp,
+)
 
 treeargs = dict(
     tree=(
         ('1.tar.gz', (
             ('crcns_pfc-1_data', (('CR24A', (
                                     ('behaving1', {'1 f.txt': '1 f load'}),)),)),
             ('crcns_pfc-1_data', (('CR24C', (
@@ -72,55 +100,61 @@
 )
 
 
 @assert_cwd_unchanged(ok_to_chdir=True)
 @with_tree(**treeargs)
 @serve_path_via_http()
 @with_tempfile(mkdir=True)
-def test_add_archive_dirs(path_orig, url, repo_path):
+def test_add_archive_dirs(path_orig=None, url=None, repo_path=None):
     # change to repo_path
-    chpwd(repo_path)
-
-    # create annex repo
-    repo = AnnexRepo(repo_path, create=True, direct=False)
+    with chpwd(repo_path):
+        # create annex repo
+        ds = Dataset(repo_path).create(force=True)
+        repo = ds.repo
+        # add archive to the repo so we could test
+        with swallow_outputs():
+            repo.add_url_to_file('1.tar.gz', opj(url, '1.tar.gz'))
+        repo.commit("added 1.tar.gz")
 
-    # add archive to the repo so we could test
-    with swallow_outputs():
-        repo.add_urls([opj(url, '1.tar.gz')], options=["--pathdepth", "-1"])
-    repo.commit("added 1.tar.gz")
+        # test with excludes and annex options
+        add_archive_content('1.tar.gz',
+                            existing='archive-suffix',
+                            # Since inconsistent and seems in many cases no
+                            # leading dirs to strip, keep them as provided
+                            strip_leading_dirs=True,
+                            delete=True,
+                            leading_dirs_consider=['crcns.*', '1'],
+                            leading_dirs_depth=2,
+                            use_current_dir=False,
+                            exclude='.*__MACOSX.*')  # some junk penetrates
 
-    # test with excludes and annex options
-    add_archive_content('1.tar.gz',
-                        existing='archive-suffix',
-                        # Since inconsistent and seems in many cases no leading dirs to strip, keep them as provided
-                        strip_leading_dirs=True,
-                        delete=True,
-                        leading_dirs_consider=['crcns.*', '1'],
-                        leading_dirs_depth=2,
-                        use_current_dir=False,
-                        exclude='.*__MACOSX.*')  # some junk penetrates
-
-    if external_versions['cmd:annex'] >= '6.20170208':
-        # should have fixed remotes
-        eq_(repo.get_description(uuid=DATALAD_SPECIAL_REMOTES_UUIDS[ARCHIVES_SPECIAL_REMOTE]),
+        eq_(repo.get_description(
+            uuid=DATALAD_SPECIAL_REMOTES_UUIDS[ARCHIVES_SPECIAL_REMOTE]),
             '[%s]' % ARCHIVES_SPECIAL_REMOTE)
 
-    all_files = sorted(find_files('.'))
-    target_files = {
-        './CR24A/behaving1/1 f.txt',
-        './CR24C/behaving3/3 f.txt',
-        './CR24D/behaving2/2 f.txt',
-    }
-    eq_(set(all_files), target_files)
-
-    # regression test: the subdir in MACOSX wasn't excluded and its name was getting stripped by leading_dir_len
-    assert_false(exists('__MACOSX'))  # if stripping and exclude didn't work this fails
-    assert_false(exists('c-1_data'))  # if exclude doesn't work then name of subdir gets stripped by leading_dir_len
-    assert_false(exists('CR24B'))     # if exclude doesn't work but everything else works this fails
-
+        all_files = sorted(find_files('.'))
+        # posixify paths to make it work on Windows as well
+        all_files = [Path(file).as_posix() for file in all_files]
+        target_files = {
+            'CR24A/behaving1/1 f.txt',
+            'CR24C/behaving3/3 f.txt',
+            'CR24D/behaving2/2 f.txt',
+            '.datalad/config',
+        }
+        eq_(set(all_files), target_files)
+
+        # regression test: the subdir in MACOSX wasn't excluded and its name was
+        # getting stripped by leading_dir_len
+        # if stripping and exclude didn't work this fails
+        assert_false(exists('__MACOSX'))
+        # if exclude doesn't work then name of subdir gets stripped by
+        # leading_dir_len
+        assert_false(exists('c-1_data'))
+        # if exclude doesn't work but everything else works this fails
+        assert_false(exists('CR24B'))
 
 # within top directory
 # archive is in subdirectory -- adding in the same (or different) directory
 
 tree1args = dict(
     tree=(
         ('1.tar.gz', (
@@ -159,113 +193,147 @@
                   '2 f.txt': '2 f'
               }
         }}),
     )
 )
 
 
-@known_failure_v6   # FIXME
+@xfail_buggy_annex_info
+@known_failure_windows
 #  apparently fails only sometimes in PY3, but in a way that's common in V6
 @assert_cwd_unchanged(ok_to_chdir=True)
 @with_tree(**tree1args)
 @serve_path_via_http()
 @with_tempfile(mkdir=True)
-def test_add_archive_content(path_orig, url, repo_path):
-    direct = False  # TODO: test on undirect, but too long ATM
-    orig_pwd = getpwd()
-    chpwd(repo_path)
-    # TODO we need to be able to pass path into add_archive_content
-    # We could mock but I mean for the API
-    assert_raises(RuntimeError, add_archive_content, "nonexisting.tar.gz") # no repo yet
-
-    repo = AnnexRepo(repo_path, create=True, direct=direct)
-    assert_raises(ValueError, add_archive_content, "nonexisting.tar.gz")
-    # we can't add a file from outside the repo ATM
-    assert_raises(FileNotInRepositoryError, add_archive_content, opj(path_orig, '1.tar.gz'))
+def test_add_archive_content(path_orig=None, url=None, repo_path=None):
+    with chpwd(repo_path):
+        # TODO we need to be able to pass path into add_archive_content
+        # We could mock but I mean for the API
 
-    # Let's add first archive to the repo so we could test
-    with swallow_outputs():
-        repo.add_urls([opj(url, '1.tar.gz')], options=["--pathdepth", "-1"])
-        for s in range(1, 5):
-            repo.add_urls([opj(url, '%du/1.tar.gz' % s)], options=["--pathdepth", "-2"])
-    repo.commit("added 1.tar.gz")
-
-    key_1tar = repo.get_file_key('1.tar.gz')  # will be used in the test later
-
-    def d1_basic_checks():
-        ok_(exists('1'))
-        ok_file_under_git('1', '1 f.txt', annexed=True)
-        ok_file_under_git(opj('1', 'd', '1d'), annexed=True)
-        ok_archives_caches(repo_path, 0)
-
-    # and by default it just does it, everything goes to annex
-    repo_ = add_archive_content('1.tar.gz')
-    eq_(repo.path, repo_.path)
-    d1_basic_checks()
-
-    # If ran again, should proceed just fine since the content is the same so no changes would be made really
-    add_archive_content('1.tar.gz')
-
-    # But that other one carries updated file, so should fail due to overwrite
-    with assert_raises(RuntimeError) as cme:
-        add_archive_content(opj('1u', '1.tar.gz'), use_current_dir=True)
-
-    # TODO: somewhat not precise since we have two possible "already exists"
-    # -- in caching and overwrite check
-    assert_in("already exists", str(cme.exception))
-    # but should do fine if overrides are allowed
-    add_archive_content(opj('1u', '1.tar.gz'), existing='overwrite', use_current_dir=True)
-    add_archive_content(opj('2u', '1.tar.gz'), existing='archive-suffix', use_current_dir=True)
-    add_archive_content(opj('3u', '1.tar.gz'), existing='archive-suffix', use_current_dir=True)
-    add_archive_content(opj('4u', '1.tar.gz'), existing='archive-suffix', use_current_dir=True)
-
-    # rudimentary test
-    assert_equal(sorted(map(basename, glob(opj(repo_path, '1', '1*')))),
-                 ['1 f-1.1.txt', '1 f-1.2.txt', '1 f-1.txt', '1 f.txt'])
-    whereis = repo.whereis(glob(opj(repo_path, '1', '1*')))
-    # they all must be the same
-    assert(all([x == whereis[0] for x in whereis[1:]]))
+        # no repo yet
+        assert_raises(NoDatasetFound, add_archive_content, "nonexisting.tar.gz")
+        ds = Dataset(repo_path).create()
+        res = ds.add_archive_content("nonexisting.tar.gz", on_failure='ignore')
+        assert_in_results(res, action='add-archive-content',
+                          status='impossible')
+        repo = ds.repo
+
+        # we can't add a file from outside the repo ATM
+        res = ds.add_archive_content(Path(path_orig) / '1.tar.gz',
+                                     on_failure='ignore')
+        assert_in_results(
+            res,
+            action='add-archive-content',
+            status='impossible',
+            type="dataset",
+            message="Can not add archive outside of the dataset"
+        )
+
+        # Let's add first archive to the repo so we could test
+        with swallow_outputs():
+            repo.add_url_to_file('1.tar.gz', opj(url, '1.tar.gz'))
+            for s in range(1, 5):
+                repo.add_url_to_file('%du/1.tar.gz' % s,
+                                     opj(url, '%du/1.tar.gz' % s))
+            repo.commit("added 1.tar.gz")
+
+        key_1tar = repo.get_file_annexinfo('1.tar.gz')['key']  # will be used in the test later
+
+        def d1_basic_checks():
+            ok_(exists('1'))
+            ok_file_under_git('1', '1 f.txt', annexed=True)
+            ok_file_under_git(opj('1', 'd', '1d'), annexed=True)
+            ok_archives_caches(repo_path, 0)
+
+        # and by default it just does it, everything goes to annex
+        res = add_archive_content('1.tar.gz')
+        assert_in_results(
+            res,
+            action='add-archive-content',
+            status='ok'
+        )
+        d1_basic_checks()
+
+        # If ran again, should proceed just fine since the content is the same
+        # so no changes would be made really
+        res = add_archive_content('1.tar.gz')
+        assert_in_results(
+            res,
+            action='add-archive-content',
+            status='ok'
+        )
+
+        # But that other one carries updated file, so should fail due to
+        # overwrite
+        res = add_archive_content(Path('1u') / '1.tar.gz',
+                                  use_current_dir=True, on_failure='ignore')
+        assert_in_results(
+            res,
+            action='add-archive-content',
+            status='error',
+        )
+        assert_in('exists, but would be overwritten by new file',
+                  res[0]['message'])
+        # but should do fine if overrides are allowed
+        add_archive_content(Path('1u') / '1.tar.gz', existing='overwrite',
+                            use_current_dir=True)
+        add_archive_content(Path('2u') / '1.tar.gz',
+                            existing='archive-suffix', use_current_dir=True)
+        add_archive_content(Path('3u') / '1.tar.gz',
+                            existing='archive-suffix', use_current_dir=True)
+        add_archive_content(Path('4u') / '1.tar.gz',
+                            existing='archive-suffix', use_current_dir=True)
+
+        # rudimentary test
+        assert_equal(sorted(map(basename, glob(opj(repo_path, '1', '1*')))),
+                     ['1 f-1.1.txt', '1 f-1.2.txt', '1 f-1.txt', '1 f.txt'])
+        whereis = repo.whereis(glob(opj(repo_path, '1', '1*')))
+        # they all must be the same
+        assert(all([x == whereis[0] for x in whereis[1:]]))
 
     # and we should be able to reference it while under subdirectory
     subdir = opj(repo_path, 'subdir')
     with chpwd(subdir, mkdir=True):
-        add_archive_content(opj(pardir, '1.tar.gz'), use_current_dir=True)
+        add_archive_content(opj(pardir, '1.tar.gz'), dataset=ds.path,
+                            use_current_dir=True)
         d1_basic_checks()
         # or we could keep relative path and also demand to keep the archive prefix
         # while extracting under original (annex root) dir
-        add_archive_content(opj(pardir, '1.tar.gz'), add_archive_leading_dir=True)
-    with chpwd('1'):
+        add_archive_content(opj(pardir, '1.tar.gz'),
+                            dataset=ds.path,
+                            add_archive_leading_dir=True)
+
+    with chpwd(opj(repo_path, '1')):
         d1_basic_checks()
 
-    # test with excludes and renames and annex options
-    add_archive_content(
-        '1.tar.gz', exclude=['d'], rename=['/ /_', '/^1/2'],
-        annex_options="-c annex.largefiles=exclude=*.txt",
-        delete=True)
-    # no conflicts since new name
-    ok_file_under_git('2', '1_f.txt', annexed=False)
-    assert_false(exists(opj('2', 'd')))
-    assert_false(exists('1.tar.gz'))  # delete was in effect
+    with chpwd(repo_path):
+        # test with excludes and renames and annex options
+        ds.add_archive_content(
+            '1.tar.gz', exclude=['d'], rename=['/ /_', '/^1/2'],
+            annex_options="-c annex.largefiles=exclude=*.txt",
+            delete=True)
+        # no conflicts since new name
+        ok_file_under_git('2', '1_f.txt', annexed=False)
+        assert_false(exists(opj('2', 'd')))
+        assert_false(exists('1.tar.gz'))  # delete was in effect
 
     # now test ability to extract within subdir
-    with chpwd('d1', mkdir=True):
+    with chpwd(opj(repo_path, 'd1'), mkdir=True):
         # Let's add first archive to the repo so we could test
         # named the same way but different content
         with swallow_outputs():
-            repo.add_urls([opj(url, 'd1', '1.tar.gz')], options=["--pathdepth", "-1"],
-                          cwd=getpwd())  # invoke under current subdir
+            repo.add_url_to_file('d1/1.tar.gz', opj(url, 'd1', '1.tar.gz'))
         repo.commit("added 1.tar.gz in d1")
 
         def d2_basic_checks():
             ok_(exists('1'))
             ok_file_under_git('1', '2 f.txt', annexed=True)
             ok_file_under_git(opj('1', 'd2', '2d'), annexed=True)
             ok_archives_caches(repo.path, 0)
-
-        add_archive_content('1.tar.gz')
+        add_archive_content('1.tar.gz', dataset=ds.path)
         d2_basic_checks()
 
     # in manual tests ran into the situation of inability to obtain on a single run
     # a file from an archive which was coming from a dropped key.  I thought it was
     # tested in custom remote tests, but I guess not sufficiently well enough
     repo.drop(opj('1', '1 f.txt'))  # should be all kosher
     repo.get(opj('1', '1 f.txt'))
@@ -276,225 +344,309 @@
     repo.drop(key_1tar, key=True)  # is available from the URL -- should be kosher
     repo.get(opj('1', '1 f.txt'))  # that what managed to not work
 
     # TODO: check if persistent archive is there for the 1.tar.gz
 
     # We should be able to drop everything since available online
     with swallow_outputs():
-        clean(dataset=repo.path)
+        clean(dataset=ds)
     repo.drop(key_1tar, key=True)  # is available from the URL -- should be kosher
-    chpwd(orig_pwd)  # just to avoid warnings ;)  move below whenever SkipTest removed
 
-    repo.drop(opj('1', '1 f.txt'))  # should be all kosher
-    repo.get(opj('1', '1 f.txt'))  # and should be able to get it again
+    ds.drop(opj('1', '1 f.txt'))  # should be all kosher
+    ds.get(opj('1', '1 f.txt'))  # and should be able to get it again
 
     # bug was that dropping didn't work since archive was dropped first
-    repo._annex_custom_command([], ["git", "annex", "drop", "--all"])
+    repo.call_annex(["drop", "--all"])
 
     # verify that we can't drop a file if archive key was dropped and online archive was removed or changed size! ;)
     repo.get(key_1tar, key=True)
     unlink(opj(path_orig, '1.tar.gz'))
-    res = repo.drop(key_1tar, key=True)
-    assert_equal(res['success'], False)
-    assert_result_values_cond(
-        [res], 'note',
-        lambda x: '(Use --force to override this check, or adjust numcopies.)' in x
-    )
+    with assert_raises(CommandError) as e:
+        repo.drop(key_1tar, key=True)
+        assert_equal(e.kwargs['stdout_json'][0]['success'], False)
+        assert_result_values_cond(
+            e.kwargs['stdout_json'], 'note',
+            lambda x: '(Use --force to override this check, or adjust numcopies.)' in x
+        )
     assert exists(opj(repo.path, repo.get_contentlocation(key_1tar)))
 
 
 @integration
 @assert_cwd_unchanged(ok_to_chdir=True)
 @with_tree(**tree1args)
 @serve_path_via_http()
 @with_tempfile(mkdir=True)
-def test_add_archive_content_strip_leading(path_orig, url, repo_path):
-    direct = False  # TODO: test on undirect, but too long ATM
-    orig_pwd = getpwd()
-    chpwd(repo_path)
-
-    repo = AnnexRepo(repo_path, create=True, direct=direct)
+def test_add_archive_content_strip_leading(path_orig=None, url=None, repo_path=None):
+    with chpwd(repo_path):
+        ds = Dataset(repo_path).create(force=True)
+        repo = ds.repo
+        # Let's add first archive to the repo so we could test
+        with swallow_outputs():
+            repo.add_url_to_file('1.tar.gz', opj(url, '1.tar.gz'))
+        repo.commit("added 1.tar.gz")
 
-    # Let's add first archive to the repo so we could test
-    with swallow_outputs():
-        repo.add_urls([opj(url, '1.tar.gz')], options=["--pathdepth", "-1"])
-    repo.commit("added 1.tar.gz")
+        add_archive_content('1.tar.gz', strip_leading_dirs=True)
+        ok_(not exists('1'))
+        ok_file_under_git(ds.path, '1 f.txt', annexed=True)
+        ok_file_under_git('d', '1d', annexed=True)
+        ok_archives_caches(ds.path, 0)
 
-    add_archive_content('1.tar.gz', strip_leading_dirs=True)
-    ok_(not exists('1'))
-    ok_file_under_git(repo.path, '1 f.txt', annexed=True)
-    ok_file_under_git('d', '1d', annexed=True)
-    ok_archives_caches(repo.path, 0)
 
-    chpwd(orig_pwd)  # just to avoid warnings ;)
+@assert_cwd_unchanged(ok_to_chdir=True)
+@with_tree(tree={"1.zip": {"dir": {"bar": "blah"}, "foo": "blahhhhh"}})
+def test_add_archive_content_zip(repo_path=None):
+    ds = Dataset(repo_path).create(force=True)
+    with chpwd(repo_path):
+        with swallow_outputs():
+            ds.save("1.zip", message="add 1.zip")
+        add_archive_content("1.zip")
+        ok_file_under_git(ds.pathobj / "1" / "foo", annexed=True)
+        ok_file_under_git(ds.pathobj / "1" / "dir" / "bar", annexed=True)
+        ok_archives_caches(ds.path, 0)
+
+
+@with_tree(tree={"ds": {"1.tar.gz": {"foo": "abc"}},
+                 "notds": {"2.tar.gz": {"bar": "def"}}})
+def test_add_archive_content_absolute_path(path=None):
+    ds = Dataset(opj(path, "ds")).create(force=True)
+    repo = ds.repo
+    ds.save("1.tar.gz", message="1.tar.gz")
+    abs_tar_gz = opj(path, "ds", "1.tar.gz")
+    add_archive_content(abs_tar_gz, dataset=ds)
+    ok_file_under_git(opj(path, "ds", "1", "foo"), annexed=True)
+    commit_msg = repo.format_commit("%B")
+    # The commit message uses relative paths.
+    assert_not_in(abs_tar_gz, commit_msg)
+    assert_in("1.tar.gz", commit_msg)
+    res = add_archive_content(opj(path, "notds", "2.tar.gz"),
+                              dataset=ds, on_failure='ignore')
+
+    assert_in_results(
+        res,
+        action='add-archive-content',
+        status='impossible',
+        message='Can not add archive outside of the dataset',
+    )
 
 
 @assert_cwd_unchanged(ok_to_chdir=True)
 @with_tree(**tree4uargs)
-def test_add_archive_use_archive_dir(repo_path):
-    direct = False  # TODO: test on undirect, but too long ATM
-    repo = AnnexRepo(repo_path, create=True, direct=direct)
+def test_add_archive_use_archive_dir(repo_path=None):
+    ds = Dataset(repo_path).create(force=True)
     with chpwd(repo_path):
         # Let's add first archive to the repo with default setting
         archive_path = opj('4u', '1.tar.gz')
         # check it gives informative error if archive is not already added
-        with assert_raises(RuntimeError) as cmr:
-            add_archive_content(archive_path)
-        assert_re_in(
-            "You should run ['\"]datalad add %s['\"] first" % archive_path,
-            str(cmr.exception), match=False
-        )
+        res = add_archive_content(archive_path, on_failure='ignore')
+        message = \
+            "Can not add an untracked archive. Run 'datalad save 4u\\1.tar.gz'"\
+        if on_windows else \
+            "Can not add an untracked archive. Run 'datalad save 4u/1.tar.gz'"
+        assert_in_results(
+            res,
+            action='add-archive-content',
+            message=message,
+            status='impossible')
+
         with swallow_outputs():
-            repo.add(archive_path)
-        repo.commit("added 1.tar.gz")
+            ds.save(archive_path)
 
-        ok_archives_caches(repo.path, 0)
-        add_archive_content(archive_path, strip_leading_dirs=True, use_current_dir=True)
+        ok_archives_caches(ds.path, 0)
+        add_archive_content(archive_path, strip_leading_dirs=True,
+                            use_current_dir=True)
         ok_(not exists(opj('4u', '1 f.txt')))
-        ok_file_under_git(repo.path, '1 f.txt', annexed=True)
-        ok_archives_caches(repo.path, 0)
+        ok_file_under_git(ds.path, '1 f.txt', annexed=True)
+        ok_archives_caches(ds.path, 0)
 
         # and now let's extract under archive dir
         add_archive_content(archive_path, strip_leading_dirs=True)
-        ok_file_under_git(repo.path, opj('4u', '1 f.txt'), annexed=True)
-        ok_archives_caches(repo.path, 0)
+        ok_file_under_git(ds.path, opj('4u', '1 f.txt'), annexed=True)
+        ok_archives_caches(ds.path, 0)
 
         add_archive_content(opj('4u', 'sub.tar.gz'))
-        ok_file_under_git(repo.path, opj('4u', 'sub', '2 f.txt'), annexed=True)
-        ok_archives_caches(repo.path, 0)
+        ok_file_under_git(ds.path, opj('4u', 'sub', '2 f.txt'), annexed=True)
+        ok_archives_caches(ds.path, 0)
 
 
-class TestAddArchiveOptions():
+@with_tree(
+    tree={
+        'archives': {
+            '1.gz': '1',
+            '2.xz': '2',
+            '3.lzma': '3',
+            # TODO: add any other stream compression we might be supporting via 7zip or patool?
+        },
+    }
+)
+def test_add_archive_single_file(repo_path=None):
+    ds = Dataset(repo_path).create(force=True)
+    with chpwd(repo_path):
+        archives = glob('archives/*')
+        ds.save(archives, message='Added archives')
+
+        for archive in archives:
+            archive_name = os.path.splitext(archive)[0]
+            archive_content = os.path.basename(archive_name)
+            ds.add_archive_content(archive)
+            ok_file_has_content(archive_name, archive_content)
+
 
+class TestAddArchiveOptions():
     # few tests bundled with a common setup/teardown to minimize boiler plate
-    @with_tree(tree={'1.tar': {'file.txt': 'load',
-                               '1.dat': 'load2'}},
-               delete=False)
-    def setup(self, repo_path):
-        self.pwd = getpwd()
-        direct = False  # TODO: test on undirect, but too long ATM
-        self.annex = annex = AnnexRepo(repo_path, create=True, direct=direct)
+    # nothing here works on windows, no even teardown(), prevent failure at the
+    # origin
+    def setup_method(self):
+        repo_path = tempfile.mkdtemp(**get_tempfile_kwargs(prefix="tree"))
+        create_tree(
+            repo_path,
+            {'1.tar': {'file.txt': 'load',
+                       '1.dat': 'load2'}})
+        self.ds = ds = Dataset(repo_path)
+        ds.create(force=True)
+        self.annex = ds.repo
         # Let's add first archive to the annex so we could test
-        annex.add('1.tar')
-        annex.commit(msg="added 1.tar")
+        ds.save('1.tar', message="added 1.tar")
 
-    def teardown(self):
-        assert_equal(self.pwd, getpwd())
-        rmtemp(self.annex.path)
+    def teardown_method(self):
+        # so we close any outstanding batch process etc
+        self.annex.precommit()
+        rmtemp(self.ds.path)
 
     def test_add_delete(self):
         # To test that .tar gets removed
-        add_archive_content('1.tar', annex=self.annex, strip_leading_dirs=True, delete=True)
-        assert_false(lexists(opj(self.annex.path, '1.tar')))
-
+        self.ds.add_archive_content('1.tar', strip_leading_dirs=True,
+                                    delete=True)
+        assert_false(lexists(self.ds.pathobj / '1.tar'))
+
+    # git-annex regression
+    # https://git-annex.branchable.com/bugs/regression__58___annex_add_of_moved_file_errors_out/
+    @skip_if(
+        '10.20220525' <= external_versions['cmd:annex'] < '10.20220706',  # approx when was fixed
+        msg="buggy git-annex release"
+    )
     def test_add_archive_leading_dir(self):
         import os
-        os.mkdir(opj(self.annex.path, 'sub'))
-        f123 = opj('sub', '123.tar')
-        os.rename(opj(self.annex.path, '1.tar'), opj(self.annex.path, f123))
+        os.mkdir(self.ds.pathobj / 'sub')
+        f123 = Path('sub') / '123.tar'
+        Path(self.ds.pathobj / '1.tar').rename(self.ds.pathobj / Path(f123))
         self.annex.remove('1.tar', force=True)
-        self.annex.add(f123)
-        self.annex.commit(msg="renamed")
-        add_archive_content(
+        self.ds.save(message="renamed")
+
+        self.ds.add_archive_content(
             f123,
-            annex=self.annex,
             add_archive_leading_dir=True,
             strip_leading_dirs=True
         )
-        ok_file_under_git(self.annex.path, opj('sub', '123', 'file.txt'), annexed=True)
 
+        ok_file_under_git(self.ds.path,
+                          str(Path('sub') / '123' / 'file.txt'),
+                          annexed=True)
+
+    # https://github.com/datalad/datalad/issues/6187
+    @skip_if_adjusted_branch
     def test_add_delete_after_and_drop(self):
         # To test that .tar gets removed
-        # but that new stuff was added to annex repo.  We know the key since default
-        # backend and content remain the same
-        key1 = 'SHA256E-s5--16d3ad1974655987dd7801d70659990b89bfe7e931a0a358964e64e901761cc0.dat'
-
+        # but that new stuff was added to annex repo.  We know the key since
+        # default backend and content remain the same
+        key1 = 'MD5E-s5--db87ebcba59a8c9f34b68e713c08a718.dat'
+        repo = self.ds.repo
         # previous state of things:
-        prev_files = list(find_files('.*', self.annex.path))
-        with assert_raises(Exception), \
-                swallow_logs():
-            self.annex.whereis(key1, key=True, output='full')
-        commits_prior = list(self.annex.get_branch_commits('git-annex'))
-        add_archive_content('1.tar', annex=self.annex, strip_leading_dirs=True, delete_after=True)
-        commits_after = list(self.annex.get_branch_commits('git-annex'))
-        # There should be a single commit for all additions +1 to initiate datalad-archives gh-1258
-        assert_equal(len(commits_after), len(commits_prior) + 2)
-        assert_equal(prev_files, list(find_files('.*', self.annex.path)))
-        w = self.annex.whereis(key1, key=True, output='full')
+        prev_files = list(find_files('.*', self.ds.path))
+        assert_equal(repo.whereis(key1, key=True, output='full'), {})
+
+        commits_prior = list(repo.get_branch_commits_('git-annex'))
+        self.ds.add_archive_content('1.tar',
+                                    strip_leading_dirs=True, delete_after=True)
+        commits_after = list(repo.get_branch_commits_('git-annex'))
+        # There should be a single commit for all additions +1 to initiate
+        # datalad-archives gh-1258
+        # If faking dates, there should be another +1 because
+        # annex.alwayscommit isn't set to false.
+        assert_equal(len(commits_after),
+                     # We expect one more when faking dates because
+                     # annex.alwayscommit isn't set to false.
+                     len(commits_prior) + 2 + repo.fake_dates_enabled)
+        assert_equal(prev_files, list(find_files('.*', self.ds.path)))
+        w = repo.whereis(key1, key=True, output='full')
         assert_equal(len(w), 2)  # in archive, and locally since we didn't drop
 
         # Let's now do the same but also drop content
-        add_archive_content('1.tar', annex=self.annex, strip_leading_dirs=True, delete_after=True,
-                            drop_after=True)
-        assert_equal(prev_files, list(find_files('.*', self.annex.path)))
-        w = self.annex.whereis(key1, key=True, output='full')
+        self.ds.add_archive_content('1.tar',
+                                    strip_leading_dirs=True, delete_after=True,
+                                    drop_after=True)
+        assert_equal(prev_files, list(find_files('.*', self.ds.path)))
+        w = repo.whereis(key1, key=True, output='full')
         assert_equal(len(w), 1)  # in archive
 
         # there should be no .datalad temporary files hanging around
         self.assert_no_trash_left_behind()
 
-    def test_add_delete_after_and_drop_subdir(self):
+    def test_add_delete_after_and_drop_subdir(self=None):
         os.mkdir(opj(self.annex.path, 'subdir'))
-        mv_out = self.annex._git_custom_command(
-            [],
-            ['git', 'mv', '1.tar', 'subdir']
+        mv_out = self.annex.call_git(
+            ['mv', '1.tar', 'subdir']
         )
         self.annex.commit("moved into subdir")
         with chpwd(self.annex.path):
             # was failing since deleting without considering if tarball
             # was extracted in that tarball directory
-            commits_prior_master = list(self.annex.get_branch_commits())
-            commits_prior = list(self.annex.get_branch_commits('git-annex'))
-            add_out = add_archive_content(
+            commits_prior_master = list(self.annex.get_branch_commits_())
+            commits_prior = list(self.annex.get_branch_commits_('git-annex'))
+            add_out = self.ds.add_archive_content(
                 opj('subdir', '1.tar'),
                 delete_after=True,
                 drop_after=True)
-            ok_clean_git(self.annex.path)
-            commits_after_master = list(self.annex.get_branch_commits())
-            commits_after = list(self.annex.get_branch_commits('git-annex'))
-            # There should be a single commit for all additions +1 to
-            # initiate datalad-archives gh-1258
-            assert_equal(len(commits_after), len(commits_prior) + 2)
-            assert_equal(len(commits_after_master), len(commits_prior_master))
-            assert(add_out is self.annex)
+            assert_repo_status(self.annex.path)
+            if not self.annex.is_managed_branch():
+                # whole counting logic here is ignorant of adjusted branches
+                commits_after_master = list(self.annex.get_branch_commits_())
+                commits_after = list(self.annex.get_branch_commits_('git-annex'))
+                # There should be a single commit for all additions +1 to
+                # initiate datalad-archives gh-1258.  If faking dates,
+                # there should be another +1 because annex.alwayscommit
+                # isn't set to false.
+                assert_equal(len(commits_after),
+                             len(commits_prior) + 2 + self.annex.fake_dates_enabled)
+                assert_equal(len(commits_after_master), len(commits_prior_master))
             # there should be no .datalad temporary files hanging around
             self.assert_no_trash_left_behind()
 
             # and if we add some untracked file, redo, there should be no changes
             # to master and file should remain not committed
             create_tree(self.annex.path, {'dummy.txt': '123'})
             assert_true(self.annex.dirty)  # untracked file
             add_out = add_archive_content(
                 opj('subdir', '1.tar'),
                 delete_after=True,
                 drop_after=True,
                 allow_dirty=True)
-            ok_clean_git(self.annex.path, untracked=['dummy.txt'])
-            assert_equal(len(list(self.annex.get_branch_commits())),
+            assert_repo_status(self.annex.path, untracked=['dummy.txt'])
+            assert_equal(len(list(self.annex.get_branch_commits_())),
                          len(commits_prior_master))
 
             # there should be no .datalad temporary files hanging around
             self.assert_no_trash_left_behind()
 
     def assert_no_trash_left_behind(self):
         assert_equal(
-            list(find_files('\.datalad..*', self.annex.path, dirs=True)),
+            list(find_files(r'\.datalad..*', self.annex.path, exclude="config",
+                            dirs=True)),
             []
         )
 
+    @xfail_buggy_annex_info
     def test_override_existing_under_git(self):
-        create_tree(self.annex.path, {'1.dat': 'load2'})
-        self.annex.add('1.dat', git=True)
-        self.annex.commit('added to git')
-        add_archive_content(
-            '1.tar', annex=self.annex, strip_leading_dirs=True,
+        create_tree(self.ds.path, {'1.dat': 'load2'})
+        self.ds.save('1.dat', to_git=True, message='added to git')
+        self.ds.add_archive_content(
+            '1.tar', strip_leading_dirs=True,
         )
         # and we did not bother adding it to annex (for now) -- just skipped
         # since we have it and it is the same
-        ok_file_under_git(self.annex.path, '1.dat', annexed=False)
+        ok_file_under_git(self.ds.path, '1.dat', annexed=False)
 
         # but if we say 'overwrite' -- we would remove and replace
-        add_archive_content(
-            '1.tar', annex=self.annex, strip_leading_dirs=True, delete=True
+        self.ds.add_archive_content(
+            '1.tar', strip_leading_dirs=True, delete=True
             , existing='overwrite'
         )
-        ok_file_under_git(self.annex.path, '1.dat', annexed=True)
+        ok_file_under_git(self.ds.path, '1.dat', annexed=True)
```

### Comparing `datalad-0.9.3/datalad/interface/tests/test_utils.py` & `datalad-1.0.0/datalad/local/remove.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,304 +1,274 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""Test interface.utils
+"""High-level interface for removing dataset content
 
 """
 
-from datalad.tests.utils import known_failure_direct_mode
+__docformat__ = 'restructuredtext'
 
-import os
 import logging
-from os.path import join as opj
-from nose.tools import assert_raises, assert_equal
-from datalad.tests.utils import with_tempfile, assert_not_equal
-from datalad.tests.utils import assert_in
-from datalad.tests.utils import assert_not_in
-from datalad.tests.utils import assert_dict_equal
-from datalad.tests.utils import with_tree
-from datalad.tests.utils import ok_clean_git
-from datalad.tests.utils import ok_
-from datalad.tests.utils import slow
-from datalad.utils import swallow_logs
-from datalad.distribution.dataset import Dataset
-from datalad.distribution.dataset import datasetmethod
-from datalad.distribution.dataset import EnsureDataset
-from datalad.support.param import Parameter
-from datalad.support.constraints import EnsureStr
-from datalad.support.constraints import EnsureNone
-from datalad.support.constraints import EnsureKeyChoice
-
-from ..base import Interface
-from ..utils import eval_results
-from datalad.interface.base import build_doc
-from ..utils import handle_dirty_dataset
-from datalad.api import create
-
-
-__docformat__ = 'restructuredtext'
-lgr = logging.getLogger('datalad.interface.tests.test_utils')
-_dirty_modes = ('fail', 'ignore', 'save-before')
+from os.path import lexists
+import warnings
 
+from datalad.core.local.save import Save
+from datalad.core.local.status import get_paths_by_ds
+from datalad.distributed.drop import Drop
+from datalad.distribution.dataset import (
+    datasetmethod,
+    require_dataset,
+    EnsureDataset,
+)
+from datalad.interface.base import (
+    Interface,
+    build_doc,
+    eval_results,
+)
+from datalad.interface.common_opts import (
+    jobs_opt,
+    save_message_opt,
+)
+from datalad.support.constraints import (
+    EnsureChoice,
+    EnsureNone,
+    EnsureStr,
+)
+from datalad.support.gitrepo import GitRepo
+from datalad.support.param import Parameter
+from datalad.utils import (
+    ensure_list,
+    rmtree,
+)
 
-def _check_all_clean(ds, state):
-    assert state is not None
-    for mode in _dirty_modes:
-        # nothing wrong, nothing saved
-        handle_dirty_dataset(ds, mode)
-        assert_equal(state, ds.repo.get_hexsha())
-
-
-def _check_auto_save(ds, orig_state):
-    handle_dirty_dataset(ds, 'ignore')
-    assert_raises(RuntimeError, handle_dirty_dataset, ds, 'fail')
-    handle_dirty_dataset(ds, 'save-before')
-    state = ds.repo.get_hexsha()
-    assert_not_equal(orig_state, state)
-    _check_all_clean(ds, state)
-    return state
-
-
-@with_tempfile(mkdir=True)
-def test_dirty(path):
-    for mode in _dirty_modes:
-        # does nothing without a dataset
-        handle_dirty_dataset(None, mode)
-    # placeholder, but not yet created
-    ds = Dataset(path)
-    # unknown mode
-    assert_raises(ValueError, handle_dirty_dataset, ds, 'MADEUP')
-    # not yet created is very dirty
-    assert_raises(RuntimeError, handle_dirty_dataset, ds, 'fail')
-    handle_dirty_dataset(ds, 'ignore')
-    assert_raises(RuntimeError, handle_dirty_dataset, ds, 'save-before')
-    # should yield a clean repo
-    ds.create()
-    orig_state = ds.repo.get_hexsha()
-    _check_all_clean(ds, orig_state)
-    # tainted: untracked
-    with open(opj(ds.path, 'something'), 'w') as f:
-        f.write('some')
-    # we don't want to auto-add untracked files by saving (anymore)
-    assert_raises(AssertionError, _check_auto_save, ds, orig_state)
-    # tainted: staged
-    ds.repo.add('something', git=True)
-    orig_state = _check_auto_save(ds, orig_state)
-    # tainted: submodule
-    # not added to super on purpose!
-    subds = ds.create('subds')
-    _check_all_clean(subds, subds.repo.get_hexsha())
-    ok_clean_git(ds.path)
-    # subdataset must be added as a submodule!
-    assert_equal(ds.subdatasets(result_xfm='relpaths'), ['subds'])
-
-
-demo_hierarchy = {
-    'a': {
-        'aa': {
-            'file_aa': 'file_aa'}},
-    'b': {
-        'ba': {
-            'file_ba': 'file_ba'},
-        'bb': {
-            'bba': {
-                'bbaa': {
-                    'file_bbaa': 'file_bbaa'}},
-            'file_bb': 'file_bb'}},
-    'c': {
-        'ca': {
-            'file_ca': 'file_ca'},
-        'file_c': 'file_c'},
-    'd': {
-        'da': {
-            'file_da': 'file_da'},
-        'db': {
-            'file_db': 'file_db'},
-        'file_d': 'file_d'},
-}
-
-
-def make_demo_hierarchy_datasets(path, tree, parent=None):
-    if parent is None:
-        parent = Dataset(path).create(force=True)
-    for node, items in tree.items():
-        if isinstance(items, dict):
-            node_path = opj(path, node)
-            nodeds = Dataset(node_path).create(force=True)
-            make_demo_hierarchy_datasets(node_path, items, parent=nodeds)
-    return parent
-
-
-@slow  # 74.4509s
-@with_tree(demo_hierarchy)
-@known_failure_direct_mode  #FIXME
-def test_save_hierarchy(path):
-    # this test doesn't use API`remove` to avoid circularities
-    ds = make_demo_hierarchy_datasets(path, demo_hierarchy)
-    ds.add('.', recursive=True)
-    ok_clean_git(ds.path)
-    ds_bb = Dataset(opj(ds.path, 'b', 'bb'))
-    ds_bba = Dataset(opj(ds_bb.path, 'bba'))
-    ds_bbaa = Dataset(opj(ds_bba.path, 'bbaa'))
-    # introduce a change at the lowest level
-    ds_bbaa.repo.remove('file_bbaa')
-    for d in (ds, ds_bb, ds_bba, ds_bbaa):
-        ok_(d.repo.dirty)
-    # need to give file specifically, otherwise it will simply just preserve
-    # staged changes
-    ds_bb.save(path=opj(ds_bbaa.path, 'file_bbaa'))
-    # it has saved all changes in the subtrees spanned
-    # by the given datasets, but nothing else
-    for d in (ds_bb, ds_bba, ds_bbaa):
-        ok_clean_git(d.path)
-    ok_(ds.repo.dirty)
-    # now with two modified repos
-    d = Dataset(opj(ds.path, 'd'))
-    da = Dataset(opj(d.path, 'da'))
-    da.repo.remove('file_da')
-    db = Dataset(opj(d.path, 'db'))
-    db.repo.remove('file_db')
-    # generator
-    d.save(recursive=True)
-    for d in (d, da, db):
-        ok_clean_git(d.path)
-    ok_(ds.repo.dirty)
-    # and now with files all over the place and saving
-    # all the way to the root
-    aa = Dataset(opj(ds.path, 'a', 'aa'))
-    aa.repo.remove('file_aa')
-    ba = Dataset(opj(ds.path, 'b', 'ba'))
-    ba.repo.remove('file_ba')
-    bb = Dataset(opj(ds.path, 'b', 'bb'))
-    bb.repo.remove('file_bb')
-    c = Dataset(opj(ds.path, 'c'))
-    c.repo.remove('file_c')
-    ca = Dataset(opj(ds.path, 'c', 'ca'))
-    ca.repo.remove('file_ca')
-    d = Dataset(opj(ds.path, 'd'))
-    d.repo.remove('file_d')
-    ds.save(
-        # append trailing slashes to the path to indicate that we want to
-        # have the staged content in the dataset saved, rather than only the
-        # subdataset state in the respective superds.
-        # an alternative would have been to pass `save` annotated paths of
-        # type {'path': dspath, 'process_content': True} for each dataset
-        # in question, but here we want to test how this would most likely
-        # by used from cmdline
-        path=[opj(p, '')
-               for p in (aa.path, ba.path, bb.path, c.path, ca.path, d.path)],
-        super_datasets=True)
+lgr = logging.getLogger('datalad.local.remove')
 
 
-# Note: class name needs to match module's name
 @build_doc
-class TestUtils(Interface):
-    """TestUtil's fake command"""
+class Remove(Interface):
+    """Remove components from datasets
 
+    Removing "unlinks" a dataset component, such as a file or subdataset, from
+    a dataset. Such a removal advances the state of a dataset, just like adding
+    new content. A remove operation can be undone, by restoring a previous
+    dataset state, but might require re-obtaining file content and subdatasets
+    from remote locations.
+
+    This command relies on the 'drop' command for safe operation. By default,
+    only file content from datasets which will be uninstalled as part of
+    a removal will be dropped. Otherwise file content is retained, such that
+    restoring a previous version also immediately restores file content access,
+    just as it is the case for files directly committed to Git. This default
+    behavior can be changed to always drop content prior removal, for cases
+    where a minimal storage footprint for local datasets installations is
+    desirable.
+
+    Removing a dataset component is always a recursive operation. Removing a
+    directory, removes all content underneath the directory too. If
+    subdatasets are located under a to-be-removed path, they will be
+    uninstalled entirely, and all their content dropped. If any subdataset
+    can not be uninstalled safely, the remove operation will fail and halt.
+
+    .. versionchanged:: 0.16
+       More in-depth and comprehensive safety-checks are now performed by
+       default.
+       The ``if_dirty||--if-dirty`` argument is ignored, will be removed in
+       a future release, and can be removed for a safe-by-default behavior. For
+       other cases consider the ``reckless||--reckless`` argument.
+       The ``save||--save`` argument is ignored and will be removed in a future
+       release, a dataset modification is now always saved. Consider save's
+       ``amend||--amend`` argument for post-remove fix-ups.
+       The ``recursive||--recursive`` argument is ignored, and will be removed
+       in a future release. Removal operations are always recursive, and the
+       parameter can be stripped from calls for a safe-by-default behavior.
+
+    .. deprecated:: 0.16
+       The ``check||--check`` argument will be removed in a future release.
+       It needs to be replaced with ``reckless||--reckless``.
+    """
     _params_ = dict(
-        number=Parameter(
-            args=("-n", "--number",),
-            doc="""It's a number""",
-            constraints=EnsureStr() | EnsureNone()),
         dataset=Parameter(
             args=("-d", "--dataset"),
-            doc=""""specify the dataset to update.  If
-            no dataset is given, an attempt is made to identify the dataset
-            based on the input and/or the current working directory""",
-            constraints=EnsureDataset() | EnsureNone()),)
+            metavar="DATASET",
+            doc="""specify the dataset to perform remove from.
+            If no dataset is given, the current working directory is used
+            as operation context""",
+            constraints=EnsureDataset() | EnsureNone()),
+        path=Parameter(
+            args=("path",),
+            metavar="PATH",
+            doc="path of a dataset or dataset component to be removed",
+            nargs="*",
+            constraints=EnsureStr() | EnsureNone()),
+        drop=Parameter(
+            args=("--drop",),
+            doc="""which dataset components to drop prior removal. This
+            parameter is passed on to the underlying drop operation as
+            its 'what' argument.""",
+            # we must not offer a 'nothing' which would bypass
+            # the `drop()` call. The implementation completely
+            # relies on `drop()` for all safety measures.
+            # instead `drop(reckless=kill)` must be used to fast-kill
+            # things
+            constraints=EnsureChoice('datasets', 'all')),
+        jobs=jobs_opt,
+        message=save_message_opt,
+        # XXX deprecate!
+        save=Parameter(
+            args=("--nosave",),
+            dest='save',
+            action="store_false",
+            doc="""DEPRECATED and IGNORED; use `save --amend` instead"""),
+        recursive=Parameter(
+            args=("--recursive", '-r',),
+            action='store_const',
+            const=None,
+            doc="""DEPRECATED and IGNORED: removal is always a recursive
+            operation"""),
+    )
+    # inherit some from Drop
+    # if_dirty and check as deprecated
+    for p in ('reckless', 'if_dirty', 'check',):
+        _params_[p] = Drop._params_[p]
+
+    _examples_ = [
+        dict(text="Permanently remove a subdataset (and all further subdatasets contained in it) from a dataset",
+             code_py="remove(dataset='path/to/dataset', path='path/to/subds')",
+             code_cmd="datalad remove -d <path/to/dataset> <path/to/subds>"),
+             dict(text="Permanently remove a superdataset (with all subdatasets) from the filesystem",
+             code_py="remove(dataset='path/to/dataset')",
+             code_cmd="datalad remove -d <path/to/dataset>"),
+             dict(text="DANGER-ZONE: Fast wipe-out a dataset and all its subdataset, bypassing all safety checks",
+             code_py="remove(dataset='path/to/dataset', reckless='kill')",
+             code_cmd="datalad remove -d <path/to/dataset> --reckless kill"),
+    ]
 
     @staticmethod
-    @datasetmethod(name='fake_command')
+    @datasetmethod(name='remove')
     @eval_results
-    def __call__(number, dataset=None):
-
-        for i in range(number):
-            # this dict will need to have the minimum info required by
-            # eval_results
-            yield {'path': 'some', 'status': 'ok', 'somekey': i, 'action': 'off'}
-
-
-def test_eval_results_plus_build_doc():
-
-    # test docs
-
-    # docstring was build already:
-    with swallow_logs(new_level=logging.DEBUG) as cml:
-        TestUtils().__call__(1)
-        assert_not_in("Building doc for", cml.out)
-    # docstring accessible both ways:
-    doc1 = Dataset.fake_command.__doc__
-    doc2 = TestUtils().__call__.__doc__
-
-    # docstring was built from Test_Util's definition:
-    assert_equal(doc1, doc2)
-    assert_in("TestUtil's fake command", doc1)
-    assert_in("Parameters", doc1)
-    assert_in("It's a number", doc1)
-
-    # docstring also contains eval_result's parameters:
-    assert_in("result_filter", doc1)
-    assert_in("return_type", doc1)
-    assert_in("list", doc1)
-    assert_in("None", doc1)
-    assert_in("return value behavior", doc1)
-    assert_in("dictionary is passed", doc1)
-
-    # test eval_results is able to determine the call, a method of which it is
-    # decorating:
-    with swallow_logs(new_level=logging.DEBUG) as cml:
-        Dataset('/does/not/matter').fake_command(3)
-        assert_in("Determined class of decorated function: {}"
-                  "".format(TestUtils().__class__), cml.out)
-
-    # test results:
-    result = TestUtils().__call__(2)
-    assert_equal(len(list(result)), 2)
-    result = Dataset('/does/not/matter').fake_command(3)
-    assert_equal(len(list(result)), 3)
-
-    # test signature:
-    from inspect import getargspec
-    assert_equal(getargspec(Dataset.fake_command)[0], ['number', 'dataset'])
-    assert_equal(getargspec(TestUtils.__call__)[0], ['number', 'dataset'])
-
-
-def test_result_filter():
-    # ensure baseline without filtering
-    assert_equal(
-        [r['somekey'] for r in TestUtils().__call__(4)],
-        [0, 1, 2, 3])
-    # test two functionally equivalent ways to filter results
-    # 1. Constraint-based -- filter by exception
-    #    we have a full set of AND and OR operators for this
-    # 2. custom filer function -- filter by boolean return value
-    for filt in (
-            EnsureKeyChoice('somekey', (0, 2)),
-            lambda x: x['somekey'] in (0, 2)):
-        assert_equal(
-            [r['somekey'] for r in TestUtils().__call__(
-                4,
-                result_filter=filt)],
-            [0, 2])
-        # constraint returns full dict
-        assert_dict_equal(
-            TestUtils().__call__(
-                4,
-                result_filter=filt)[-1],
-            {'action': 'off', 'path': 'some', 'status': 'ok', 'somekey': 2})
-
-    # test more sophisticated filters that actually get to see the
-    # API call's kwargs
-    def greatfilter(res, **kwargs):
-        assert_equal(kwargs.get('dataset', 'bob'), 'awesome')
-        return True
-    TestUtils().__call__(4, dataset='awesome', result_filter=greatfilter)
-
-    def sadfilter(res, **kwargs):
-        assert_equal(kwargs.get('dataset', 'bob'), None)
-        return True
-    TestUtils().__call__(4, result_filter=sadfilter)
+    def __call__(
+            path=None,
+            *,
+            dataset=None,
+            drop='datasets',
+            reckless=None,
+            message=None,
+            jobs=None,
+            # deprecated below
+            recursive=None,
+            check=None,
+            save=None,
+            if_dirty=None):
+
+        # deprecate checks
+        if if_dirty is not None:
+            warnings.warn(
+                "The `if_dirty` argument of `datalad remove` is ignored, "
+                "it can be removed for a safe-by-default behavior. For "
+                "other cases consider the `reckless` argument.",
+                DeprecationWarning)
+
+        if save is not None:
+            warnings.warn(
+                "The `save` argument of `datalad remove` is ignored. "
+                "A dataset modification is always saved. Consider "
+                "`save --amend` if post-remove fix-ups are needed.",
+                DeprecationWarning)
+
+        if recursive is not None:
+            warnings.warn(
+                "The `recursive` argument of `datalad remove` is ignored. "
+                "Removal operations are always recursive, and the parameter "
+                "can be stripped from calls for a safe-by-default behavior. ",
+                DeprecationWarning)
+
+        if check is not None:
+            warnings.warn(
+                "The `check` argument of `datalad remove` is deprecated, "
+                "use the `reckless` argument instead.",
+                DeprecationWarning)
+
+        if check is False:
+            if reckless is not None:
+                raise ValueError(
+                    'Must not use deprecated `check` argument, and new '
+                    '`reckless` argument together with `datalad remove`.')
+            reckless = 'availability'
+
+        refds = require_dataset(dataset, check_installed=True,
+                                purpose='remove')
+        # same path resolution that drop will do
+        paths_by_ds, errors = get_paths_by_ds(
+            refds, dataset, ensure_list(path),
+            # super-mode will readily tell us which datasets to
+            # save as the end
+            subdsroot_mode='super')
+
+        drop_success = True
+        for res in Drop.__call__(
+                dataset=dataset,
+                path=path,
+                what=drop,
+                reckless=reckless,
+                recursive=True,
+                recursion_limit=None,
+                jobs=jobs,
+                result_xfm=None,
+                return_type='generator',
+                result_renderer='disabled',
+                # delegate error handling here
+                on_failure='ignore'):
+            if res.get('status') not in ('ok', 'notneeded'):
+                drop_success = False
+            yield res
+
+        if not drop_success:
+            # there will be 'rm -rf' below, so play safe
+            lgr.debug('Observed drop failure, will not attempt remove')
+            return
+
+        for dpath, paths in paths_by_ds.items():
+            for delpath in ([dpath] if paths is None else paths):
+                if lexists(str(delpath)):
+                    # here we still have something around on the
+                    # filesystem. There is no need to fiddle with
+                    # Git, just wipe it out. A later save() will
+                    # act on it properly
+                    if delpath.is_dir():
+                        lgr.debug('Remove directory: %s', delpath)
+                        rmtree(delpath)
+                    # cannot use .exists() must forsee dead symlinks
+                    else:
+                        lgr.debug('Remove file: %s', delpath)
+                        delpath.unlink()
+                    continue
+                else:
+                    # if we get here, there is nothing on the file system
+                    # anymore at this path. Either because the parent
+                    # dataset vanished already, or because we dropped a
+                    # dataset. `save()` will properly unregistered
+                    # from the parents at the end. nothing else to do
+                    pass
+
+        if not refds.is_installed():
+            # we already dropped the whole thing
+            return
+
+        for res in Save.__call__(
+                dataset=dataset,
+                path=path,
+                # we might have removed the reference dataset by now, recheck
+                message=message if message else '[DATALAD] removed content',
+                return_type='generator',
+                result_renderer='disabled',
+                result_xfm=None,
+                result_filter=None,
+                on_failure='ignore'):
+            if res.get('action') == 'delete':
+                # normalize to previous remove results
+                res['action'] = 'remove'
+            yield res
```

### Comparing `datalad-0.9.3/datalad/interface/tests/test_docs.py` & `datalad-1.0.0/datalad/interface/tests/test_docs.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,27 +1,30 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 """Tests for interface doc wranglers.
 
 """
 
 __docformat__ = 'restructuredtext'
 
-from datalad.interface.base import dedent_docstring
-from datalad.interface.base import alter_interface_docs_for_api
-from datalad.interface.base import alter_interface_docs_for_cmdline
-from datalad.tests.utils import assert_true, assert_false, assert_in, \
-    assert_not_in, eq_
-
+from datalad.interface.base import (
+    alter_interface_docs_for_api,
+    dedent_docstring,
+)
+from datalad.tests.utils_pytest import (
+    assert_false,
+    assert_in,
+    assert_not_in,
+)
 
 demo_doc = """\
     Bla bla summary
 
     Generic intro blurb. Ping pong ping pong ping pong ping pong.  Ping pong ping
     pong ping pong ping pong. Ping pong ping pong ping pong ping pong. Ping pong
     ping pong ping pong ping pong. Ping pong ping pong ping pong ping pong. Ping
@@ -30,38 +33,53 @@
     || CMDLINE >>
     || REFLOW >>
     Something for the cmdline only
     Multiline!
     << REFLOW ||
     << CMDLINE ||
 
+    || REFLOW >>
+    a
+    b
+    << REFLOW ||
+    not
+       reflowed
+    || REFLOW >>
+    c
+    << REFLOW ||
+
     || PYTHON >>
 
     || REFLOW >>
     Some Python-only bits
     Multiline!
     << REFLOW ||
 
     << PYTHON ||
 
     And an example for in-line markup: [PY: just for Python PY] and
-    the other one [CMD: just for the command line CMD]. End of demo.
+    the other one [CMD: just for the command line CMD]. [PY: multiline
+    python-only with [ brackets [] ] PY][CMD: multiline cli-only with [ brackets
+    [] ] CMD]. End of demo.
 
     Generic appendix. Ding dong ding dong ding dong.  Ding dong ding dong ding
     dong.  Ding dong ding dong ding dong.  Ding dong ding dong ding dong.  Ding
     dong ding dong ding dong.
 
 """
 
 demo_paramdoc = """\
 
     Parameters
     ----------
     dataset : Dataset or None, optional
-      something [PY: python only PY] inbetween [CMD: cmdline only CMD] appended [PY: more python PY]
+      something [PY: python only PY] in between [CMD: cmdline only CMD] appended
+      Brackets can also be within and we can deal with [PY: multiline
+      python-only with [ some brackets [] PY] [CMD: multiline cli-only [
+      brackets included [ can we also have || ?] CMD].
       dataset is given, an attempt is made to identify the dataset based
       Dataset (e.g. a path), or value must be `None`. [Default: None]
 """
 
 demo_argdoc = """\
     specify the dataset to perform the install operation
     on. If no dataset is given, an attempt is made to
@@ -76,50 +94,32 @@
 def test_dedent():
     assert_false(dedent_docstring("one liner").endswith("\n"))
 
 
 def test_alter_interface_docs_for_api():
     alt = alter_interface_docs_for_api(demo_doc)
     alt_l = alt.split('\n')
-    # dedented
+    # de-dented
     assert_false(alt_l[0].startswith(' '))
     assert_false(alt_l[-1].startswith(' '))
     assert_not_in('CMD', alt)
     assert_not_in('PY', alt)
     assert_not_in('REFLOW', alt)
+    assert_in('a b', alt)
+    assert_in('not\n   reflowed', alt)
     assert_in("Some Python-only bits Multiline!", alt)
+    assert_in("Some Python-only bits", alt)
+    assert_in("just for Python", alt)
+    assert_not_in("just for the command line", alt)
+    assert_not_in("multiline cli-only with [ brackets\n[] ]", alt)
+    assert_in("multiline\npython-only with [ brackets [] ]", alt)
 
     altpd = alter_interface_docs_for_api(demo_paramdoc)
+    assert_not_in("PY", altpd)
+    assert_not_in("CMD", altpd)
     assert_in('python', altpd)
-    assert_in('inbetween', altpd)
+    assert_in('in between', altpd)
     assert_in('appended', altpd)
+    assert_in("multiline\n  python-only with [ some brackets []", altpd)
     assert_not_in('cmdline', altpd)
-
-
-def test_alter_interface_docs_for_cmdline():
-    alt = alter_interface_docs_for_cmdline(demo_doc)
-    alt_l = alt.split('\n')
-    # dedented
-    assert_false(alt_l[0].startswith(' '))
-    assert_false(alt_l[-1].startswith(' '))
-    assert_not_in('PY', alt)
-    assert_not_in('CMD', alt)
-    assert_not_in('REFLOW', alt)
-    assert_in("Something for the cmdline only Multiline!", alt)
-    # args
-    altarg = alter_interface_docs_for_cmdline(demo_argdoc)
-    # RST role markup
-    eq_(alter_interface_docs_for_cmdline(':murks:`me and my buddies`'),
-        'me and my buddies')
-    # spread across lines
-    eq_(alter_interface_docs_for_cmdline(':term:`Barbara\nStreisand`'),
-        'Barbara\nStreisand')
-    # multiple on one line
-    eq_(alter_interface_docs_for_cmdline(
-        ':term:`one` bla bla :term:`two` bla'),
-        'one bla bla two bla')
-
-    altpd = alter_interface_docs_for_cmdline(demo_paramdoc)
-    assert_not_in('python', altpd)
-    assert_in('inbetween', altpd)
-    assert_in('appended', altpd)
-    assert_in('cmdline', altpd)
+    assert_not_in("multiline cli-only [\n  brackets included "
+                  "[ can we also have || ?]", altpd)
```

### Comparing `datalad-0.9.3/datalad/interface/run.py` & `datalad-1.0.0/datalad/local/add_readme.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,224 +1,267 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""Run arbitrary commands and track how they modify a dataset"""
+"""add a README file to a dataset"""
 
 __docformat__ = 'restructuredtext'
 
-
 import logging
-import json
-
-from argparse import REMAINDER
-from os.path import join as opj
-from os.path import curdir
-from os.path import normpath
-from os.path import relpath
 
 from datalad.interface.base import Interface
-from datalad.interface.utils import eval_results
 from datalad.interface.base import build_doc
-from datalad.interface.results import get_status_dict
-from datalad.interface.common_opts import save_message_opt
-
-from datalad.support.constraints import EnsureNone
-from datalad.support.exceptions import CommandError
-from datalad.support.param import Parameter
-
-from datalad.distribution.dataset import require_dataset
-from datalad.distribution.dataset import EnsureDataset
-from datalad.distribution.dataset import datasetmethod
-
-from datalad.utils import get_dataset_root
-from datalad.utils import getpwd
+from datalad.support.annexrepo import AnnexRepo
 
-lgr = logging.getLogger('datalad.interface.run')
+lgr = logging.getLogger('datalad.local.add_readme')
 
 
 @build_doc
-class Run(Interface):
-    """Run an arbitrary command and record its impact on a dataset.
+class AddReadme(Interface):
+    """Add basic information about DataLad datasets to a README file
 
-    It is recommended to craft the command such that it can run in the root
-    directory of the dataset that the command will be recorded in. However,
-    as long as the command is executed somewhere underneath the dataset root,
-    the exact location will be recorded relative to the dataset root.
+    The README file is added to the dataset and the addition is saved
+    in the dataset.
+    Note: Make sure that no unsaved modifications to your dataset's
+    .gitattributes file exist.
 
-    If the executed command did not alter the dataset in any way, no record of
-    the command execution is made.
-
-    If the given command errors, a `CommandError` exception with the same exit
-    code will be raised, and no modifications will be saved.
     """
+    from datalad.support.param import Parameter
+    from datalad.distribution.dataset import datasetmethod
+    from datalad.interface.base import eval_results
+    from datalad.distribution.dataset import EnsureDataset
+    from datalad.support.constraints import (
+        EnsureChoice,
+        EnsureNone,
+        EnsureStr,
+    )
+
     _params_ = dict(
-        cmd=Parameter(
-            args=("cmd",),
-            nargs=REMAINDER,
-            metavar='SHELL COMMAND',
-            doc="command for execution"),
         dataset=Parameter(
             args=("-d", "--dataset"),
-            doc="""specify the dataset to record the command results in.
-            An attempt is made to identify the dataset based on the current
-            working directory. If a dataset is given, the command will be
-            executed in the root directory of this dataset.""",
+            doc="""Dataset to add information to. If no dataset is given, an
+            attempt is made to identify the dataset based on the current
+            working directory.""",
             constraints=EnsureDataset() | EnsureNone()),
-        message=save_message_opt,
-        rerun=Parameter(
-            args=('--rerun',),
-            action='store_true',
-            doc="""re-run the command recorded in the last saved change (if any).
-            Note: This option is deprecated since version 0.9.2 and
-            will be removed in a later release. Use `datalad rerun`
-            instead."""),
+        filename=Parameter(
+            args=("filename",),
+            metavar="PATH",
+            nargs='?',
+            doc="""Path of the README file within the dataset.""",
+            constraints=EnsureStr()),
+        existing=Parameter(
+            args=("--existing",),
+            doc="""How to react if a file with the target name already exists:
+            'skip': do nothing; 'append': append information to the existing
+            file; 'replace': replace the existing file with new content.""",
+            constraints=EnsureChoice("skip", "append", "replace")),
     )
 
     @staticmethod
-    @datasetmethod(name='run')
+    @datasetmethod(name='add_readme')
     @eval_results
-    def __call__(
-            cmd=None,
-            dataset=None,
-            message=None,
-            rerun=False):
-        if rerun:
-            if cmd:
-                lgr.warning("Ignoring provided command in --rerun mode")
-            lgr.warning("The --rerun option is deprecated since version 0.9.2. "
-                        "Use `datalad rerun` instead.")
-            from datalad.interface.rerun import Rerun
-            for r in Rerun.__call__(dataset=dataset, message=message):
-                yield r
-        else:
-            if cmd:
-                for r in run_command(cmd, dataset, message):
-                    yield r
-            else:
-                lgr.warning("No command given")
-
-
-# This helper function is used to add the rerun_info argument.
-def run_command(cmd, dataset=None, message=None, rerun_info=None):
-    rel_pwd = rerun_info.get('pwd') if rerun_info else None
-    if rel_pwd and dataset:
-        # recording is relative to the dataset
-        pwd = normpath(opj(dataset.path, rel_pwd))
-        rel_pwd = relpath(pwd, dataset.path)
-    elif dataset:
-        pwd = dataset.path
-        rel_pwd = curdir
-    else:
-        # act on the whole dataset if nothing else was specified
-        dataset = get_dataset_root(curdir)
-        # Follow our generic semantic that if dataset is specified,
-        # paths are relative to it, if not -- relative to pwd
-        pwd = getpwd()
-        if dataset:
-            rel_pwd = relpath(pwd, dataset)
-        else:
-            rel_pwd = pwd  # and leave handling on deciding either we
-                           # deal with it or crash to checks below
-
-    ds = require_dataset(
-        dataset, check_installed=True,
-        purpose='tracking outcomes of a command')
-    # not needed ATM
-    #refds_path = ds.path
-
-    # delayed imports
-    from datalad.cmd import Runner
-    from datalad.tests.utils import ok_clean_git
-
-    lgr.debug('tracking command output underneath %s', ds)
-    if not rerun_info:  # Rerun already takes care of this.
-        try:
-            # base assumption is that the animal smells superb
-            ok_clean_git(ds.path)
-        except AssertionError:
-            yield get_status_dict(
-                'run',
-                ds=ds,
-                status='impossible',
-                message=('unsaved modifications present, '
-                         'cannot detect changes by command'))
+    def __call__(filename='README.md',
+                 *,
+                 dataset=None,
+                 existing='skip'):
+        from os.path import lexists
+        from os.path import join as opj
+        from io import open
+
+        from datalad.distribution.dataset import require_dataset
+        from datalad.utils import ensure_list
+
+        dataset = require_dataset(dataset, check_installed=True,
+                                  purpose='add README')
+
+        fpath = opj(dataset.path, filename)
+        res_kwargs = dict(action='add_readme', path=fpath)
+
+        if lexists(fpath) and existing == 'skip':
+            yield dict(
+                res_kwargs,
+                status='notneeded',
+                message='file already exists, and not appending content')
             return
 
-    # anticipate quoted compound shell commands
-    cmd = cmd[0] if isinstance(cmd, list) and len(cmd) == 1 else cmd
-
-    # TODO do our best to guess which files to unlock based on the command string
-    #      in many cases this will be impossible (but see rerun). however,
-    #      generating new data (common case) will be just fine already
-
-    # we have a clean dataset, let's run things
-    cmd_exitcode = None
-    runner = Runner(cwd=pwd)
-    try:
-        lgr.info("== Command start (output follows) =====")
-        runner.run(
-            cmd,
-            # immediate output
-            log_online=True,
-            # not yet sure what we should do with the command output
-            # IMHO `run` itself should be very silent and let the command talk
-            log_stdout=False,
-            log_stderr=False,
-            expect_stderr=True,
-            expect_fail=True,
-            # TODO stdin
+        # unlock, file could be annexed
+        if lexists(fpath):
+            yield from dataset.unlock(
+                fpath,
+                return_type='generator',
+                result_renderer='disabled'
+            )
+        if not lexists(fpath):
+            # if we have an annex repo, shall the README go to Git or annex?
+
+            if isinstance(dataset.repo, AnnexRepo) \
+                and 'annex.largefiles' not in \
+                    dataset.repo.get_gitattributes(filename).get(filename, {}):
+                # configure the README to go into Git
+                dataset.repo.set_gitattributes(
+                    [(filename, {'annex.largefiles': 'nothing'})])
+                yield from dataset.save(
+                    path='.gitattributes',
+                    message="[DATALAD] Configure README to be in Git",
+                    to_git=True,
+                    return_type='generator',
+                    result_renderer='disabled'
+                )
+
+        # get any metadata on the dataset itself
+        meta = _get_dataset_metadata(dataset)
+        metainfo = ''
+        for label, content in (
+                ('', meta.get('description', meta.get('shortdescription', ''))),
+                ('Author{}'.format('s' if isinstance(meta.get('author', None), list) else ''),
+                    u'\n'.join([u'- {}'.format(a) for a in ensure_list(meta.get('author', []))])),
+                ('Homepage', meta.get('homepage', '')),
+                ('Reference', meta.get('citation', '')),
+                ('License', meta.get('license', '')),
+                ('Keywords', u', '.join([u'`{}`'.format(k) for k in ensure_list(meta.get('tag', []))])),
+                ('Funding', meta.get('fundedby', '')),
+                ):
+            if label and content:
+                metainfo += u'\n\n### {}\n\n{}'.format(label, content)
+            elif content:
+                metainfo += u'\n\n{}'.format(content)
+
+        for key in 'title', 'name', 'shortdescription':
+            if 'title' in meta:
+                break
+            if key in meta:
+                meta['title'] = meta[key]
+
+        default_content=u"""\
+# {title}{metainfo}
+
+## General information
+
+This is a DataLad dataset{id}.
+
+## DataLad datasets and how to use them
+
+This repository is a [DataLad](https://www.datalad.org/) dataset. It provides
+fine-grained data access down to the level of individual files, and allows for
+tracking future updates. In order to use this repository for data retrieval,
+[DataLad](https://www.datalad.org/) is required. It is a free and open source
+command line tool, available for all major operating systems, and builds up on
+Git and [git-annex](https://git-annex.branchable.com/) to allow sharing,
+synchronizing, and version controlling collections of large files.
+
+More information on how to install DataLad and [how to install](http://handbook.datalad.org/en/latest/intro/installation.html)
+it can be found in the [DataLad Handbook](https://handbook.datalad.org/en/latest/index.html).
+
+### Get the dataset
+
+A DataLad dataset can be `cloned` by running
+
+```
+datalad clone <url>
+```
+
+Once a dataset is cloned, it is a light-weight directory on your local machine.
+At this point, it contains only small metadata and information on the identity
+of the files in the dataset, but not actual *content* of the (sometimes large)
+data files.
+
+### Retrieve dataset content
+
+After cloning a dataset, you can retrieve file contents by running
+
+```
+datalad get <path/to/directory/or/file>
+```
+
+This command will trigger a download of the files, directories, or subdatasets
+you have specified.
+
+DataLad datasets can contain other datasets, so called *subdatasets*.  If you
+clone the top-level dataset, subdatasets do not yet contain metadata and
+information on the identity of files, but appear to be empty directories. In
+order to retrieve file availability metadata in subdatasets, run
+
+```
+datalad get -n <path/to/subdataset>
+```
+
+Afterwards, you can browse the retrieved metadata to find out about subdataset
+contents, and retrieve individual files with `datalad get`.  If you use
+`datalad get <path/to/subdataset>`, all contents of the subdataset will be
+downloaded at once.
+
+### Stay up-to-date
+
+DataLad datasets can be updated. The command `datalad update` will *fetch*
+updates and store them on a different branch (by default
+`remotes/origin/master`). Running
+
+```
+datalad update --merge
+```
+
+will *pull* available updates and integrate them in one go.
+
+### Find out what has been done
+
+DataLad datasets contain their history in the ``git log``.  By running ``git
+log`` (or a tool that displays Git history) in the dataset or on specific
+files, you can find out what has been done to the dataset or to individual
+files by whom, and when.
+""".format(
+            title='Dataset "{}"'.format(meta['title']) if 'title' in meta else 'About this dataset',
+            metainfo=metainfo,
+            id=u' (id: {})'.format(dataset.id) if dataset.id else '',
+            )
+
+        with open(fpath, 'a' if existing == 'append' else 'w', encoding='utf-8') as fp:
+            fp.write(default_content)
+            yield dict(
+                status='ok',
+                path=fpath,
+                type='file',
+                action='add_readme')
+
+        yield from dataset.save(
+                fpath,
+                message='[DATALAD] added README',
+                result_filter=None,
+                result_xfm=None,
+                return_type='generator',
+                result_renderer='disabled'
         )
-    except CommandError as e:
-        # strip our own info from the exception. The original command output
-        # went to stdout/err -- we just have to exitcode in the same way
-        cmd_exitcode = e.code
-
-        if not rerun_info or rerun_info.get("exit", 0) != cmd_exitcode:
-            # we failed during a fresh run, or in a different way during a rerun
-            # the latter can easily happen if we try to alter a locked file
-            #
-            # let's fail here, the command could have had a typo or some
-            # other undesirable condition. If we would `add` nevertheless,
-            # we would need to rerun and aggregate annex content that we
-            # likely don't want
-            # TODO add switch to ignore failure (some commands are stupid)
-            # TODO add the ability to `git reset --hard` the dataset tree on failure
-            # we know that we started clean, so we could easily go back, needs gh-1424
-            # to be able to do it recursively
-            raise CommandError(code=cmd_exitcode)
-
-    lgr.info("== Command exit (modification check follows) =====")
-
-    # ammend commit message with `run` info:
-    # - pwd if inside the dataset
-    # - the command itself
-    # - exit code of the command
-    run_info = {
-        'cmd': cmd,
-        'exit': cmd_exitcode if cmd_exitcode is not None else 0,
-        'chain': rerun_info["chain"] if rerun_info else [],
-    }
-    if rel_pwd is not None:
-        # only when inside the dataset to not leak information
-        run_info['pwd'] = rel_pwd
-
-    # compose commit message
-    cmd_shorty = (' '.join(cmd) if isinstance(cmd, list) else cmd)
-    cmd_shorty = '{}{}'.format(
-        cmd_shorty[:40],
-        '...' if len(cmd_shorty) > 40 else '')
-    msg = '[DATALAD RUNCMD] {}\n\n=== Do not change lines below ===\n{}\n^^^ Do not change lines above ^^^'.format(
-        message if message is not None else cmd_shorty,
-        json.dumps(run_info, indent=1), sort_keys=True, ensure_ascii=False, encoding='utf-8')
-
-    for r in ds.add('.', recursive=True, message=msg):
-        yield r
-
-    # TODO bring back when we can ignore a command failure
-    #if cmd_exitcode:
-    #    # finally raise due to the original command error
-    #    raise CommandError(code=cmd_exitcode)
+
+
+def _get_dataset_metadata(dataset):
+    """Implement this function to perform metadata reporting for a dataset
+
+    This implementation reports no metadata.
+
+    Returns
+    -------
+    dict
+        Can contain keys like 'description', 'shortdescription', 'author',
+        'homepage', 'citation', 'license', 'tag', 'fundedby'
+    """
+
+    meta = {}
+    if hasattr(dataset, 'metadata'):
+        dsinfo = dataset.metadata(
+            '.',
+            reporton='datasets',
+            return_type='item-or-list',
+            result_renderer='disabled',
+            on_failure='ignore')
+        if not isinstance(dsinfo, dict) or dsinfo.get('status', None) != 'ok':
+            lgr.warning("Could not obtain dataset metadata, proceeding without")
+        else:
+            # flatten possibly existing multiple metadata sources
+            for src in dsinfo['metadata']:
+                if src.startswith('@'):
+                    # not a source
+                    continue
+                meta.update(dsinfo['metadata'][src])
+    return meta
```

### Comparing `datalad-0.9.3/datalad/interface/add_archive_content.py` & `datalad-1.0.0/datalad/local/add_archive_content.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,535 +1,713 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 """High-level interface for adding content of an archive under annex control
 
 """
-from __future__ import print_function
 
 __docformat__ = 'restructuredtext'
 
 
-import re
 import os
-import shlex
+import re
 import tempfile
-
-from os.path import join as opj, realpath, curdir, exists, lexists, relpath, basename
-from os.path import commonprefix
+import warnings
+from os.path import (
+    basename,
+    curdir,
+    exists,
+)
+from os.path import join as opj
+from os.path import lexists
 from os.path import sep as opsep
-from os.path import islink
-from os.path import isabs
-from os.path import dirname
-from os.path import normpath
-
-from .base import Interface
-from datalad.interface.base import build_doc
-from .common_opts import allow_dirty
-from ..consts import ARCHIVES_SPECIAL_REMOTE
-from ..support.param import Parameter
-from ..support.constraints import EnsureStr, EnsureNone
-
-from ..support.annexrepo import AnnexRepo
-from ..support.strings import apply_replacement_rules
-from ..support.stats import ActivityStats
-from ..cmdline.helpers import get_repo_instance
-from ..utils import getpwd, rmtree, file_basename
-from ..utils import md5sum
-from ..utils import assure_tuple_or_list
 
-from datalad.customremotes.base import init_datalad_remote
+from datalad.consts import ARCHIVES_SPECIAL_REMOTE
+from datalad.customremotes.base import ensure_datalad_remote
+from datalad.distribution.dataset import (
+    EnsureDataset,
+    datasetmethod,
+    require_dataset,
+    resolve_path,
+)
+from datalad.interface.base import (
+    Interface,
+    build_doc,
+    eval_results,
+)
+from datalad.interface.common_opts import allow_dirty
+from datalad.interface.results import get_status_dict
+from datalad.log import (
+    log_progress,
+    logging,
+)
+from datalad.support.annexrepo import AnnexRepo
+from datalad.support.constraints import (
+    EnsureNone,
+    EnsureStr,
+)
+from datalad.support.param import Parameter
+from datalad.support.stats import ActivityStats
+from datalad.support.strings import apply_replacement_rules
+from datalad.utils import (
+    Path,
+    ensure_tuple_or_list,
+    file_basename,
+    getpwd,
+    md5sum,
+    rmtree,
+    split_cmdline,
+)
 
-from six import string_types
-
-from ..log import logging
-lgr = logging.getLogger('datalad.interfaces.add_archive_content')
+lgr = logging.getLogger('datalad.local.add_archive_content')
 
 
 # Shortcut note
 _KEY_OPT = "[PY: `key=True` PY][CMD: --key CMD]"
 _KEY_OPT_NOTE = "Note that it will be of no effect if %s is given" % _KEY_OPT
 
 # TODO: may be we could enable separate logging or add a flag to enable
 # all but by default to print only the one associated with this given action
 
 
 @build_doc
 class AddArchiveContent(Interface):
     """Add content of an archive under git annex control.
 
-    This results in the files within archive (which must be already under annex
-    control itself) added under annex referencing original archive via
-    custom special remotes mechanism
-
-    Example:
-
-        annex-repo$ datalad add-archive-content my_big_tarball.tar.gz
+    Given an already annex'ed archive, extract and add its files to the
+    dataset, and reference the original archive as a custom special remote.
 
     """
+    _examples_ = [
+        dict(text="""Add files from the archive 'big_tarball.tar.gz', but
+                     keep big_tarball.tar.gz in the index""",
+             code_py="add_archive_content(path='big_tarball.tar.gz')",
+             code_cmd="datalad add-archive-content big_tarball.tar.gz"),
+        dict(text="""Add files from the archive 'tarball.tar.gz', and
+                     remove big_tarball.tar.gz from the index""",
+             code_py="add_archive_content(path='big_tarball.tar.gz', delete=True)",
+             code_cmd="datalad add-archive-content big_tarball.tar.gz --delete"),
+        dict(text="""Add files from the archive 's3.zip' but remove the leading
+                     directory""",
+             code_py="add_archive_content(path='s3.zip', strip_leading_dirs=True)",
+             code_cmd="datalad add-archive-content s3.zip --strip-leading-dirs"),
+        ]
+
     # XXX prevent common args from being added to the docstring
     _no_eval_results = True
     _params_ = dict(
+        dataset=Parameter(
+            args=("-d", "--dataset"),
+            doc=""""specify the dataset to save""",
+            constraints=EnsureDataset() | EnsureNone()),
         delete=Parameter(
-            args=("-d", "--delete"),
+            args=("-D", "--delete"),
             action="store_true",
-            doc="""flag to delete original archive from the filesystem/git in current tree.
-                   %s""" % _KEY_OPT_NOTE),
+            doc="""delete original archive from the filesystem/Git in current
+            tree. %s""" % _KEY_OPT_NOTE),
         add_archive_leading_dir=Parameter(
             args=("--add-archive-leading-dir",),
             action="store_true",
-            doc="""flag to place extracted content under a directory which would correspond
-                   to archive name with suffix stripped.  E.g. for archive `example.zip` its
-                   content will be extracted under a directory `example/`"""),
+            doc="""place extracted content under a directory which would
+            correspond to the archive name with all suffixes stripped. E.g. the
+            content of `archive.tar.gz` will be extracted under `archive/`"""),
         strip_leading_dirs=Parameter(
             args=("--strip-leading-dirs",),
             action="store_true",
-            doc="""flag to move all files directories up, from how they were stored in an archive,
-                   if that one contained a number (possibly more than 1 down) single leading
-                   directories"""),
+            doc="""remove one or more leading directories from the archive
+            layout on extraction"""),
         leading_dirs_depth=Parameter(
             args=("--leading-dirs-depth",),
             action="store",
             type=int,
-            doc="""maximal depth to strip leading directories to.  If not specified (None), no limit"""),
+            doc="""maximum depth of leading directories to strip.
+            If not specified (None), no limit"""),
         leading_dirs_consider=Parameter(
             args=("--leading-dirs-consider",),
             action="append",
-            doc="""regular expression(s) for directories to consider to strip away""",
+            doc="""regular expression(s) for directories to consider to strip
+            away""",
             constraints=EnsureStr() | EnsureNone(),
         ),
         use_current_dir=Parameter(
             args=("--use-current-dir",),
             action="store_true",
-            doc="""flag to extract archive under the current directory,  not the directory where archive is located.
-                   %s""" % _KEY_OPT_NOTE),
+            doc="""extract the archive under the current directory, not the
+             directory where the archive is located. This parameter is applied
+             automatically if [PY: `key=True` PY][CMD: --key CMD] was used."""),
         # TODO: add option to extract under archive's original directory. Currently would extract in curdir
         existing=Parameter(
             args=("--existing",),
             choices=('fail', 'overwrite', 'archive-suffix', 'numeric-suffix'),
             default="fail",
-            doc="""what operation to perform a file from archive tries to overwrite an existing
-             file with the same name.  'fail' (default) leads to RuntimeError exception.
-             'overwrite' silently replaces existing file.  'archive-suffix' instructs to add
-             a suffix (prefixed with a '-') matching archive name from which file gets extracted,
-             and if that one present, 'numeric-suffix' is in effect in addition, when incremental
-             numeric suffix (prefixed with a '.') is added until no name collision is longer detected"""
+            doc="""what operation to perform if a file from an archive tries to
+            overwrite an existing file with the same name.  'fail' (default)
+            leads to an error result, 'overwrite' silently replaces
+            existing file, 'archive-suffix' instructs to add a suffix (prefixed
+            with a '-') matching archive name from which file gets extracted,
+            and if that one is present as well, 'numeric-suffix' is in effect in
+            addition, when incremental numeric suffix (prefixed with a '.') is
+            added until no name collision is longer detected"""
         ),
         exclude=Parameter(
             args=("-e", "--exclude"),
             action='append',
-            doc="""regular expressions for filenames which to exclude from being added to annex.
-            Applied after --rename if that one is specified.  For exact matching, use anchoring""",
+            doc="""regular expressions for filenames which to exclude from being
+            added to annex. Applied after --rename if that one is specified.
+            For exact matching, use anchoring""",
             constraints=EnsureStr() | EnsureNone()
         ),
         rename=Parameter(
             args=("-r", "--rename"),
             action='append',
-            doc="""regular expressions to rename files before being added under git.
-            First letter defines how to split provided string into two parts:
-            Python regular expression (with groups), and replacement string""",
+            doc="""regular expressions to rename files before added them under
+            to Git. The first defines how to split provided string into
+            two parts: Python regular expression (with groups), and replacement
+            string""",
             constraints=EnsureStr(min_len=2) | EnsureNone()
         ),
         annex_options=Parameter(
             args=("-o", "--annex-options"),
-            doc="""additional options to pass to git-annex""",
+            doc="""additional options to pass to git-annex """,
             constraints=EnsureStr() | EnsureNone()
         ),
-        # TODO: Python only???
         annex=Parameter(
-            doc="""annex instance to use"""
-            #constraints=EnsureStr() | EnsureNone()
+            doc="""DEPRECATED. Use the 'dataset' parameter instead."""
         ),
         # TODO: Python only!
         stats=Parameter(
             doc="""ActivityStats instance for global tracking""",
         ),
         key=Parameter(
             args=("--key",),
             action="store_true",
-            doc="""flag to signal if provided archive is not actually a filename on its own but an annex key"""),
+            doc="""signal if provided archive is not actually a filename on its
+            own but an annex key. The archive will be extracted in the current
+            directory."""),
         copy=Parameter(
             args=("--copy",),
             action="store_true",
-            doc="""flag to copy the content of the archive instead of moving"""),
+            doc="""copy the content of the archive instead of moving"""),
         allow_dirty=allow_dirty,
         commit=Parameter(
             args=("--no-commit",),
             action="store_false",
             dest="commit",
-            doc="""flag to not commit upon completion"""),
+            doc="""don't commit upon completion"""),
         drop_after=Parameter(
             args=("--drop-after",),
             action="store_true",
             doc="""drop extracted files after adding to annex""",
         ),
         delete_after=Parameter(
             args=("--delete-after",),
             action="store_true",
-            doc="""extract under a temporary directory, git-annex add, and delete after.  To
-             be used to "index" files within annex without actually creating corresponding
-             files under git.  Note that `annex dropunused` would later remove that load"""),
+            doc="""extract under a temporary directory, git-annex add, and
+            delete afterwards. To be used to "index" files within annex without
+            actually creating corresponding files under git. Note that
+            `annex dropunused` would later remove that load"""),
 
         # TODO: interaction with archives cache whenever we make it persistent across runs
         archive=Parameter(
+            args=("archive",),
             doc="archive file or a key (if %s specified)" % _KEY_OPT,
             constraints=EnsureStr()),
     )
 
-        # use-case from openfmri pipeline
-        #     ExtractArchives(
-        #         # will do the merge of 'replace' strategy
-        #         source_branch="incoming",
-        #         regex="\.(tgz|tar\..*)$",
-        #         renames=[
-        #             ("^[^/]*/(.*)", "\1") # e.g. to strip leading dir, or could prepend etc
-        #         ],
-        #         #exclude="license.*",  # regexp
-        #     ),
-
     @staticmethod
-    def __call__(archive, annex=None,
-                 add_archive_leading_dir=False,
-                 strip_leading_dirs=False, leading_dirs_depth=None, leading_dirs_consider=None,
-                 use_current_dir=False,
-                 delete=False, key=False, exclude=None, rename=None, existing='fail',
-                 annex_options=None, copy=False, commit=True, allow_dirty=False,
-                 stats=None, drop_after=False, delete_after=False):
-        """
-        Returns
-        -------
-        annex
-        """
+    @datasetmethod(name='add_archive_content')
+    @eval_results
+    def __call__(
+            archive,
+            *,
+            dataset=None,
+            annex=None,
+            add_archive_leading_dir=False,
+            strip_leading_dirs=False,
+            leading_dirs_depth=None,
+            leading_dirs_consider=None,
+            use_current_dir=False,
+            delete=False,
+            key=False,
+            exclude=None,
+            rename=None,
+            existing='fail',
+            annex_options=None,
+            copy=False,
+            commit=True,
+            allow_dirty=False,
+            stats=None,
+            drop_after=False,
+            delete_after=False):
+
         if exclude:
-            exclude = assure_tuple_or_list(exclude)
+            exclude = ensure_tuple_or_list(exclude)
         if rename:
-            rename = assure_tuple_or_list(rename)
-
-        # TODO: actually I see possibly us asking user either he wants to convert
-        # his git repo into annex
-        archive_path = archive
-        pwd = getpwd()
-        if annex is None:
-            annex = get_repo_instance(pwd, class_=AnnexRepo)
-            if not isabs(archive):
-                # if not absolute -- relative to wd and thus
-                archive_path = normpath(opj(realpath(pwd), archive))
-                # abspath(archive) is not "good" since dereferences links in the path
-                # archive_path = abspath(archive)
-        elif not isabs(archive):
-            # if we are given an annex, then assume that given path is within annex, not
-            # relative to PWD
-            archive_path = opj(annex.path, archive)
-        annex_path = annex.path
-
-        # _rpath below should depict paths relative to the top of the annex
-        archive_rpath = relpath(archive_path, annex_path)
-
-        if archive in annex.untracked_files:
-            raise RuntimeError(
-                "The archive is not under annex yet. You should run 'datalad "
-                "add {}' first".format(archive))
+            rename = ensure_tuple_or_list(rename)
+        ds = require_dataset(dataset,
+                             check_installed=True,
+                             purpose='add-archive-content')
+
+        # set up common params for result records
+        res_kwargs = {
+            'action': 'add-archive-content',
+            'logger': lgr,
+        }
+
+        if not isinstance(ds.repo, AnnexRepo):
+            yield get_status_dict(
+                ds=ds,
+                status='impossible',
+                message="Can't operate in a pure Git repository",
+                **res_kwargs
+            )
+            return
+        if annex:
+            warnings.warn(
+                "datalad add_archive_content's `annex` parameter is "
+                "deprecated and will be removed in a future release. "
+                "Use the 'dataset' parameter instead.",
+                DeprecationWarning)
+        annex = ds.repo
+        # get the archive path relative from the ds root
+        archive_path = resolve_path(archive, ds=dataset)
+        # let Status decide whether we can act on the given file
+        for s in ds.status(
+                path=archive_path,
+                on_failure='ignore',
+                result_renderer='disabled'):
+            if s['status'] == 'error':
+                if 'path not underneath the reference dataset %s' in s['message']:
+                    yield get_status_dict(
+                        ds=ds,
+                        status='impossible',
+                        message='Can not add archive outside of the dataset',
+                        **res_kwargs)
+                    return
+                # status errored & we haven't anticipated the cause. Bubble up
+                yield s
+                return
+            elif s['state'] == 'untracked':
+                # we can't act on an untracked file
+                message = (
+                    "Can not add an untracked archive. "
+                    "Run 'datalad save {}'".format(archive)
+                )
+                yield get_status_dict(
+                           ds=ds,
+                           status='impossible',
+                           message=message,
+                           **res_kwargs)
+                return
 
-        # TODO: somewhat too cruel -- may be an option or smth...
         if not allow_dirty and annex.dirty:
-            # already saved me once ;)
-            raise RuntimeError("You better commit all the changes and untracked files first")
+            # error out here if the dataset contains untracked changes
+            yield get_status_dict(
+                ds=ds,
+                status='impossible',
+                message=(
+                    'clean dataset required. '
+                    'Use `datalad status` to inspect unsaved changes'),
+                **res_kwargs
+            )
+            return
+
+        # ensure the archive exists, status doesn't error on a non-existing file
+        if not key and not lexists(archive_path):
+            yield get_status_dict(
+                ds=ds,
+                status='impossible',
+                message=(
+                    'No such file: {}'.format(archive_path),
+                ),
+                **res_kwargs
+            )
+            return
 
         if not key:
-            # we were given a file which must exist
-            if not exists(archive_path):
-                raise ValueError("Archive {} does not exist".format(archive))
+            check_path = archive_path.relative_to(ds.pathobj)
             # TODO: support adding archives content from outside the annex/repo
             origin = 'archive'
-            key = annex.get_file_key(archive_rpath)
-            archive_dir = dirname(archive_path)
+            # can become get_file_annexinfo once #6104 is merged
+            key = annex.get_file_annexinfo(check_path)['key']
+            if not key:
+                raise RuntimeError(
+                    f"Archive must be an annexed file in {ds}")
+            archive_dir = Path(archive_path).parent
         else:
             origin = 'key'
             key = archive
-            archive_dir = None  # We must not have anything to do with the location under .git/annex
+            # We must not have anything to do with the location under .git/annex
+            archive_dir = None
+            # instead, we will go from the current directory
+            use_current_dir = True
 
         archive_basename = file_basename(archive)
 
         if not key:
-            # TODO: allow for it to be under git???  how to reference then?
+            # if we didn't manage to get a key, the file must be in Git
             raise NotImplementedError(
-                "Provided file %s is not under annex.  We don't support yet adding everything "
-                "straight to git" % archive
+                "Provided file %s does not seem to be under annex control. "
+                "We don't support adding everything straight to Git" % archive
             )
 
+        # figure out our location
+        pwd = getpwd()
         # are we in a subdirectory of the repository?
-        pwd_under_annex = commonprefix([pwd, annex_path]) == annex_path
-        #  then we should add content under that
-        # subdirectory,
+        pwd_in_root = annex.path == archive_dir
+        # then we should add content under that subdirectory,
         # get the path relative to the repo top
         if use_current_dir:
-            # if outside -- extract to the top of repo
-            extract_rpath = relpath(pwd, annex_path) \
-                if pwd_under_annex \
+            # extract the archive under the current directory, not the directory
+            # where the archive is located
+            extract_rpath = Path(pwd).relative_to(ds.path) \
+                if not pwd_in_root \
                 else None
         else:
-            extract_rpath = relpath(archive_dir, annex_path)
+            extract_rpath = archive_dir.relative_to(ds.path)
 
         # relpath might return '.' as the relative path to curdir, which then normalize_paths
         # would take as instructions to really go from cwd, so we need to sanitize
         if extract_rpath == curdir:
-            extract_rpath = None  # no special relpath from top of the repo
+            extract_rpath = None
 
-        # and operate from now on the key or whereever content available "canonically"
         try:
-            key_rpath = annex.get_contentlocation(key)  # , relative_to_top=True)
+            key_rpath = annex.get_contentlocation(key)
         except:
-            raise RuntimeError("Content of %s seems to be N/A.  Fetch it first" % key)
+            # the only probable reason for this to fail is that there is no
+            # content present
+            raise RuntimeError(
+                "Content of %s seems to be N/A.  Fetch it first" % key
+            )
 
         # now we simply need to go through every file in that archive and
-        lgr.info("Adding content of the archive %s into annex %s", archive, annex)
+        lgr.info(
+            "Adding content of the archive %s into annex %s", archive, annex
+        )
 
         from datalad.customremotes.archives import ArchiveAnnexCustomRemote
+
         # TODO: shouldn't we be able just to pass existing AnnexRepo instance?
         # TODO: we will use persistent cache so we could just (ab)use possibly extracted archive
-        annexarchive = ArchiveAnnexCustomRemote(path=annex_path, persistent_cache=True)
+        # OK, let's ignore that the following class is actually a special
+        # remote implementation, and use it only to work with its cache
+        annexarchive = ArchiveAnnexCustomRemote(annex=None,
+                                                path=annex.path,
+                                                persistent_cache=True)
         # We will move extracted content so it must not exist prior running
         annexarchive.cache.allow_existing = True
         earchive = annexarchive.cache[key_rpath]
-
-        # TODO: check if may be it was already added
-        if ARCHIVES_SPECIAL_REMOTE not in annex.get_remotes():
-            init_datalad_remote(annex, ARCHIVES_SPECIAL_REMOTE, autoenable=True)
-        else:
-            lgr.debug("Special remote {} already exists".format(ARCHIVES_SPECIAL_REMOTE))
+        # make sure there is an enabled datalad-archives special remote
+        ensure_datalad_remote(ds.repo, remote=ARCHIVES_SPECIAL_REMOTE,
+                              autoenable=True)
 
         precommitted = False
+        old_always_commit = annex.always_commit
+        # batch mode is disabled when faking dates, we want to always commit
+        annex.always_commit = annex.fake_dates_enabled
+        if annex_options:
+            if isinstance(annex_options, str):
+                annex_options = split_cmdline(annex_options)
         delete_after_rpath = None
-        try:
-            old_always_commit = annex.always_commit
-            annex.always_commit = False
 
-            if annex_options:
-                if isinstance(annex_options, string_types):
-                    annex_options = shlex.split(annex_options)
-
-            leading_dir = earchive.get_leading_directory(
-                depth=leading_dirs_depth, exclude=exclude, consider=leading_dirs_consider) \
-                if strip_leading_dirs else None
-            leading_dir_len = len(leading_dir) + len(opsep) if leading_dir else 0
-
-            # we need to create a temporary directory at the top level which would later be
-            # removed
-            prefix_dir = basename(tempfile.mktemp(prefix=".datalad", dir=annex_path)) \
-                if delete_after \
-                else None
-
-            # dedicated stats which would be added to passed in (if any)
-            outside_stats = stats
-            stats = ActivityStats()
+        prefix_dir = basename(tempfile.mkdtemp(prefix=".datalad",
+                                               dir=annex.path)) \
+            if delete_after \
+            else None
+
+        # dedicated stats which would be added to passed in (if any)
+        outside_stats = stats
+        stats = ActivityStats()
 
-            for extracted_file in earchive.get_extracted_files():
+        # start a progress bar for extraction
+        pbar_id = f'add-archive-{archive_path}'
+        try:
+            # keep track of extracted files for progress bar logging
+            file_counter = 0
+            # iterative over all files in the archive
+            extracted_files = list(earchive.get_extracted_files())
+            log_progress(
+                lgr.info, pbar_id, 'Extracting archive',
+                label="Extracting archive",
+                unit=' Files',
+                total = len(extracted_files),
+                noninteractive_level = logging.INFO)
+            for extracted_file in extracted_files:
+                file_counter += 1
+                files_left = len(extracted_files) - file_counter
+                log_progress(
+                    lgr.info, pbar_id,
+                    "Files to extract %i ", files_left,
+                    update=1,
+                    increment=True,
+                    noninteractive_level=logging.DEBUG)
                 stats.files += 1
-                extracted_path = opj(earchive.path, extracted_file)
+                extracted_path = Path(earchive.path) / Path(extracted_file)
 
-                if islink(extracted_path):
-                    link_path = realpath(extracted_path)
-                    if not exists(link_path):  # TODO: config  addarchive.symlink-broken='skip'
-                        lgr.warning("Path %s points to non-existing file %s" % (extracted_path, link_path))
+                if extracted_path.is_symlink():
+                    link_path = str(extracted_path.resolve())
+                    if not exists(link_path):
+                        # TODO: config  addarchive.symlink-broken='skip'
+                        lgr.warning(
+                            "Path %s points to non-existing file %s" %
+                            (extracted_path, link_path)
+                        )
                         stats.skipped += 1
                         continue
-                        # TODO: check if points outside of the archive -- warning and skip
+                        # TODO: check if points outside of archive - warn & skip
+
+                url = annexarchive.get_file_url(
+                    archive_key=key,
+                    file=extracted_file,
+                    size=os.stat(extracted_path).st_size)
 
                 # preliminary target name which might get modified by renames
-                target_file_orig = target_file = extracted_file
+                target_file_orig = target_file = Path(extracted_file)
 
-                # strip leading dirs
-                target_file = target_file[leading_dir_len:]
+                # stream archives would not have had the original filename
+                # information in them, so would be extracted under a name
+                # derived from their annex key.
+                # Provide ad-hoc handling for such cases
+                if (len(extracted_files) == 1 and
+                    Path(archive).suffix in ('.xz', '.gz', '.lzma') and
+                        Path(key_rpath).name.startswith(Path(
+                            extracted_file).name)):
+                    # take archive's name without extension for filename & place
+                    # where it was originally extracted
+                    target_file = \
+                        Path(extracted_file).parent / Path(archive).stem
+
+                if strip_leading_dirs:
+                    leading_dir = earchive.get_leading_directory(
+                        depth=leading_dirs_depth, exclude=exclude,
+                        consider=leading_dirs_consider)
+                    leading_dir_len = \
+                        len(leading_dir) + len(opsep) if leading_dir else 0
+                    target_file = str(target_file)[leading_dir_len:]
 
                 if add_archive_leading_dir:
-                    target_file = opj(archive_basename, target_file)
+                    # place extracted content under a directory corresponding to
+                    # the archive name with suffix stripped.
+                    target_file = Path(archive_basename) / target_file
 
                 if rename:
-                    target_file = apply_replacement_rules(rename, target_file)
+                    target_file = apply_replacement_rules(rename,
+                                                          str(target_file))
 
                 # continue to next iteration if extracted_file in excluded
                 if exclude:
                     try:  # since we need to skip outside loop from inside loop
                         for regexp in exclude:
                             if re.search(regexp, extracted_file):
                                 lgr.debug(
-                                    "Skipping {extracted_file} since contains {regexp} pattern".format(**locals()))
+                                    "Skipping %s since contains %s pattern",
+                                    extracted_file, regexp)
                                 stats.skipped += 1
                                 raise StopIteration
                     except StopIteration:
                         continue
 
-                if prefix_dir:
-                    target_file = opj(prefix_dir, target_file)
+                if delete_after:
+                    # place target file in a temporary directory
+                    target_file = Path(prefix_dir) / Path(target_file)
                     # but also allow for it in the orig
-                    target_file_orig = opj(prefix_dir, target_file_orig)
-
-                target_file_path_orig = opj(annex.path, target_file_orig)
-
-                url = annexarchive.get_file_url(archive_key=key, file=extracted_file, size=os.stat(extracted_path).st_size)
+                    target_file_orig = Path(prefix_dir) / Path(target_file_orig)
 
-                # lgr.debug("mv {extracted_path} {target_file}. URL: {url}".format(**locals()))
+                target_file_path_orig = annex.pathobj / target_file_orig
 
-                target_file_path = opj(extract_rpath, target_file) \
+                # If we were invoked in a subdirectory, patch together the
+                # correct path
+                target_file_path = extract_rpath / target_file \
                     if extract_rpath else target_file
+                target_file_path = annex.pathobj / target_file_path
 
-                target_file_path = opj(annex.path, target_file_path)
-
+                # when the file already exists...
                 if lexists(target_file_path):
                     handle_existing = True
-                    if md5sum(target_file_path) == md5sum(extracted_path):
-                        if not annex.is_under_annex(extracted_path):
+                    if md5sum(str(target_file_path)) == \
+                            md5sum(str(extracted_path)):
+                        if not annex.is_under_annex(str(extracted_path)):
                             # if under annex -- must be having the same content,
                             # we should just add possibly a new extra URL
                             # but if under git -- we cannot/should not do
                             # anything about it ATM
                             if existing != 'overwrite':
                                 continue
                         else:
                             handle_existing = False
                     if not handle_existing:
                         pass  # nothing... just to avoid additional indentation
                     elif existing == 'fail':
-                        raise RuntimeError(
-                            "File {} already exists, but new (?) file {} was instructed "
-                            "to be placed there while overwrite=False".format
-                                (target_file_path, extracted_file)
-                        )
+                        message = \
+                            "{} exists, but would be overwritten by new file " \
+                            "{}. Consider adjusting --existing".format\
+                            (target_file_path, extracted_file)
+                        yield get_status_dict(
+                            ds=ds,
+                            status='error',
+                            message=message,
+                            **res_kwargs)
+                        return
                     elif existing == 'overwrite':
                         stats.overwritten += 1
-                        # to make sure it doesn't conflict -- might have been a tree
+                        # to make sure it doesn't conflict -- might have been a
+                        # tree
                         rmtree(target_file_path)
                     else:
+                        # an elaborate dance to piece together new archive names
                         target_file_path_orig_ = target_file_path
 
-                        # To keep extension intact -- operate on the base of the filename
+                        # To keep extension intact -- operate on the base of the
+                        # filename
                         p, fn = os.path.split(target_file_path)
                         ends_with_dot = fn.endswith('.')
                         fn_base, fn_ext = file_basename(fn, return_ext=True)
 
                         if existing == 'archive-suffix':
                             fn_base += '-%s' % archive_basename
                         elif existing == 'numeric-suffix':
                             pass  # archive-suffix will have the same logic
                         else:
+                            # we shouldn't get here, argparse should catch a
+                            # non-existing value for --existing right away
                             raise ValueError(existing)
-                        # keep incrementing index in the suffix until file doesn't collide
+                        # keep incrementing index in the suffix until file
+                        # doesn't collide
                         suf, i = '', 0
                         while True:
-                            target_file_path_new = opj(p, fn_base + suf + ('.' if (fn_ext or ends_with_dot) else '') + fn_ext)
+                            connector = \
+                                ('.' if (fn_ext or ends_with_dot) else '')
+                            file = fn_base + suf + connector + fn_ext
+                            target_file_path_new =  \
+                                Path(p) / Path(file)
                             if not lexists(target_file_path_new):
+                                # we found a file name that is not yet taken
                                 break
-                            lgr.debug("File %s already exists" % target_file_path_new)
+                            lgr.debug("Iteration %i of file name finding. "
+                                      "File %s already exists", i,
+                                      target_file_path_new)
                             i += 1
                             suf = '.%d' % i
                         target_file_path = target_file_path_new
                         lgr.debug("Original file %s will be saved into %s"
                                   % (target_file_path_orig_, target_file_path))
                         # TODO: should we reserve smth like
                         # stats.clobbed += 1
 
                 if target_file_path != target_file_path_orig:
                     stats.renamed += 1
 
-                #target_path = opj(getpwd(), target_file)
                 if copy:
-                    raise NotImplementedError("Not yet copying from 'persistent' cache")
-                else:
-                    # os.renames(extracted_path, target_path)
-                    # addurl implementation relying on annex'es addurl below would actually copy
-                    pass
+                    raise NotImplementedError(
+                        "Not yet copying from 'persistent' cache"
+                    )
 
-                lgr.debug("Adding %s to annex pointing to %s and with options %r",
-                          target_file_path, url, annex_options)
+                lgr.debug("Adding %s to annex pointing to %s and with options "
+                          "%r", target_file_path, url, annex_options)
 
                 out_json = annex.add_url_to_file(
                     target_file_path,
                     url, options=annex_options,
                     batch=True)
 
-                if 'key' in out_json and out_json['key'] is not None:  # annex.is_under_annex(target_file, batch=True):
+                if 'key' in out_json and out_json['key'] is not None:
+                    # annex.is_under_annex(target_file, batch=True):
                     # due to http://git-annex.branchable.com/bugs/annex_drop_is_not___34__in_effect__34___for_load_which_was___34__addurl_--batch__34__ed_but_not_yet_committed/?updated
                     # we need to maintain a list of those to be dropped files
                     if drop_after:
+                        # drop extracted files after adding to annex
                         annex.drop_key(out_json['key'], batch=True)
                         stats.dropped += 1
                     stats.add_annex += 1
                 else:
-                    lgr.debug("File {} was added to git, not adding url".format(target_file_path))
+                    lgr.debug("File %s was added to git, not adding url",
+                        target_file_path)
                     stats.add_git += 1
 
                 if delete_after:
-                    # delayed removal so it doesn't interfer with batched processes since any pure
-                    # git action invokes precommit which closes batched processes. But we like to count
+                    # we count the removal here, but don't yet perform it
+                    # to not interfere with batched processes - any pure Git
+                    # action invokes precommit which closes batched processes.
                     stats.removed += 1
 
-                # # chaining 3 annex commands, 2 of which not batched -- less efficient but more bullet proof etc
-                # annex.add(target_path, options=annex_options)
-                # # above action might add to git or to annex
-                # if annex.file_has_content(target_path):
-                #     # if not --  it was added to git, if in annex, it is present and output is True
-                #     annex.add_url_to_file(target_file, url, options=['--relaxed'], batch=True)
-                #     stats.add_annex += 1
-                # else:
-                #     lgr.debug("File {} was added to git, not adding url".format(target_file))
-                #     stats.add_git += 1
-                # # TODO: actually check if it is anyhow different from a previous version. If not
-                # # then it wasn't really added
-
-                del target_file  # Done with target_file -- just to have clear end of the loop
+                # Done with target_file -- just to have clear end of the loop
+                del target_file
 
             if delete and archive and origin != 'key':
-                lgr.debug("Removing the original archive {}".format(archive))
+                lgr.debug("Removing the original archive %s", archive)
                 # force=True since some times might still be staged and fail
-                annex.remove(archive_rpath, force=True)
+                annex.remove(str(archive_path), force=True)
 
-            lgr.info("Finished adding %s: %s" % (archive, stats.as_str(mode='line')))
+            lgr.info("Finished adding %s: %s", archive, stats.as_str(mode='line'))
 
             if outside_stats:
                 outside_stats += stats
             if delete_after:
                 # force since not committed. r=True for -r (passed into git call
                 # to recurse)
-                delete_after_rpath = opj(extract_rpath, prefix_dir) if extract_rpath else prefix_dir
+                delete_after_rpath = opj(extract_rpath, prefix_dir) \
+                    if extract_rpath else prefix_dir
+                delete_after_rpath = resolve_path(delete_after_rpath,
+                                                  ds=dataset)
                 lgr.debug(
                     "Removing extracted and annexed files under %s",
                     delete_after_rpath
                 )
-                annex.remove(delete_after_rpath, r=True, force=True)
+                annex.remove(str(delete_after_rpath), r=True, force=True)
             if commit:
+                archive_rpath = archive_path.relative_to(ds.path)
                 commit_stats = outside_stats if outside_stats else stats
-                annex.precommit()  # so batched ones close and files become annex symlinks etc
+                # so batched ones close and files become annex symlinks etc
+                annex.precommit()
                 precommitted = True
-                if annex.is_dirty(untracked_files=False):
+                if any(r.get('state', None) != 'clean'
+                       for p, r in annex.status(untracked='no').items()):
                     annex.commit(
                         "Added content extracted from %s %s\n\n%s" %
-                        (origin, archive, commit_stats.as_str(mode='full')),
+                        (origin, archive_rpath,
+                         commit_stats.as_str(mode='full')),
                         _datalad_msg=True
                     )
                     commit_stats.reset()
+            else:
+                # don't commit upon completion
+                pass
         finally:
+            # take down the progress bar
+            log_progress(
+                lgr.info, pbar_id,
+                'Finished extraction',
+                noninteractive_level=logging.INFO)
             # since we batched addurl, we should close those batched processes
             # if haven't done yet.  explicitly checked to avoid any possible
             # "double-action"
             if not precommitted:
                 annex.precommit()
 
             if delete_after_rpath:
-                delete_after_path = opj(annex_path, delete_after_rpath)
+                delete_after_path = opj(annex.path, delete_after_rpath)
+                delete_after_rpath = resolve_path(delete_after_rpath,
+                                                  ds=dataset)
                 if exists(delete_after_path):  # should not be there
                     # but for paranoid yoh
                     lgr.warning(
                         "Removing temporary directory under which extracted "
                         "files were annexed and should have been removed: %s",
                         delete_after_path)
                     rmtree(delete_after_path)
 
             annex.always_commit = old_always_commit
             # remove what is left and/or everything upon failure
             earchive.clean(force=True)
-
+            # remove tempfile directories (not cleaned up automatically):
+            if prefix_dir is not None and lexists(prefix_dir):
+                os.rmdir(prefix_dir)
+        yield get_status_dict(
+            ds=ds,
+            status='ok',
+            **res_kwargs)
         return annex
```

### Comparing `datalad-0.9.3/datalad/interface/clean.py` & `datalad-1.0.0/datalad/local/no_annex.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,115 +1,144 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
+"""configure which dataset parts to never put in the annex"""
 
-__docformat__ = 'restructuredtext'
 
+__docformat__ = 'restructuredtext'
 
-from os.path import join as opj
-from glob import glob
-from .base import Interface
-from ..utils import rmtree
-from ..support.param import Parameter
-from ..consts import ARCHIVES_TEMP_DIR
-from ..consts import ANNEX_TEMP_DIR
-
-from datalad.support.constraints import EnsureNone
-from datalad.distribution.dataset import EnsureDataset
-from datalad.distribution.dataset import require_dataset
-from datalad.distribution.dataset import datasetmethod
-from datalad.interface.annotate_paths import AnnotatePaths
-from datalad.interface.common_opts import recursion_flag
-from datalad.interface.common_opts import recursion_limit
-from datalad.interface.results import get_status_dict
-from datalad.interface.utils import eval_results
+from datalad.interface.base import Interface
 from datalad.interface.base import build_doc
 
-from logging import getLogger
-lgr = getLogger('datalad.api.clean')
-
 
 @build_doc
-class Clean(Interface):
-    """Clean up after DataLad (possible temporary files etc.)
+class NoAnnex(Interface):
+    """Configure a dataset to never put some content into the dataset's annex
+
+    This can be useful in mixed datasets that also contain textual data, such
+    as source code, which can be efficiently and more conveniently managed
+    directly in Git.
+
+    Patterns generally look like this::
+
+      code/*
 
-    Removes extracted temporary archives, etc.
+    which would match all file in the code directory. In order to match all
+    files under ``code/``, including all its subdirectories use such a
+    pattern::
 
-    Examples:
+      code/**
 
-      $ datalad clean
+    Note that this command works incrementally, hence any existing configuration
+    (e.g. from a previous plugin run) is amended, not replaced.
     """
+    from datalad.support.param import Parameter
+    from datalad.distribution.dataset import datasetmethod
+    from datalad.interface.base import eval_results
+    from datalad.distribution.dataset import EnsureDataset
+    from datalad.support.constraints import EnsureNone
 
     _params_ = dict(
         dataset=Parameter(
             args=("-d", "--dataset"),
-            doc="""specify the dataset to perform the clean operation on.  If
-                no dataset is given, an attempt is made to identify the dataset
-                in current working directory""",
+            doc=""""specify the dataset to configure. If no dataset is given,
+            an attempt is made to identify the dataset based on the current
+            working directory.""",
             constraints=EnsureDataset() | EnsureNone()),
-        # TODO:  --info  -- which performs dry run just summarizing what is to be cleaned up
-        # TODO: Python only???
-        what=Parameter(
-            args=("--what",),
-            dest='what',
-            choices=('cached-archives', 'annex-tmp'),
-            nargs="*",
-            doc="""What to clean.  If none specified -- all known targets are
-            cleaned"""),
-        recursive=recursion_flag,
-        recursion_limit=recursion_limit,
+        pattern=Parameter(
+            args=("--pattern",),
+            nargs='+',
+            doc="""list of path patterns. Any content whose path is matching
+            any pattern will not be annexed when added to a dataset, but
+            instead will be tracked directly in Git. Path pattern have to be
+            relative to the directory given by the `ref_dir` option. By
+            default, patterns should be relative to the root of the dataset."""),
+        ref_dir=Parameter(
+            args=("--ref-dir",),
+            doc="""Relative path (within the dataset) to the directory that is
+            to be configured. All patterns are interpreted relative to this
+            path, and configuration is written to a ``.gitattributes`` file in
+            this directory."""),
+        makedirs=Parameter(
+            args=("--makedirs",),
+            action='store_true',
+            doc="""If set, any missing directories will be created in order to
+            be able to place a file into ``--ref-dir``."""),
     )
 
     @staticmethod
-    @datasetmethod(name='clean')
+    @datasetmethod(name='no_annex')
     @eval_results
-    def __call__(dataset=None, what=None, recursive=False, recursion_limit=None):
-        ds = require_dataset(dataset, purpose='clean-up')
-        res_kwargs = dict(action='clean', logger=lgr, refds=ds.path)
-        for ap in AnnotatePaths.__call__(
-                dataset=ds.path,
-                recursive=recursive,
-                recursion_limit=recursion_limit,
-                action='clean',
-                unavailable_path_status='impossible',
-                nondataset_path_status='impossible',
-                return_type='generator',
-                on_failure='ignore'):
-            if ap.get('status', None):
-                yield ap
-                continue
-            if ap.get('type', None) != 'dataset':
-                ap.update(status='impossible',
-                          message='only datasets can be cleaned')
-                yield ap
-                continue
-            d = ap['path']
-            for dirpath, flag, msg, sing_pl in [
-                (ARCHIVES_TEMP_DIR, "cached-archives",
-                 "temporary archive", ("directory", "directories")),
-                (ANNEX_TEMP_DIR, "annex-tmp",
-                 "temporary annex", ("file", "files")),
-            ]:
-                topdir = opj(d, dirpath)
-                lgr.debug("Considering to clean %s:%s", d, dirpath)
-                if not ((what is None) or (flag in what)):
-                    yield get_status_dict(
-                        path=topdir, status='notneeded', type='dir', **res_kwargs)
-                    continue
-                paths = glob(opj(topdir, '*'))
-                if not paths:
-                    yield get_status_dict(
-                        path=topdir, status='notneeded', type='dir', **res_kwargs)
-                    continue
-                pl = len(paths) > 1
-                message = ("Removed %d %s %s: %s",
-                           len(paths), msg, sing_pl[int(pl)],
-                           ", ".join(sorted([x[len(topdir) + 1:] for x in paths])))
-                rmtree(topdir)
-                yield get_status_dict(
-                    path=topdir, status='ok', type='dir', message=message,
-                    **res_kwargs)
+    # TODO*: make dataset, pattern into kwargs after *,?
+    def __call__(dataset, pattern, ref_dir='.', makedirs=False):
+        # could be extended to accept actual largefile expressions
+        from os.path import join as opj
+        from os.path import isabs
+        from os.path import exists
+        from os import makedirs as makedirsfx
+        from datalad.distribution.dataset import require_dataset
+        from datalad.support.annexrepo import AnnexRepo
+        from datalad.utils import ensure_list
+
+        pattern = ensure_list(pattern)
+        ds = require_dataset(dataset, check_installed=True,
+                             purpose='no_annex configuration')
+
+        res_kwargs = dict(
+            path=ds.path,
+            type='dataset',
+            action='no_annex',
+        )
+
+        # all the ways we refused to cooperate
+        if not isinstance(ds.repo, AnnexRepo):
+            yield dict(
+                res_kwargs,
+                status='notneeded',
+                message='dataset has no annex')
+            return
+        if any(isabs(p) for p in pattern):
+            yield dict(
+                res_kwargs,
+                status='error',
+                message=('path pattern for `no_annex` configuration must be relative paths: %s',
+                         pattern))
+            return
+        if isabs(ref_dir):
+            yield dict(
+                res_kwargs,
+                status='error',
+                message=('`ref_dir` for `no_annex` configuration must be a relative path: %s',
+                         ref_dir))
+            return
+
+        gitattr_dir = opj(ds.path, ref_dir)
+        if not exists(gitattr_dir):
+            if makedirs:
+                makedirsfx(gitattr_dir)
+            else:
+                yield dict(
+                    res_kwargs,
+                    status='error',
+                    message='target directory for `no_annex` does not exist (consider makedirs=True)')
+                return
+
+        gitattr_file = opj(gitattr_dir, '.gitattributes')
+        ds.repo.set_gitattributes(
+            [(p, {'annex.largefiles': 'nothing'}) for p in pattern],
+            attrfile=gitattr_file)
+        yield dict(res_kwargs, status='ok')
+
+        yield from ds.save(
+            gitattr_file,
+            to_git=True,
+            message="[DATALAD] exclude paths from annex'ing",
+            result_filter=None,
+            result_xfm=None,
+            return_type='generator',
+            result_renderer='disabled',
+        )
```

### Comparing `datalad-0.9.3/datalad/interface/results.py` & `datalad-1.0.0/datalad/dataset/repo.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,360 +1,304 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""Interface result handling functions
+""" utility classes for repositories
 
 """
 
-__docformat__ = 'restructuredtext'
-
 import logging
+import threading
 
-from os.path import isdir
-from os.path import isabs
-from os.path import join as opj
-from os.path import relpath
-from os.path import abspath
-from os.path import normpath
-from datalad.utils import assure_list
-from datalad.utils import with_pathsep as _with_sep
-from datalad.utils import path_is_subpath
-
-from datalad.distribution.dataset import Dataset
-
-
-lgr = logging.getLogger('datalad.interface.results')
-
-
-# which status is a success , which is failure
-success_status_map = {
-    'ok': 'success',
-    'notneeded': 'success',
-    'impossible': 'failure',
-    'error': 'failure',
-}
-
-
-def get_status_dict(action=None, ds=None, path=None, type=None, logger=None,
-                    refds=None, status=None, message=None, **kwargs):
-    # `type` is intentionally not `type_` or something else, as a mismatch
-    # with the dict key 'type' causes too much pain all over the place
-    # just for not shadowing the builtin `type` in this function
-    """Helper to create a result dictionary.
-
-    Most arguments match their key in the resulting dict. Only exceptions are
-    listed here.
-
-    Parameters
-    ----------
-    ds : Dataset instance
-      If given, the `path` and `type` values are populated with the path of the
-      datasets and 'dataset' as the type. Giving additional values for both
-      keys will overwrite these pre-populated values.
-
-    Returns
-    -------
-    dict
-    """
+from datalad.support.exceptions import InvalidInstanceRequestError
+from datalad.support.network import RI
+from datalad import utils as ut
 
-    d = {}
-    if action is not None:
-        d['action'] = action
-    if ds:
-        d['path'] = ds.path
-        d['type'] = 'dataset'
-    # now overwrite automatic
-    if path is not None:
-        d['path'] = path
-    if type:
-        d['type'] = type
-    if logger:
-        d['logger'] = logger
-    if refds:
-        d['refds'] = refds
-    if status is not None:
-        # TODO check for known status label
-        d['status'] = status
-    if message is not None:
-        d['message'] = message
-    if kwargs:
-        d.update(kwargs)
-    return d
+lgr = logging.getLogger('datalad.repo')
 
 
-def results_from_paths(paths, action=None, type=None, logger=None, refds=None,
-                       status=None, message=None):
-    """
-    Helper to yield analog result dicts for each path in a sequence.
+class Flyweight(type):
+    """Metaclass providing an implementation of the flyweight pattern.
 
-    Parameters
-    ----------
-    message: str
-      A result message. May contain `%s` which will be replaced by the
-      respective `path`.
-
-    Returns
-    -------
-    generator
+    Since the flyweight is very similar to a singleton, we occasionally use this
+    term to make clear there's only one instance (at a time).
+    This integrates the "factory" into the actual classes, which need
+    to have a class attribute `_unique_instances` (WeakValueDictionary).
+    By providing an implementation of __call__, you don't need to call a
+    factory's get_xy_repo() method to get a singleton. Instead this is called
+    when you simply instantiate via MyClass(). So, you basically don't even need
+    to know there were singletons. Therefore it is also less likely to sabotage
+    the concept by not being aware of how to get an appropriate object.
 
-    """
-    for p in assure_list(paths):
-        yield get_status_dict(
-            action, path=p, type=type, logger=logger, refds=refds,
-            status=status, message=(message, p) if '%s' in message else message)
+    Multiple instances, pointing to the same physical repository can cause a
+    lot of trouble. This is why this class exists. You should be very aware of
+    the implications, if you want to circumvent that mechanism.
 
+    To use this pattern, you need to add this class as a metaclass to the class
+    you want to use it with. Additionally there needs to be a class attribute
+    `_unique_instances`, which should be a `WeakValueDictionary`. Furthermore
+    implement `_flyweight_id_from_args` method to determine, what should be the
+    identifying criteria to consider two requested instances the same.
 
-def is_ok_dataset(r):
-    """Convenience test for a non-failure dataset-related result dict"""
-    return r.get('status', None) == 'ok' and r.get('type', None) == 'dataset'
+    Example:
 
+    from weakref import WeakValueDictionary
 
-class ResultXFM(object):
-    """Abstract definition of the result transformer API"""
-    def __call__(self, res):
-        """This is called with one result dict at a time"""
-        raise NotImplementedError
+    class MyFlyweightClass(object, metaclass=Flyweight):
 
+        _unique_instances = WeakValueDictionary()
 
-class YieldDatasets(ResultXFM):
-    """Result transformer to return a Dataset instance from matching result.
+        @classmethod
+        def _flyweight_id_from_args(cls, *args, **kwargs):
 
-    If the `success_only` flag is given only dataset with 'ok' or 'notneeded'
-    status are returned'.
+            id = kwargs.pop('id')
+            return id, args, kwargs
 
-    `None` is returned for any other result.
-    """
-    def __init__(self, success_only=False):
-        self.success_only = success_only
+        def __init__(self, some, someother=None):
+            pass
 
-    def __call__(self, res):
-        if res.get('type', None) == 'dataset':
-            if not self.success_only or \
-                    res.get('status', None) in ('ok', 'notneeded'):
-                return Dataset(res['path'])
-        else:
-            lgr.debug('rejected by return value configuration: %s', res)
+    a = MyFlyweightClass('bla', id=1)
+    b = MyFlyweightClass('blubb', id=1)
+    assert a is b
+    c = MyFlyweightClass('whatever', id=2)
+    assert c is not a
+    """
 
+    # to avoid parallel creation of (identical) instances
+    _lock = threading.Lock()
 
-class YieldRelativePaths(ResultXFM):
-    """Result transformer to return relative paths for a result
+    def _flyweight_id_from_args(cls, *args, **kwargs):
+        """create an ID from arguments passed to `__call__`
 
-    Relative paths are determined from the 'refds' value in the result. If
-    no such value is found, `None` is returned.
-    """
-    def __call__(self, res):
-        refpath = res.get('refds', None)
-        if refpath:
-            return relpath(res['path'], start=refpath)
+        Subclasses need to implement this method. The ID it returns is used to
+        determine whether or not there already is an instance of that kind and
+        as key in the `_unique_instances` dictionary.
 
+        Besides the ID this should return args and kwargs, which can be modified
+        herein and will be passed on to the constructor of a requested instance.
 
-class YieldField(ResultXFM):
-    """Result transformer to return an arbitrary value from a result dict"""
-    def __init__(self, field):
+        Parameters
+        ----------
+        args:
+         positional arguments passed to __call__
+        kwargs:
+         keyword arguments passed to __call__
+
+        Returns
+        -------
+        hashable, args, kwargs
+          id, optionally manipulated args and kwargs to be passed to __init__
         """
+        raise NotImplementedError
+
+    #       TODO: - We might want to remove the classmethod from Flyweight altogether and replace by an
+    #             requirement to implement an actual method, since the purpose of it is actually about a
+    #             particular, existing instance
+    #             - Done. But update docs!
+    # def _flyweight_invalid(cls, id):
+    #     """determines whether or not an instance with `id` became invalid and
+    #     therefore has to be instantiated again.
+    #
+    #     Subclasses can implement this method to provide an additional condition
+    #     on when to create a new instance besides there is none yet.
+    #
+    #     Parameter
+    #     ---------
+    #     id: hashable
+    #       ID of the requested instance
+    #
+    #     Returns
+    #     -------
+    #     bool
+    #       whether to consider an existing instance with that ID invalid and
+    #       therefore create a new instance. Default implementation always returns
+    #       False.
+    #     """
+    #     return False
+
+    # TODO: document the suggestion to implement a finalizer!
+
+    def _flyweight_reject(cls, id, *args, **kwargs):
+        """decides whether to reject a request for an instance
+
+        This gives the opportunity to detect a conflict of an instance request
+        with an already existing instance, that is not invalidated by
+        `_flyweight_invalid`. In case the return value is not `None`, it will be
+        used as the message for an `InvalidInstanceRequestError`,
+        raised by `__call__`
+
         Parameters
         ----------
-        field : str
-          Key of the field to return.
+        id: hashable
+          the ID of the instance in question as calculated by
+          `_flyweight_id_from_args`
+        args:
+        kwargs:
+          (keyword) arguments to the original call
+
+        Returns:
+        --------
+        None or str
+        """
+        return None
+
+    def __call__(cls, *args, **kwargs):
+
+        id_, new_args, new_kwargs = cls._flyweight_id_from_args(*args, **kwargs)
+        # Thread lock following block so we do not fall victim to
+        # race condition across threads trying to instantiate multiple
+        # instances. In principle we better have a lock per id_ but that mean we
+        # might race at getting "name specific lock" (Yarik did not research much),
+        # so keeping it KISS -- just lock instantiation altogether, but could be
+        # made smarter later on.
+        with cls._lock:
+            instance = cls._unique_instances.get(id_, None)
+
+            if instance is None or instance._flyweight_invalid():
+                # we have no such instance yet or the existing one is invalidated,
+                # so we instantiate:
+                instance = type.__call__(cls, *new_args, **new_kwargs)
+                cls._unique_instances[id_] = instance
+            else:
+                # we have an instance already that is not invalid itself; check
+                # whether there is a conflict, otherwise return existing one:
+                # TODO
+                # Note, that this might (and probably should) go away, when we
+                # decide how to deal with currently possible invalid constructor
+                # calls for the repo classes. In particular this is about calling
+                # it with different options than before, that might lead to
+                # fundamental changes in the repository (like annex repo version
+                # change or re-init of git)
+
+                # force? may not mean the same thing
+                msg = cls._flyweight_reject(id_, *new_args, **new_kwargs)
+                if msg is not None:
+                    raise InvalidInstanceRequestError(id_, msg)
+        return instance
+
+
+class PathBasedFlyweight(Flyweight):
+
+    def _flyweight_preproc_path(cls, path):
+        """perform any desired path preprocessing (e.g., aliases)
+
+        By default nothing is done
         """
-        self.field = field
+        return path
+
+    def _flyweight_postproc_path(cls, path):
+        """perform any desired path post-processing (e.g., dereferencing etc)
 
-    def __call__(self, res):
-        if self.field in res:
-            return res[self.field]
+        By default - realpath to guarantee reuse. Derived classes (e.g.,
+        Dataset) could override to allow for symlinked datasets to have
+        individual instances for multiple symlinks
+        """
+        # resolve symlinks to make sure we have exactly one instance per
+        # physical repository at a time
+        # do absolute() in addition to always get an absolute path
+        # even with non-existing paths on windows
+        resolved = str(ut.Path(path).resolve().absolute())
+        if ut.on_windows and resolved.startswith('\\\\'):
+            # resolve() ended up converting a mounted network drive into a UNC path.
+            # such paths are not supoprted (e.g. as cmd.exe CWD), hence redo and take
+            # absolute path at face value. This has the consequence we cannot determine
+            # repo duplicates mounted on different drives, but this is no worse than
+            # on UNIX
+            return str(ut.Path(path).absolute())
+        return resolved
+
+    def _flyweight_id_from_args(cls, *args, **kwargs):
+
+        if args:
+            # to a certain degree we need to simulate an actual call to __init__
+            # and make sure, passed arguments are fitting:
+            # TODO: Figure out, whether there is a cleaner way to do this in a
+            # generic fashion
+            assert('path' not in kwargs)
+            path = args[0]
+            args = args[1:]
+        elif 'path' in kwargs:
+            path = kwargs.pop('path')
         else:
-            lgr.debug('rejected by return value configuration: %s', res)
+            raise TypeError("__init__() requires argument `path`")
+
+        if path is None:
+            lgr.debug("path is None. args: %s, kwargs: %s", args, kwargs)
+            raise ValueError("path must not be None")
+
+        # Custom handling for few special abbreviations if defined by the class
+        path_ = cls._flyweight_preproc_path(path)
+
+        # Sanity check for argument `path`:
+        # raise if we cannot deal with `path` at all or
+        # if it is not a local thing:
+        localpath = RI(path_).localpath
 
+        path_postproc = cls._flyweight_postproc_path(localpath)
 
-# a bunch of convenience labels for common result transformers
-# the API `result_xfm` argument understand any of these labels and
-# applied the corresponding callable
-known_result_xfms = {
-    'datasets': YieldDatasets(),
-    'successdatasets-or-none': YieldDatasets(success_only=True),
-    'paths': YieldField('path'),
-    'relpaths': YieldRelativePaths(),
-}
-
-translate_annex_notes = {
-    '(Use --force to override this check, or adjust numcopies.)':
-        'configured minimum number of copies not found',
-}
-
-
-def annexjson2result(d, ds, **kwargs):
-    """Helper to convert an annex JSON result to a datalad result dict
-
-    Info from annex is rather heterogenous, partly because some of it
-    our support functions are faking.
-
-    This helper should be extended with all needed special cases to
-    homogenize the information.
-
-    Parameters
-    ----------
-    d : dict
-      Annex info dict.
-    ds : Dataset instance
-      Used to determine absolute paths for `file` results. This dataset
-      is not used to set `refds` in the result, pass this as a separate
-      kwarg if needed.
-    **kwargs
-      Passes as-is to `get_status_dict`. Must not contain `refds`.
+        kwargs['path'] = path_postproc
+        return path_postproc, args, kwargs
+    # End Flyweight
+
+
+
+# TODO: see issue #1100
+class RepoInterface(object):
+    """common operations for annex and plain git repositories
+
+    Especially provides "annex operations" on plain git repos, that just do
+    (or return) the "right thing"
     """
-    lgr.debug('received JSON result from annex: %s', d)
-    res = get_status_dict(**kwargs)
-    res['status'] = 'ok' if d.get('success', False) is True else 'error'
-    # we cannot rely on any of these to be available as the feed from
-    # git annex (or its wrapper) is not always homogeneous
-    if 'file' in d:
-        res['path'] = opj(ds.path, d['file'])
-    if 'command' in d:
-        res['action'] = d['command']
-    if 'key' in d:
-        res['annexkey'] = d['key']
-    # avoid meaningless standard messages
-    if 'note' in d and (
-            d['note'] != 'checksum...' and
-            not d['note'].startswith('checking file')):
-        res['message'] = translate_annex_notes.get(d['note'], d['note'])
-    return res
-
-
-def count_results(res, **kwargs):
-    """Return number if results that match all property values in kwargs"""
-    return sum(
-        all(k in r and r[k] == v for k, v in kwargs.items()) for r in res)
-
-
-def only_matching_paths(res, **kwargs):
-    # TODO handle relative paths by using a contained 'refds' value
-    paths = assure_list(kwargs.get('path', []))
-    respath = res.get('path', None)
-    return respath in paths
-
-
-# needs decorator, as it will otherwise bind to the command classes that use it
-@staticmethod
-def is_result_matching_pathsource_argument(res, **kwargs):
-    # we either have any non-zero number of "paths" (that could be anything), or
-    # we have one path and one source
-    # we don't do any error checking here, done by the command itself
-    source = kwargs.get('source', None)
-    if source is not None:
-        # we want to be able to deal with Dataset instances given as 'source':
-        if isinstance(source, Dataset):
-            source = source.path
-        # if there was a source, it needs to be recorded in the result
-        # otherwise this is not what we are looking for
-        return source == res.get('source_url', None)
-    # the only thing left is a potentially heterogeneous list of paths/URLs
-    paths = assure_list(kwargs.get('path', []))
-    # three cases left:
-    # 1. input arg was an absolute path -> must match 'path' property
-    # 2. input arg was relative to a dataset -> must match refds/relpath
-    # 3. something nifti with a relative input path that uses PWD as the
-    #    reference
-    respath = res.get('path', None)
-    if respath in paths:
-        # absolute match, pretty sure we want this
-        return True
-    elif kwargs.get('dataset', None) and YieldRelativePaths()(res) in paths:
-        # command was called with a reference dataset, and a relative
-        # path of a result matches in input argument -- not 100% exhaustive
-        # test, but could be good enough
-        return True
-    elif any(abspath(p) == respath for p in paths):
-        # one absolutified input path matches the result path
-        # I'd say: got for it!
-        return True
-    elif any(p == res.get('source_url', None) for p in paths):
-        # this was installed from a URL that was given, we'll take that too
-        return True
-    else:
-        False
-
-
-def results_from_annex_noinfo(ds, requested_paths, respath_by_status, dir_fail_msg,
-                              noinfo_dir_msg, noinfo_file_msg, **kwargs):
-    """Helper to yield results based on what information git annex did no give us.
-
-    The helper assumes that the annex command returned without an error code,
-    and interprets which of the requested paths we have heard nothing about,
-    and assumes that git annex was happy with their current state.
-
-    Parameters
-    ==========
-    ds : Dataset
-      All results have to be concerning this single dataset (used to resolve
-      relpaths).
-    requested_paths : list
-      List of path arguments sent to `git annex`
-    respath_by_status : dict
-      Mapping of 'success' or 'failure' labels to lists of result paths
-      reported by `git annex`. Everything that is not in here, we assume
-      that `git annex` was happy about.
-    dir_fail_msg : str
-      Message template to inject into the result for a requested directory where
-      a failure was reported for some of its content. The template contains two
-      string placeholders that will be expanded with 1) the path of the
-      directory, and 2) the content failure paths for that directory
-    noinfo_dir_msg : str
-      Message template to inject into the result for a requested directory that
-      `git annex` was silent about (incl. any content). There must be one string
-      placeholder that is expanded with the path of that directory.
-    noinfo_file_msg : str
-      Message to inject into the result for a requested file that `git
-      annex` was silent about.
-    **kwargs
-      Any further kwargs are included in the yielded result dictionary.
+
+    # Note: Didn't find a way yet, to force GitRepo as well as AnnexRepo to
+    # implement a method defined herein, since AnnexRepo inherits from GitRepo.
+    # Would be much nicer, but still - I'd prefer to have a central place for
+    # these anyway.
+
+    # Note 2: Seems possible. There is MRO magic:
+    # http://pybites.blogspot.de/2009/01/mro-magic.html
+    # http://stackoverflow.com/questions/20822850/change-python-mro-at-runtime
+
+    # Test!
+    pass
+
+
+def path_based_str_repr(cls):
+    """A helper decorator for a class to define str and repr based on its .path
+
+    For the rationale/discussion on why to bother distinguishing the two is
+    in https://github.com/datalad/datalad/pull/4439 . The idea  is that
+    `__str__` should provide cut/pasteable to shell representation of the path,
+    with all necessary escapes for characters shell might care about.
+    `__repr__` to provide string representation consumable in Python.
     """
-    for p in requested_paths:
-        # any relpath is relative to the currently processed dataset
-        # not the global reference dataset
-        p = p if isabs(p) else normpath(opj(ds.path, p))
-        if any(p in ps for ps in respath_by_status.values()):
-            # we have a report for this path already
-            continue
-        common_report = dict(path=p, **kwargs)
-        if isdir(p):
-            # `annex` itself will not report on directories, but if a
-            # directory was requested, we want to say something about
-            # it in the results.  we are inside a single, existing
-            # repo, hence all directories are already present, if not
-            # we had an error
-            # do we have any failures in a subdir of the requested dir?
-            failure_results = [
-                fp for fp in respath_by_status.get('failure', [])
-                if path_is_subpath(fp, p)]
-            if failure_results:
-                # we were not able to process all requested_paths, let's label
-                # this 'impossible' to get a warning-type report
-                # after all we have the directory itself, but not
-                # (some) of its requested_paths
-                yield get_status_dict(
-                    status='impossible', type='directory',
-                    message=(dir_fail_msg, p, failure_results),
-                    **common_report)
-            else:
-                # otherwise cool, but how cool?
-                success_results = [
-                    fp for fp in respath_by_status.get('success', [])
-                    if path_is_subpath(fp, p)]
-                yield get_status_dict(
-                    status='ok' if success_results else 'notneeded',
-                    message=None if success_results else (noinfo_dir_msg, p),
-                    type='directory', **common_report)
-            continue
-        else:
-            # not a directory, and we have had no word from `git annex`,
-            # yet no exception, hence the file was most probably
-            # already in the desired state
-            yield get_status_dict(
-                status='notneeded', type='file',
-                message=noinfo_file_msg,
-                **common_report)
+
+    # %s is used over .format since it is more performant. In Python 3.7.6 I get
+    # In [2]: %timeit "%s" % ("buga")
+    # 29 ns ± 0.179 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)
+    # In [3]: %timeit "{}".format("buga")
+    # 62 ns ± 0.345 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)
+    # and similarly 58ns vs 97ns for %r vs !r
+    def __str__(self):
+        s = self._str
+        if s is None:
+            s = self._str = \
+                '%s(%s)' % (self.__class__.__name__, ut.quote_cmdlinearg(self.path))
+        return s
+
+    def __repr__(self):
+        s = self._repr
+        if s is None:
+            s = self._repr = \
+                '%s(%r)' % (self.__class__.__name__, self.path)
+        return s
+
+    cls._str = None
+    cls.__str__ = __str__
+    cls._repr = None
+    cls.__repr__ = __repr__
+    return cls
+
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `datalad-0.9.3/datalad/interface/ls.py` & `datalad-1.0.0/datalad/cmd.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,668 +1,630 @@
-# emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil; coding: utf-8 -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""Helper utility to list things.  ATM lists datasets and S3 bucket URLs
 """
-__docformat__ = 'restructuredtext'
+Class the starts a subprocess and keeps it around to communicate with it
+via stdin. For each instruction send over stdin, a response is read and
+returned. The response structure is determined by "output_proc"
 
-import humanize
+"""
+from __future__ import annotations
+
+import logging
+import os
+import queue
 import sys
-import string
-import time
+import warnings
+from datetime import datetime
+from operator import attrgetter
+from queue import Queue
+from subprocess import TimeoutExpired
+from typing import (
+    Any,
+    Callable,
+    List,
+    Optional,
+    Tuple,
+    Union,
+)
+from weakref import (
+    ReferenceType,
+    WeakValueDictionary,
+    ref,
+)
+
+from datalad import cfg
+# start of legacy import block
+# to avoid breakage of code written before datalad.runner
+from datalad.runner.coreprotocols import (
+    KillOutput,
+    NoCapture,
+    StdErrCapture,
+    StdOutCapture,
+    StdOutErrCapture,
+)
+from datalad.runner.gitrunner import (
+    GIT_SSH_COMMAND,
+    GitRunnerBase,
+    GitWitlessRunner,
+)
+from datalad.runner.nonasyncrunner import (
+    STDERR_FILENO,
+    STDOUT_FILENO,
+    _ResultGenerator,
+    run_command,
+)
+from datalad.runner.protocol import (
+    GeneratorMixIn,
+    WitlessProtocol,
+)
+from datalad.runner.runner import WitlessRunner
+from datalad.runner.utils import LineSplitter
+from datalad.support.exceptions import CommandError
+from datalad.utils import (
+    auto_repr,
+    ensure_unicode,
+)
+
+# end of legacy import block
+
+
+
+__docformat__ = "restructuredtext"
+
+
+_cfg_var = "datalad.runtime.stalled-external"
+_cfg_val = cfg.obtain(_cfg_var)
+
+
+class BatchedCommandError(CommandError):
+    def __init__(self,
+                 cmd="",
+                 last_processed_request="",
+                 msg="",
+                 code=None,
+                 stdout="",
+                 stderr="",
+                 cwd=None,
+                 **kwargs):
+        """
+        This exception extends a CommandError that is raised by the command,
+        that is executed by `BatchedCommand`. It extends the `CommandError` by
+        `last_processed_request`. This attribute contains the last request, i.e.
+        argument to `BatchedCommand.__call__()`, that was successfully
+        processed, i.e. for which a result was received from the command (that
+        does not imply that the result was positive).
+
+        :param last_processed_request: the last request for which a response was
+            received from the underlying command. This could be used to restart
+            an interrupted process.
 
-from os.path import exists, lexists, join as opj, abspath, isabs
-from os.path import curdir, isfile, islink, isdir, realpath
-from os.path import relpath
-from os import lstat
-
-from six.moves.urllib.request import urlopen, Request
-from six.moves.urllib.error import HTTPError
-
-from ..utils import auto_repr
-from .base import Interface
-from datalad.interface.base import build_doc
-from ..ui import ui
-from ..utils import safe_print
-from ..dochelpers import exc_str
-from ..support.param import Parameter
-from ..support import ansi_colors
-from ..support.constraints import EnsureStr, EnsureNone
-from ..distribution.dataset import Dataset
-
-from datalad.support.annexrepo import AnnexRepo
-from datalad.support.annexrepo import GitRepo
-from datalad.utils import is_interactive
-
-from logging import getLogger
-lgr = getLogger('datalad.api.ls')
-
-
-@build_doc
-class Ls(Interface):
-    """List summary information about URLs and dataset(s)
+        For all other arguments see `CommandError`.
+        """
+        CommandError.__init__(
+            self,
+            cmd=cmd,
+            msg=msg,
+            code=code,
+            stdout=stdout,
+            stderr=stderr,
+            cwd=cwd,
+            **kwargs
+        )
+        self.last_processed_request = last_processed_request
+
+
+lgr = logging.getLogger('datalad.cmd')
+
+# TODO unused?
+# In python3 to split byte stream on newline, it must be bytes
+linesep_bytes = os.linesep.encode()
+
+# TODO unused?
+_TEMP_std = sys.stdout, sys.stderr
+
+# TODO unused?
+# To be used in the temp file name to distinguish the ones we create
+# in Runner so we take care about their removal, in contrast to those
+# which might be created outside and passed into Runner
+_MAGICAL_OUTPUT_MARKER = "_runneroutput_"
+
+
+def readline_rstripped(stdout):
+    warnings.warn("the function `readline_rstripped()` is deprecated "
+                  "and will be removed in a future release",
+                  DeprecationWarning)
+    return _readline_rstripped(stdout)
+
+
+def _readline_rstripped(stdout):
+    """Internal helper for BatchedCommand"""
+    return stdout.readline().rstrip()
+
+
+class BatchedCommandProtocol(GeneratorMixIn, StdOutErrCapture):
+    def __init__(self,
+                 batched_command: "BatchedCommand",
+                 done_future: Any = None,
+                 encoding: Optional[str] = None,
+                 output_proc: Optional[Callable] = None,
+                 ):
+        GeneratorMixIn.__init__(self)
+        StdOutErrCapture.__init__(self, done_future, encoding)
+        self.batched_command = batched_command
+        self.output_proc = output_proc
+        self.line_splitter = LineSplitter()
+
+    def pipe_data_received(self, fd: int, data: bytes):
+        if fd == STDERR_FILENO:
+            self.send_result((fd, data))
+        elif fd == STDOUT_FILENO:
+            for line in self.line_splitter.process(data.decode(self.encoding)):
+                self.send_result((fd, line))
+        else:
+            raise ValueError(f"unknown file descriptor: {fd}")
 
-    ATM only s3:// URLs and datasets are supported
+    def pipe_connection_lost(self, fd: int, exc: Optional[BaseException]):
+        if fd == STDOUT_FILENO:
+            remaining_line = self.line_splitter.finish_processing()
+            if remaining_line is not None:
+                lgr.debug("unterminated line: %s", remaining_line)
+                self.send_result((fd, remaining_line))
+
+    def timeout(self, fd: Optional[int]) -> bool:
+        timeout_error = self.batched_command.get_timeout_exception(fd)
+        if timeout_error:
+            raise timeout_error
+        self.send_result(("timeout", fd))
+        return False
 
-    Examples:
 
-      $ datalad ls s3://openfmri/tarballs/ds202  # to list S3 bucket
-      $ datalad ls                               # to list current dataset
+class ReadlineEmulator:
     """
-    # XXX prevent common args from being added to the docstring
-    _no_eval_results = True
+    This class implements readline() on the basis of an instance of
+    BatchedCommand. Its purpose is to emulate stdout's for output_procs,
+    This allows us to provide a BatchedCommand API that is identical
+    to the old version, but with an implementation that is based on the
+    threaded runner.
+    """
+    def __init__(self,
+                 batched_command: "BatchedCommand"):
+        self.batched_command = batched_command
 
-    # TODO: during big RF refactor this one away since it must not be instance's
-    # attribute.  For now introduced to make `datalad ls` be relatively usable
-    # in terms of speed
-    _cached_subdatasets = {}
-
-    _params_ = dict(
-        loc=Parameter(
-            doc="URL or path to list, e.g. s3://...",
-            metavar='PATH/URL',
-            nargs="*",
-            constraints=EnsureStr() | EnsureNone(),
-        ),
-        recursive=Parameter(
-            args=("-r", "--recursive"),
-            action="store_true",
-            doc="recurse into subdirectories",
-        ),
-        fast=Parameter(
-            args=("-F", "--fast"),
-            action="store_true",
-            doc="only perform fast operations.  Would be overridden by --all",
-        ),
-        all_=Parameter(
-            args=("-a", "--all"),
-            dest='all_',
-            action="store_true",
-            doc="list all (versions of) entries, not e.g. only latest entries "
-                "in case of S3",
-        ),
-        long_=Parameter(
-            args=("-L", "--long"),
-            dest='long_',
-            action="store_true",
-            doc="list more information on entries (e.g. acl, urls in s3, annex "
-                "sizes etc)",
-        ),
-        config_file=Parameter(
-            doc="""path to config file which could help the 'ls'.  E.g. for s3://
-            URLs could be some ~/.s3cfg file which would provide credentials""",
-            constraints=EnsureStr() | EnsureNone()
-        ),
-        list_content=Parameter(
-            choices=(None, 'first10', 'md5', 'full'),
-            doc="""list also the content or only first 10 bytes (first10), or md5
-            checksum of an entry.  Might require expensive transfer and dump
-            binary output to your screen.  Do not enable unless you know what you
-            are after""",
-            default=None
-        ),
-        json=Parameter(
-            choices=('file', 'display', 'delete'),
-            doc="""metadata json of dataset for creating web user interface.
-            display: prints jsons to stdout or
-            file: writes each subdir metadata to json file in subdir of dataset or
-            delete: deletes all metadata json files in dataset""",
-        ),
-    )
-
-    @staticmethod
-    def __call__(loc, recursive=False, fast=False, all_=False, long_=False,
-                 config_file=None, list_content=False, json=None):
-        if json:
-            from datalad.interface.ls_webui import _ls_json
-
-        if isinstance(loc, list) and not len(loc):
-            # nothing given, CWD assumed -- just like regular ls
-            loc = '.'
-
-        kw = dict(fast=fast, recursive=recursive, all_=all_, long_=long_)
-        if isinstance(loc, list):
-            return [Ls.__call__(loc_, config_file=config_file,
-                                list_content=list_content, json=json, **kw)
-                    for loc_ in loc]
-
-        # TODO: do some clever handling of kwargs as to remember what were defaults
-        # and what any particular implementation actually needs, and then issuing
-        # warning if some custom value/option was specified which doesn't apply to the
-        # given url
-
-        # rename to not angry Python gods who took all_ good words
-        kw['long_'] = kw.pop('long_')
-
-        loc_type = "unknown"
-        if loc.startswith('s3://'):
-            return _ls_s3(loc, config_file=config_file, list_content=list_content,
-                          **kw)
-        elif lexists(loc):
-            if isdir(loc):
-                ds = Dataset(loc)
-                if ds.is_installed():
-                    return _ls_json(loc, json=json, **kw) if json else _ls_dataset(loc, **kw)
-                    loc_type = False
-                else:
-                    loc_type = "dir"  # we know that so far for sure
-                    # it might have been an uninstalled dataset within super-dataset
-                    superds = ds.get_superdataset()
-                    if superds:
-                        try:
-                            subdatasets = Ls._cached_subdatasets[superds.path]
-                        except KeyError:
-                            subdatasets = Ls._cached_subdatasets[superds.path] \
-                                = superds.subdatasets(result_xfm='relpaths')
-                        if relpath(ds.path, superds.path) in subdatasets:
-                            loc_type = "not installed"
-            else:
-                loc_type = "file"
-                # could list properties -- under annex or git, either clean/dirty
-                # etc
-                # repo = get_repo_instance(dirname(loc))
-
-        if loc_type:
-            #raise ValueError("ATM supporting only s3:// URLs and paths to local datasets")
-            # TODO: unify all_ the output here -- _ls functions should just return something
-            # to be displayed
-            ui.message(
-                "{}  {}".format(
-                    ansi_colors.color_word(loc, ansi_colors.DATASET),
-                    ansi_colors.color_word(
-                        loc_type,
-                        ansi_colors.RED
-                        if loc_type in {'unknown', 'not installed'}
-                        else ansi_colors.BLUE)
-                )
-            )
+    def readline(self):
+        """
+        Read from the stdout provider until we have a line or None (which
+        indicates some error).
+        """
+        return self.batched_command.get_one_line()
 
 
-#
-# Dataset listing
-#
+class SafeDelCloseMixin(object):
+    """A helper class to use where __del__ would call .close() which might
+    fail if "too late in GC game"
+    """
+    def __del__(self):
+        try:
+            self.close()
+        except (TypeError, ImportError):
+            # ImportError could be raised when the interpreter is shutting down.
+            if os.fdopen is None or lgr.debug is None:
+                # if we are late in the game and things already gc'ed in py3,
+                # it is Ok
+                return
+            raise
+
 
 @auto_repr
-class AbsentRepoModel(object):
-    """Just a base for those where repo wasn't installed yet"""
+class BatchedCommand(SafeDelCloseMixin):
+    """
+    Container for a running subprocess. Supports communication with the
+    subprocess via stdin and stdout.
+    """
 
-    def __init__(self, path):
-        self.path = path
-        self.repo = None
-
-    @property
-    def type(self):
-        return "N/A"
+    # Collection of active BatchedCommands as a mapping from object IDs to
+    # instances
+    _active_instances: WeakValueDictionary[int, BatchedCommand] = WeakValueDictionary()
+
+    def __init__(self,
+                 cmd: Union[str, Tuple, List],
+                 path: Optional[str] = None,
+                 output_proc: Optional[Callable] = None,
+                 timeout: Optional[float] = None,
+                 exception_on_timeout: bool = False,
+                 ):
+
+        command = cmd
+        self.command: list = [command] if not isinstance(command, List) else command
+        self.path: Optional[str] = path
+        self.output_proc: Optional[Callable] = output_proc
+        self.timeout: Optional[float] = timeout
+        self.exception_on_timeout: bool = exception_on_timeout
+
+        self.stderr_output = b""
+        self.runner: Optional[WitlessRunner] = None
+        self.encoding = None
+        self.wait_timed_out = None
+        self.return_code: Optional[int] = None
+        self._abandon_cache = None
+        self.last_request: Optional[str] = None
+
+        self._active = 0
+        self._active_last = _now()
+        self.clean_inactive()
+        assert id(self) not in self._active_instances
+        self._active_instances[id(self)] = self
+
+        # pure declarations
+        self.stdin_queue: Queue
+        self.generator: _ResultGenerator
+
+    @classmethod
+    def clean_inactive(cls):
+        from . import cfg
+        max_batched = cfg.obtain("datalad.runtime.max-batched")
+        max_inactive_age = cfg.obtain("datalad.runtime.max-inactive-age")
+        if len(cls._active_instances) > max_batched:
+            active_qty = 0
+            inactive = []
+            for c in cls._active_instances.values():
+                if c._active:
+                    active_qty += 1
+                else:
+                    inactive.append(c)
+            inactive.sort(key=attrgetter("_active_last"))
+            to_close = len(cls._active_instances) - max_batched
+            if to_close <= 0:
+                return
+            too_young = 0
+            now = _now()
+            for i, c in enumerate(inactive):
+                if (now - c._active_last).total_seconds() <= max_inactive_age:
+                    too_young = len(inactive) - i
+                    break
+                elif c._active:
+                    active_qty += 1
+                else:
+                    c.close()
+                    cls._active_instances.pop(id(c), None)
+                    to_close -= 1
+                    if to_close <= 0:
+                        break
+            if to_close > 0:
+                lgr.debug(
+                    "Too many BatchedCommands remaining after cleanup;"
+                    " %d active, %d went inactive recently",
+                    active_qty,
+                    too_young,
+                )
 
+    def _initialize(self):
 
-@auto_repr
-class GitModel(object):
-    """A base class for models which have some .repo available"""
+        lgr.debug("Starting new runner for %s", repr(self))
+        lgr.log(5, "Command: %s", self.command)
 
-    __slots__ = ['_branch', 'repo', '_path']
+        self.stdin_queue = queue.Queue()
+        self.stderr_output = b""
+        self.wait_timed_out = None
+        self.return_code = None
+        self.last_request = None
+
+        self.runner = WitlessRunner(
+            cwd=self.path,
+            env=GitRunnerBase.get_git_environ_adjusted()
+        )
+        self.generator = self.runner.run(
+            cmd=self.command,
+            protocol=BatchedCommandProtocol,
+            stdin=self.stdin_queue,
+            cwd=self.path,
+            # This mimics the behavior of the old implementation w.r.t
+            # timeouts when waiting for the closing process
+            timeout=self.timeout or 11.0,
+            # Keyword arguments for the protocol
+            batched_command=self,
+            output_proc=self.output_proc,
+        )
+        self.encoding = self.generator.runner.protocol.encoding
+
+        self._active_last = _now()
+
+    def process_running(self) -> bool:
+        if self.runner:
+            if self.generator.runner.process is None:
+                return False
+            result = self.generator.runner.process.poll()
+            if result is None:
+                return True
+            self.return_code = result
+            self.runner = None
+            if result != 0:
+                raise BatchedCommandError(
+                    cmd=" ".join(self.command),
+                    last_processed_request=self.last_request,
+                    msg=f"{type(self).__name__}: exited with {result} after "
+                        f"request: {self.last_request}",
+                    code=result
+                ) from CommandError
+        return False
 
-    def __init__(self, repo):
-        self.repo = repo
-        # lazy evaluation variables
-        self._branch = None
-        self._path = None
-
-    @property
-    def path(self):
-        return self.repo.path if self._path is None else self._path
-
-    @path.setter
-    def path(self, v):
-        self._path = v
-
-    @property
-    def branch(self):
-        if self._branch is None:
-            try:
-                self._branch = self.repo.get_active_branch()
-            except:  # MIH: InvalidGitRepositoryError?
-                return None
-        return self._branch
-
-    @property
-    def clean(self):
-        return not self.repo.dirty
-
-    @property
-    def describe(self):
-        return self.repo.describe(tags=True)
-
-    @property
-    def date(self):
-        """Date of the last commit
-        """
-        return self.repo.get_commit_date()
-
-    @property
-    def count_objects(self):
-        return self.repo.count_objects
-
-    @property
-    def git_local_size(self):
-        count_objects = self.count_objects
-        return count_objects['size'] if count_objects else None
-
-    @property
-    def type(self):
-        return {False: 'git', True: 'annex'}[isinstance(self.repo, AnnexRepo)]
+    def __call__(self,
+                 cmds: Union[str, Tuple, List]):
+        """
+        Send requests to the subprocess and return the responses. We expect one
+        response per request. How the response is structured is determined by
+        output_proc. If output_proc returns not-None, the responses is
+        considered to be a response.
+
+        If output_proc is not provided, we assume that a single response is
+        a single line.
+
+        If the subprocess does not exist yet it is started before the first
+        command is sent.
+
+        Parameters
+        ----------
+        cmds : str or tuple or list of (str or tuple)
+            request for the subprocess
+
+        Returns
+        -------
+        (return_type[self.output_proc] | str)
+        | list[(return_type[self.output_proc] | str)]
+
+            Responses received from process. Either a single element, or a list
+            of elements, if `cmds` was a list.
+            The type of the elements is `str`, if `self.output_proc` is `None`.
+            If `self.output_proc` is not `None`, the result type of
+            `self.output_proc` determines the type of the elements.
+        """
+        self._active += 1
+        requests = cmds
 
+        input_multiple = isinstance(requests, list)
+        if not input_multiple:
+            requests = [requests]
 
-@auto_repr
-class AnnexModel(GitModel):
+        responses = []
+        try:
+            # This code assumes that each processing request is
+            # a single line and leads to a response that triggers a
+            # `send_result` in the protocol.
+            for request in requests:
+                while True:
+                    try:
+                        responses.append(self.process_request(request))
+                        self.last_request = request
+                        break
+                    except StopIteration:
+                        # The process finished executing, store the last return
+                        # code and restart the process.
+                        lgr.debug("%s: command exited", self)
+                        self.return_code = self.generator.return_code
+                        self.runner = None
+
+        except CommandError as command_error:
+            # Convert CommandError into BatchedCommandError
+            self.runner = None
+            self.return_code = command_error.code
+            raise BatchedCommandError(
+                cmd=command_error.cmd,
+                last_processed_request=self.last_request,
+                msg=command_error.msg,
+                code=command_error.code,
+                stdout=command_error.stdout,
+                stderr=command_error.stderr,
+                cwd=command_error.cwd,
+                **command_error.kwargs
+            ) from command_error
+
+        finally:
+            self._active -= 1
+        return responses if input_multiple else responses[0] if responses else None
 
-    __slots__ = ['_info'] + GitModel.__slots__
+    def process_request(self,
+                        request: Union[Tuple, str]) -> Any | None:
 
-    def __init__(self, *args, **kwargs):
-        super(AnnexModel, self).__init__(*args, **kwargs)
-        self._info = None
-
-    @property
-    def info(self):
-        if self._info is None and self.type == 'annex':
-            self._info = self.repo.repo_info()
-        return self._info
-
-    @property
-    def annex_worktree_size(self):
-        info = self.info
-        return info['size of annexed files in working tree'] if info else 0.0
-
-    @property
-    def annex_local_size(self):
-        info = self.info
-        return info['local annex size'] if info else 0.0
+        self._active += 1
+        try:
 
+            if not self.process_running():
+                self._initialize()
 
-@auto_repr
-class FsModel(AnnexModel):
+            # Remember request and send it to subprocess
+            if not isinstance(request, str):
+                request = ' '.join(request)
+            self.stdin_queue.put((request + "\n").encode())
+
+            # Get the response from the generator. We only consider
+            # data received on stdout as a response.
+            if self.output_proc:
+                # If we have an output procedure, let the output procedure
+                # read stdout and decide about the nature of the response
+                response = self.output_proc(ReadlineEmulator(self))
+            else:
+                # If there is no output procedure we assume that a response
+                # is one line.
+                response = self.get_one_line()
+                if response is not None:
+                    response = response.rstrip()
+            return response
 
-    __slots__ = AnnexModel.__slots__
+        finally:
+            self._active -= 1
 
-    def __init__(self, path, *args, **kwargs):
-        super(FsModel, self).__init__(*args, **kwargs)
-        self._path = path
-
-    @property
-    def path(self):
-        return self._path
-
-    @property
-    def symlink(self):
-        """if symlink returns path the symlink points to else returns None"""
-        if islink(self._path):                    # if symlink
-            target_path = realpath(self._path)    # find link target
-            # convert to absolute path if not
-            return target_path if exists(target_path) else None
-        return None
+    def proc1(self,
+              single_command: str):
+        """
+        Simulate the old interface. This method is used only once in
+        AnnexRepo.get_metadata()
+        """
+        self._active += 1
+        try:
+            assert isinstance(single_command, str)
+            return self(single_command)
+        finally:
+            self._active -= 1
 
-    @property
-    def date(self):
-        """Date of last modification"""
-        if self.type_ is not ['git', 'annex']:
-            return lstat(self._path).st_mtime
-        else:
-            return super(self.__class__, self).date
+    def get_one_line(self) -> Optional[str]:
+        """
+        Get a single stdout line from the generator.
 
-    @property
-    def size(self):
-        """Size of the node computed based on its type"""
-        type_ = self.type_
-        sizes = {'total': 0.0,
-                 'ondisk': 0.0,
-                 'git': 0.0,
-                 'annex': 0.0,
-                 'annex_worktree': 0.0}
-
-        if type_ in ['file', 'link', 'link-broken']:
-            # if node is under annex, ask annex for node size, ondisk_size
-            if isinstance(self.repo, AnnexRepo) and self.repo.is_under_annex(self._path):
-                size = self.repo.info(self._path, batch=True)['size']
-                ondisk_size = size \
-                    if self.repo.file_has_content(self._path) \
-                    else 0
-            # else ask fs for node size (= ondisk_size)
+        If timeout was specified, and exception_on_timeout is False,
+        and if a timeout occurs, return None. Otherwise, return the
+        string that was read from the generator.
+        """
+
+        # Implementation remarks:
+        # 1. We know that BatchedCommandProtocol only returns complete lines on
+        #    stdout, that makes this code simple.
+        # 2. stderr is handled transparently within this method,
+        #    by adding all stderr-content to an internal buffer.
+        while True:
+            source, data = self.generator.send(None)
+            if source == STDERR_FILENO:
+                self.stderr_output += data
+            elif source == STDOUT_FILENO:
+                return data
+            elif source == "timeout":
+                # TODO: we should restart the subprocess on timeout, otherwise
+                #  we might end up with results from a previous instruction,
+                #  when handling multiple instructions at once. Until this is
+                #  done properly, communication timeouts are ignored in order
+                #  to avoid errors.
+                if data is None:
+                    lgr.debug('BatchedCommand: timeout on process')
+                else:
+                    lgr.debug('BatchedCommand: timeout on file descriptor %d', data)
             else:
-                size = ondisk_size = 0 \
-                    if type_ == 'link-broken' \
-                    else lstat(self.symlink or self._path).st_size
-
-            sizes.update({'total': size, 'ondisk': ondisk_size})
-
-        if self.repo.path == self._path:
-            sizes.update({'git': self.git_local_size,
-                          'annex': self.annex_local_size,
-                          'annex_worktree': self.annex_worktree_size})
-        return sizes
-
-    @property
-    def type_(self):
-        """outputs the node type
-
-        Types: link, link-broken, file, dir, annex-repo, git-repo"""
-        if islink(self.path):
-            return 'link' if self.symlink else 'link-broken'
-        elif isfile(self.path):
-            return 'file'
-        elif exists(opj(self.path, ".git", "annex")):
-            return 'annex'
-        elif exists(opj(self.path, ".git")):
-            return 'git'
-        elif isdir(self.path):
-            return 'dir'
-        else:
-            return None
+                raise ValueError(f"{self}: unknown source: {source}")
 
+    def close(self, return_stderr=False):
+        """
+        Close communication and wait for process to terminate. If the "timeout"
+        parameter to the constructor was not None, and if the configuration
+        setting "datalad.runtime.stalled-external" is set to "abandon",
+        the method will return latest after "timeout" seconds. If the subprocess
+        did not exit within this time, the attribute "wait_timed_out" will
+        be set to "True".
+
+        Parameters
+        ----------
+        return_stderr: bool
+          if set to "True", the call will return all collected stderr content
+          as string. In addition, if return_stderr is True and the log level
+          is 5 or lower, and the configuration setting "datalad.log.outputs"
+          evaluates to "True", the content of stderr will be logged.
+
+        Returns
+        -------
+        str, optional
+          stderr output if return_stderr is True, None otherwise
+        """
 
-class LsFormatter(string.Formatter):
-    # condition by interactive
-    if is_interactive():
-        BLUE = ansi_colors.COLOR_SEQ % ansi_colors.BLUE
-        RED = ansi_colors.COLOR_SEQ % ansi_colors.RED
-        GREEN = ansi_colors.COLOR_SEQ % ansi_colors.GREEN
-        RESET = ansi_colors.RESET_SEQ
-        DATASET = ansi_colors.COLOR_SEQ % ansi_colors.UNDERLINE
-    else:
-        BLUE = RED = GREEN = RESET = DATASET = u""
-
-    # TODO: we might want to just ignore and force utf8 while explicitly .encode()'ing output!
-    # unicode versions which look better but which blow during tests etc
-    # Those might be reset by the constructor
-    OK = 'OK'   # u"✓"
-    NOK = 'X'  # u"✗"
-    NONE = '-'  # u"✗"
-
-    def __init__(self, *args, **kwargs):
-        super(LsFormatter, self).__init__(*args, **kwargs)
-        if sys.stdout.encoding is None:
-            lgr.debug("encoding not set, using safe alternatives")
-        elif not sys.stdout.isatty():
-            lgr.debug("stdout is not a tty, using safe alternatives")
-        else:
+        if self.runner:
+
+            abandon = self._get_abandon()
+
+            # Close stdin to let the process know that we want to end
+            # communication. We also close stdout and stderr to inform
+            # the generator that we do not care about them anymore. This
+            # will trigger process wait timeouts.
+            self.generator.runner.close_stdin()
+
+            # Process all remaining messages until the subprocess exits.
+            remaining = []
+            timeout = False
             try:
-                u"✓".encode(sys.stdout.encoding)
-            except UnicodeEncodeError:
-                lgr.debug("encoding %s does not support unicode, "
-                          "using safe alternatives",
-                          sys.stdout.encoding)
-            else:
-                self.OK = u"✓"
-                self.NOK = u"✗"
-                self.NONE = u"✗"
-
-    def convert_field(self, value, conversion):
-        #print("%r->%r" % (value, conversion))
-        if conversion == 'D':  # Date
-            if value is not None:
-                return time.strftime(u"%Y-%m-%d/%H:%M:%S", time.localtime(value))
-            else:
-                return u'-'
-        elif conversion == 'S':  # Human size
-            #return value
-            if value is not None:
-                return humanize.naturalsize(value)
-            else:
-                return u'-'
-        elif conversion == 'X':  # colored bool
-            chr, col = (self.OK, self.GREEN) if value else (self.NOK, self.RED)
-            return u"%s%s%s" % (col, chr, self.RESET)
-        elif conversion == 'N':  # colored Red - if None
-            if value is None:
-                # return "%s✖%s" % (self.RED, self.RESET)
-                return u"%s%s%s" % (self.RED, self.NONE, self.RESET)
-            return value
-        elif conversion in {'B', 'R', 'U'}:
-            return u"%s%s%s" % ({'B': self.BLUE, 'R': self.RED, 'U': self.DATASET}[conversion], value, self.RESET)
-
-        return super(LsFormatter, self).convert_field(value, conversion)
-
-    def format_field(self, value, format_spec):
-        # TODO: move all the "coloring" into formatting, so we could correctly indent
-        # given the format and only then color it up
-        # print "> %r, %r" % (value, format_spec)
-        return super(LsFormatter, self).format_field(value, format_spec)
-
-
-def format_ds_model(formatter, ds_model, format_str, format_exc):
-    try:
-        #print("WORKING ON %s" % ds_model.path)
-        if not exists(ds_model.path) or not ds_model.repo:
-            return formatter.format(format_exc, ds=ds_model, msg=u"not installed")
-        ds_formatted = formatter.format(format_str, ds=ds_model)
-        #print("FINISHED ON %s" % ds_model.path)
-        return ds_formatted
-    except Exception as exc:
-        return formatter.format(format_exc, ds=ds_model, msg=exc_str(exc))
-
-# from joblib import Parallel, delayed
-
-
-def _ls_dataset(loc, fast=False, recursive=False, all_=False, long_=False):
-    isabs_loc = isabs(loc)
-    topdir = '' if isabs_loc else abspath(curdir)
-
-    topds = Dataset(loc)
-    dss = [topds] + (
-        [Dataset(opj(loc, sm))
-         for sm in topds.subdatasets(recursive=recursive, result_xfm='relpaths')]
-        if recursive else [])
-
-    dsms = []
-    for ds in dss:
-        if not ds.is_installed():
-            dsm = AbsentRepoModel(ds.path)
-        elif isinstance(ds.repo, AnnexRepo):
-            dsm = AnnexModel(ds.repo)
-        elif isinstance(ds.repo, GitRepo):
-            dsm = GitModel(ds.repo)
-        else:
-            raise RuntimeError("Got some dataset which don't know how to handle %s"
-                               % ds)
-        dsms.append(dsm)
-
-    # adjust path strings
-    for ds_model in dsms:
-        #path = ds_model.path[len(topdir) + 1 if topdir else 0:]
-        path = relpath(ds_model.path, topdir) if topdir else ds_model.path
-        if not path:
-            path = '.'
-        ds_model.path = path
-    dsms = sorted(dsms, key=lambda m: m.path)
-
-    maxpath = max(len(ds_model.path) for ds_model in dsms)
-    path_fmt = u"{ds.path!U:<%d}" % (maxpath + (11 if is_interactive() else 0))  # + to accommodate ansi codes
-    pathtype_fmt = path_fmt + u"  [{ds.type}]"
-    full_fmt = pathtype_fmt + u"  {ds.branch!N}  {ds.describe!N} {ds.date!D}"
-    if (not fast) or long_:
-        full_fmt += u"  {ds.clean!X}"
-
-    fmts = {
-        AbsentRepoModel: pathtype_fmt,
-        GitModel: full_fmt,
-        AnnexModel: full_fmt
-    }
-    if long_:
-        fmts[AnnexModel] += u"  {ds.annex_local_size!S}/{ds.annex_worktree_size!S}"
-
-    formatter = LsFormatter()
-    # weird problems happen in the parallel run -- TODO - figure it out
-    # for out in Parallel(n_jobs=1)(
-    #         delayed(format_ds_model)(formatter, dsm, full_fmt, format_exc=path_fmt + "  {msg!R}")
-    #         for dsm in dss):
-    #     print(out)
-    for dsm in dsms:
-        fmt = fmts[dsm.__class__]
-        ds_str = format_ds_model(formatter, dsm, fmt, format_exc=path_fmt + u"  {msg!R}")
-        safe_print(ds_str)
-        # workaround for explosion of git cat-file --batch processes
-        # https://github.com/datalad/datalad/issues/1888
-        if dsm.repo is not None:
-            dsm.repo.repo.close()
-            del dsm.repo
-            dsm.repo = None
+                for source, data in self.generator:
+                    if source == STDERR_FILENO:
+                        self.stderr_output += data
+                    elif source == STDOUT_FILENO:
+                        remaining.append(data)
+                    elif source == "timeout":
+                        if data is None and abandon is True:
+                            timeout = True
+                            break
+                    else:
+                        raise ValueError(f"{self}: unknown source: {source}")
+                self.return_code = self.generator.return_code
 
+            except CommandError as command_error:
+                lgr.error(
+                    "%s subprocess exited with %s (%s)",
+                    self,
+                    repr(command_error.code),
+                    command_error
+                )
+                self.return_code = command_error.code
 
-#
-# S3 listing
-#
-def _ls_s3(loc, fast=False, recursive=False, all_=False, long_=False,
-           config_file=None, list_content=False):
-    """List S3 bucket content"""
-    if loc.startswith('s3://'):
-        bucket_prefix = loc[5:]
-    else:
-        raise ValueError("passed location should be an s3:// url")
-
-    import boto
-    from hashlib import md5
-    from boto.s3.key import Key
-    from boto.s3.prefix import Prefix
-    from boto.exception import S3ResponseError
-    from ..support.configparserinc import SafeConfigParser  # provides PY2,3 imports
-
-    if '/' in bucket_prefix:
-        bucket_name, prefix = bucket_prefix.split('/', 1)
-    else:
-        bucket_name, prefix = bucket_prefix, None
-
-    if prefix and '?' in prefix:
-        ui.message("We do not care about URL options ATM, they get stripped")
-        prefix = prefix[:prefix.index('?')]
-
-    ui.message("Connecting to bucket: %s" % bucket_name)
-    if config_file:
-        config = SafeConfigParser()
-        config.read(config_file)
-        access_key = config.get('default', 'access_key')
-        secret_key = config.get('default', 'secret_key')
+            if remaining:
+                lgr.debug("%s: remaining content: %s", self, remaining)
 
-        # TODO: remove duplication -- reuse logic within downloaders/s3.py to get connected
-        conn = boto.connect_s3(access_key, secret_key)
-        try:
-            bucket = conn.get_bucket(bucket_name)
-        except S3ResponseError as e:
-            ui.message("E: Cannot access bucket %s by name" % bucket_name)
-            all_buckets = conn.get_all_buckets()
-            all_bucket_names = [b.name for b in all_buckets]
-            ui.message("I: Found following buckets %s" % ', '.join(all_bucket_names))
-            if bucket_name in all_bucket_names:
-                bucket = all_buckets[all_bucket_names.index(bucket_name)]
-            else:
-                raise RuntimeError("E: no bucket named %s thus exiting" % bucket_name)
-    else:
-        # TODO: expose credentials
-        # We don't need any provider here really but only credentials
-        from datalad.downloaders.providers import Providers
-        providers = Providers.from_config_files()
-        provider = providers.get_provider(loc)
-
-        if not provider:
-            raise ValueError(
-                "Don't know how to deal with this url %s -- no provider defined for %s. "
-                "Define a new provider (DOCS: TODO) or specify just s3cmd config file instead for now."
-                % loc
-            )
-        downloader = provider.get_downloader(loc)
-
-        # should authenticate etc, and when ready we will ask for a bucket ;)
-        bucket = downloader.access(lambda url: downloader.bucket, loc)
-
-    info = []
-    for iname, imeth in [
-        ("Versioning", bucket.get_versioning_status),
-        ("   Website", bucket.get_website_endpoint),
-        ("       ACL", bucket.get_acl),
-    ]:
-        try:
-            ival = imeth()
-        except Exception as e:
-            ival = str(e).split('\n')[0]
-        info.append(" {iname}: {ival}".format(**locals()))
-    ui.message("Bucket info:\n %s" % '\n '.join(info))
-
-    kwargs = {} if recursive else {'delimiter': '/'}
-
-    ACCESS_METHODS = [
-        bucket.list_versions,
-        bucket.list
-    ]
+            self.wait_timed_out = timeout is True
+            if self.wait_timed_out:
+                lgr.debug(
+                    "%s: timeout while waiting for subprocess to exit", self)
+                lgr.warning(
+                    "Batched process (%s) "
+                    "did not finish, abandoning it without killing it",
+                    self.generator.runner.process.pid,
+                )
 
-    prefix_all_versions = None
-    for acc in ACCESS_METHODS:
-        try:
-            prefix_all_versions = list(acc(prefix, **kwargs))
-            break
-        except Exception as exc:
-            lgr.debug("Failed to access via %s: %s", acc, exc_str(exc))
-
-    if not prefix_all_versions:
-        ui.error("No output was provided for prefix %r" % prefix)
-    else:
-        max_length = max((len(e.name) for e in prefix_all_versions))
-        max_size_length = max((len(str(getattr(e, 'size', 0))) for e in prefix_all_versions))
-
-    results = []
-    for e in prefix_all_versions:
-        results.append(e)
-        if isinstance(e, Prefix):
-            ui.message("%s" % (e.name, ),)
-            continue
-
-        base_msg = ("%%-%ds %%s" % max_length) % (e.name, e.last_modified)
-        if isinstance(e, Key):
-            if not (e.is_latest or all_):
-                # Skip this one
-                continue
-            ui.message(base_msg + " %%%dd" % max_size_length % e.size, cr=' ')
-            # OPT: delayed import
-            from ..support.s3 import get_key_url
-            url = get_key_url(e, schema='http')
-            try:
-                _ = urlopen(Request(url))
-                urlok = "OK"
-            except HTTPError as err:
-                urlok = "E: %s" % err.code
+        result = self.get_requested_error_output(return_stderr)
+        self.runner = None
+        self.stderr_output = b""
+        return result
 
-            try:
-                acl = e.get_acl()
-            except S3ResponseError as exc:
-                acl = exc.code if exc.code in ('AccessDenied',) else str(exc)
-
-            content = ""
-            if list_content:
-                # IO intensive, make an option finally!
-                try:
-                    # _ = e.next()[:5]  if we are able to fetch the content
-                    kwargs = dict(version_id=e.version_id)
-                    if list_content in {'full', 'first10'}:
-                        if list_content in 'first10':
-                            kwargs['headers'] = {'Range': 'bytes=0-9'}
-                        content = repr(e.get_contents_as_string(**kwargs))
-                    elif list_content == 'md5':
-                        digest = md5()
-                        digest.update(e.get_contents_as_string(**kwargs))
-                        content = digest.hexdigest()
-                    else:
-                        raise ValueError(list_content)
-                    # content = "[S3: OK]"
-                except S3ResponseError as err:
-                    content = str(err)
-                finally:
-                    content = " " + content
-            ui.message(
-                "ver:%-32s  acl:%s  %s [%s]%s"
-                % (getattr(e, 'version_id', None),
-                   acl, url, urlok, content)
-                if long_ else ''
-            )
-        else:
-            ui.message(base_msg + " " + str(type(e)).split('.')[-1].rstrip("\"'>"))
-    return results
+    def get_requested_error_output(self, return_stderr: bool):
+        if not self.runner:
+            return None
+
+        stderr_content = ensure_unicode(self.stderr_output)
+        if lgr.isEnabledFor(5):
+            from . import cfg
+            if cfg.getbool("datalad.log", "outputs", default=False):
+                stderr_lines = stderr_content.splitlines()
+                lgr.log(
+                    5,
+                    "stderr of %s had %d lines:",
+                    self.generator.runner.process.pid if self.generator.runner.process else 'terminated',
+                    len(stderr_lines))
+                for line in stderr_lines:
+                    lgr.log(5, "| " + line)
+        if return_stderr:
+            return stderr_content
+        return None
+
+    def get_timeout_exception(self,
+                              fd: Optional[int]
+                              ) -> Optional[TimeoutExpired]:
+        """
+        Get a process timeout exception if timeout exceptions should
+        be generated for a process that continues longer than timeout
+        seconds after self.close() was initiated.
+        """
+        if self.timeout is None \
+                or fd is not None \
+                or self.exception_on_timeout is False\
+                or self._get_abandon() == "wait":
+            return None
+        return TimeoutExpired(
+            cmd=self.command,
+            timeout=self.timeout or 11.0,
+            stderr=self.stderr_output)
+
+    def _get_abandon(self):
+        if self._abandon_cache is None:
+            if _cfg_val not in ("wait", "abandon"):
+                raise ValueError(f"Unexpected value: {_cfg_var}={_cfg_val!r}")
+            self._abandon_cache = _cfg_val == "abandon"
+        return self._abandon_cache
+
+
+def _now():
+    return datetime.now().astimezone()
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `datalad-0.9.3/datalad/interface/__init__.py` & `datalad-1.0.0/datalad/interface/__init__.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 """High-level interface definition
@@ -13,72 +13,78 @@
 __docformat__ = 'restructuredtext'
 
 # ORDER MATTERS FOLKS!
 
 # the following should be series of import definitions for interface implementations
 # that shall be exposed in the Python API and the cmdline interface
 # all interfaces should be associated with (at least) one of the groups below
-_group_dataset = (
-    'Commands for dataset operations',
+# the name of the `_group_*` variable determines the sorting in the command overview
+# alphanum ascending order
+_group_0dataset = (
+    'Essential',
     [
         # source module, source object[, dest. cmdline name[, dest python name]]
         # src module can be relative, but has to be relative to the main 'datalad' package
-        ('datalad.distribution.create', 'Create'),
-        ('datalad.distribution.install', 'Install'),
+        ('datalad.core.local.create', 'Create'),
+        ('datalad.core.local.save', 'Save', 'save'),
+        ('datalad.core.local.status', 'Status', 'status'),
+        ('datalad.core.distributed.clone', 'Clone'),
         ('datalad.distribution.get', 'Get'),
-        ('datalad.distribution.add', 'Add'),
-        ('datalad.distribution.publish', 'Publish'),
-        ('datalad.distribution.uninstall', 'Uninstall', 'uninstall', 'uninstall'),
-        ('datalad.distribution.drop', 'Drop', 'drop', 'drop'),
-        ('datalad.distribution.remove', 'Remove', 'remove', 'remove'),
-        # N/I ATM
-        # ('datalad.distribution.move', 'Move'),
+        ('datalad.core.distributed.push', 'Push', 'push'),
+        ('datalad.core.local.run', 'Run', 'run'),
+        ('datalad.core.local.diff', 'Diff', 'diff'),
+    ])
+
+_group_1siblings = (
+    'Collaborative workflows',
+    [
+        ('datalad.distributed.create_sibling_github', 'CreateSiblingGithub'),
+        ('datalad.distributed.create_sibling_gitlab', 'CreateSiblingGitlab'),
+        ('datalad.distributed.create_sibling_gogs', 'CreateSiblingGogs'),
+        ('datalad.distributed.create_sibling_gin', 'CreateSiblingGin'),
+        ('datalad.distributed.create_sibling_gitea', 'CreateSiblingGitea'),
+        ('datalad.distributed.create_sibling_ria', 'CreateSiblingRia'),
+        ('datalad.distribution.create_sibling', 'CreateSibling'),
+        ('datalad.distribution.siblings', 'Siblings', 'siblings'),
         ('datalad.distribution.update', 'Update'),
-        ('datalad.distribution.create_sibling',
-         'CreateSibling',
-         'create-sibling'),
-        ('datalad.distribution.create_sibling_github',
-         'CreateSiblingGithub',
-         'create-sibling-github'),
-        ('datalad.interface.unlock', 'Unlock', 'unlock'),
-        ('datalad.interface.save', 'Save', 'save'),
-        ('datalad.plugin', 'Plugin', 'plugin'),
     ])
 
-_group_metadata = (
-    'Commands for meta data handling',
+_group_2dataset = (
+    'Dataset operations',
     [
-        ('datalad.metadata.search', 'Search',
-         'search', 'search'),
-        ('datalad.metadata.metadata', 'Metadata',
-         'metadata'),
-        ('datalad.metadata.aggregate', 'AggregateMetaData',
-         'aggregate-metadata', 'aggregate_metadata'),
+        ('datalad.local.subdatasets', 'Subdatasets'),
+        ('datalad.distributed.drop', 'Drop'),
+        ('datalad.local.remove', 'Remove'),
+        ('datalad.local.addurls', 'Addurls'),
+        ('datalad.local.copy_file', 'CopyFile'),
+        ('datalad.local.download_url', 'DownloadURL'),
+        ('datalad.local.foreach_dataset', 'ForEachDataset', 'foreach-dataset'),
+        ('datalad.distribution.install', 'Install'),
+        ('datalad.local.rerun', 'Rerun'),
+        ('datalad.local.run_procedure', 'RunProcedure'),
     ])
 
-_group_misc = (
-    'Miscellaneous commands',
+_group_3misc = (
+    'Miscellaneous',
     [
-        ('datalad.interface.test', 'Test'),
-        ('datalad.interface.crawl', 'Crawl'),
-        ('datalad.interface.crawl_init', 'CrawlInit', 'crawl-init'),
-        ('datalad.interface.ls', 'Ls'),
-        ('datalad.interface.clean', 'Clean'),
-        ('datalad.interface.add_archive_content', 'AddArchiveContent',
-         'add-archive-content'),
-        ('datalad.interface.download_url', 'DownloadURL', 'download-url'),
-        ('datalad.interface.run', 'Run', 'run'),
-        ('datalad.interface.rerun', 'Rerun', 'rerun'),
+        ('datalad.local.configuration', 'Configuration'),
+        ('datalad.local.wtf', 'WTF'),
+        ('datalad.local.clean', 'Clean'),
+        ('datalad.local.add_archive_content', 'AddArchiveContent'),
+        ('datalad.local.add_readme', 'AddReadme'),
+        ('datalad.local.export_archive', 'ExportArchive'),
+        ('datalad.distributed.export_archive_ora', 'ExportArchiveORA'),
+        ('datalad.distributed.export_to_figshare', 'ExportToFigshare'),
+        ('datalad.local.no_annex', 'NoAnnex'),
+        ('datalad.local.check_dates', 'CheckDates'),
+        ('datalad.local.unlock', 'Unlock'),
+        ('datalad.distribution.uninstall', 'Uninstall'),
     ])
 
-_group_plumbing = (
-    'Plumbing commands',
+_group_4plumbing = (
+    'Plumbing',
     [
-        ('datalad.interface.annotate_paths', 'AnnotatePaths', 'annotate-paths'),
-        ('datalad.distribution.clone', 'Clone'),
         ('datalad.distribution.create_test_dataset', 'CreateTestDataset',
          'create-test-dataset'),
-        ('datalad.interface.diff', 'Diff', 'diff'),
-        ('datalad.distribution.siblings', 'Siblings', 'siblings'),
         ('datalad.support.sshrun', 'SSHRun', 'sshrun'),
-        ('datalad.distribution.subdatasets', 'Subdatasets', 'subdatasets'),
+        ('datalad.interface.shell_completion', 'ShellCompletion', 'shell-completion'),
     ])
```

### Comparing `datalad-0.9.3/datalad/interface/save.py` & `datalad-1.0.0/datalad/core/local/save.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,416 +1,400 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""For now just a wrapper for Dataset.save()
+"""Interface to add content, and save modifications to a dataset
 
 """
 
 __docformat__ = 'restructuredtext'
 
 import logging
-from os import curdir
-from os.path import abspath
-from os.path import relpath
-from os.path import lexists
+from functools import partial
+from pathlib import Path
 
-
-from datalad.utils import unique
-from datalad.support.annexrepo import AnnexRepo
-from datalad.support.constraints import EnsureStr
-from datalad.support.constraints import EnsureNone
-from datalad.support.param import Parameter
+import datalad.utils as ut
+from datalad.distribution.dataset import (
+    Dataset,
+    EnsureDataset,
+    datasetmethod,
+    require_dataset,
+)
+from datalad.interface.base import (
+    Interface,
+    build_doc,
+    eval_results,
+)
+from datalad.interface.common_opts import (
+    jobs_opt,
+    recursion_flag,
+    recursion_limit,
+    save_message_opt,
+)
+from datalad.interface.utils import (
+    discover_dataset_trace_to_targets,
+    get_tree_roots,
+)
+from datalad.support.constraints import (
+    EnsureNone,
+    EnsureStr,
+)
 from datalad.support.exceptions import CommandError
-from datalad.distribution.dataset import Dataset
-from datalad.distribution.dataset import EnsureDataset
-from datalad.distribution.dataset import datasetmethod
-from datalad.interface.annotate_paths import AnnotatePaths
-from datalad.interface.annotate_paths import annotated2content_by_ds
-from datalad.interface.common_opts import recursion_limit, recursion_flag
-from datalad.interface.common_opts import super_datasets_flag
-from datalad.interface.common_opts import save_message_opt
-from datalad.interface.results import get_status_dict
-from datalad.interface.utils import eval_results
-from datalad.interface.base import build_doc
-from datalad.interface.utils import get_tree_roots
-from datalad.interface.utils import discover_dataset_trace_to_targets
-
-from .base import Interface
-
-lgr = logging.getLogger('datalad.interface.save')
-
-
-def save_dataset(
-        ds,
-        paths,
-        message=None):
-    """Save changes in a single dataset.
-
-    Parameters
-    ----------
-    ds : Dataset
-      The dataset to be saved.
-    paths : list
-      Annotated paths to dataset components to be saved.
-    message: str, optional
-      (Commit) message to be attached to the saved state.
-
-    Returns
-    -------
-    bool
-      Whether a new state was saved. If all to be saved content was unmodified
-      no new state will be saved.
-    """
-    # XXX paths must be in the given ds, no further sanity checks!
+from datalad.support.parallel import (
+    ProducerConsumerProgressLog,
+    no_subds_in_futures,
+)
+from datalad.support.param import Parameter
+from datalad.utils import ensure_list
+
+from .status import Status
 
-    # make sure that all pending changes (batched annex operations, etc.)
-    # are actually reflected in Git
-    ds.repo.precommit()
-
-    # track what is to be committed, so it becomes
-    # possible to decide when/what to save further down
-    # and one level up
-    orig_hexsha = ds.repo.get_hexsha()
-
-    # check whether we want to commit anything staged, or individual pieces
-    # this is independent of actually staging individual bits
-    save_entire_ds = False
-    for ap in paths:
-        if ap['path'] == ds.path:
-            save_entire_ds = True
-            break
-
-    # asking yourself why we need to `add` at all? For example, freshly
-    # unlocked files in a v5 repo are listed as "typechange" and commit
-    # refuses to touch them without an explicit `add`
-    to_gitadd = [ap['path'] for ap in paths
-                 # if not flagged as staged
-                 if not ap.get('staged', False) and
-                 # must exist, anything else needs no staging, can be committed directly
-                 lexists(ap['path']) and
-                 # not an annex repo, hence no choice other than git
-                 (not isinstance(ds.repo, AnnexRepo) or
-                  # even in an annex repo we want to use `git add` for submodules
-                  (ap.get('type', None) == 'dataset' and not ap['path'] == ds.path))]
-    to_annexadd = [ap['path'] for ap in paths
-                   # not passed to git add
-                   if ap['path'] not in to_gitadd and
-                   # if not flagged as staged
-                   not ap.get('staged', False) and
-                   # prevent `git annex add .` in a subdataset, if not desired
-                   not ap.get('process_updated_only', False) and
-                   # must exist, anything else needs no staging, can be committed directly
-                   lexists(ap['path'])]
-
-    if to_gitadd or save_entire_ds:
-        ds.repo.add(to_gitadd, git=True, commit=False,
-                    # this makes sure that pending submodule updates are added too
-                    update=save_entire_ds)
-    if to_annexadd:
-        ds.repo.add(to_annexadd, commit=False)
-
-    _datalad_msg = False
-    if not message:
-        message = 'Recorded existing changes'
-        _datalad_msg = True
-
-    # we will blindly call commit not knowing if there is anything to
-    # commit -- this is cheaper than to anticipate all possible ways
-    # a repo in whatever mode is dirty
-    paths_to_commit = None
-    if not save_entire_ds:
-        paths_to_commit = []
-        for ap in paths:
-            paths_to_commit.append(ap['path'])
-            # was file renamed?
-            path_src = ap.get('path_src')
-            if path_src and path_src != ap['path']:
-                paths_to_commit.append(path_src)
-
-    ds.repo.commit(
-        message,
-        files=paths_to_commit,
-        _datalad_msg=_datalad_msg,
-        careless=True)
-
-    current_hexsha = ds.repo.get_hexsha()
-    _was_modified = current_hexsha != orig_hexsha
-    return current_hexsha if _was_modified else None
+lgr = logging.getLogger('datalad.core.local.save')
 
 
 @build_doc
 class Save(Interface):
     """Save the current state of a dataset
 
-    Saving the state of a dataset records all changes that have been made
-    to it. This change record is annotated with a user-provided description.
+    Saving the state of a dataset records changes that have been made to it.
+    This change record is annotated with a user-provided description.
     Optionally, an additional tag, such as a version, can be assigned to the
-    saved state. Such tag enables straightforward retrieval of past versions
-    at a later point in time.
+    saved state. Such tag enables straightforward retrieval of past versions at
+    a later point in time.
 
-    || PYTHON >>
-    Returns
-    -------
-    commit or None
-      `None` if nothing was saved, the resulting commit otherwise.
-    << PYTHON ||
+    .. note::
+      Before Git v2.22, any Git repository without an initial commit located
+      inside a Dataset is ignored, and content underneath it will be saved to
+      the respective superdataset. DataLad datasets always have an initial
+      commit, hence are not affected by this behavior.
     """
+    # note above documents that out behavior is like that of `git add`, but
+    # does not explicitly mention the connection to keep it simple.
+
+    _examples_ = [
+        dict(text="""Save any content underneath the current directory, without
+             altering any potential subdataset""",
+             code_py="save(path='.')",
+             code_cmd="datalad save ."),
+        dict(text="""Save specific content in the dataset""",
+             code_py="save(path='myfile.txt')",
+             code_cmd="datalad save myfile.txt"),
+        dict(text="""Attach a commit message to save""",
+             code_py="save(path='myfile.txt', message='add file')",
+             code_cmd="datalad save -m 'add file' myfile.txt"),
+        dict(text="""Save any content underneath the current directory, and
+             recurse into any potential subdatasets""",
+             code_py="save(path='.', recursive=True)",
+             code_cmd="datalad save . -r"),
+        dict(text="Save any modification of known dataset content in the "
+                  "current directory, but leave untracked files (e.g. temporary files) "
+                  "untouched",
+             code_py="""save(path='.', updated=True)""",
+             code_cmd="""datalad save -u ."""),
+        dict(text="Tag the most recent saved state of a dataset",
+             code_py="save(version_tag='bestyet')",
+             code_cmd="datalad save --version-tag 'bestyet'"),
+        dict(text="Save a specific change but integrate into last commit keeping "
+                  "the already recorded commit message",
+             code_py="save(path='myfile.txt', amend=True)",
+             code_cmd="datalad save myfile.txt --amend")
+    ]
 
     _params_ = dict(
         dataset=Parameter(
             args=("-d", "--dataset"),
-            doc=""""specify the dataset to save. If a dataset is given, but
-            no `files`, the entire dataset will be saved.""",
+            doc=""""specify the dataset to save""",
             constraints=EnsureDataset() | EnsureNone()),
         path=Parameter(
             args=("path",),
             metavar='PATH',
             doc="""path/name of the dataset component to save. If given, only
             changes made to those components are recorded in the new state.""",
             nargs='*',
             constraints=EnsureStr() | EnsureNone()),
         message=save_message_opt,
-        all_changes=Parameter(
-            args=("-a", "--all-changes"),
-            doc="""save all changes (even to not yet added files) of all components
-            in datasets that contain any of the given paths [DEPRECATED!].""",
-            action="store_true"),
-        all_updated=Parameter(
-            args=("-u", "--all-updated"),
-            doc="""if no explicit paths are given, save changes of all known
-            components in a datasets""",
-            action="store_true"),
+        message_file=Parameter(
+            args=("-F", "--message-file"),
+            doc="""take the commit message from this file. This flag is
+            mutually exclusive with -m.""",
+            constraints=EnsureStr() | EnsureNone()),
         version_tag=Parameter(
-            args=("--version-tag",),
+            args=("-t", "--version-tag",),
             metavar='ID',
-            doc="""an additional marker for that state.""",
+            doc="""an additional marker for that state. Every dataset that
+            is touched will receive the tag.""",
             constraints=EnsureStr() | EnsureNone()),
-        super_datasets=super_datasets_flag,
         recursive=recursion_flag,
         recursion_limit=recursion_limit,
+        updated=Parameter(
+            args=('-u', '--updated',),
+            action='store_true',
+            doc="""if given, only saves previously tracked paths."""),
+        to_git=Parameter(
+            args=("--to-git",),
+            action='store_true',
+            doc="""flag whether to add data directly to Git, instead of
+            tracking data identity only.  Use with caution, there is no
+            guarantee that a file put directly into Git like this will
+            not be annexed in a subsequent save operation.
+            If not specified, it will be up to git-annex to decide how
+            a file is tracked, based on a dataset's configuration
+            to track particular paths,
+            file types, or file sizes with either Git or git-annex.
+            (see https://git-annex.branchable.com/tips/largefiles).
+            """),
+        jobs=jobs_opt,
+        amend=Parameter(
+            args=('--amend',),
+            action='store_true',
+            doc="""if set, changes are not recorded in a new, separate
+            commit, but are integrated with the changeset of the previous
+            commit, and both together are recorded by replacing that
+            previous commit. This is mutually exclusive with recursive
+            operation.
+            """),
     )
 
     @staticmethod
     @datasetmethod(name='save')
     @eval_results
-    def __call__(message=None, path=None, dataset=None,
-                 all_updated=True, all_changes=None, version_tag=None,
-                 recursive=False, recursion_limit=None, super_datasets=False
+    def __call__(path=None,
+                 *,
+                 message=None, dataset=None,
+                 version_tag=None,
+                 recursive=False, recursion_limit=None,
+                 updated=False,
+                 message_file=None,
+                 to_git=None,
+                 jobs=None,
+                 amend=False,
                  ):
-        if all_changes is not None:
-            from datalad.support.exceptions import DeprecatedError
-            raise DeprecatedError(
-                new="all_updated option where fits and/or datalad add",
-                version="0.5.0",
-                msg="RF: all_changes option passed to the save"
-            )
-        if not dataset and not path:
-            # we got nothing at all -> save what is staged in the repo in "this" directory?
-            # we verify that there is an actual repo next
-            dataset = abspath(curdir)
-        refds_path = Interface.get_refds_path(dataset)
-
-        to_process = []
-        got_nothing = True
-        for ap in AnnotatePaths.__call__(
+        if message and message_file:
+            raise ValueError(
+                "Both a message and message file were specified for save()")
+
+        if amend and recursive:
+            raise ValueError("Cannot amend a commit recursively.")
+
+        path = ensure_list(path)
+
+        if message_file:
+            with open(message_file) as mfh:
+                message = mfh.read()
+
+        # we want 'normal' to achieve the most compact argument list
+        # for git calls
+        # untracked_mode = 'no' if updated else 'normal'
+        # TODO however, Repo.add() would refuse to add any dotfiles
+        # in a directory that is itself untracked, hence the only
+        # choice is to go with potentially crazy long lists
+        # until https://github.com/datalad/datalad/issues/1454
+        # has a resolution
+        untracked_mode = 'no' if updated else 'all'
+
+        # there are three basic scenarios:
+        # 1. save modifications to any already tracked content
+        # 2. save any content (including removal of deleted content)
+        #    to bring things to a clean state
+        # 3. like (2), but only operate on a given subset of content
+        #    identified by paths
+        # - all three have to work in conjunction with --recursive
+        # - the difference between (1) and (2) should be no more
+        #   that a switch from --untracked=no to --untracked=all
+        #   in Repo.save()
+
+        # we do not support
+        # - simultaneous operations on multiple datasets from disjoint
+        #   dataset hierarchies, hence a single reference dataset must be
+        #   identifiable from the either
+        #   - curdir or
+        #   - the `dataset` argument.
+        #   This avoids complex annotation loops and hierarchy tracking.
+        # - any modification upwards from the root dataset
+
+        ds = require_dataset(dataset, check_installed=True, purpose='save')
+
+        # use status() to do all discovery and annotation of paths
+        paths_by_ds = {}
+        for s in Status()(
+                # ATTN: it is vital to pass the `dataset` argument as it,
+                # and not a dataset instance in order to maintain the path
+                # semantics between here and the status() call
+                dataset=dataset,
                 path=path,
-                dataset=refds_path,
+                untracked=untracked_mode,
                 recursive=recursive,
                 recursion_limit=recursion_limit,
-                action='save',
-                unavailable_path_status='impossible',
-                unavailable_path_msg="path does not exist: %s",
-                nondataset_path_status='impossible',
-                modified='HEAD' if not path and recursive else None,
+                on_failure='ignore',
+                # for save without recursion only commit matters
+                eval_subdataset_state='full' if recursive else 'commit',
                 return_type='generator',
-                on_failure='ignore'):
-            if ap.get('state', None) == 'untracked' and not ap.get('raw_input', False):
-                # this path was found untracked, but not explicitly given to save
-                # we will silently ignore this
+                # this could be, but for now only 'error' results are handled
+                # below
+                #on_failure='ignore',
+                result_renderer='disabled'):
+            if s['status'] == 'error':
+                # Downstream code can't do anything with these. Let the caller
+                # decide their fate.
+                yield s
                 continue
-            got_nothing = False
-            # next check should not be done during annotation, as it is possibly expensive
-            # and not generally useful
-            if ap.get('status', None) == 'impossible' and \
-                    ap.get('state', None) == 'absent' and \
-                    ap.get('parentds', None):
-                # this is not here anymore, but it might actually have been a deleted
-                # component
-                if relpath(ap['path'], start=ap['parentds']) \
-                        in Dataset(ap['parentds']).repo.get_deleted_files():
-                    # ok, this is a staged deletion that we want to save
-                    ap['status'] = ''
-                    del ap['message']
-            if ap.get('status', None):
-                # this is done
-                yield ap
-                continue
-            # for things like: `ds.save()`
-            # or recursively discovered datasets
-            if ap['path'] == refds_path or \
-                    (ap.get('type', None) == 'dataset' and
-                     not ap.get('raw_input', False) and
-                     not ap.get('state', None) == 'absent'):
-                ap['process_content'] = True
-                ap['process_updated_only'] = all_updated
-            to_process.append(ap)
-        lgr.log(2, "save, to_process=%r", to_process)
-        if got_nothing and recursive and refds_path:
-            # path annotation yielded nothing, most likely cause is that nothing
-            # was found modified, we need to say something about the reference
-            # dataset
-            yield get_status_dict(
-                'save',
-                status='notneeded',
-                path=refds_path,
-                type='dataset',
-                logger=lgr)
-            return
 
-        if not to_process:
-            # nothing left to do, potentially all errored before
-            return
-
-        if super_datasets:
-            # search for the topmost superdatasets of any path
-            dss = [Dataset(ap.get('parentds', ap['path'])) for ap in to_process]
-            superdss = [ds.get_superdataset(topmost=True)
-                        for ds in dss]
-            superdss = get_tree_roots(
-                unique(ds.path for ds in dss + superdss if ds))
-            if dataset:
-                # need to adjust the reference to the new superds
-                # if we had one ref before, we should still have exactly one
-                assert len(superdss) <= 1
-                dataset = list(superdss.keys())[0]
-                refds_path = dataset
-        elif refds_path:
-            # there is a single superdataset
-            superdss = {
-                refds_path: unique([ap['parentds']
-                                    for ap in to_process if 'parentds' in ap])}
-        else:
-            # sort all datasets under their potential superdatasets
-            # start from the top to get all subdatasets down the line
-            # and collate them into as few superdatasets as possible
-            # this is quick, just string operations
-            superdss = get_tree_roots(
-                unique([ap['parentds'] for ap in to_process if 'parentds' in ap]))
-        # for each "superdataset" check the tree of subdatasets and make sure
-        # we gather all datasets between the super and any subdataset
-        # so we can save them all bottom-up in order to be able to properly
-        # save the superdataset
-        # if this is called from e.g. `add` this is actually not necessary,
-        # but in the general case we cannot avoid it
-        # TODO maybe introduce a switch?
-        discovered = {}
-        for superds_path in superdss:
-            target_subs = superdss[superds_path]
+            # fish out status dict for this parent dataset
+            ds_status = paths_by_ds.get(s['parentds'], {})
+            # reassemble path status info as repo.status() would have made it
+            ds_status[ut.Path(s['path'])] = \
+                {k: v for k, v in s.items()
+                 if k not in (
+                     'path', 'parentds', 'refds', 'status', 'action',
+                     'logger')}
+            paths_by_ds[s['parentds']] = ds_status
+
+        lgr.debug('Determined %i datasets for saving from input arguments',
+                  len(paths_by_ds))
+        # figure out what datasets to process, start with the ones containing
+        # the paths that were given as arguments
+        discovered_datasets = list(paths_by_ds.keys())
+        if dataset:
+            # if a reference dataset was given we want to save all the way up
+            # to it, so let's throw it into the mix
+            discovered_datasets.append(ds.path)
+        # sort the datasets into (potentially) disjoint hierarchies,
+        # or a single one, if a reference dataset was given
+        dataset_hierarchies = get_tree_roots(discovered_datasets)
+        for rootds, children in dataset_hierarchies.items():
+            edges = {}
             discover_dataset_trace_to_targets(
-                # from here
-                superds_path,
-                # to all
-                target_subs,
-                [],
-                discovered)
-        # create a new minimally annotated path for each discovered dataset
-        discovered_added = set()
-        for parentds in discovered:
-            for subds in discovered[parentds]:
-                to_process.append(dict(
-                    path=subds,
-                    parentds=parentds,
-                    type='dataset'))
-                discovered_added.add(subds)
-        # make sure we have an entry for each dataset, including those
-        # tha are just parents
-        for parentds in discovered:
-            if parentds not in discovered_added:
-                to_process.append(dict(
-                    path=parentds,
-                    type='dataset',
-                    # make sure we save content of superds later on
-                    process_content=True,
-                    # but not do nasty things, like adding untracked content
-                    # just because we discovered this dataset
-                    process_updated_only=True))
-
-        # now re-annotate all paths, this will be fast for already annotated ones
-        # and will amend the annotation for others, deduplication happens here too
-        annotated_paths = AnnotatePaths.__call__(
-            path=to_process,
-            dataset=dataset,
-            # never recursion, done already
-            recursive=False,
-            action='save',
-            unavailable_path_status='',
-            nondataset_path_status='impossible',
-            return_type='generator',
-            # if there is an error now, we made this mistake in here
-            on_failure='stop')
-
-        # now sort into datasets so we can process them one by one
-        content_by_ds, ds_props, completed, nondataset_paths = \
-            annotated2content_by_ds(
-                annotated_paths,
-                refds_path=refds_path)
-        assert(not completed)
-
-        # iterate over all datasets, starting at the bottom
-        for dspath in sorted(content_by_ds.keys(), reverse=True):
-            ds = Dataset(dspath)
-            res = get_status_dict('save', ds=ds, logger=lgr)
-            if not ds.is_installed():
-                # TODO This is likely impossible now
-                res['status'] = 'impossible'
-                res['message'] = ('dataset %s is not installed', ds)
-                yield res
-                continue
-            saved_state = save_dataset(
-                ds,
-                content_by_ds[dspath],
-                message=message)
-            res['status'] = 'ok' if saved_state else 'notneeded'
-            # MIH: let's tag even if there was nothing commit. I'd forget this
-            # option too often...
-            if version_tag:
-                try:
-                    # TODO: check whether comment below is still true after
-                    # removing the log swallowing:
-                    # again cannot help but force-silence low-level code, because
-                    # it screams like a made man instead of allowing top-level
-                    # code an orderly error report
-                    ds.repo.tag(version_tag)
-                    # even if we haven't saved anything
-                    res['status'] = 'ok'
+                rootds, children, [], edges, includeds=children)
+            for superds, subdss in edges.items():
+                superds_status = paths_by_ds.get(superds, {})
+                for subds in subdss:
+                    subds_path = ut.Path(subds)
+                    sub_status = superds_status.get(subds_path, {})
+                    if not (sub_status.get("state") == "clean" and
+                            sub_status.get("type") == "dataset"):
+                        # start from an entry that may already exist in the
+                        # status record
+                        superds_status[subds_path] = superds_status.get(
+                            subds_path,
+                            # if we got nothing yet:
+                            # shot from the hip, some status config
+                            # to trigger this specific super/sub
+                            # relation to be saved
+                            dict(state='untracked', type='dataset')
+                        )
+                paths_by_ds[superds] = superds_status
+
+        def save_ds(args, version_tag=None):
+            pdspath, paths = args
+
+            pds = Dataset(pdspath)
+            pds_repo = pds.repo
+            # pop status for this dataset, we are not coming back to it
+            pds_status = {
+                # for handing over to the low-level code, we recode any
+                # path relative to the real repo location, this avoid
+                # cumbersome symlink handling without context in the
+                # lower levels
+                pds_repo.pathobj / p.relative_to(pdspath): props
+                for p, props in paths.items()}
+            start_commit = pds_repo.get_hexsha()
+            if not all(p['state'] == 'clean' for p in pds_status.values()) or \
+                    (amend and message):
+                for res in pds_repo.save_(
+                        message=message,
+                        # make sure to have the `path` arg be None, as we want
+                        # to prevent and bypass any additional repo.status()
+                        # calls
+                        paths=None,
+                        # prevent whining of GitRepo
+                        git=True if not hasattr(pds_repo, 'uuid')
+                        else to_git,
+                        # we are supplying the full status already, do not
+                        # detect anything else
+                        untracked='no',
+                        _status=pds_status,
+                        amend=amend):
+                    # TODO remove stringification when datalad-core can handle
+                    # path objects, or when PY3.6 is the lowest supported
+                    # version
+                    for k in ('path', 'refds'):
+                        if k in res:
+                            res[k] = str(
+                                # recode path back to dataset path anchor
+                                pds.pathobj / Path(res[k]).relative_to(
+                                    pds_repo.pathobj)
+                            )
                     yield res
-                except CommandError as e:
-                    if saved_state:
-                        # first we yield the result for the actual save
-                        yield res
-                    # and now complain that tagging didn't work
-                    yield get_status_dict(
-                        'save',
-                        ds=ds,
-                        logger=lgr,
-                        status='error',
-                        message=(
-                            'cannot tag this version: %s',
-                            e.stderr.strip()))
-            else:
-                yield res
+            # report on the dataset itself
+            dsres = dict(
+                action='save',
+                type='dataset',
+                path=pds.path,
+                refds=ds.path,
+                status='ok'
+                if start_commit != pds_repo.get_hexsha()
+                else 'notneeded',
+                logger=lgr,
+            )
+            if not version_tag:
+                yield dsres
+                return
+            try:
+                # method requires str
+                version_tag = str(version_tag)
+                pds_repo.tag(version_tag)
+                dsres.update(
+                    status='ok',
+                    version_tag=version_tag)
+                yield dsres
+            except CommandError as e:
+                if dsres['status'] == 'ok':
+                    # first we yield the result for the actual save
+                    # TODO: we will get duplicate dataset/save record obscuring
+                    # progress reporting.  yoh thought to decouple "tag" from "save"
+                    # messages but was worrying that original authors would disagree
+                    yield dsres.copy()
+                # and now complain that tagging didn't work
+                dsres.update(
+                    status='error',
+                    message=('cannot tag this version: %s', e.stderr.strip()))
+                yield dsres
+
+        if not paths_by_ds:
+            # Special case: empty repo. There's either an empty commit only or
+            # none at all. An empty one we can amend otherwise there's nothing
+            # to do.
+            if amend and ds.repo.get_hexsha():
+                yield from save_ds((ds.pathobj, dict()), version_tag=version_tag)
 
-    @staticmethod
-    def custom_result_renderer(res, **kwargs):
-        from datalad.ui import ui
-        if not res or res.get('type', None) != 'dataset' or 'path' not in res:
+            else:
+                yield dict(action='save',
+                           type='dataset',
+                           path=ds.path,
+                           refds=ds.path,
+                           status='notneeded',
+                           logger=lgr)
             return
-        ds = Dataset(res['path'])
-        commit = ds.repo.get_hexsha()
-        ui.message('Saved state: {0} for {1}'.format(
-            commit,
-            ds))
+
+        # TODO: in principle logging could be improved to go not by a dataset
+        # but by path(s) within subdatasets. That should provide a bit better ETA
+        # and more "dynamic" feedback than jumpy datasets count.
+        # See addurls where it is implemented that way by providing agg and another
+        # log_filter
+        yield from ProducerConsumerProgressLog(
+            sorted(paths_by_ds.items(), key=lambda v: v[0], reverse=True),
+            partial(save_ds, version_tag=version_tag),
+            safe_to_consume=no_subds_in_futures,
+            producer_future_key=lambda ds_items: ds_items[0],
+            jobs=jobs,
+            log_filter=_log_filter_save_dataset,
+            unit="datasets",
+            lgr=lgr,
+        )
+
+
+def _log_filter_save_dataset(res):
+    return res.get('type') == 'dataset' and res.get('action') == 'save'
```

### Comparing `datalad-0.9.3/datalad/interface/utils.py` & `datalad-1.0.0/datalad/core/local/create.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,538 +1,574 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""Interface utility functions
+"""High-level interface for dataset creation
 
 """
 
-__docformat__ = 'restructuredtext'
-
-import inspect
+import os
 import logging
-import wrapt
-import sys
-import re
-import shlex
-from os import curdir
-from os import pardir
+import random
+import uuid
+import warnings
+from argparse import (
+    REMAINDER,
+)
+
 from os import listdir
-from os.path import join as opj
-from os.path import isdir
-from os.path import relpath
-from os.path import sep
-from os.path import split as psplit
-from itertools import chain
-from six import PY2
-
-import json
-
-# avoid import from API to not get into circular imports
-from datalad.utils import with_pathsep as _with_sep  # TODO: RF whenever merge conflict is not upon us
-from datalad.utils import path_startswith
-from datalad.utils import path_is_subpath
-from datalad.support.gitrepo import GitRepo
-from datalad.support.exceptions import IncompleteResultsError
-from datalad import cfg as dlcfg
-from datalad.dochelpers import exc_str
+import os.path as op
 
+from datalad import cfg
+from datalad import _seed
+from datalad.interface.base import Interface
+from datalad.interface.base import (
+    build_doc,
+    eval_results,
+)
+from datalad.interface.common_opts import (
+    location_description,
+)
+from datalad.support.constraints import (
+    EnsureStr,
+    EnsureNone,
+    EnsureKeyChoice,
+)
+from datalad.support.param import Parameter
+from datalad.utils import (
+    getpwd,
+    ensure_list,
+    get_dataset_root,
+    Path,
+)
+
+from datalad.distribution.dataset import (
+    Dataset,
+    datasetmethod,
+    EnsureDataset,
+    resolve_path,
+    path_under_rev_dataset,
+    require_dataset,
+)
 
-from datalad.support.constraints import Constraint
+from datalad.support.gitrepo import GitRepo
+from datalad.support.annexrepo import AnnexRepo
 
-from datalad.ui import ui
-import datalad.support.ansi_colors as ac
 
-from datalad.interface.base import Interface
-from datalad.interface.base import default_logchannels
-from datalad.interface.base import get_allargs_as_kwargs
-from datalad.interface.common_opts import eval_params
-from datalad.interface.common_opts import eval_defaults
-from .results import known_result_xfms
+__docformat__ = 'restructuredtext'
 
+lgr = logging.getLogger('datalad.core.local.create')
 
-lgr = logging.getLogger('datalad.interface.utils')
 
+@build_doc
+class Create(Interface):
+    """Create a new dataset from scratch.
+
+    This command initializes a new dataset at a given location, or the
+    current directory. The new dataset can optionally be registered in an
+    existing superdataset (the new dataset's path needs to be located
+    within the superdataset for that, and the superdataset needs to be given
+    explicitly via [PY: `dataset` PY][CMD: --dataset CMD]). It is recommended
+    to provide a brief description to label the dataset's nature *and*
+    location, e.g. "Michael's music on black laptop". This helps humans to
+    identify data locations in distributed scenarios.  By default an identifier
+    comprised of user and machine name, plus path will be generated.
+
+    This command only creates a new dataset, it does not add existing content
+    to it, even if the target directory already contains additional files or
+    directories.
+
+    Plain Git repositories can be created via [PY: `annex=False` PY][CMD: --no-annex CMD].
+    However, the result will not be a full dataset, and, consequently,
+    not all features are supported (e.g. a description).
+
+    || REFLOW >>
+    To create a local version of a remote dataset use the
+    :func:`~datalad.api.install` command instead.
+    << REFLOW ||
+
+    .. note::
+      Power-user info: This command uses :command:`git init` and
+      :command:`git annex init` to prepare the new dataset. Registering to a
+      superdataset is performed via a :command:`git submodule add` operation
+      in the discovered superdataset.
+    """
 
-def cls2cmdlinename(cls):
-    "Return the cmdline command name from an Interface class"
-    r = re.compile(r'([a-z0-9])([A-Z])')
-    return r.sub('\\1-\\2', cls.__name__).lower()
+    # in general this command will yield exactly one result
+    return_type = 'item-or-list'
+    # in general users expect to get an instance of the created dataset
+    result_xfm = 'datasets'
+    # result filter
+    result_filter = \
+        EnsureKeyChoice('action', ('create',)) & \
+        EnsureKeyChoice('status', ('ok', 'notneeded'))
+
+    _examples_ = [
+        dict(text="Create a dataset 'mydataset' in the current directory",
+             code_py="create(path='mydataset')",
+             code_cmd="datalad create mydataset"),
+        dict(text="Apply the text2git procedure upon creation of a dataset",
+             code_py="create(path='mydataset', cfg_proc='text2git')",
+             code_cmd="datalad create -c text2git mydataset"),
+        dict(text="Create a subdataset in the root of an existing dataset",
+             code_py="create(dataset='.', path='mysubdataset')",
+             code_cmd="datalad create -d . mysubdataset"),
+        dict(text="Create a dataset in an existing, non-empty directory",
+             code_py="create(force=True)",
+             code_cmd="datalad create --force"),
+        dict(text="Create a plain Git repository",
+             code_py="create(path='mydataset', annex=False)",
+             code_cmd="datalad create --no-annex mydataset"),
+    ]
+
+    _params_ = dict(
+        path=Parameter(
+            args=("path",),
+            nargs='?',
+            metavar='PATH',
+            doc="""path where the dataset shall be created, directories
+            will be created as necessary. If no location is provided, a dataset
+            will be created in the location specified by [PY: `dataset`
+            PY][CMD: --dataset CMD] (if given) or the current working
+            directory. Either way the command will error if the target
+            directory is not empty. Use [PY: `force` PY][CMD: --force CMD] to
+            create a dataset in a non-empty directory.""",
+            # put dataset 2nd to avoid useless conversion
+            constraints=EnsureStr() | EnsureDataset() | EnsureNone()),
+        initopts=Parameter(
+            args=("initopts",),
+            metavar='INIT OPTIONS',
+            nargs=REMAINDER,
+            doc="""options to pass to :command:`git init`. [PY: Options can be
+            given as a list of command line arguments or as a GitPython-style
+            option dictionary PY][CMD: Any argument specified after the
+            destination path of the repository will be passed to git-init
+            as-is CMD]. Note that not all options will lead to viable results.
+            For example '--bare' will not yield a repository where DataLad
+            can adjust files in its working tree."""),
+        dataset=Parameter(
+            args=("-d", "--dataset"),
+            metavar='DATASET',
+            doc="""specify the dataset to perform the create operation on. If
+            a dataset is given along with `path`, a new subdataset will be created
+            in it at the `path` provided to the create command. If a dataset is
+            given but `path` is unspecified, a new dataset will be created at the
+            location specified by this option.""",
+            constraints=EnsureDataset() | EnsureNone()),
+        force=Parameter(
+            args=("-f", "--force",),
+            doc="""enforce creation of a dataset in a non-empty directory""",
+            action='store_true'),
+        description=location_description,
+        annex=Parameter(
+            args=("--no-annex",),
+            dest='annex',
+            doc="""if [CMD: set CMD][PY: disabled PY], a plain Git repository
+            will be created without any annex""",
+            action='store_false'),
+        # TODO seems to only cause a config flag to be set, this could be done
+        # in a procedure
+        fake_dates=Parameter(
+            args=('--fake-dates',),
+            action='store_true',
+            doc="""Configure the repository to use fake dates. The date for a
+            new commit will be set to one second later than the latest commit
+            in the repository. This can be used to anonymize dates."""),
+        cfg_proc=Parameter(
+            args=("-c", "--cfg-proc"),
+            metavar="PROC",
+            action='append',
+            doc="""Run cfg_PROC procedure(s) (can be specified multiple times)
+            on the created dataset. Use
+            [PY: `run_procedure(discover=True)` PY][CMD: run-procedure --discover CMD]
+            to get a list of available procedures, such as cfg_text2git.
+            """
+        )
+    )
+
+    @staticmethod
+    @datasetmethod(name='create')
+    @eval_results
+    def __call__(
+            path=None,
+            initopts=None,
+            *,
+            force=False,
+            description=None,
+            dataset=None,
+            annex=True,
+            fake_dates=False,
+            cfg_proc=None
+    ):
+        # we only perform negative tests below
+        no_annex = not annex
+
+        if dataset:
+            if isinstance(dataset, Dataset):
+                ds = dataset
+            else:
+                ds = Dataset(dataset)
+            refds_path = ds.path
+        else:
+            ds = refds_path = None
 
+        # two major cases
+        # 1. we got a `dataset` -> we either want to create it (path is None),
+        #    or another dataset in it (path is not None)
+        # 2. we got no dataset -> we want to create a fresh dataset at the
+        #    desired location, either at `path` or PWD
+
+        # sanity check first
+        if no_annex:
+            if description:
+                raise ValueError("Incompatible arguments: cannot specify "
+                                 "description for annex repo and declaring "
+                                 "no annex repo.")
+
+        if (isinstance(initopts, (list, tuple)) and '--bare' in initopts) or (
+                isinstance(initopts, dict) and 'bare' in initopts):
+            raise ValueError(
+                "Creation of bare repositories is not supported. Consider "
+                "one of the create-sibling commands, or use "
+                "Git to init a bare repository and push an existing dataset "
+                "into it.")
+
+        if path:
+            path = resolve_path(path, dataset)
+
+        path = path if path \
+            else getpwd() if ds is None \
+            else refds_path
+
+        # we know that we need to create a dataset at `path`
+        assert(path is not None)
+
+        # assure cfg_proc is a list (relevant if used via Python API)
+        cfg_proc = ensure_list(cfg_proc)
+
+        # prep for yield
+        res = dict(action='create', path=str(path),
+                   logger=lgr, type='dataset',
+                   refds=refds_path)
+
+        refds = None
+        if refds_path and refds_path != str(path):
+            refds = require_dataset(
+                refds_path, check_installed=True,
+                purpose='create a subdataset')
+
+            path_inrefds = path_under_rev_dataset(refds, path)
+            if path_inrefds is None:
+                yield dict(
+                    res,
+                    status='error',
+                    message=(
+                        "dataset containing given paths is not underneath "
+                        "the reference dataset %s: %s",
+                        ds, str(path)),
+                )
+                return
 
-def handle_dirty_dataset(ds, mode, msg=None):
-    """Detect and treat unsaved changes as instructed by `mode`
+        # try to locate an immediate parent dataset
+        # we want to know this (irrespective of whether we plan on adding
+        # this new dataset to a parent) in order to avoid conflicts with
+        # a potentially absent/uninstalled subdataset of the parent
+        # in this location
+        # it will cost some filesystem traversal though...
+        parentds_path = get_dataset_root(
+            op.normpath(op.join(str(path), os.pardir)))
+        if parentds_path:
+            prepo = GitRepo(parentds_path)
+            parentds_path = Path(parentds_path)
+            # we cannot get away with a simple
+            # GitRepo.get_content_info(), as we need to detect
+            # uninstalled/added subdatasets too
+            check_path = Path(path)
+            pstatus = prepo.status(
+                untracked='no',
+                # limit query to target path for a potentially massive speed-up
+                paths=[check_path.relative_to(parentds_path)])
+            if (not pstatus.get(check_path, {}).get("type") == "dataset" and
+                any(check_path == p or check_path in p.parents
+                    for p in pstatus)):
+                # redo the check in a slower fashion, it is already broken
+                # let's take our time for a proper error message
+                conflict = [
+                    p for p in pstatus
+                    if check_path == p or check_path in p.parents]
+                res.update({
+                    'status': 'error',
+                    'message': (
+                        'collision with content in parent dataset at %s: %s',
+                        str(parentds_path),
+                        [str(c) for c in conflict])})
+                yield res
+                return
+            if not force:
+                # another set of check to see whether the target path is pointing
+                # into a known subdataset that is not around ATM
+                subds_status = {
+                    parentds_path / k.relative_to(prepo.path)
+                    for k, v in pstatus.items()
+                    if v.get('type', None) == 'dataset'}
+                check_paths = [check_path]
+                check_paths.extend(check_path.parents)
+                if any(p in subds_status for p in check_paths):
+                    conflict = [p for p in check_paths if p in subds_status]
+                    res.update({
+                        'status': 'error',
+                        'message': (
+                            'collision with %s (dataset) in dataset %s',
+                            str(conflict[0]),
+                            str(parentds_path))})
+                    yield res
+                    return
+
+        # important to use the given Dataset object to avoid spurious ID
+        # changes with not-yet-materialized Datasets
+        tbds = ds if isinstance(ds, Dataset) and \
+            ds.path == path else Dataset(str(path))
+
+        # don't create in non-empty directory without `force`:
+        if op.isdir(tbds.path) and listdir(tbds.path) != [] and not force:
+            res.update({
+                'status': 'error',
+                'message':
+                    'will not create a dataset in a non-empty directory, use '
+                    '`--force` option to ignore'})
+            yield res
+            return
+
+        # Check if specified cfg_proc(s) can be discovered, storing
+        # the results so they can be used when the time comes to run
+        # the procedure. If a procedure cannot be found, raise an
+        # error to prevent creating the dataset.
+        cfg_proc_specs = []
+        if cfg_proc:
+            discovered_procs = tbds.run_procedure(
+                discover=True,
+                result_renderer='disabled',
+                return_type='list',
+            )
+            for cfg_proc_ in cfg_proc:
+                for discovered_proc in discovered_procs:
+                    if discovered_proc['procedure_name'] == 'cfg_' + cfg_proc_:
+                        cfg_proc_specs.append(discovered_proc)
+                        break
+                else:
+                    raise ValueError("Cannot find procedure with name "
+                                     "'%s'" % cfg_proc_)
 
-    Parameters
-    ----------
-    ds : Dataset or None
-      Dataset to be inspected. Does nothing if `None`.
-    mode : {'fail', 'ignore', 'save-before'}
-      How to act upon discovering unsaved changes.
-    msg : str or None
-      Custom message to use for a potential commit.
+        if initopts is not None and isinstance(initopts, list):
+            initopts = {'_from_cmdline_': initopts}
 
-    Returns
-    -------
-    None
-    """
-    if ds is None:
-        # nothing to be handled
-        return
-    if msg is None:
-        msg = '[DATALAD] auto-saved changes'
-
-    # make sure that all pending changes (batched annex operations, etc.)
-    # are actually reflected in Git
-    if ds.repo:
-        ds.repo.precommit()
-
-    if mode == 'ignore':
-        return
-    elif mode == 'fail':
-        if not ds.repo or ds.repo.is_dirty(index=True,
-                                           untracked_files=True,
-                                           submodules=True):
-            raise RuntimeError('dataset {} has unsaved changes'.format(ds))
-    elif mode == 'save-before':
-        if not ds.is_installed():
-            raise RuntimeError('dataset {} is not yet installed'.format(ds))
-        from datalad.interface.save import Save
-        Save.__call__(dataset=ds, message=msg)
-    else:
-        raise ValueError("unknown if-dirty mode '{}'".format(mode))
-
-
-def get_tree_roots(paths):
-    """Return common root paths for a set of paths
-
-    This function determines the smallest set of common root
-    paths and sorts all given paths under the respective
-    root.
+        # Note for the code below:
+        # OPT: be "smart" and avoid re-resolving .repo -- expensive in DataLad
+        # Reuse tbrepo instance, do not use tbds.repo
+
+        # create and configure desired repository
+        # also provides initial set of content to be tracked with git (not annex)
+        if no_annex:
+            tbrepo, add_to_git = _setup_git_repo(path, initopts, fake_dates)
+        else:
+            tbrepo, add_to_git = _setup_annex_repo(
+                path, initopts, fake_dates, description)
 
-    Returns
-    -------
-    dict
-      paths by root
-    """
-    paths_ws = [_with_sep(p) for p in paths]
-    # sort all paths under their potential roots
-    roots = {}
-    # start from the top to get all paths down the line
-    # and collate them into as few roots as possible
-    for s in sorted(paths_ws):
-        if any([s.startswith(r) for r in roots]):
-            # this path is already covered by a known root
-            continue
-        # find all sub paths
-        subs = [p for p in paths if p.startswith(s)]
-        roots[s.rstrip(sep)] = subs
-    return roots
+        # OPT: be "smart" and avoid re-resolving .repo -- expensive in DataLad
+        # Note, must not happen earlier (before if) since "smart" it would not be
+        tbds_config = tbds.config
+
+        # record an ID for this repo for the afterlife
+        # to be able to track siblings and children
+        id_var = 'datalad.dataset.id'
+        # Note, that Dataset property `id` will change when we unset the
+        # respective config. Therefore store it before:
+        tbds_id = tbds.id
+        if id_var in tbds_config:
+            # make sure we reset this variable completely, in case of a
+            # re-create
+            tbds_config.unset(id_var, scope='branch')
+
+        if _seed is None:
+            # just the standard way
+            # use a fully random identifier (i.e. UUID version 4)
+            uuid_id = str(uuid.uuid4())
+        else:
+            # Let's generate preseeded ones
+            uuid_id = str(uuid.UUID(int=random.getrandbits(128)))
+        tbds_config.add(
+            id_var,
+            tbds_id if tbds_id is not None else uuid_id,
+            scope='branch',
+            reload=False)
+
+        # make config overrides permanent in the repo config
+        # this is similar to what `annex init` does
+        # we are only doing this for config overrides and do not expose
+        # a dedicated argument, because it is sufficient for the cmdline
+        # and unnecessary for the Python API (there could simply be a
+        # subsequence ds.config.add() call)
+        for k, v in tbds_config.overrides.items():
+            tbds_config.add(k, v, scope='local', reload=False)
+
+        # all config manipulation is done -> fll reload
+        tbds_config.reload()
+
+        # must use the repo.pathobj as this will have resolved symlinks
+        add_to_git[tbrepo.pathobj / '.datalad'] = {
+            'type': 'directory',
+            'state': 'untracked'}
+
+        # save everything, we need to do this now and cannot merge with the
+        # call below, because we may need to add this subdataset to a parent
+        # but cannot until we have a first commit
+        tbrepo.save(
+            message='[DATALAD] new dataset',
+            git=True,
+            # we have to supply our own custom status, as the repo does
+            # not have a single commit yet and the is no HEAD reference
+            # TODO make `GitRepo.status()` robust to this state.
+            _status=add_to_git,
+        )
+
+        for cfg_proc_spec in cfg_proc_specs:
+            yield from tbds.run_procedure(
+                cfg_proc_spec,
+                result_renderer='disabled',
+                return_type='generator',
+            )
+
+        # the next only makes sense if we saved the created dataset,
+        # otherwise we have no committed state to be registered
+        # in the parent
+        if isinstance(refds, Dataset) and refds.path != tbds.path:
+            # we created a dataset in another dataset
+            # -> make submodule
+            yield from refds.save(
+                path=tbds.path,
+                return_type='generator',
+                result_renderer='disabled',
+            )
+        else:
+            # if we do not save, we touch the root directory of the new
+            # dataset to signal a change in the nature of the directory.
+            # this is useful for apps like datalad-gooey (or other
+            # inotify consumers) to pick up on such changes.
+            tbds.pathobj.touch()
+
+        res.update({'status': 'ok'})
+        yield res
 
 
-def path_is_under(values, path=None):
-    """Whether a given path is a subdirectory of any of the given test values
+def _setup_git_repo(path, initopts=None, fake_dates=False):
+    """Create and configure a repository at `path`
 
     Parameters
     ----------
-    values : sequence or dict
-      Paths to be tested against. This can be a dictionary in which case
-      all values from all keys will be tested against.
-    path : path or None
-      Test path. If None is given, the process' working directory is
-      used.
+    path: str or Path
+      Path of the repository
+    initopts: dict, optional
+      Git options to be passed to the GitRepo constructor
+    fake_dates: bool, optional
+      Passed to the GitRepo constructor
 
     Returns
     -------
-    bool
+    GitRepo, dict
+      Created repository and records for any repo component that needs to be
+      passed to git-add as a result of the setup procedure.
     """
-    if path is None:
-        from datalad.utils import getpwd
-        path = getpwd()
-    if isinstance(values, dict):
-        values = chain(*values.values())
-    for p in values:
-        rpath = relpath(p, start=path)
-        if rpath == curdir \
-                or rpath == pardir \
-                or set(psplit(rpath)) == {pardir}:
-            # first match is enough
-            return True
-    return False
-
+    tbrepo = GitRepo(
+        path,
+        create=True,
+        create_sanity_checks=False,
+        git_opts=initopts,
+        fake_dates=fake_dates)
+    # place a .noannex file to indicate annex to leave this repo alone
+    stamp_path = Path(tbrepo.path) / '.noannex'
+    stamp_path.touch()
+    add_to_git = {
+        stamp_path: {
+            'type': 'file',
+            'state': 'untracked',
+        }
+    }
+    return tbrepo, add_to_git
+
+
+def _setup_annex_repo(path, initopts=None, fake_dates=False,
+                      description=None):
+    """Create and configure a repository at `path`
 
-def discover_dataset_trace_to_targets(basepath, targetpaths, current_trace, spec):
-    """Discover the edges and nodes in a dataset tree to given target paths
+    This includes a default setup of annex.largefiles.
 
     Parameters
     ----------
-    basepath : path
-      Path to a start or top-level dataset. Really has to be a path to a
-      dataset!
-    targetpaths : list(path)
-      Any non-zero number of path that are termination points for the
-      search algorithm. Can be paths to datasets, directories, or files
-      (and any combination thereof).
-    current_trace : list
-      For a top-level call this should probably always be `[]`
-    spec : dict
-      `content_by_ds`-style dictionary that will receive information about the
-      discovered datasets. Specifically, for each discovered dataset there
-      will be in item with its path under the key (path) of the respective
-      superdataset.
+    path: str or Path
+      Path of the repository
+    initopts: dict, optional
+      Git options to be passed to the AnnexRepo constructor
+    fake_dates: bool, optional
+      Passed to the AnnexRepo constructor
+    description: str, optional
+      Passed to the AnnexRepo constructor
 
     Returns
     -------
-    None
-      Function calls itself recursively and populates `spec` in-place.
+    AnnexRepo, dict
+      Created repository and records for any repo component that needs to be
+      passed to git-add as a result of the setup procedure.
     """
-    # this beast walks the directory tree from a given `basepath` until
-    # it discovers any of the given `targetpaths`
-    # if it finds one, it commits any accummulated trace of visited
-    # datasets on this edge to the spec
-    valid_repo = GitRepo.is_valid_repo(basepath)
-    if valid_repo:
-        # we are passing into a new dataset, extend the dataset trace
-        current_trace = current_trace + [basepath]
-    if basepath in targetpaths:
-        # found a targetpath, commit the trace
-        for i, p in enumerate(current_trace[:-1]):
-            # TODO RF prepare proper annotated path dicts
-            spec[p] = list(set(spec.get(p, []) + [current_trace[i + 1]]))
-    if not isdir(basepath):
-        # nothing underneath this one -> done
-        return
-    # this edge is not done, we need to try to reach any downstream
-    # dataset
-    for p in listdir(basepath):
-        if valid_repo and p == '.git':
-            # ignore gitdir to speed things up
-            continue
-        p = opj(basepath, p)
-        if all(t != p and not path_startswith(t, p) for t in targetpaths):
-            # OPT listdir might be large and we could have only few items
-            # in `targetpaths` -- so traverse only those in spec which have
-            # leading dir basepath
-            continue
-        # we need to call this even for non-directories, to be able to match
-        # file target paths
-        discover_dataset_trace_to_targets(p, targetpaths, current_trace, spec)
-
-
-def eval_results(func):
-    """Decorator for return value evaluation of datalad commands.
-
-    Note, this decorator is only compatible with commands that return
-    status dict sequences!
-
-    Two basic modes of operation are supported: 1) "generator mode" that
-    `yields` individual results, and 2) "list mode" that returns a sequence of
-    results. The behavior can be selected via the kwarg `return_type`.
-    Default is "list mode".
-
-    This decorator implements common functionality for result rendering/output,
-    error detection/handling, and logging.
-
-    Result rendering/output can be triggered via the
-    `datalad.api.result-renderer` configuration variable, or the
-    `result_renderer` keyword argument of each decorated command. Supported
-    modes are: 'default' (one line per result with action, status, path,
-    and an optional message); 'json' (one object per result, like git-annex),
-    'json_pp' (like 'json', but pretty-printed spanning multiple lines),
-    'tailored' custom output formatting provided by each command
-    class (if any).
-
-    Error detection works by inspecting the `status` item of all result
-    dictionaries. Any occurrence of a status other than 'ok' or 'notneeded'
-    will cause an IncompleteResultsError exception to be raised that carries
-    the failed actions' status dictionaries in its `failed` attribute.
-
-    Status messages will be logged automatically, by default the following
-    association of result status and log channel will be used: 'ok' (debug),
-    'notneeded' (debug), 'impossible' (warning), 'error' (error).  Logger
-    instances included in the results are used to capture the origin of a
-    status report.
-
-    Parameters
-    ----------
-    func: function
-      __call__ method of a subclass of Interface,
-      i.e. a datalad command definition
-    """
-
-    @wrapt.decorator
-    def eval_func(wrapped, instance, args, kwargs):
-        # for result filters and pre/post plugins
-        # we need to produce a dict with argname/argvalue pairs for all args
-        # incl. defaults and args given as positionals
-        allkwargs = get_allargs_as_kwargs(wrapped, args, kwargs)
-        # determine class, the __call__ method of which we are decorating:
-        # Ben: Note, that this is a bit dirty in PY2 and imposes restrictions on
-        # when and how to use eval_results as well as on how to name a command's
-        # module and class. As of now, we are inline with these requirements as
-        # far as I'm aware.
-        mod = sys.modules[wrapped.__module__]
-        if PY2:
-            # we rely on:
-            # - decorated function is method of a subclass of Interface
-            # - the name of the class matches the last part of the module's name
-            #   if converted to lower
-            # for example:
-            # ..../where/ever/mycommand.py:
-            # class MyCommand(Interface):
-            #     @eval_results
-            #     def __call__(..)
-            command_class_names = \
-                [i for i in mod.__dict__
-                 if type(mod.__dict__[i]) == type and
-                 issubclass(mod.__dict__[i], Interface) and
-                 i.lower() == wrapped.__module__.split('.')[-1].replace('_', '')]
-            assert(len(command_class_names) == 1)
-            command_class_name = command_class_names[0]
-        else:
-            command_class_name = wrapped.__qualname__.split('.')[-2]
-        _func_class = mod.__dict__[command_class_name]
-        lgr.debug("Determined class of decorated function: %s", _func_class)
-
-        # retrieve common options from kwargs, and fall back on the command
-        # class attributes, or general defaults if needed
-        common_params = {
-            p_name: kwargs.pop(
-                p_name,
-                getattr(_func_class, p_name, eval_defaults[p_name]))
-            for p_name in eval_params}
-        # short cuts and configured setup for common options
-        on_failure = common_params['on_failure']
-        return_type = common_params['return_type']
-        # resolve string labels for transformers too
-        result_xfm = common_params['result_xfm']
-        if result_xfm in known_result_xfms:
-            result_xfm = known_result_xfms[result_xfm]
-        result_renderer = common_params['result_renderer']
-        # TODO remove this conditional branch entirely, done outside
-        if not result_renderer:
-            result_renderer = dlcfg.get('datalad.api.result-renderer', None)
-        # wrap the filter into a helper to be able to pass additional arguments
-        # if the filter supports it, but at the same time keep the required interface
-        # as minimal as possible. Also do this here, in order to avoid this test
-        # to be performed for each return value
-        result_filter = common_params['result_filter']
-        _result_filter = result_filter
-        if result_filter:
-            if isinstance(result_filter, Constraint):
-                _result_filter = result_filter.__call__
-            if (PY2 and inspect.getargspec(_result_filter).keywords) or \
-                    (not PY2 and inspect.getfullargspec(_result_filter).varkw):
-
-                def _result_filter(res):
-                    return result_filter(res, **allkwargs)
-
-        def _get_plugin_specs(param_key=None, cfg_key=None):
-            spec = common_params.get(param_key, None)
-            if spec is not None:
-                # this is already a list of lists
-                return spec
-
-            spec = dlcfg.get(cfg_key, None)
-            if spec is None:
-                return
-            elif not isinstance(spec, tuple):
-                spec = [spec]
-            return [shlex.split(s) for s in spec]
-
-        # query cfg for defaults
-        cmdline_name = cls2cmdlinename(_func_class)
-        run_before = _get_plugin_specs(
-            'run_before',
-            'datalad.{}.run-before'.format(cmdline_name))
-        run_after = _get_plugin_specs(
-            'run_after',
-            'datalad.{}.run-after'.format(cmdline_name))
-
-        # this internal helper function actually drives the command
-        # generator-style, it may generate an exception if desired,
-        # on incomplete results
-        def generator_func(*_args, **_kwargs):
-            from datalad.plugin import Plugin
-
-            # flag whether to raise an exception
-            incomplete_results = []
-            # track what actions were performed how many times
-            action_summary = {}
-
-            for pluginspec in run_before or []:
-                lgr.debug('Running pre-proc plugin %s', pluginspec)
-                for r in _process_results(
-                        Plugin.__call__(
-                            pluginspec,
-                            dataset=allkwargs.get('dataset', None),
-                            return_type='generator'),
-                        _func_class, action_summary,
-                        on_failure, incomplete_results,
-                        result_renderer, result_xfm, result_filter,
-                        **_kwargs):
-                    yield r
-
-            # process main results
-            for r in _process_results(
-                    wrapped(*_args, **_kwargs),
-                    _func_class, action_summary,
-                    on_failure, incomplete_results,
-                    result_renderer, result_xfm, _result_filter, **_kwargs):
-                yield r
-
-            for pluginspec in run_after or []:
-                lgr.debug('Running post-proc plugin %s', pluginspec)
-                for r in _process_results(
-                        Plugin.__call__(
-                            pluginspec,
-                            dataset=allkwargs.get('dataset', None),
-                            return_type='generator'),
-                        _func_class, action_summary,
-                        on_failure, incomplete_results,
-                        result_renderer, result_xfm, result_filter,
-                        **_kwargs):
-                    yield r
-
-            # result summary before a potential exception
-            if result_renderer == 'default' and action_summary and \
-                    sum(sum(s.values()) for s in action_summary.values()) > 1:
-                # give a summary in default mode, when there was more than one
-                # action performed
-                ui.message("action summary:\n  {}".format(
-                    '\n  '.join('{} ({})'.format(
-                        act,
-                        ', '.join('{}: {}'.format(status, action_summary[act][status])
-                                  for status in sorted(action_summary[act])))
-                                for act in sorted(action_summary))))
-
-            if incomplete_results:
-                raise IncompleteResultsError(
-                    failed=incomplete_results,
-                    msg="Command did not complete successfully")
-
-        if return_type == 'generator':
-            # hand over the generator
-            return generator_func(*args, **kwargs)
-        else:
-            @wrapt.decorator
-            def return_func(wrapped_, instance_, args_, kwargs_):
-                results = wrapped_(*args_, **kwargs_)
-                if inspect.isgenerator(results):
-                    # unwind generator if there is one, this actually runs
-                    # any processing
-                    results = list(results)
-                # render summaries
-                if not result_xfm and result_renderer == 'tailored':
-                    # cannot render transformed results
-                    if hasattr(_func_class, 'custom_result_summary_renderer'):
-                        _func_class.custom_result_summary_renderer(results)
-                if return_type == 'item-or-list' and \
-                        len(results) < 2:
-                    return results[0] if results else None
-                else:
-                    return results
-
-            return return_func(generator_func)(*args, **kwargs)
-
-    return eval_func(func)
-
-
-def _process_results(
-        results, cmd_class,
-        action_summary, on_failure, incomplete_results,
-        result_renderer, result_xfm, result_filter, **kwargs):
-    # private helper pf @eval_results
-    # loop over results generated from some source and handle each
-    # of them according to the requested behavior (logging, rendering, ...)
-    for res in results:
-        if 'action' not in res:
-            # XXX Yarik has to no clue on how to track the origin of the
-            # record to figure out WTF, so he just skips it
-            continue
-        actsum = action_summary.get(res['action'], {})
-        if res['status']:
-            actsum[res['status']] = actsum.get(res['status'], 0) + 1
-            action_summary[res['action']] = actsum
-        ## log message, if a logger was given
-        # remove logger instance from results, as it is no longer useful
-        # after logging was done, it isn't serializable, and generally
-        # pollutes the output
-        res_lgr = res.pop('logger', None)
-        if isinstance(res_lgr, logging.Logger):
-            # didn't get a particular log function, go with default
-            res_lgr = getattr(res_lgr, default_logchannels[res['status']])
-        if res_lgr and 'message' in res:
-            msg = res['message']
-            msgargs = None
-            if isinstance(msg, tuple):
-                msgargs = msg[1:]
-                msg = msg[0]
-            if 'path' in res:
-                msg = '{} [{}({})]'.format(
-                    msg, res['action'], res['path'])
-            if msgargs:
-                # support string expansion of logging to avoid runtime cost
-                res_lgr(msg, *msgargs)
-            else:
-                res_lgr(msg)
-        ## error handling
-        # looks for error status, and report at the end via
-        # an exception
-        if on_failure in ('continue', 'stop') \
-                and res['status'] in ('impossible', 'error'):
-            incomplete_results.append(res)
-            if on_failure == 'stop':
-                # first fail -> that's it
-                # raise will happen after the loop
-                break
-        if result_filter:
-            try:
-                if not result_filter(res):
-                    raise ValueError('excluded by filter')
-            except ValueError as e:
-                lgr.debug('not reporting result (%s)', exc_str(e))
-                continue
-        ## output rendering
-        # TODO RF this in a simple callable that gets passed into this function
-        if result_renderer is None or result_renderer == 'disabled':
-            pass
-        elif result_renderer == 'default':
-            # TODO have a helper that can expand a result message
-            ui.message('{action}({status}): {path}{type}{msg}'.format(
-                action=ac.color_word(res['action'], ac.BOLD),
-                status=ac.color_status(res['status']),
-                path=relpath(res['path'],
-                             res['refds']) if res.get('refds', None) else res['path'],
-                type=' ({})'.format(
-                    ac.color_word(res['type'], ac.MAGENTA)
-                    ) if 'type' in res else '',
-                msg=' [{}]'.format(
-                    res['message'][0] % res['message'][1:]
-                    if isinstance(res['message'], tuple) else res['message'])
-                if 'message' in res else ''))
-        elif result_renderer in ('json', 'json_pp'):
-            ui.message(json.dumps(
-                {k: v for k, v in res.items()
-                 if k not in ('message', 'logger')},
-                sort_keys=True,
-                indent=2 if result_renderer.endswith('_pp') else None))
-        elif result_renderer == 'tailored':
-            if hasattr(cmd_class, 'custom_result_renderer'):
-                cmd_class.custom_result_renderer(res, **kwargs)
-        elif hasattr(result_renderer, '__call__'):
-            try:
-                result_renderer(res, **kwargs)
-            except Exception as e:
-                lgr.warn('Result rendering failed for: %s [%s]',
-                         res, exc_str(e))
-        else:
-            raise ValueError('unknown result renderer "{}"'.format(result_renderer))
-        if result_xfm:
-            res = result_xfm(res)
-            if res is None:
-                continue
-        yield res
+    # always come with annex when created from scratch
+    tbrepo = AnnexRepo(
+        path,
+        create=True,
+        create_sanity_checks=False,
+        # do not set backend here, to avoid a dedicated commit
+        backend=None,
+        # None causes version to be taken from config
+        version=None,
+        description=description,
+        git_opts=initopts,
+        fake_dates=fake_dates
+    )
+    # set the annex backend in .gitattributes as a staged change
+    tbrepo.set_default_backend(
+        cfg.obtain('datalad.repo.backend'),
+        persistent=True, commit=False)
+    add_to_git = {
+        tbrepo.pathobj / '.gitattributes': {
+            'type': 'file',
+            'state': 'added',
+        }
+    }
+    # make sure that v6 annex repos never commit content under .datalad
+    attrs_cfg = (
+        ('config', 'annex.largefiles', 'nothing'),
+    )
+    attrs = tbrepo.get_gitattributes(
+        [op.join('.datalad', i[0]) for i in attrs_cfg])
+    set_attrs = []
+    for p, k, v in attrs_cfg:
+        if not attrs.get(
+                op.join('.datalad', p), {}).get(k, None) == v:
+            set_attrs.append((p, {k: v}))
+    if set_attrs:
+        tbrepo.set_gitattributes(
+            set_attrs,
+            attrfile=op.join('.datalad', '.gitattributes'))
+
+    # prevent git annex from ever annexing .git* stuff (gh-1597)
+    attrs = tbrepo.get_gitattributes('.git')
+    if not attrs.get('.git', {}).get(
+            'annex.largefiles', None) == 'nothing':
+        tbrepo.set_gitattributes([
+            ('**/.git*', {'annex.largefiles': 'nothing'})])
+        # must use the repo.pathobj as this will have resolved symlinks
+        add_to_git[tbrepo.pathobj / '.gitattributes'] = {
+            'type': 'file',
+            'state': 'untracked'}
+    return tbrepo, add_to_git
```

### Comparing `datalad-0.9.3/datalad/interface/common_opts.py` & `datalad-1.0.0/datalad/interface/common_opts.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,46 +1,81 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 """Common interface options
 
 """
 
 __docformat__ = 'restructuredtext'
 
 from datalad.interface.results import known_result_xfms
 from datalad.support.param import Parameter
-from datalad.support.constraints import EnsureInt, EnsureNone, EnsureStr
-from datalad.support.constraints import EnsureChoice
-from datalad.support.constraints import EnsureCallable
-
+from datalad.support.constraints import (
+    EnsureBool,
+    EnsureCallable,
+    EnsureChoice,
+    EnsureInt,
+    EnsureNone,
+    EnsureStr,
+    EnsureStrPrefix,
+)
 
 location_description = Parameter(
     args=("-D", "--description",),
     constraints=EnsureStr() | EnsureNone(),
     doc="""short description to use for a dataset location. Its primary
     purpose is to help humans to identify a dataset copy (e.g., "mike's dataset
     on lab server"). Note that when a dataset is published, this information
     becomes available on the remote side.""")
 
 recursion_flag = Parameter(
     args=("-r", "--recursive",),
     action="store_true",
-    doc="""if set, recurse into potential subdataset""")
+    doc="""if set, recurse into potential subdatasets""")
 
 recursion_limit = Parameter(
-    args=("--recursion-limit",),
+    args=("-R", "--recursion-limit",),
     metavar="LEVELS",
     constraints=EnsureInt() | EnsureNone(),
-    doc="""limit recursion into subdataset to the given number of levels""")
+    doc="""limit recursion into subdatasets to the given number of levels""")
+
+contains = Parameter(
+    args=('--contains',),
+    metavar='PATH',
+    action='append',
+    doc="""limit to the subdatasets containing the
+    given path. If a root path of a subdataset is given, the last
+    considered dataset will be the subdataset itself.[CMD:  This
+    option can be given multiple times CMD][PY:  Can be a list with
+    multiple paths PY], in which case datasets that
+    contain any of the given paths will be considered.""",
+    constraints=EnsureStr() | EnsureNone())
+
+fulfilled = Parameter(
+    args=("--fulfilled",),
+    doc="""DEPRECATED: use [CMD: --state CMD][PY: `state` PY]
+    instead. If given, must be a boolean flag indicating whether
+    to consider either only locally present or absent datasets.
+    By default all subdatasets are considered regardless of their
+    status.""",
+    constraints=EnsureBool() | EnsureNone())
+
+dataset_state = Parameter(
+    args=("--state",),
+    doc="""indicate which (sub)datasets to consider: either only locally present,
+    absent, or any of those two kinds.
+    """,
+    # yoh: intentionally left out the description of default since might be
+    # command specific
+    constraints=EnsureChoice('present', 'absent', 'any'))
 
 shared_access_opt = Parameter(
     args=('--shared-access',),
     metavar='MODE',
     doc="""configure shared access to a dataset, see `git init --shared`
     documentation for complete details on the supported scenarios. Possible
     values include: 'false', 'true', 'group', and 'all'""")
@@ -106,39 +141,70 @@
     'ignore' let's datalad proceed as if the dataset would not have unsaved
     changes.""")
 
 nosave_opt = Parameter(
     args=("--nosave",),
     dest='save',
     action="store_false",
-    doc="""by default all modifications to a dataset are immediately saved. Given
+    doc="""by default all modifications to a dataset are immediately saved. Giving
     this option will disable this behavior.""")
 
 save_message_opt = Parameter(
     args=("-m", "--message",),
     metavar='MESSAGE',
     doc="""a description of the state or the changes made to a dataset.""",
     constraints=EnsureStr() | EnsureNone())
 
+message_file_opt = Parameter(
+    args=("-F", "--message-file"),
+    doc="""take the commit message from this file. This flag is
+    mutually exclusive with -m.""",
+    constraints=EnsureStr() | EnsureNone())
+
 reckless_opt = Parameter(
     args=("--reckless",),
-    action="store_true",
-    doc="""Set up the dataset to be able to obtain content in the
-    cheapest/fastest possible way, even if this poses a potential
-    risk the data integrity (e.g. hardlink files from a local clone
-    of the dataset). Use with care, and limit to "read-only" use
-    cases. With this flag the installed dataset will be marked as
-    untrusted.""")
+    # if no specific mode is given, set to auto
+    const='auto',
+    nargs='?',
+    # boolean types only for backward compatibility
+    constraints=
+    EnsureChoice(None, True, False, 'auto', 'ephemeral') | \
+    EnsureStrPrefix('shared-'),
+    metavar='auto|ephemeral|shared-...',
+    doc="""Obtain a dataset or subdatset and set it up in a potentially 
+    unsafe way for performance, or access reasons. 
+    Use with care, any dataset is marked as 'untrusted'.
+    The reckless mode is stored in a dataset's local configuration under
+    'datalad.clone.reckless', and will be inherited to any of its subdatasets.
+    Supported modes are:
+    ['auto']: hard-link files between local clones. In-place
+    modification in any clone will alter original annex content.
+    ['ephemeral']: symlink annex to origin's annex and discard local
+    availability info via git-annex-dead 'here' and declares this annex private.
+    Shares an annex between origin and clone w/o git-annex being aware of it.
+    In case of a change in origin you need to update the clone before you're
+    able to save new content on your end.
+    Alternative to 'auto' when hardlinks are not an option, or number of consumed
+    inodes needs to be minimized. Note that this mode can only be used with clones from
+    non-bare repositories or a RIA store! Otherwise two different annex object tree
+    structures (dirhashmixed vs dirhashlower) will be used simultaneously, and annex keys
+    using the respective other structure will be inaccessible.
+    ['shared-<mode>']: set up repository and annex permission to enable multi-user
+    access. This disables the standard write protection of annex'ed files.
+    <mode> can be any value support by 'git init --shared=', such as 'group', or
+    'all'.""")
 
 jobs_opt = Parameter(
     args=("-J", "--jobs"),
     metavar="NJOBS",
-    default=None,
+    default='auto',
     constraints=EnsureInt() | EnsureNone() | EnsureChoice('auto'),
-    doc="""how many parallel jobs (where possible) to use.""")
+    doc="""how many parallel jobs (where possible) to use. "auto" corresponds
+    to the number defined by 'datalad.runtime.max-annex-jobs' configuration
+    item""")
 
 verbose = Parameter(
     args=("-v", "--verbose",),
     action="store_true",
     doc="""print out more detailed information while executing a command""")
 
 
@@ -243,14 +309,15 @@
 
 eval_params = dict(
     return_type=Parameter(
         doc="""return value behavior switch. If 'item-or-list' a single
         value is returned instead of a one-item return value list, or a
         list in case of multiple return values. `None` is return in case
         of an empty list.""",
+        default='list',
         constraints=EnsureChoice('generator', 'list', 'item-or-list')),
     result_filter=Parameter(
         doc="""if given, each to-be-returned
         status dictionary is passed to this callable, and is only
         returned if the callable's return value does not
         evaluate to False or a ValueError exception is raised. If the given
         callable supports `**kwargs` it will additionally be passed the
@@ -263,43 +330,47 @@
         `result_filter`, as it can perform arbitrary transformation of the
         result value. This is mostly useful for top-level command invocations
         that need to provide the results in a particular format. Instead of
         a callable, a label for a pre-crafted result transformation can be
         given.""",
         constraints=EnsureChoice(*list(known_result_xfms.keys())) | EnsureCallable() | EnsureNone()),
     result_renderer=Parameter(
-        doc="""format of return value rendering on stdout""",
-        constraints=EnsureChoice('default', 'json', 'json_pp', 'tailored') | EnsureNone()),
+        doc="""select rendering mode command results.
+        'tailored' enables a command-specific rendering style that is typically
+        tailored to human consumption, if there is one for a specific
+        command, or otherwise falls back on the the 'generic' result renderer;
+        'generic' renders each result in one line  with key info like action,
+        status, path, and an optional message);
+        'json' a complete JSON line serialization of the full result record;
+        'json_pp' like 'json', but pretty-printed spanning multiple lines;
+        'disabled' turns off result rendering entirely;
+        '<template>' reports any value(s) of any result properties in any
+        format indicated by the template (e.g. '{path}', compare with JSON
+        output for all key-value choices). The template syntax follows the
+        Python "format() language". It is possible to report individual
+        dictionary values, e.g. '{metadata[name]}'. If a 2nd-level key contains
+        a colon, e.g. 'music:Genre', ':' must be substituted by '#' in the
+        template, like so: '{metadata[music#Genre]}'.""",
+        default='tailored'),
     on_failure=Parameter(
         doc="""behavior to perform on failure: 'ignore' any failure is reported,
         but does not cause an exception; 'continue' if any failure occurs an
         exception will be raised at the end, but processing other actions will
         continue for as long as possible; 'stop': processing will stop on first
         failure and an exception is raised. A failure is any result with status
         'impossible' or 'error'. Raised exception is an IncompleteResultsError
         that carries the result dictionaries of the failures in its `failed`
         attribute.""",
+        default='continue',
         constraints=EnsureChoice('ignore', 'continue', 'stop')),
-    run_before=Parameter(
-        doc="""DataLad plugin to run before the command. PLUGINSPEC is a list
-        comprised of a plugin name plus optional 2-tuples of key-value pairs
-        with arguments for the plugin call (see `plugin` command documentation
-        for details).
-        PLUGINSPECs must be wrapped in list where each item configures
-        one plugin call. Plugins are called in the order defined by this list.
-        For running plugins that require a `dataset` argument it is important
-        to provide the respective dataset as the `dataset` argument of the main
-        command, if it is not in the list of plugin arguments."""),
-    run_after=Parameter(
-        doc="""Like `run_before`, but plugins are executed after the main command
-        has finished."""),
 )
 
-eval_defaults = dict(
-    return_type='list',
-    result_filter=None,
-    result_renderer=None,
-    result_xfm=None,
-    on_failure='continue',
-    run_before=None,
-    run_after=None,
-)
+eval_defaults = {
+    k: p.cmd_kwargs.get('default', None)
+    for k, p in eval_params.items()
+}
+"""\
+.. deprecated:: 0.16
+   This variable will be removed in a future release. The default values for
+   all Parameters (possibly overriding by command-specific settings) are now
+   available as :class:`Interface` attributes.
+"""
```

### Comparing `datalad-0.9.3/datalad/interface/annotate_paths.py` & `datalad-1.0.0/datalad/core/distributed/clone.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,757 +1,914 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""Plumbing command for analyzing and acting upon input paths"""
+"""Plumbing command for dataset installation"""
 
-__docformat__ = 'restructuredtext'
-
-from collections import defaultdict
 
 import logging
-import textwrap
+from argparse import REMAINDER
+from typing import Dict
 
-from os import curdir
-from os.path import join as opj
-from os.path import lexists
-from os.path import isdir
-from os.path import dirname
-from os.path import pardir
-from os.path import normpath
-from os.path import sep as dirsep
-
-from datalad.interface.base import Interface
-from datalad.interface.utils import eval_results
-from datalad.interface.base import build_doc
+from datalad.cmd import CommandError
+from datalad.config import ConfigManager
+from datalad.distribution.dataset import (
+    Dataset,
+    EnsureDataset,
+    datasetmethod,
+    require_dataset,
+    resolve_path,
+)
+from datalad.interface.base import (
+    Interface,
+    build_doc,
+    eval_results
+)
+from datalad.interface.common_opts import (
+    location_description,
+    reckless_opt,
+)
 from datalad.interface.results import get_status_dict
-from datalad.support.constraints import EnsureStr
-from datalad.support.constraints import EnsureBool
-from datalad.support.constraints import EnsureNone
+from datalad.support.annexrepo import AnnexRepo
+from datalad.support.constraints import (
+    EnsureKeyChoice,
+    EnsureNone,
+    EnsureStr,
+)
+from datalad.support.exceptions import CapturedException
+from datalad.support.network import (
+    RI,
+    PathRI,
+)
 from datalad.support.param import Parameter
-from datalad.support.gitrepo import GitRepo
-from datalad.interface.common_opts import recursion_flag
-from datalad.interface.common_opts import recursion_limit
-
-from datalad.distribution.dataset import Dataset
-from datalad.distribution.dataset import resolve_path
-from datalad.distribution.dataset import EnsureDataset
-from datalad.distribution.dataset import datasetmethod
+from datalad.utils import (
+    knows_annex,
+    PurePath,
+    rmtree,
+)
+
+from .clone_utils import (  # needed because other code imports it from here
+    _check_autoenable_special_remotes,
+    _format_clone_errors,
+    _generate_candidate_clone_sources,
+    _get_remote,
+    _get_tracking_source,
+    _map_urls,
+    _test_existing_clone_target,
+    _try_clone_candidates,
+    decode_source_spec,
+    # RIA imports needed b/c datalad-next imports it from here ATM;
+    # Remove after core was released and next dropped the ria patch.
+    postclone_preannex_cfg_ria,
+    postclonecfg_ria,
+)
 
-from datalad.utils import get_dataset_root
-from datalad.utils import with_pathsep as _with_sep
-from datalad.utils import path_startswith
-from datalad.utils import path_is_subpath
-from datalad.utils import assure_list
+__docformat__ = 'restructuredtext'
 
-from datalad.consts import PRE_INIT_COMMIT_SHA
+lgr = logging.getLogger('datalad.core.distributed.clone')
 
 
-lgr = logging.getLogger('datalad.interface.annotate_paths')
+@build_doc
+class Clone(Interface):
+    """Obtain a dataset (copy) from a URL or local directory
 
+    The purpose of this command is to obtain a new clone (copy) of a dataset
+    and place it into a not-yet-existing or empty directory. As such `clone`
+    provides a strict subset of the functionality offered by `install`. Only a
+    single dataset can be obtained, and immediate recursive installation of
+    subdatasets is not supported. However, once a (super)dataset is installed
+    via `clone`, any content, including subdatasets can be obtained by a
+    subsequent `get` command.
+
+    Primary differences over a direct `git clone` call are 1) the automatic
+    initialization of a dataset annex (pure Git repositories are equally
+    supported); 2) automatic registration of the newly obtained dataset as a
+    subdataset (submodule), if a parent dataset is specified; 3) support
+    for additional resource identifiers (DataLad resource identifiers as used
+    on datasets.datalad.org, and RIA store URLs as used for store.datalad.org
+    - optionally in specific versions as identified by a branch or a tag; see
+    examples); and 4) automatic configurable generation of alternative access
+    URL for common cases (such as appending '.git' to the URL in case the
+    accessing the base URL failed).
+
+    In case the clone is registered as a subdataset, the original URL passed to
+    `clone` is recorded in `.gitmodules` of the parent dataset in addition
+    to the resolved URL used internally for git-clone. This allows to preserve
+    datalad specific URLs like ria+ssh://... for subsequent calls to `get` if
+    the subdataset was locally removed later on.
+
+    || PYTHON >>By default, the command returns a single Dataset instance for
+    an installed dataset, regardless of whether it was newly installed ('ok'
+    result), or found already installed from the specified source ('notneeded'
+    result).<< PYTHON ||
+
+    URL mapping configuration
+
+    'clone' supports the transformation of URLs via (multi-part) substitution
+    specifications. A substitution specification is defined as a configuration
+    setting 'datalad.clone.url-substition.<seriesID>' with a string containing
+    a match and substitution expression, each following Python's regular
+    expression syntax. Both expressions are concatenated to a single string
+    with an arbitrary delimiter character. The delimiter is defined by
+    prefixing the string with the delimiter. Prefix and delimiter are stripped
+    from the expressions (Example: ",^http://(.*)$,https://\\1").  This setting
+    can be defined multiple times, using the same '<seriesID>'.  Substitutions
+    in a series will be applied incrementally, in order of their definition.
+    The first substitution in such a series must match, otherwise no further
+    substitutions in a series will be considered. However, following the first
+    match all further substitutions in a series are processed, regardless
+    whether intermediate expressions match or not. Substitution series themselves
+    have no particular order, each matching series will result in a candidate
+    clone URL. Consequently, the initial match specification in a series should
+    be as precise as possible to prevent inflation of candidate URLs.
 
-def annotated2ds_props(annotated):
-    """Return a dict with properties of all datasets in `annotated`.
+    .. seealso::
 
-    Returns
-    -------
-    dict
+      :ref:`handbook:3-001`
+        More information on Remote Indexed Archive (RIA) stores
     """
-    props = {}
-    for a in annotated:
-        if a.get('type', None) == 'dataset':
-            dp = props.get(a['path'], {})
-            dp.update(a)
-            props[a['path']]
-    return props
+    # by default ignore everything but install results
+    # i.e. no "add to super dataset"
+    result_filter = EnsureKeyChoice('action', ('install',))
+    # very frequently this command will yield exactly one installed dataset
+    # spare people the pain of going through a list by default
+    return_type = 'item-or-list'
+    # as discussed in #1409 and #1470, we want to return dataset instances
+    # matching what is actually available after command completion (and
+    # None for any failed dataset installation)
+    result_xfm = 'successdatasets-or-none'
+
+    _examples_ = [
+        dict(text="Install a dataset from GitHub into the current directory",
+             code_py="clone("
+             "source='https://github.com/datalad-datasets/longnow"
+             "-podcasts.git')",
+             code_cmd="datalad clone "
+             "https://github.com/datalad-datasets/longnow-podcasts.git"),
+        dict(text="Install a dataset into a specific directory",
+             code_py="""\
+             clone(source='https://github.com/datalad-datasets/longnow-podcasts.git',
+                   path='myfavpodcasts')""",
+             code_cmd="""\
+             datalad clone https://github.com/datalad-datasets/longnow-podcasts.git \\
+             myfavpodcasts"""),
+        dict(text="Install a dataset as a subdataset into the current dataset",
+             code_py="""\
+             clone(dataset='.',
+                   source='https://github.com/datalad-datasets/longnow-podcasts.git')""",
+             code_cmd="datalad clone -d . "
+             "https://github.com/datalad-datasets/longnow-podcasts.git"),
+        dict(text="Install the main superdataset from datasets.datalad.org",
+             code_py="clone(source='///')",
+             code_cmd="datalad clone ///"),
+        dict(text="Install a dataset identified by a literal alias from store.datalad.org",
+             code_py="clone(source='ria+http://store.datalad.org#~hcp-openaccess')",
+             code_cmd="datalad clone ria+http://store.datalad.org#~hcp-openaccess"),
+        dict(
+            text="Install a dataset in a specific version as identified by a "
+                 "branch or tag name from store.datalad.org",
+            code_py="clone(source='ria+http://store.datalad.org#76b6ca66-36b1-11ea-a2e6-f0d5bf7b5561@myidentifier')",
+            code_cmd="datalad clone ria+http://store.datalad.org#76b6ca66-36b1-11ea-a2e6-f0d5bf7b5561@myidentifier"),
+        dict(
+            text="Install a dataset with group-write access permissions",
+            code_py=\
+            "clone(source='http://example.com/dataset', reckless='shared-group')",
+            code_cmd=\
+            "datalad clone http://example.com/dataset --reckless shared-group"),
+    ]
 
+    _params_ = dict(
+        dataset=Parameter(
+            args=("-d", "--dataset"),
+            doc="""(parent) dataset to clone into. If given, the newly cloned
+            dataset is registered as a subdataset of the parent. Also, if given,
+            relative paths are interpreted as being relative to the parent
+            dataset, and not relative to the working directory.""",
+            constraints=EnsureDataset() | EnsureNone()),
+        source=Parameter(
+            args=("source",),
+            metavar='SOURCE',
+            doc="""URL, DataLad resource identifier, local path or instance of
+            dataset to be cloned""",
+            constraints=EnsureStr()),
+        path=Parameter(
+            args=("path",),
+            metavar='PATH',
+            nargs="?",
+            doc="""path to clone into.  If no `path` is provided a
+            destination path will be derived from a source URL
+            similar to :command:`git clone`"""),
+        git_clone_opts=Parameter(
+            args=("git_clone_opts",),
+            metavar='GIT CLONE OPTIONS',
+            nargs=REMAINDER,
+            doc="""[PY: A list of command line arguments PY][CMD: Options CMD]
+            to pass to :command:`git clone`. [CMD: Any argument specified after
+            SOURCE and the optional PATH will be passed to git-clone. CMD] Note
+            that not all options will lead to viable results. For example
+            '--single-branch' will not result in a functional annex repository
+            because both a regular branch and the git-annex branch are
+            required. Note that a version in a RIA URL takes precedence over
+            '--branch'."""),
+        description=location_description,
+        reckless=reckless_opt,
+    )
 
-def annotated2content_by_ds(annotated, refds_path):
-    """Helper to convert annotated paths into an old-style content_by_ds dict
-
-    Only items with an `status` property value not equal to 'ok', 'notneeded',
-    'impossible', or 'error' are sorted. All others are considered as
-    already processed and are returned in a separate list.
-
-    Parameters
-    ----------
-    annotated : list or generator
-      Dicts with annotated path information.
-    refds_path : str
-      Path to the reference dataset the original path annotation was based on.
-
-    Returns
-    -------
-    dict, dict, list, list
-      Dict keys are dataset paths, values are full info dicts.
-      The keys in the second dict are paths to dataset, values are
-      dicts with all known properties about those datasets.
-      The first list contains all already "processed" results, which
-      typically need to be re-yielded. The second list contains items (same
-      type as dict values) for all annotated paths that have no associated
-      parent dataset (i.e. nondataset paths) -- this list will be empty by
-      default, unless `nondataset_path_status` was set to ''."""
-    content_by_ds = defaultdict(list)
-    ds_props = {}
-    nondataset_paths = []
-    completed = []
-    for r in annotated:
-        r_path = r['path']
-        if r.get('type', None) == 'dataset':
-            # collect all properties of all known datasets from the annotated
-            # paths
-            dp = ds_props.get(r_path, {})
-            dp.update(r)
-            ds_props[r_path] = dp
-        if r.get('status', None) in ('ok', 'notneeded', 'impossible', 'error'):
-            completed.append(r)
-            continue
-        parentds = r.get('parentds', None)
-        appendto = []  # what entries, if any, to append r to
-        if r.get('type', None) == 'dataset':
-            # to dataset handling first, it is the more complex beast
-            orig_request = r.get('orig_request', None)
-            if parentds is None or refds_path is None or \
-                    r.get('process_content', False) or (orig_request and (
-                    orig_request == curdir or
-                    orig_request.endswith(dirsep) or
-                    orig_request.endswith('{}{}'.format(dirsep, curdir)))):
-                # a dataset that floats by on its own OR
-                # behave similar to rsync, a trailing '/' indicates the
-                # content rather then the dataset itself
-                # in both cases we want to process this part as part
-                # of the same dataset, and not any potential parent
-                appendto += [r_path]
-            if parentds and refds_path and \
-                    path_startswith(parentds, refds_path):
-                # put also in parentds record if there is any, and the parent
-                # is underneath or identical to the reference dataset
-                appendto += [parentds]
-        else:
-            # files and dirs
-            # common case, something with a parentds
-            appendto += [parentds]
-
-        for e in appendto:
-            content_by_ds[e] += [r]
-
-    return content_by_ds, ds_props, completed, nondataset_paths
-
-
-def yield_recursive(ds, path, action, recursion_limit):
-    # make sure we get everything relevant in all _checked out_
-    # subdatasets, obtaining of previously unavailable subdataset
-    # is elsewhere
-    for subd_res in ds.subdatasets(
-            recursive=True,
-            recursion_limit=recursion_limit,
-            return_type='generator'):
-        # this check is not the same as subdatasets --contains=path
-        # because we want all subdataset below a path, not just the
-        # containing one
-        if path_is_subpath(subd_res['path'], path):
-            # this subdatasets is underneath the search path
-            # be careful to not overwrite anything, in case
-            # this subdataset has been processed before
-            subd_res['action'] = action
-            # mark as "notprocessed"
-            subd_res['status'] = ''
-            # we know that this is a known subdataset, that is how
-            # we got here, make a record
-            subd_res['registered_subds'] = True
-            yield subd_res
-
-
-def get_modified_subpaths(aps, refds, revision, recursion_limit=None,
-                          report_no_revision_change=True,
-                          report_untracked='all'):
-    """
-    Parameters
-    ----------
-    aps : list
-    refds : Dataset
-    revision : str
-      Commit-ish
-    """
-    # TODO needs recursion limit
-    # NOTE this is implemented as a generator despite that fact that we need
-    # to sort through _all_ the inputs initially, diff'ing each involved
-    # dataset takes time that we can use to already act on intermediate
-    # result paths, without having to wait for 100% completion
-    if revision is None:
-        # we want all, subds not matching the ref are assumed to have been
-        # sorted out before (e.g. one level up)
-        for r in aps:
-            yield r
-
-    # life is simple: we diff the base dataset
-    modified = []
-    for r in refds.diff(
-            # we cannot really limit the diff paths easily because we might get
-            # or miss content (e.g. subdatasets) if we don't figure out which
-            # ones are known -- and we don't want that
+    @staticmethod
+    @datasetmethod(name='clone')
+    @eval_results
+    def __call__(
+            source,
             path=None,
-            # `revision` can be anything that Git support for `diff`
-            # `True` is code for diff without revision
-            revision=revision if revision is not True else None,
-            # it is important that staged is False, otherwise we would miss unstaged
-            # changes when e.g. diffing against HEAD (save does that)
-            staged=False,
-            # we might want to consider putting 'untracked' here
-            # maybe that is a little faster, not tested yet
-            ignore_subdatasets='none',
-            # by default, we want to see any individual untracked file, this simplifies further
-            # processing dramatically, but may require subsequent filtering
-            # in order to avoid flooding user output with useless info
-            report_untracked=report_untracked,
-            # no recursion, we needs to update `revision` for every subdataset
-            # before we can `diff`
-            recursive=False,
-            return_type='generator',
-            result_renderer=None,
-            # need to be able to yield the errors
-            on_failure='ignore'):
-        if r['status'] in ('impossible', 'error'):
-            # something unexpected, tell daddy
-            yield r
-            continue
-        # if asked, and no change in revision -- skip
-        if not report_no_revision_change \
-                and (r.get('revision_src') or r.get('revision')) \
-                and (r.get('revision_src') == r.get('revision')):
-            continue
-        r['status'] = ''
-        modified.append(r)
+            git_clone_opts=None,
+            *,
+            dataset=None,
+            description=None,
+            reckless=None,
+        ):
+        # did we explicitly get a dataset to install into?
+        # if we got a dataset, path will be resolved against it.
+        # Otherwise path will be resolved first.
+        ds = require_dataset(
+            dataset, check_installed=True, purpose='clone') \
+            if dataset is not None else dataset
+        refds_path = ds.path if ds else None
+
+        # legacy compatibility
+        if reckless is True:
+            # so that we can forget about how things used to be
+            reckless = 'auto'
+
+        if isinstance(source, Dataset):
+            source = source.path
+
+        if source == path:
+            # even if they turn out to be identical after resolving symlinks
+            # and more sophisticated witchcraft, it would still happily say
+            # "it appears to be already installed", so we just catch an
+            # obviously pointless input combination
+            raise ValueError(
+                "clone `source` and destination `path` are identical [{}]. "
+                "If you are trying to add a subdataset simply use `save`".format(
+                    path))
+
+        if path is not None:
+            path = resolve_path(path, dataset)
+
+        # derive target from source:
+        if path is None:
+            # we got nothing but a source. do something similar to git clone
+            # and derive the path from the source and continue
+            # since this is a relative `path`, resolve it:
+            # we are not going to reuse the decoded URL, as this is done for
+            # all source candidates in clone_dataset(), we just use to determine
+            # a destination path here in order to perform a bunch of additional
+            # checks that shall not pollute the helper function
+            source_ = decode_source_spec(
+                source, cfg=None if ds is None else ds.config)
+            path = resolve_path(source_['default_destpath'], dataset)
+            lgr.debug("Determined clone target path from source")
+        lgr.debug("Resolved clone target path to: '%s'", path)
+
+        # there is no other way -- my intoxicated brain tells me
+        assert(path is not None)
+
+        result_props = dict(
+            action='install',
+            logger=lgr,
+            refds=refds_path,
+            source_url=source)
 
-    if not len(modified):
-        # nothing modified nothing to report
-        return
+        try:
+            # this will implicitly cause pathlib to run a bunch of checks
+            # whether the present path makes any sense on the platform
+            # we are running on -- we don't care if the path actually
+            # exists at this point, but we want to abort early if the path
+            # spec is determined to be useless
+            # we can do strict=False since we are 3.6+
+            path.resolve(strict=False)
+        except OSError as e:
+            ce = CapturedException(e)
+            yield get_status_dict(
+                status='error',
+                path=path,
+                message=('cannot handle target path: %s', ce),
+                exception=ce,
+                **result_props)
+            return
 
-    # now we can grab the APs that are in this dataset and yield them
-    for ap in aps:
-        # need to preserve pristine info first
-        ap = ap if isinstance(ap, dict) else rawpath2ap(ap, refds.path)
-        for m in modified:
-            if ap['path'] == m['path']:
-                # is directly modified, yield input AP
-                # but update with what we learned about the modification
-                ap.update(m)
-                yield ap
-                break
-            if path_is_subpath(m['path'], ap['path']):
-                # a modified path is underneath this AP
-                # yield the modified one instead
-                yield m
-                continue
+        destination_dataset = Dataset(path)
+        result_props['ds'] = destination_dataset
 
-    mod_subs = [m for m in modified if m.get('type', None) == 'dataset']
-    if not mod_subs or (recursion_limit is not None and recursion_limit < 1):
-        return
+        if ds is not None and ds.pathobj not in path.parents:
+            yield get_status_dict(
+                status='error',
+                message=("clone target path '%s' not in specified target dataset '%s'",
+                         path, ds),
+                **result_props)
+            return
 
-    aps = [ap if isinstance(ap, dict) else rawpath2ap(ap, refds.path) for ap in aps]
-    # now for all submodules that were found modified
-    for sub in [m for m in modified if m.get('type', None) == 'dataset']:
-        sub_path_ = _with_sep(sub['path'])
-        # these AP match something inside this submodule, or the whole submodule
-        sub_aps = [ap for ap in aps if _with_sep(ap['path']).startswith(sub_path_)]
-        if not sub_aps:
-            continue
-        # we are interested in the modifications within this subdataset
-        # from the state we previously had on record, till the state
-        # we have in record now
-        diff_range = '{}..{}'.format(
-            sub['revision_src'] if sub['revision_src'] else PRE_INIT_COMMIT_SHA,
-            sub['revision'] if sub['revision'] else '')
-        if sub['revision_src'] and sub['revision_src'] == sub['revision']:
-            # this is a special case, where subdataset reported changes without
-            # a change in state/commit -- this is code for uncommited changes
-            # in the subdataset (including staged ones). In such a case, we
-            # must not provide a diff range, but only the source commit we want
-            # to diff against
-            # XXX if this is changed, likely the same logic in diff needs
-            # changing too!
-            diff_range = sub['revision_src']
-
-        for r in get_modified_subpaths(
-                sub_aps,
-                Dataset(sub['path']),
-                diff_range,
-                recursion_limit=(recursion_limit - 1) if recursion_limit is not None else None ):
+        # perform the actual cloning operation
+        clone_failure = False
+        for r in clone_dataset(
+                [source],
+                destination_dataset,
+                reckless,
+                description,
+                result_props,
+                cfg=None if ds is None else ds.config,
+                clone_opts=git_clone_opts,
+                ):
+            if r['status'] in ['error', 'impossible']:
+                clone_failure = True
             yield r
 
+        if clone_failure:
+            # do not proceed saving anything if cloning failed
+            return
 
-def rawpath2ap(path, refds_path):
-    orig_path_request = path
-    # this is raw, resolve
-    path = resolve_path(path, refds_path)
-    # collect info on this path
-    path_props = dict(
-        path=path,
-        # path was requested as input, and not somehow discovered
-        raw_input=True,
-        # make a record of what actually came in, sorting into
-        # dataset might later need to distinguish between a path
-        # that pointed to a dataset as a whole vs. a path that
-        # pointed to the dataset's content -- just do not destroy
-        # any information on the way down
-        orig_request=orig_path_request)
-    return path_props
-
-
-# "complete" list of recognized properties, there could be other ones
-# as any command can inject anything
-known_props = {
-    'annexkey': 'annex key for the content of a file',
-    'action': 'label of the action that triggered the path annotation',
-    'logger': 'logger for reporting a message',
-    'message': 'message (plus possible tsring expansion arguments)',
-    'orig_request': 'original input by which a path was determined',
-    'parentds':
-        'path of dataset containing the annotated path (superdataset for subdatasets)',
-    'path': 'absolute path that is annotated',
-    'process_content': 'flag that content underneath the path is to be processed',
-    'process_updated_only':
-        'flag that only known dataset components are to be processed',
-    'raw_input': 'flag whether this path was given as raw (non-annotated) input',
-    'revision': 'the recorded commit for a subdataset in a superdataset',
-    'revision_descr': 'a human-readable description of `revision`',
-    'refds': 'path of a reference/base dataset the annotated path is part of',
-    'registered_subds': 'flag whether a dataset is known to be a true subdataset of `parentds`',
-    'source_url': 'URL a dataset was installed from',  # unify with `url`?
-    'staged': 'flag whether a path is known to be "staged" in its containing dataset',
-    'state':
-        'state indicator for a path in its containing dataset (clean, modified, absent (also for files), conflict)',
-    'status': 'action result status (ok, notneeded, impossible, error)',
-    'type': 'nature of the path (file, directory, dataset)',
-    'url': 'registered URL for a subdataset in a superdataset',  # unify with `source_url`?
-}
-
-
-@build_doc
-class AnnotatePaths(Interface):
-    """Analyze and act upon input paths
+        # TODO handle any 'version' property handling and verification using a
+        # dedicated public helper
 
-    Given paths (or more generally location requests) are inspected and
-    annotated with a number of properties. A list of recognized properties
-    is provided below.
+        if ds is not None:
+            # we created a dataset in another dataset
+            # -> make submodule
+            actually_saved_subds = False
+            for r in ds.save(
+                    path,
+                    # Note, that here we know we don't save anything but a new
+                    # subdataset. Hence, don't go with default commit message,
+                    # but be more specific.
+                    message="[DATALAD] Added subdataset",
+                    return_type='generator',
+                    result_filter=None,
+                    result_xfm=None,
+                    result_renderer='disabled',
+                    on_failure='ignore'):
+                actually_saved_subds = actually_saved_subds or (
+                        r['action'] == 'save' and
+                        r['type'] == 'dataset' and
+                        r['refds'] == ds.path and
+                        r['status'] == 'ok')
+                yield r
+
+            # Modify .gitmodules to contain originally given url. This is
+            # particularly relevant for postclone routines on a later `get`
+            # for that subdataset. See gh-5256.
+
+            if isinstance(RI(source), PathRI):
+                # ensure posix paths; Windows paths would neither be meaningful
+                # as a committed path nor are they currently stored correctly
+                # (see gh-7182).
+                # Restricted to when 'source' is identified as a path, b/c this
+                # wouldn't work with file-URLs (ria or not):
+                #
+                # PureWindowsPath("file:///C:/somewhere/path").as_posix() ->
+                # 'file:/C:/somewhere/path'
+                source = PurePath(source).as_posix()
+            if actually_saved_subds:
+                # New subdataset actually saved. Amend the modification
+                # of .gitmodules.
+                # Note, that we didn't allow deviating from git's default
+                # behavior WRT a submodule's name vs its path when we made this
+                # a new subdataset.
+                subds_name = path.relative_to(ds.pathobj)
+                ds.repo.call_git(
+                    ['config',
+                     '--file',
+                     '.gitmodules',
+                     '--replace-all',
+                     'submodule.{}.{}'.format(subds_name,
+                                              "datalad-url"),
+                     source]
+                )
+                yield from ds.save('.gitmodules',
+                                   amend=True, to_git=True,
+                                   result_renderer='disabled',
+                                   return_type='generator')
+            else:
+                # We didn't really commit. Just call `subdatasets`
+                # in that case to have the modification included in the
+                # post-clone state (whatever that may be).
+                ds.subdatasets(path, set_property=[("datalad-url", source)])
+
+
+def clone_dataset(
+        srcs,
+        destds,
+        reckless=None,
+        description=None,
+        result_props=None,
+        cfg=None,
+        checkout_gitsha=None,
+        clone_opts=None):
+    """Internal helper to perform cloning without sanity checks (assumed done)
 
-    || PYTHON >>Input `paths` for this command can either be un-annotated
-    (raw) path strings, or already (partially) annotated paths. In the latter
-    case, further annotation is limited to yet-unknown properties, and is
-    potentially faster than initial annotation.<< PYTHON ||
+    This helper does not handle any saving of subdataset modification or adding
+    in a superdataset.
 
+    Parameters
+    ----------
+    srcs : list
+      Any suitable clone source specifications (paths, URLs)
+    destds : Dataset
+      Dataset instance for the clone destination
+    reckless : {None, 'auto', 'ephemeral', 'shared-...'}, optional
+      Mode switch to put cloned dataset into unsafe/throw-away configurations, i.e.
+      sacrifice data safety for performance or resource footprint. When None
+      and `cfg` is specified, use the value of `datalad.clone.reckless`.
+    description : str, optional
+      Location description for the annex of the dataset clone (if there is any).
+    result_props : dict, optional
+      Default properties for any yielded result, passed on to get_status_dict().
+    cfg : ConfigManager, optional
+      Configuration for parent dataset. This will be queried instead
+      of the global DataLad configuration.
+    checkout_gitsha : str, optional
+      If given, a specific commit, identified by shasum, will be checked out after
+      cloning. A dedicated follow-up fetch will be performed, if the initial clone
+      did not obtain the commit object. Should the checkout of the target commit
+      cause a detached HEAD, the previously active branch will be reset to the
+      target commit.
+    clone_opts : list of str, optional
+      Options passed to git-clone. Note that for RIA URLs, the version is
+      translated to a --branch argument, and that will take precedence over a
+      --branch argument included in this value.
 
-    *Recognized path properties*
+    Yields
+    ------
+    dict
+      DataLad result records
+    """
+    # apply the two in-house patches, do local to avoid circular imports
+    from . import (
+        clone_ephemeral,
+        clone_ria,
+    )
+
+    if not result_props:
+        # in case the caller had no specific idea on how results should look
+        # like, provide sensible defaults
+        result_props = dict(
+            action='install',
+            logger=lgr,
+            ds=destds,
+        )
+    else:
+        result_props = result_props.copy()
+
+    candidate_sources = _generate_candidate_clone_sources(
+        destds, srcs, cfg)
+
+    # important test!
+    # based on this `rmtree` will happen below after failed clone
+    dest_path_existed, stop_props = _test_existing_clone_target(
+        destds, candidate_sources)
+    if stop_props:
+        # something happened that indicates we cannot continue
+        # yield and return
+        result_props.update(stop_props)
+        yield get_status_dict(**result_props)
+        return
 
-    {proplist}
+    if reckless is None and cfg:
+        # if reckless is not explicitly given, but we operate on a
+        # superdataset, query whether it has been instructed to operate
+        # in a reckless mode, and inherit it for the coming clone
+        reckless = cfg.get('datalad.clone.reckless', None)
+
+    last_candidate, error_msgs, stop_props = _try_clone_candidates(
+        destds=destds,
+        candidate_sources=candidate_sources,
+        clone_opts=clone_opts or [],
+        dest_path_existed=dest_path_existed,
+    )
+    if stop_props:
+        # no luck, report and stop
+        result_props.update(stop_props)
+        yield get_status_dict(**result_props)
+        return
+    else:
+        # we can record the last attempt as the candidate URL that gave
+        # a successful clone
+        result_props['source'] = last_candidate
+
+    if not destds.is_installed():
+        # we do not have a clone, stop, provide aggregate error message
+        # covering all attempts
+        yield get_status_dict(
+            status='error',
+            message=_format_clone_errors(
+                destds, error_msgs, last_candidate['giturl']),
+            **result_props)
+        return
 
-    In the case of enabled modification detection the results may contain
-    additional properties regarding the nature of the modification. See the
-    documentation of the `diff` command for details.
+    #
+    # At minimum all further processing is all candidate for extension
+    # patching.  wrap the whole thing in try-except, catch any exceptions
+    # report it as an error results `rmtree` any intermediate and return
+    #
+    try:
+        yield from _post_gitclone_processing_(
+            destds=destds,
+            cfg=cfg,
+            gitclonerec=last_candidate,
+            reckless=reckless,
+            checkout_gitsha=checkout_gitsha,
+            description=description,
+        )
+    except Exception as e:
+        ce = CapturedException(e)
+        # the rational for turning any exception into an error result is that
+        # we are hadly able to distinguish user-error from an other errors
+        yield get_status_dict(
+            status='error',
+            # XXX A test in core insists on the wrong message type to be used
+            #error_message=ce.message,
+            message=ce.message,
+            exception=ce,
+            **result_props,
+        )
+        rmtree(destds.path, children_only=dest_path_existed)
+        return
 
+    # yield successful clone of the base dataset now, as any possible
+    # subdataset clone down below will not alter the Git-state of the
+    # parent
+    yield get_status_dict(status='ok', **result_props)
+
+
+def _post_gitclone_processing_(
+        *,
+        destds: Dataset,
+        cfg: ConfigManager,
+        gitclonerec: Dict,
+        reckless: None or str,
+        checkout_gitsha: None or str,
+        description: None or str,
+):
+    """Perform git-clone post-processing
+
+    This is helper is called immediately after a Git clone was established.
+
+    The properties of that clone are passed via `gitclonerec`.
+
+    Yields
+    ------
+    DataLad result records
     """
-    _docs_ = dict(
-        proplist='\n\n    '.join(
-            '"{}"\n{}'.format(
-                k,
-                textwrap.fill(known_props[k],
-                              initial_indent='        ',
-                              subsequent_indent='        '))
-            for k in sorted(known_props)))
+    dest_repo = destds.repo
+    remote = _get_remote(dest_repo)
 
-    _params_ = dict(
-        path=Parameter(
-            args=("path",),
-            metavar="PATH",
-            doc="""path to be annotated""",
-            nargs="*",
-            constraints=EnsureStr() | EnsureNone()),
-        dataset=Parameter(
-            args=("-d", "--dataset"),
-            doc="""an optional reference/base dataset for the paths""",
-            constraints=EnsureDataset() | EnsureNone()),
-        recursive=recursion_flag,
-        recursion_limit=recursion_limit,
-        action=Parameter(
-            args=("--action",),
-            metavar="LABEL",
-            doc="""an "action" property value to include in the
-            path annotation""",
-            constraints=EnsureStr() | EnsureNone()),
-        unavailable_path_status=Parameter(
-            args=("--unavailable-path-status",),
-            metavar="LABEL",
-            doc="""a "status" property value to include in the
-            annotation for paths that are underneath a dataset, but
-            do not exist on the filesystem""",
-            constraints=EnsureStr() | EnsureNone()),
-        unavailable_path_msg=Parameter(
-            args=("--unavailable-path-msg",),
-            metavar="message",
-            doc="""a "message" property value to include in the
-            annotation for paths that are underneath a dataset, but
-            do not exist on the filesystem""",
-            constraints=EnsureStr() | EnsureNone()),
-        nondataset_path_status=Parameter(
-            args=("--nondataset-path-status",),
-            metavar="LABEL",
-            doc="""a "status" property value to include in the
-            annotation for paths that are not underneath any dataset""",
-            constraints=EnsureStr() | EnsureNone()),
-        force_parentds_discovery=Parameter(
-            args=("--no-parentds-discovery",),
-            dest='force_parentds_discovery',
-            action='store_false',
-            doc="""Flag to disable reports of parent dataset information for any
-            path, in particular dataset root paths. Disabling saves on command
-            run time, if this information is not needed."""),
-        force_subds_discovery=Parameter(
-            args=("--no-subds-discovery",),
-            action='store_false',
-            dest='force_subds_discovery',
-            doc="""Flag to disable reporting type='dataset' for subdatasets, even
-            when they are not installed, or their mount point directory doesn't
-            exist. Disabling saves on command run time, if this information is
-            not needed."""),
-        force_untracked_discovery=Parameter(
-            args=("--no-untracked-discovery",),
-            action='store_false',
-            dest='force_untracked_discovery',
-            doc="""Flag to disable discovery of untracked changes.
-                Disabling saves on command run time, if this information is
-                not needed."""),
-        force_no_revision_change_discovery=Parameter(
-            args=("--revision-change-discovery",),
-            action='store_false',
-            dest='force_no_revision_change_discovery',
-            doc="""Flag to disable discovery of changes which were not yet committed.
-            Disabling saves on command run time, if this information is
-            not needed."""),
-        modified=Parameter(
-            args=("--modified",),
-            nargs='?',
-            const=True,
-            constraints=EnsureStr() | EnsureBool() | EnsureNone(),
-            doc="""comparison reference specification for modification detection.
-            This can be (mostly) anything that `git diff` understands (commit,
-            treeish, tag, etc). See the documentation of `datalad diff --revision`
-            for details. Unmodified paths will not be annotated. If a requested
-            path was not modified but some content underneath it was, then the
-            request is replaced by the modified paths and those are annotated
-            instead. This option can be used [PY: with `True` as PY][CMD: without CMD]
-            an argument to test against changes that have been made, but have not
-            yet been staged for a commit."""))
+    yield from _post_git_init_processing_(
+        destds=destds,
+        cfg=cfg,
+        gitclonerec=gitclonerec,
+        remote=remote,
+        reckless=reckless,
+    )
+
+    if knows_annex(destds.path):
+        # init annex when traces of a remote annex can be detected
+        yield from _pre_annex_init_processing_(
+            destds=destds,
+            cfg=cfg,
+            gitclonerec=gitclonerec,
+            remote=remote,
+            reckless=reckless,
+        )
+        dest_repo = _annex_init(
+            destds=destds,
+            cfg=cfg,
+            gitclonerec=gitclonerec,
+            remote=remote,
+            description=description,
+        )
+        yield from _post_annex_init_processing_(
+            destds=destds,
+            cfg=cfg,
+            gitclonerec=gitclonerec,
+            remote=remote,
+            reckless=reckless,
+        )
+
+    if checkout_gitsha and \
+       dest_repo.get_hexsha(
+            dest_repo.get_corresponding_branch()) != checkout_gitsha:
+        try:
+            postclone_checkout_commit(dest_repo, checkout_gitsha,
+                                      remote=remote)
+        except Exception:
+            # We were supposed to clone a particular version but failed to.
+            # This is particularly pointless in case of subdatasets and
+            # potentially fatal with current implementation of recursion.
+            # see gh-5387
+            lgr.debug(
+                "Failed to checkout %s, removing this clone attempt at %s",
+                checkout_gitsha, destds.path)
+            raise
+
+    yield from _pre_final_processing_(
+        destds=destds,
+        cfg=cfg,
+        gitclonerec=gitclonerec,
+        remote=remote,
+        reckless=reckless,
+    )
+
+
+def _post_git_init_processing_(
+        *,
+        destds: Dataset,
+        cfg: ConfigManager,
+        gitclonerec: Dict,
+        remote: str,
+        reckless: None or str,
+):
+    """Any post-git-init processing that need not be concerned with git-annex
+    """
+    if not gitclonerec.get("version"):
+        postclone_check_head(destds, remote=remote)
 
-    @staticmethod
-    @datasetmethod(name='annotate_paths')
-    @eval_results
-    def __call__(
-            path=None,
-            dataset=None,
-            recursive=False,
-            recursion_limit=None,
-            action=None,
-            unavailable_path_status='',
-            unavailable_path_msg=None,
-            nondataset_path_status='error',
-            force_parentds_discovery=True,
-            force_subds_discovery=True,
-            force_no_revision_change_discovery=True,
-            force_untracked_discovery=True,
-            modified=None):
-        # upfront check for the fastest possible response
-        if not path and dataset is None:
-            # nothing given, try "here", but do not use `require_dataset`, as
-            # it will determine the root dataset of `curdir` and further down
-            # lead to path annotation of upstairs directories
-            dataset = curdir
+    # act on --reckless=shared-...
+    # must happen prior git-annex-init, where we can cheaply alter the repo
+    # setup through safe re-init'ing
+    if reckless and reckless.startswith('shared-'):
+        lgr.debug(
+            'Reinitializing %s to enable shared access permissions',
+            destds)
+        destds.repo.call_git(['init', '--shared={}'.format(reckless[7:])])
+
+    # trick to have the function behave like a generator, even if it
+    # (currently) doesn't actually yield anything.
+    # but a patched version might want to...so for uniformity with
+    # _post_annex_init_processing_() let's do this
+    if False:
+        yield
+
+
+def _pre_annex_init_processing_(
+        *,
+        destds: Dataset,
+        cfg: ConfigManager,
+        gitclonerec: Dict,
+        remote: str,
+        reckless: None or str,
+):
+    """Pre-processing a to-be-initialized annex repository"""
+    if reckless == 'auto':
+        lgr.debug(
+            "Instruct annex to hardlink content in %s from local "
+            "sources, if possible (reckless)", destds.path)
+        destds.config.set(
+            'annex.hardlink', 'true', scope='local', reload=True)
+
+    # trick to have the function behave like a generator, even if it
+    # (currently) doesn't actually yield anything.
+    if False:
+        yield
+
+
+def _annex_init(
+        *,
+        destds: Dataset,
+        cfg: ConfigManager,
+        gitclonerec: Dict,
+        remote: str,
+        description: None or str,
+):
+    """Initializing an annex repository"""
+    lgr.debug("Initializing annex repo at %s", destds.path)
+    # Note, that we cannot enforce annex-init via AnnexRepo().
+    # If such an instance already exists, its __init__ will not be executed.
+    # Therefore do quick test once we have an object and decide whether to call
+    # its _init().
+    #
+    # Additionally, call init if we need to add a description (see #1403),
+    # since AnnexRepo.__init__ can only do it with create=True
+    repo = AnnexRepo(destds.path, init=True)
+    if not repo.is_initialized() or description:
+        repo._init(description=description)
+    return repo
+
+
+def _post_annex_init_processing_(
+        *,
+        destds: Dataset,
+        cfg: ConfigManager,
+        gitclonerec: Dict,
+        remote: str,
+        reckless: None or str,
+):
+    """Post-processing an annex repository"""
+    # convenience aliases
+    repo = destds.repo
+    ds = destds
+
+    if reckless == 'auto' or (reckless and reckless.startswith('shared-')):
+        repo.call_annex(['untrust', 'here'])
+
+    _check_autoenable_special_remotes(repo)
+
+    # we have just cloned the repo, so it has a remote `remote`, configure any
+    # reachable origin of origins
+    yield from configure_origins(ds, ds, remote=remote)
+
+
+def _pre_final_processing_(
+        *,
+        destds: Dataset,
+        cfg: ConfigManager,
+        gitclonerec: Dict,
+        remote: str,
+        reckless: None or str,
+):
+    """Any post-processing after Git and git-annex pieces are fully initialized
+    """
+    if reckless:
+        # store the reckless setting in the dataset to make it
+        # known to later clones of subdatasets via get()
+        destds.config.set(
+            'datalad.clone.reckless', reckless,
+            scope='local',
+            reload=True)
+    else:
+        # We would still want to reload configuration to ensure that any of the
+        # above git invocations could have potentially changed the config
+        # TODO: might no longer be necessary if 0.14.0 adds reloading upon
+        # non-readonly commands invocation
+        destds.config.reload()
+
+    # trick to have the function behave like a generator, even if it
+    # (currently) doesn't actually yield anything.
+    if False:
+        yield
 
-        if force_subds_discovery and not force_parentds_discovery:
-            raise ValueError(
-                'subdataset discovery requires parent dataset discovery')
 
-        # CONCEPT: yield with no status to indicate further processing
+def postclone_checkout_commit(repo, target_commit, remote="origin"):
+    """Helper to check out a specific target commit in a fresh clone.
 
-        # everything in one big loop to be able too yield as fast a possible
-        # without any precomputing for all paths
-        refds_path = Interface.get_refds_path(dataset)
-        if modified is not None and (refds_path is None or not GitRepo.is_valid_repo(refds_path)):
+    Will not check (again) if current commit and target commit are already
+    the same!
+    """
+    # record what branch we were on right after the clone
+    active_branch = repo.get_active_branch()
+    corr_branch = repo.get_corresponding_branch(branch=active_branch)
+    was_adjusted = bool(corr_branch)
+    repo_orig_branch = corr_branch or active_branch
+    # if we are on a branch this hexsha will be the tip of that branch
+    repo_orig_hexsha = repo.get_hexsha(repo_orig_branch)
+    # make sure we have the desired commit locally
+    # expensive and possibly error-prone fetch conditional on cheap
+    # local check
+    if not repo.commit_exists(target_commit):
+        try:
+            repo.fetch(remote=remote, refspec=target_commit)
+        except CommandError as e:
+            CapturedException(e)
+            pass
+        # instead of inspecting the fetch results for possible ways
+        # with which it could failed to produced the desired result
+        # let's verify the presence of the commit directly, we are in
+        # expensive-land already anyways
+        if not repo.commit_exists(target_commit):
+            # there is nothing we can do about this
+            # MIH thinks that removing the clone is not needed, as a likely
+            # next step will have to be a manual recovery intervention
+            # and not another blind attempt
             raise ValueError(
-                "modification detection only works with a base dataset (non-given or found)")
-
-        # prep common result props
-        res_kwargs = dict(
-            action=action if action else 'annotate_path',
-            refds=refds_path,
-            logger=lgr)
-
-        # handle the case of recursion into a single dataset without any
-        # extra fancy processing first -- full recursion can be done
-        # faster than manual recursion, hence we gain quite some speed
-        # from these few lines of extra code
-        if not modified and not path and refds_path:
-            if not GitRepo.is_valid_repo(refds_path):
-                yield get_status_dict(
-                    # doesn't matter if the path is in another dataset
-                    # it was given as reference dataset
-                    status=nondataset_path_status,
-                    message='given reference dataset is not a dataset',
-                    path=refds_path,
-                    **res_kwargs)
-                return
-
-            refds = Dataset(refds_path)
-            path = []
-            # yield the dataset itself
-            r = get_status_dict(ds=refds, status='', **res_kwargs)
-            yield r
-
-            if recursive:
-                # if we have nothing given, but need recursion, we need to feed
-                # the dataset path itself
-                for r in yield_recursive(
-                        refds,
-                        refds_path,
-                        action,
-                        recursion_limit):
-                    r.update(res_kwargs)
-                    if 'refds' in r and not r['refds']:
-                        # avoid cruft
-                        del r['refds']
-                    yield r
-            return
-
-        # goal: structure in a way that makes most information on any path
-        # available in a single pass, at the cheapest possible cost
-        reported_paths = {}
-        requested_paths = assure_list(path)
-
-        if modified is not None:
-            # modification detection would silently kill all nondataset paths
-            # but we have to complain about them, hence doing it here
-            if requested_paths and refds_path:
-                for r in requested_paths:
-                    p = r['path'] if isinstance(r, dict) else r
-                    p = resolve_path(p, ds=refds_path)
-                    if path_startswith(p, refds_path):
-                        # all good
-                        continue
-                    # not the refds
-                    path_props = r if isinstance(r, dict) else {}
-                    res = get_status_dict(
-                        **dict(res_kwargs, **path_props))
-                    res['status'] = nondataset_path_status
-                    res['message'] = 'path not associated with reference dataset'
-                    reported_paths[r] = res
-                    yield res
-
-            # preserve non-existing paths to be silently killed by modification
-            # detection and append them to requested_paths again after detection.
-            # TODO: This might be melted in with treatment of non dataset paths
-            # above. Re-appending those paths seems to be better than yielding
-            # directly to avoid code duplication, since both cases later on are
-            # dealt with again.
-            preserved_paths = []
-            if requested_paths:
-                [preserved_paths.append(r)
-                 for r in requested_paths
-                 if not lexists(r['path'] if isinstance(r, dict) else r)]
-
-            # replace the requested paths by those paths that were actually
-            # modified underneath or at a requested location
-            requested_paths = get_modified_subpaths(
-                # either the request, or the base dataset, if there was no request
-                requested_paths if requested_paths else [refds_path],
-                refds=Dataset(refds_path),
-                revision=modified,
-                report_no_revision_change=force_no_revision_change_discovery,
-                report_untracked='all' if force_untracked_discovery else 'no',
-                recursion_limit=recursion_limit)
-
-            from itertools import chain
-            # re-append the preserved paths:
-            requested_paths = chain(requested_paths, iter(preserved_paths))
-
-        # do not loop over unique(), this could be a list of dicts
-        # we avoid duplicates manually below via `reported_paths`
-        for path in requested_paths:
-            if not isinstance(path, dict):
-                path = rawpath2ap(path, refds_path)
-            # this is now an annotated path!
-            path_props = path
-            path = path['path']
-            # we need to mark our territory, who knows where this has been
-            path_props.update(res_kwargs)
-
-            if path in reported_paths:
-                # we already recorded this path in the output
-                # this can happen, whenever `path` is a subdataset, that was
-                # discovered via recursive processing of another path before
-                continue
-            # the path exists in some shape or form
-            # TODO if we have path_props already we could skip this test
-            if isdir(path):
-                # keep any existing type info, previously a more expensive run
-                # could have discovered an uninstalled 'dataset', and we don't
-                # want it to be relabeled to a directory
-                path_props['type'] = \
-                    path_props.get(
-                        'type',
-                        'dataset' if GitRepo.is_valid_repo(path) else 'directory')
-                # this could contain all types of additional content
-                containing_dir = path
-            else:
-                if lexists(path):
-                    path_props['type'] = 'file'
-                else:
-                    path_props['state'] = 'absent'
-                # for everything else we are interested in the container
-                containing_dir = dirname(path)
-                if not containing_dir:
-                    containing_dir = curdir
-
-            dspath = parent = get_dataset_root(containing_dir)
-            if dspath:
-                if path_props.get('type', None) == 'dataset':
-                    # for a dataset the root is not the parent, for anything else
-                    # it is
-                    parent = path_props.get('parentds', None)
-                    oneupdir = normpath(opj(containing_dir, pardir))
-                    if parent is None and (force_parentds_discovery or (
-                            refds_path and _with_sep(oneupdir).startswith(
-                                _with_sep(refds_path)))):
-                        # either forced, or only if we have a reference dataset, and
-                        # only if we stay within this refds when searching for the
-                        # parent
-                        parent = get_dataset_root(normpath(opj(containing_dir, pardir)))
-                        # NOTE the `and refds_path` is critical, as it will determine
-                        # whether a top-level dataset that was discovered gets the
-                        # parent property or not, it won't get it without a common
-                        # base dataset, and that is how we always rolled
-                    if parent and refds_path:
-                        path_props['parentds'] = parent
-                        # don't check whether this is actually a true subdataset of the
-                        # parent, done further down
-                else:
-                    # set parent, but prefer existing property
-                    path_props['parentds'] = path_props.get('parentds', dspath)
-
-            # test for `dspath` not `parent`, we only need to know whether there is
-            # ANY dataset, not which one is the true parent, logic below relies on
-            # the fact that we end here, if there is no dataset at all
-            if not dspath:
-                # not in any dataset
-                res = get_status_dict(
-                    **dict(res_kwargs, **path_props))
-                res['status'] = nondataset_path_status
-                res['message'] = 'path not associated with any dataset'
-                reported_paths[path] = res
-                yield res
+                'Target commit %s does not exist in the clone, and '
+                'a fetch that commit from remote failed'
+                % target_commit[:8])
+    # checkout the desired commit
+    repo.call_git(['checkout', target_commit])
+    # did we detach?
+    if repo_orig_branch and not repo.get_active_branch():
+        # trace if current state is a predecessor of the branch_hexsha
+        lgr.debug(
+            "Detached HEAD after resetting worktree of %s "
+            "(original branch: %s)", repo, repo_orig_branch)
+        if repo.get_merge_base(
+                [repo_orig_hexsha, target_commit]) == target_commit:
+            # we assume the target_commit to be from the same branch,
+            # because it is an ancestor -- update that original branch
+            # to point to the target_commit, and update HEAD to point to
+            # that location
+            lgr.info(
+                "Reset branch '%s' to %s (from %s) to "
+                "avoid a detached HEAD",
+                repo_orig_branch, target_commit[:8], repo_orig_hexsha[:8])
+            branch_ref = 'refs/heads/%s' % repo_orig_branch
+            repo.update_ref(branch_ref, target_commit)
+            repo.update_ref('HEAD', branch_ref, symbolic=True)
+            if was_adjusted:
+                # Note: The --force is needed because the adjust branch already
+                # exists.
+                repo.adjust(options=["--unlock", "--force"])
+        else:
+            lgr.warning(
+                "%s has a detached HEAD, because the target commit "
+                "%s has no unique ancestor with branch '%s'",
+                repo, target_commit[:8], repo_orig_branch)
+
+
+def postclone_check_head(ds, remote="origin"):
+    repo = ds.repo
+    if not repo.commit_exists("HEAD"):
+        # HEAD points to an unborn branch. A likely cause of this is that the
+        # remote's main branch is something other than master but HEAD wasn't
+        # adjusted accordingly.
+        #
+        # Let's choose the most recently updated remote ref (according to
+        # commit date). In the case of a submodule, switching to a ref with
+        # commits prevents .update_submodule() from failing. It is likely that
+        # the ref includes the registered commit, but we don't have the
+        # information here to know for sure. If it doesn't, .update_submodule()
+        # will check out a detached HEAD.
+        remote_branches = (
+            b["refname:strip=2"] for b in repo.for_each_ref_(
+                fields="refname:strip=2", sort="-committerdate",
+                pattern="refs/remotes/" + remote))
+        for rbranch in remote_branches:
+            if rbranch in [remote + "/git-annex", "HEAD"]:
                 continue
-
-            # check that we only got SUBdatasets
-            if refds_path and not path_startswith(dspath, refds_path):
-                res = get_status_dict(**dict(res_kwargs, **path_props))
-                res['status'] = nondataset_path_status
-                res['message'] = \
-                    ('path not part of the reference dataset at %s', refds_path)
-                reported_paths[path] = res
-                yield res
+            if rbranch.startswith(remote + "/adjusted/"):
+                # If necessary for this file system, a downstream
+                # git-annex-init call will handle moving into an
+                # adjusted state.
                 continue
+            repo.call_git(["checkout", "-b",
+                           rbranch[len(remote) + 1:],  # drop "<remote>/"
+                           "--track", rbranch])
+            lgr.debug("Checked out local branch from %s", rbranch)
+            return
+        lgr.warning("Cloned %s but could not find a branch "
+                    "with commits", ds.path)
 
-            if path_props.get('type', None) == 'file':
-                # nothing else we can learn about this
-                res = get_status_dict(**dict(res_kwargs, **path_props))
-                if 'status' not in res:
-                    res['status'] = ''
-                reported_paths[path] = res
-                yield res
-                continue
 
-            containing_ds = None
-            path_type = path_props.get('type', None)
-            if parent and force_subds_discovery and (
-                    (path_type == 'dataset' and 'registered_subds' not in path_props) or
-                    path_type == 'directory' or
-                    not lexists(path)):
-                # if the path doesn't exist, or is labeled a directory, or a dataset even
-                # a dataset (without this info) -> record whether this is a known subdataset
-                # to its parent
-                containing_ds = Dataset(parent)
-                subdss = containing_ds.subdatasets(
-                    fulfilled=None, recursive=False,
-                    result_xfm=None, result_filter=None, return_type='list')
-                if path in [s['path'] for s in subdss]:
-                    if path_type == 'directory' or not lexists(path):
-                        # first record that it isn't here, if just a dir or not here at all
-                        path_props['state'] = 'absent'
-                    # this must be a directory, and it is not installed
-                    path_props['type'] = 'dataset'
-                    path_props['registered_subds'] = True
-
-            if not lexists(path) or \
-                    (path_props.get('type', None) == 'dataset' and
-                     path_props.get('state', None) == 'absent'):
-                # not there (yet)
-                message = unavailable_path_msg if unavailable_path_msg else None
-                if message and '%s' in message:
-                    message = (message, path)
-                path_props['message'] = message
-                res = get_status_dict(**dict(res_kwargs, **path_props))
-                # assign given status, but only if the props don't indicate a status
-                # already
-                res['status'] = path_props.get(
-                    'status', unavailable_path_status)
-                reported_paths[path] = res
-                yield res
-                continue
+def configure_origins(cfgds, probeds, label=None, remote="origin"):
+    """Configure any discoverable local dataset sibling as a remote
 
-            # we know everything we can, report
-            res = get_status_dict(**dict(res_kwargs, **path_props))
-            if 'status' not in res:
-                res['status'] = ''
-            reported_paths[path] = res
-            yield res
-
-            rec_paths = []
-            if recursive:
-                # here we need to consider the special case that `path` is
-                # a dataset itself, if a recursion_limit is given (e.g.
-                # `remove` will do that by default), we need to recurse
-                # from the dataset itself, and not its parent to get things
-                # right -- this will also avoid needless discovery of
-                # unrelated subdatasets
-                if path_props.get('type', None) == 'dataset':
-                    containing_ds = Dataset(path)
-                else:
-                    # regular parent, we might have a dataset already
-                    containing_ds = Dataset(parent) if containing_ds is None else containing_ds
-                for r in yield_recursive(containing_ds, path, action, recursion_limit):
-                    # capture reported paths
-                    r.update(res_kwargs)
-                    if 'refds' in r and not r['refds']:
-                        # avoid cruft
-                        del r['refds']
-                    reported_paths[r['path']] = r
-                    if modified is not None:
-                        # we cannot yield right away, maybe it wasn't modified
-                        rec_paths.append(r)
-                    else:
-                        yield r
-            if modified is not None and rec_paths:
-                # replace the recursively discovered paths by those paths that
-                # were actually modified underneath or at a requested location
-                for r in get_modified_subpaths(
-                        rec_paths,
-                        refds=Dataset(refds_path),
-                        revision=modified,
-                        report_no_revision_change=force_no_revision_change_discovery,
-                        report_untracked='all' if force_untracked_discovery else 'no',
-                        recursion_limit=recursion_limit):
-                    res = get_status_dict(**dict(r, **res_kwargs))
-                    reported_paths[res['path']] = res
-                    yield res
+    Parameters
+    ----------
+    cfgds : Dataset
+      Dataset to receive the remote configurations
+    probeds : Dataset
+      Dataset to start looking for `remote` remotes. May be identical with
+      `cfgds`.
+    label : int, optional
+      Each discovered remote will be configured as a remote under the name
+      '<remote>-<label>'. If no label is given, '2' will be used by default,
+      given that there is typically a remote named `remote` already.
+    remote : str, optional
+      Name of the default remote on clone.
+    """
+    if label is None:
+        label = 1
+    # let's look at the URL for that remote and see if it is a local
+    # dataset
+    origin_url = probeds.config.get(f'remote.{remote}.url')
+    if not origin_url:
+        # no remote with default name, nothing to do
+        return
+    if not cfgds.config.obtain(
+            'datalad.install.inherit-local-origin',
+            default=True):
+        # no inheritance wanted
         return
+    if not isinstance(RI(origin_url), PathRI):
+        # not local path
+        return
+
+    # no need to reconfigure original/direct remote again
+    if cfgds != probeds:
+        # prevent duplicates
+        known_remote_urls = set(
+            cfgds.config.get(r + '.url', None)
+            for r in cfgds.config.sections()
+            if r.startswith('remote.')
+        )
+        if origin_url not in known_remote_urls:
+            yield from cfgds.siblings(
+                'configure',
+                # no chance for conflict, can only be the second configured
+                # remote
+                name='{}-{}'.format(remote, label),
+                url=origin_url,
+                # fetch to get all annex info
+                fetch=True,
+                result_renderer='disabled',
+                on_failure='ignore',
+            )
+    # and dive deeper
+    # given the clone source is a local dataset, we can have a
+    # cheap look at it, and configure its own `remote` as a remote
+    # (if there is any), and benefit from additional annex availability
+    yield from configure_origins(
+        cfgds,
+        Dataset(probeds.pathobj / origin_url),
+        label=label + 1,
+        remote=remote)
```

### Comparing `datalad-0.9.3/datalad/interface/diff.py` & `datalad-1.0.0/datalad/interface/results.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,416 +1,460 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""Plumbing command for reporting changes in datasets"""
+"""Interface result handling functions
 
-__docformat__ = 'restructuredtext'
+"""
+
+from __future__ import annotations
 
+__docformat__ = 'restructuredtext'
 
 import logging
-import stat
+from collections.abc import (
+    Iterable,
+    Iterator,
+)
+from os.path import (
+    isabs,
+    isdir,
+)
 from os.path import join as opj
-from os.path import curdir
-from os.path import isdir
-from os.path import relpath
-from os.path import normpath
-
-
-from datalad.interface.annotate_paths import AnnotatePaths
-from datalad.interface.annotate_paths import annotated2content_by_ds
-from datalad.interface.base import Interface
-from datalad.interface.utils import eval_results
-from datalad.interface.base import build_doc
-from datalad.support.constraints import EnsureNone
-from datalad.support.constraints import EnsureStr
-from datalad.support.constraints import EnsureChoice
-from datalad.support.exceptions import CommandError
-from datalad.support.param import Parameter
-from datalad.interface.common_opts import recursion_flag
-from datalad.interface.common_opts import recursion_limit
-from datalad.cmd import GitRunner
-
-from datalad.distribution.dataset import EnsureDataset
-from datalad.distribution.dataset import datasetmethod
-
-from datalad.utils import with_pathsep as _with_sep
-from datalad.utils import path_startswith
-
-from datalad.consts import PRE_INIT_COMMIT_SHA
-
-
-lgr = logging.getLogger('datalad.interface.diff')
-
-
-# from Git docs
-state_map = {
-    'A': 'added',
-    'C': 'copied',
-    'D': 'deleted',
-    'M': 'modified',
-    'R': 'renamed',
-    'T': 'typechange',
-    'U': 'unmerged',
-    'X': 'unknown_potentialbug',
+from os.path import (
+    normpath,
+    relpath,
+)
+from typing import (
+    Any,
+    Optional,
+)
+
+from datalad.distribution.dataset import Dataset
+from datalad.support.exceptions import (
+    CapturedException,
+    CommandError,
+    format_oneline_tb,
+)
+from datalad.support.path import robust_abspath
+from datalad.utils import (
+    PurePosixPath,
+    ensure_list,
+    path_is_subpath,
+)
+
+lgr = logging.getLogger('datalad.interface.results')
+lgr.log(5, "Importing datalad.interface.results")
+
+# which status is a success , which is failure
+success_status_map = {
+    'ok': 'success',
+    'notneeded': 'success',
+    'impossible': 'failure',
+    'error': 'failure',
 }
 
 
-def _translate_status(label, ap):
-    if label[0] in ('C', 'R', 'M') and len(label) > 1:
-        ap['perc_similarity'] = float(label[1:])
-        label = label[0]
-    ap['state'] = state_map[label]
+def get_status_dict(
+    action: Optional[str] = None,
+    ds: Optional[Dataset] = None,
+    path: Optional[str] = None,
+    type: Optional[str] = None,
+    logger: Optional[logging.Logger] = None,
+    refds: Optional[str] = None,
+    status: Optional[str] = None,
+    message: str | tuple | None = None,
+    exception: Exception | CapturedException | None = None,
+    error_message: str | tuple | None = None,
+    **kwargs: Any,
+) -> dict[str, Any]:
+    # `type` is intentionally not `type_` or something else, as a mismatch
+    # with the dict key 'type' causes too much pain all over the place
+    # just for not shadowing the builtin `type` in this function
+    """Helper to create a result dictionary.
+
+    Most arguments match their key in the resulting dict, and their given
+    values are simply assigned to the result record under these keys.  Only
+    exceptions are listed here.
+
+    Parameters
+    ----------
+    ds
+      If given, the `path` and `type` values are populated with the path of the
+      datasets and 'dataset' as the type. Giving additional values for both
+      keys will overwrite these pre-populated values.
+    exception
+      Exceptions that occurred while generating a result should be captured
+      by immediately instantiating a CapturedException. This instance can
+      be passed here to yield more comprehensive error reporting, including
+      an auto-generated traceback (added to the result record under an
+      'exception_traceback' key). Exceptions of other types are also supported.
+
+    Returns
+    -------
+    dict
+    """
 
+    d: dict[str, Any] = {}
+    if action is not None:
+        d['action'] = action
+    if ds:
+        d['path'] = ds.path
+        d['type'] = 'dataset'
+    # now overwrite automatic
+    if path is not None:
+        d['path'] = path
+    if type:
+        d['type'] = type
+    if logger:
+        d['logger'] = logger
+    if refds:
+        d['refds'] = refds
+    if status is not None:
+        # TODO check for known status label
+        d['status'] = status
+    if message is not None:
+        d['message'] = message
+    if error_message is not None:
+        d['error_message'] = error_message
+    if exception is not None:
+        d['exception'] = exception
+        d['exception_traceback'] = exception.format_oneline_tb(
+            include_str=False) \
+            if isinstance(exception, CapturedException) \
+            else format_oneline_tb(
+                exception, include_str=False)
+        if error_message is None and isinstance(exception, CapturedException):
+            d['error_message'] = exception.message
+        if isinstance(exception, CommandError):
+            d['exit_code'] = exception.code
+    if kwargs:
+        d.update(kwargs)
+    return d
+
+
+def results_from_paths(
+    paths: str | list[str],
+    action: Optional[str] = None,
+    type: Optional[str] = None,
+    logger: Optional[logging.Logger] = None,
+    refds: Optional[str]=None,
+    status: Optional[str] = None,
+    message: Optional[str] = None,
+    exception: Exception | CapturedException | None = None,
+) -> Iterator[dict[str, Any]]:
+    """
+    Helper to yield analog result dicts for each path in a sequence.
 
-def _translate_type(mode, ap, prop):
-    if mode == 0:
-        ap[prop] = None
-    elif mode == stat.S_IFDIR | stat.S_IFLNK:
-        ap[prop] = 'dataset'
-    elif stat.S_ISDIR(mode):
-        # not sure if this can happen at all, at least not in the tests...
-        ap[prop] = 'directory'
-    else:
-        ap[prop] = 'file'
+    Parameters
+    ----------
+    message: str
+      A result message. May contain `%s` which will be replaced by the
+      respective `path`.
+
+    Returns
+    -------
+    generator
 
+    """
+    for p in ensure_list(paths):
+        yield get_status_dict(
+            action, path=p, type=type, logger=logger, refds=refds,
+            status=status, message=(message, p) if message is not None and '%s' in message else message,
+            exception=exception
+        )
 
-def _get_untracked_content(dspath, report_untracked, paths=None):
-    cmd = ['git', '--work-tree=.', 'status', '--porcelain',
-           # file names NULL terminated
-           '-z',
-           # we never want to touch submodules, they cannot be untracked
-           '--ignore-submodules=all',
-           # fully untracked dirs as such, the rest as files
-           '--untracked={}'.format(report_untracked)]
-    try:
-        stdout, stderr = GitRunner(cwd=dspath).run(
-            cmd,
-            log_stderr=True,
-            log_stdout=True,
-            log_online=False,
-            expect_stderr=False,
-            shell=False,
-            expect_fail=True)
-    except CommandError as e:
-        # TODO should we catch any and handle them in here?
-        raise e
-
-    if paths:
-        paths = [r['path'] for r in paths]
-        if len(paths) == 1 and paths[0] == dspath:
-            # nothing to filter
-            paths = None
 
-    for line in stdout.split('\0'):
-        if not line:
-            continue
-        if not line.startswith('?? '):
-            # nothing untracked, ignore, task of `diff`
-            continue
-        apath = opj(
-            dspath,
-            # strip state marker
-            line[3:])
-        norm_apath = normpath(apath)
-        if paths and not any([norm_apath == p or path_startswith(apath, p) for p in paths]):
-            # we got a whitelist for paths, don't report any other
+def is_ok_dataset(r: dict) -> bool:
+    """Convenience test for a non-failure dataset-related result dict"""
+    return r.get('status', None) == 'ok' and r.get('type', None) == 'dataset'
+
+
+class ResultXFM:
+    """Abstract definition of the result transformer API"""
+
+    def __call__(self, res: dict[str, Any]) -> Any:
+        """This is called with one result dict at a time"""
+        raise NotImplementedError
+
+
+class YieldDatasets(ResultXFM):
+    """Result transformer to return a Dataset instance from matching result.
+
+    If the `success_only` flag is given only dataset with 'ok' or 'notneeded'
+    status are returned'.
+
+    `None` is returned for any other result.
+    """
+    def __init__(self, success_only: bool = False) -> None:
+        self.success_only = success_only
+
+    def __call__(self, res: dict[str, Any]) -> Optional[Dataset]:
+        if res.get('type', None) == 'dataset':
+            if not self.success_only or \
+                    res.get('status', None) in ('ok', 'notneeded'):
+                return Dataset(res['path'])
+            else:
+                return None
+        else:
+            lgr.debug('rejected by return value configuration: %s', res)
+            return None
+
+
+class YieldRelativePaths(ResultXFM):
+    """Result transformer to return relative paths for a result
+
+    Relative paths are determined from the 'refds' value in the result. If
+    no such value is found, `None` is returned.
+    """
+    def __call__(self, res: dict[str, Any]) -> Optional[str]:
+        refpath = res.get('refds', None)
+        if refpath:
+            return relpath(res['path'], start=refpath)
+        else:
+            return None
+
+
+class YieldField(ResultXFM):
+    """Result transformer to return an arbitrary value from a result dict"""
+    def __init__(self, field: str) -> None:
+        """
+        Parameters
+        ----------
+        field : str
+          Key of the field to return.
+        """
+        self.field = field
+
+    def __call__(self, res: dict[str, Any]) -> Any:
+        if self.field in res:
+            return res[self.field]
+        else:
+            lgr.debug('rejected by return value configuration: %s', res)
+            return None
+
+
+# a bunch of convenience labels for common result transformers
+# the API `result_xfm` argument understand any of these labels and
+# applied the corresponding callable
+known_result_xfms = {
+    'datasets': YieldDatasets(),
+    'successdatasets-or-none': YieldDatasets(success_only=True),
+    'paths': YieldField('path'),
+    'relpaths': YieldRelativePaths(),
+    'metadata': YieldField('metadata'),
+}
+
+translate_annex_notes = {
+    '(Use --force to override this check, or adjust numcopies.)':
+        'configured minimum number of copies not found',
+}
+
+
+def annexjson2result(d: dict[str, Any], ds: Dataset, **kwargs: Any) -> dict[str, Any]:
+    """Helper to convert an annex JSON result to a datalad result dict
+
+    Info from annex is rather heterogeneous, partly because some of it
+    our support functions are faking.
+
+    This helper should be extended with all needed special cases to
+    homogenize the information.
+
+    Parameters
+    ----------
+    d : dict
+      Annex info dict.
+    ds : Dataset instance
+      Used to determine absolute paths for `file` results. This dataset
+      is not used to set `refds` in the result, pass this as a separate
+      kwarg if needed.
+    **kwargs
+      Passes as-is to `get_status_dict`. Must not contain `refds`.
+    """
+    lgr.debug('received JSON result from annex: %s', d)
+    messages = []
+    res = get_status_dict(**kwargs)
+    res['status'] = 'ok' if d.get('success', False) is True else 'error'
+    # we cannot rely on any of these to be available as the feed from
+    # git annex (or its wrapper) is not always homogeneous
+    if d.get('file'):
+        res['path'] = str(ds.pathobj / PurePosixPath(d['file']))
+    if 'command' in d:
+        res['action'] = d['command']
+    if 'key' in d:
+        res['annexkey'] = d['key']
+    if 'fields' in d:
+        # this is annex metadata, filter out timestamps
+        res['metadata'] = {k: v[0] if isinstance(v, list) and len(v) == 1 else v
+                           for k, v in d['fields'].items()
+                           if not k.endswith('lastchanged')}
+    if d.get('error-messages', None):
+        res['error_message'] = '\n'.join(m.strip() for m in d['error-messages'])
+    # avoid meaningless standard messages, and collision with actual error
+    # messages
+    elif 'note' in d:
+        note = "; ".join(ln for ln in d['note'].splitlines()
+                         if ln != 'checksum...'
+                         and not ln.startswith('checking file'))
+        if note:
+            messages.append(translate_annex_notes.get(note, note))
+    if messages:
+        res['message'] = '\n'.join(m.strip() for m in messages)
+    return res
+
+
+def count_results(res: Iterable[dict[str, Any]], **kwargs: Any) -> int:
+    """Return number of results that match all property values in kwargs"""
+    return sum(
+        all(k in r and r[k] == v for k, v in kwargs.items()) for r in res)
+
+
+def only_matching_paths(res: dict[str, Any], **kwargs: Any) -> bool:
+    # TODO handle relative paths by using a contained 'refds' value
+    paths = ensure_list(kwargs.get('path', []))
+    respath = res.get('path', None)
+    return respath in paths
+
+
+# needs decorator, as it will otherwise bind to the command classes that use it
+@staticmethod  # type: ignore[misc]
+def is_result_matching_pathsource_argument(res: dict[str, Any], **kwargs: Any) -> bool:
+    # we either have any non-zero number of "paths" (that could be anything), or
+    # we have one path and one source
+    # we don't do any error checking here, done by the command itself
+    if res.get('action', None) not in ('install', 'get'):
+        # this filter is only used in install, reject anything that comes
+        # in that could not possibly be a 'install'-like result
+        # e.g. a sibling being added in the process
+        return False
+    source = kwargs.get('source', None)
+    if source is not None:
+        # we want to be able to deal with Dataset instances given as 'source':
+        if isinstance(source, Dataset):
+            source = source.path
+        # if there was a source, it needs to be recorded in the result
+        # otherwise this is not what we are looking for
+        return source == res.get('source_url', None)
+    # the only thing left is a potentially heterogeneous list of paths/URLs
+    paths = ensure_list(kwargs.get('path', []))
+    # three cases left:
+    # 1. input arg was an absolute path -> must match 'path' property
+    # 2. input arg was relative to a dataset -> must match refds/relpath
+    # 3. something nifti with a relative input path that uses PWD as the
+    #    reference
+    respath = res.get('path', None)
+    if respath in paths:
+        # absolute match, pretty sure we want this
+        return True
+    elif isinstance(kwargs.get('dataset', None), Dataset) and \
+            YieldRelativePaths()(res) in paths:
+        # command was called with a reference dataset, and a relative
+        # path of a result matches in input argument -- not 100% exhaustive
+        # test, but could be good enough
+        return True
+    elif any(robust_abspath(p) == respath for p in paths):
+        # one absolutified input path matches the result path
+        # I'd say: got for it!
+        return True
+    elif any(p == res.get('source_url', None) for p in paths):
+        # this was installed from a URL that was given, we'll take that too
+        return True
+    else:
+        return False
+
+
+def results_from_annex_noinfo(
+    ds: Dataset,
+    requested_paths: list[str],
+    respath_by_status: dict[str, list[str]],
+    dir_fail_msg: str,
+    noinfo_dir_msg: str,
+    noinfo_file_msg: str,
+    noinfo_status: str = 'notneeded',
+    **kwargs: Any
+) -> Iterator[dict[str, Any]]:
+    """Helper to yield results based on what information git annex did no give us.
+
+    The helper assumes that the annex command returned without an error code,
+    and interprets which of the requested paths we have heard nothing about,
+    and assumes that git annex was happy with their current state.
+
+    Parameters
+    ==========
+    ds : Dataset
+      All results have to be concerning this single dataset (used to resolve
+      relpaths).
+    requested_paths : list
+      List of path arguments sent to `git annex`
+    respath_by_status : dict
+      Mapping of 'success' or 'failure' labels to lists of result paths
+      reported by `git annex`. Everything that is not in here, we assume
+      that `git annex` was happy about.
+    dir_fail_msg : str
+      Message template to inject into the result for a requested directory where
+      a failure was reported for some of its content. The template contains two
+      string placeholders that will be expanded with 1) the path of the
+      directory, and 2) the content failure paths for that directory
+    noinfo_dir_msg : str
+      Message template to inject into the result for a requested directory that
+      `git annex` was silent about (incl. any content). There must be one string
+      placeholder that is expanded with the path of that directory.
+    noinfo_file_msg : str
+      Message to inject into the result for a requested file that `git
+      annex` was silent about.
+    noinfo_status : str
+      Status to report when annex provides no information
+    **kwargs
+      Any further kwargs are included in the yielded result dictionary.
+    """
+    for p in requested_paths:
+        # any relpath is relative to the currently processed dataset
+        # not the global reference dataset
+        p = p if isabs(p) else normpath(opj(ds.path, p))
+        if any(p in ps for ps in respath_by_status.values()):
+            # we have a report for this path already
             continue
-        ap = dict(
-            path=norm_apath,
-            parentds=dspath,
-            state='untracked',
-            type='directory' if isdir(apath) else 'file')
-        yield ap
-
-
-def _parse_git_diff(dspath, diff_thingie=None, paths=None,
-                    ignore_submodules='none', staged=False):
-    # use '--work-tree=.' to get direct omde to cooperate
-    cmd = ['git', '--work-tree=.', 'diff', '--raw',
-           # file names NULL terminated
-           '-z',
-           # how to treat submodules (see git diff docs)
-           '--ignore-submodules={}'.format(ignore_submodules),
-           # never abbreviate sha sums
-           '--abbrev=40']
-    if staged:
-        cmd.append('--staged')
-    if diff_thingie:
-        cmd.append(diff_thingie)
-    if paths:
-        cmd.append('--')
-        cmd.extend(ap['path'] for ap in paths if ap.get('raw_input', False))
-
-    try:
-        stdout, stderr = GitRunner(cwd=dspath).run(
-            cmd,
-            log_stderr=True,
-            log_stdout=True,
-            log_online=False,
-            expect_stderr=False,
-            shell=False,
-            expect_fail=True)
-    except CommandError as e:
-        if 'bad revision' in e.stderr:
-            yield dict(
-                path=dspath,
-                type='dataset',
-                status='impossible',
-                message=e.stderr.strip())
-            return
-        raise e
-
-    ap = None
-    for line in stdout.split('\0'):
-        if not line:
+        common_report = dict(path=p, **kwargs)
+        if isdir(p):
+            # `annex` itself will not report on directories, but if a
+            # directory was requested, we want to say something about
+            # it in the results.  we are inside a single, existing
+            # repo, hence all directories are already present, if not
+            # we had an error
+            # do we have any failures in a subdir of the requested dir?
+            failure_results = [
+                fp for fp in respath_by_status.get('failure', [])
+                if path_is_subpath(fp, p)]
+            if failure_results:
+                # we were not able to process all requested_paths, let's label
+                # this 'impossible' to get a warning-type report
+                # after all we have the directory itself, but not
+                # (some) of its requested_paths
+                yield get_status_dict(
+                    status='impossible', type='directory',
+                    message=(dir_fail_msg, p, failure_results),
+                    **common_report)
+            else:
+                # otherwise cool, but how cool?
+                success_results = [
+                    fp for fp in respath_by_status.get('success', [])
+                    if path_is_subpath(fp, p)]
+                yield get_status_dict(
+                    status='ok' if success_results else noinfo_status,
+                    message=None if success_results else (noinfo_dir_msg, p),
+                    type='directory', **common_report)
             continue
-        if line.startswith(':'):
-            # a new path
-            # yield any existing one
-            if ap:
-                yield ap
-                ap = None
-            # start new record
-            m_src, m_dst, sha_src, sha_dst, status = \
-                line[1:].split()
-            ap = dict(
-                mode_src=int(m_src, base=8),
-                mode=int(m_dst, base=8),
-                revision_src=sha_src if sha_src != '0' * 40 else None,
-                revision=sha_dst if sha_dst != '0' * 40 else None,
-                parentds=dspath)
-            _translate_status(status, ap)
-            _translate_type(ap['mode'], ap, 'type')
-            _translate_type(ap['mode_src'], ap, 'type_src')
         else:
-            # a filename
-            if 'path' in ap:
-                ap['path_src'] = ap['path']
-            ap['path'] = opj(dspath, line)
-    if ap:
-        yield ap
-
-
-@build_doc
-class Diff(Interface):
-    """Report changes of dataset components.
-
-    Reports can be generated for changes between recorded revisions, or
-    between a revision and the state of a dataset's work tree.
-
-    Unlike 'git diff', this command also reports untracked content when
-    comparing a revision to the state of the work tree. Such content is
-    marked with the property `state='untracked'` in the command results.
-
-    The following types of changes are distinguished and reported via the
-    `state` result property:
-
-    - added
-    - copied
-    - deleted
-    - modified
-    - renamed
-    - typechange
-    - unmerged
-    - untracked
-
-    Whenever applicable, source and/or destination revisions are reported
-    to indicate when exactly within the requested revision range a particular
-    component changed its status.
+            # not a directory, and we have had no word from `git annex`,
+            # yet no exception, hence the file was most probably
+            # already in the desired state
+            yield get_status_dict(
+                status=noinfo_status, type='file',
+                message=noinfo_file_msg,
+                **common_report)
 
-    Optionally, the reported changes can be limited to a subset of paths
-    within a dataset.
-    """
 
-    # make the custom renderer the default one, as the global default renderer
-    # does not yield meaningful output for this command
-    result_renderer = 'tailored'
-
-    _params_ = dict(
-        dataset=Parameter(
-            args=("-d", "--dataset"),
-            doc="""specify the dataset to query.  If
-            no dataset is given, an attempt is made to identify the dataset
-            based on the input and/or the current working directory""",
-            constraints=EnsureDataset() | EnsureNone()),
-        path=Parameter(
-            args=("path",),
-            metavar="PATH",
-            doc="""path to be evaluated""",
-            nargs="*",
-            constraints=EnsureStr() | EnsureNone()),
-        revision=Parameter(
-            args=('--revision',),
-            metavar='REVISION EXPRESSION',
-            nargs='?',
-            doc="""comparison reference specification. Three modes are
-            supported: 1) <revision> changes you have in your working tree
-            relative to the named revision (this can also be a branch name,
-            tag, commit or any label Git can understand). 2) <revision>..<revision>
-            changes between two arbitrary revisions. 3) <revision>...<revision>
-            changes on the branch containing and up to the second <revision>,
-            starting at a common ancestor of both revisions."""),
-        staged=Parameter(
-            args=("--staged",),
-            action="store_true",
-            doc="""get the changes already staged for a commit relative
-            to an optionally given revision (by default the most recent one)"""),
-        ignore_subdatasets=Parameter(
-            args=('--ignore-subdatasets',),
-            constraints=EnsureChoice('none', 'untracked', 'dirty', 'all'),
-            doc="""speed up execution by (partially) not evaluating the state of
-            subdatasets in a parent dataset. With "none" a subdataset is
-            considered modified when it either contains untracked or modified
-            content or its last saved state differs from that recorded in the
-            parent dataset. When "untracked" is used subdatasets are not
-            considered modified when they only contain untracked content (but
-            they are still scanned for modified content). Using "dirty" ignores
-            all changes to the work tree of subdatasets, only changes to the
-            revisions stored in the parent dataset are shown. Using "all" hides
-            all changes to subdatasets. Note, even with "all" recursive
-            execution will still report other changes in any existing
-            subdataset, only the subdataset record in a parent dataset
-            is not  evaluated."""),
-        report_untracked=Parameter(
-            args=('--report-untracked',),
-            constraints=EnsureChoice('no', 'normal', 'all'),
-            doc="""If and how untracked content is reported when comparing
-            a revision to the state of the work tree. 'no': no untracked files
-            are reported; 'normal': untracked files and entire untracked
-            directories are reported as such; 'all': report individual files
-            even in fully untracked directories."""),
-        recursive=recursion_flag,
-        recursion_limit=recursion_limit)
-
-    @staticmethod
-    @datasetmethod(name='diff')
-    @eval_results
-    def __call__(
-            path=None,
-            dataset=None,
-            revision=None,
-            staged=False,
-            ignore_subdatasets='none',
-            report_untracked='normal',
-            recursive=False,
-            recursion_limit=None):
-        if not dataset and not path:
-            # act on the whole dataset if nothing else was specified
-            dataset = curdir
-        refds_path = Interface.get_refds_path(dataset)
-
-        to_process = []
-        # tracked what commit ranges we want to diff per dataset
-        ds_diffies = {}
-        for ap in AnnotatePaths.__call__(
-                path=path,
-                dataset=refds_path,
-                recursive=recursive,
-                recursion_limit=recursion_limit,
-                action='diff',
-                # unavailable is OK, because we might query for a deleted file
-                unavailable_path_status='',
-                nondataset_path_status='impossible',
-                # must not use `modified`, infinite loop otherwise
-                modified=None,
-                return_type='generator',
-                on_failure='ignore'):
-            if ap.get('status', None):
-                # we know what to report already
-                yield ap
-                continue
-            if ap.get('type', None) == 'dataset':
-                ap['process_content'] = True
-            if ap.get('raw_input', False) or ap['path'] == refds_path:
-                # prepopulate the revision specs for all input paths
-                ds_diffies[ap['path']
-                           if ap.get('type', None) == 'dataset'
-                           else ap['parentds']] = revision
-            to_process.append(ap)
-
-        # sort into datasets
-        content_by_ds, ds_props, completed, nondataset_paths = \
-            annotated2content_by_ds(
-                to_process,
-                refds_path=refds_path)
-        assert(not completed)
-
-        for ds_path in sorted(content_by_ds.keys()):
-            if ds_path not in ds_diffies:
-                # we don't know how to diff
-                # this was not neither an input path, not did we see it
-                # when diffing its parent
-                continue
-            content_paths = content_by_ds[ds_path]
-            revision = ds_diffies[ds_path]
-            for r in _parse_git_diff(
-                    ds_path,
-                    diff_thingie=ds_diffies[ds_path],
-                    paths=content_paths,
-                    ignore_submodules=ignore_subdatasets,
-                    staged=staged):
-                r.update(dict(
-                    action='diff',
-                    logger=lgr))
-                if refds_path:
-                    r['refds'] = refds_path
-                if 'status' not in r:
-                    r['status'] = 'ok'
-                if r.get('type', None) == 'dataset':
-                    # this is a subdataset report
-                    # we need to use the reported commit range to properly adjust the
-                    # query once we hit that subdataset
-                    from_rev = r.get('revision_src', '')
-                    to_rev = r.get('revision', '')
-                    subrev = '{}..{}'.format(
-                        from_rev if from_rev else PRE_INIT_COMMIT_SHA,
-                        to_rev if to_rev else '',
-                    )
-                    if from_rev and from_rev == to_rev:
-                        # this is a special case, where subdataset reported changes without
-                        # a change in state/commit -- this is code for uncommited changes
-                        # in the subdataset (including staged ones). In such a case, we
-                        # must not provide a diff range, but only the source commit we want
-                        # to diff against
-                        # XXX if this is changed, likely the same logic in annotate_paths needs
-                        # changing too!
-                        subrev = from_rev
-                    ds_diffies[r['path']] = subrev
-                yield r
-            if (revision and '..' in revision) or report_untracked == 'no':
-                # don't look for untracked content, we got a revision range
-                continue
-            for r in _get_untracked_content(
-                    ds_path,
-                    report_untracked,
-                    paths=content_paths):
-                r.update(dict(
-                    action='diff',
-                    logger=lgr))
-                if refds_path:
-                    r['refds'] = refds_path
-                if 'status' not in r:
-                    r['status'] = 'ok'
-                yield r
-
-    @staticmethod
-    def custom_result_renderer(res, **kwargs):
-        from datalad.ui import ui
-        if not res['status'] == 'ok':
-            # logging reported already
-            return
-        path = relpath(res['path'], start=res['refds']) \
-            if res.get('refds', None) else res['path']
-        type_ = res.get('type', res.get('type_src', ''))
-        max_len = len('untracked(directory)')
-        state_msg = '{}{}'.format(
-            res['state'],
-            '({})'.format(type_ if type_ else ''))
-        ui.message('{fill}{state_msg}: {path}'.format(
-            fill=' ' * max(0, max_len - len(state_msg)),
-            state_msg=state_msg,
-            path=path))
+lgr.log(5, "Done importing datalad.interface.results")
```

### Comparing `datalad-0.9.3/datalad/interface/ls_webui.py` & `datalad-1.0.0/datalad/core/local/diff.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,414 +1,448 @@
-# emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil; coding: utf-8 -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""Helper to create listing for web UI such as on http://datasets.datalad.org
-"""
-__docformat__ = 'restructuredtext'
-
-import hashlib
-import humanize
-import json as js
-import time
-from genericpath import isdir, exists, getmtime
-from os import makedirs, remove, listdir
-from os.path import split, abspath, basename, join as opj, realpath, relpath, \
-    isabs, dirname
-
-from datalad.consts import METADATA_DIR, METADATA_FILENAME
-from datalad.distribution.dataset import Dataset
-from datalad.interface.ls import FsModel, lgr, GitModel
-from datalad.utils import safe_print, with_pathsep
-
-
-def machinesize(humansize):
-    """convert human-size string to machine-size"""
-    try:
-        size_str, size_unit = humansize.split(" ")
-    except AttributeError:
-        return float(humansize)
-    unit_converter = {
-        'Byte': 0, 'Bytes': 0, 'kB': 1, 'MB': 2, 'GB': 3, 'TB': 4, 'PB': 5
-    }
-    machinesize = float(size_str) * (1000 ** unit_converter[size_unit])
-    return machinesize
-
-
-def leaf_name(path):
-    """takes a relative or absolute path and returns name of node at that location"""
-    head, tail = split(abspath(path))
-    return tail or basename(head)
-
-
-def ignored(path, only_hidden=False):
-    """if path is in the ignorelist return True
-
-    ignore list includes hidden files and git or annex maintained folders
-    when only_hidden set, only ignores hidden files and folders not git or annex
-     maintained folders
-    """
-    if isdir(opj(path, ".git")) and not only_hidden:
-        return True
-    return '.' == leaf_name(path)[0] or leaf_name(path) == 'index.html'
-
-
-def metadata_locator(fs_metadata=None, path=None, ds_path=None, metadata_path=None):
-    """path to metadata file of node associated with the fs_metadata dictionary
+"""Report differences between two states of a dataset (hierarchy)"""
 
-    Parameters
-    ----------
-    fs_metadata: dict
-      Metadata json of a node
-    path: str
-      Path to directory of metadata to be rendered
-    ds_path: str
-      Path to dataset root
-    metadata_path: str
-      Path to metadata root. Calculated relative to ds_path
-
-    Returns
-    -------
-    str
-      path to metadata of current node
-    """
-
-    # use implicit paths unless paths explicitly specified
-    # Note: usage of ds_path as if it was the Repo's path. Therefore use
-    # realpath, since we switched to have symlinks resolved in repos but not in
-    # datasets
-    ds_path = realpath(ds_path) if ds_path else fs_metadata['repo']
-    path = path or fs_metadata['path']
-    metadata_path = metadata_path or '.git/datalad/metadata'
-    # directory metadata directory tree location
-    metadata_dir = opj(ds_path, metadata_path)
-    # relative path of current directory wrt dataset root
-    dir_path = relpath(path, ds_path) if isabs(path) else path
-    # normalize to / -- TODO, switch to '.' which is now actually the name since path is relative in web meta?
-    if dir_path in ('.', None, ''):
-        dir_path = '/'
-    # create md5 hash of current directory's relative path
-    metadata_hash = hashlib.md5(dir_path.encode('utf-8')).hexdigest()
-    # construct final path to metadata file
-    metadata_file = opj(metadata_dir, metadata_hash)
-
-    return metadata_file
+__docformat__ = 'restructuredtext'
 
 
-def fs_extract(nodepath, repo, basepath='/'):
-    """extract required info of nodepath with its associated parent repository and returns it as a dictionary
+import logging
+import os.path as op
+from datalad.utils import (
+    ensure_list,
+    ensure_unicode,
+    get_dataset_root,
+)
+from datalad.interface.base import (
+    Interface,
+    build_doc,
+    eval_results,
+)
+
+from datalad.distribution.dataset import (
+    Dataset,
+    datasetmethod,
+    require_dataset,
+    resolve_path,
+    path_under_rev_dataset,
+)
+
+from datalad.support.constraints import (
+    EnsureNone,
+    EnsureStr,
+)
+from datalad.support.param import Parameter
+
+from datalad.core.local.status import (
+    Status,
+    _common_diffstatus_params,
+)
+from datalad.support.exceptions import (
+    InvalidGitReferenceError,
+)
+
+lgr = logging.getLogger('datalad.core.local.diff')
+
+
+@build_doc
+class Diff(Interface):
+    """Report differences between two states of a dataset (hierarchy)
+
+    The two to-be-compared states are given via the --from and --to options.
+    These state identifiers are evaluated in the context of the (specified
+    or detected) dataset. In the case of a recursive report on a dataset
+    hierarchy, corresponding state pairs for any subdataset are determined
+    from the subdataset record in the respective superdataset. Only changes
+    recorded in a subdataset between these two states are reported, and so on.
+
+    Any paths given as additional arguments will be used to constrain the
+    difference report. As with Git's diff, it will not result in an error when
+    a path is specified that does not exist on the filesystem.
 
-    Parameters
-    ----------
-    nodepath : str
-        Full path to the location we are exploring (must be a directory within
-        `repo`)
-    repo : GitRepo
-        Is the repository nodepath belongs to
+    Reports are very similar to those of the `status` command, with the
+    distinguished content types and states being identical.
     """
-    # Create FsModel from filesystem nodepath and its associated parent repository
-    node = FsModel(nodepath, repo)
-    pretty_size = {stype: humanize.naturalsize(svalue)
-                   for stype, svalue in node.size.items()}
-    pretty_date = time.strftime(u"%Y-%m-%d %H:%M:%S", time.localtime(node.date))
-    name = leaf_name(node._path) \
-        if leaf_name(node._path) != "" \
-        else leaf_name(node.repo.path)
-    rec = {
-        "name": name,
-        "path": relpath(node._path, basepath),
-        "type": node.type_,
-        "size": pretty_size,
-        "date": pretty_date,
-    }
-    # if there is meta-data for the dataset (done by aggregate-metadata)
-    # we include it
-    metadata_path = opj(nodepath, METADATA_DIR, METADATA_FILENAME)
-    if exists(metadata_path):
-        # might need flattening!  TODO: flatten when aggregating?  why wasn't done?
-        with open(metadata_path) as t:
-            metadata = js.load(t)
-        # might be too heavy to carry around, so will do basic flattening manually
-        # and in a basic fashion
-        # import jsonld
-        metadata_reduced = metadata[0]
-        for m in metadata[1:]:
-            metadata_reduced.update(m)
-        # but verify that they all had the same id
-        if metadata:
-            metaid = metadata[0]['@id']
-            assert all(m['@id'] == metaid for m in metadata)
-        rec["metadata"] = metadata_reduced
-    return rec
-
+    # make the custom renderer the default one, as the global default renderer
+    # does not yield meaningful output for this command
+    result_renderer = 'tailored'
+
+    _params_ = dict(
+        _common_diffstatus_params,
+        path=Parameter(
+            args=("path",),
+            metavar="PATH",
+            doc="""path to constrain the report to""",
+            nargs="*",
+            constraints=EnsureStr() | EnsureNone()),
+        fr=Parameter(
+            args=("-f", "--from",),
+            dest='fr',
+            metavar="REVISION",
+            doc="""original state to compare to, as given by any identifier
+            that Git understands.""",
+            constraints=EnsureStr()),
+        to=Parameter(
+            args=("-t", "--to",),
+            metavar="REVISION",
+            doc="""state to compare against the original state, as given by
+            any identifier that Git understands. If none is specified,
+            the state of the working tree will be compared.""",
+            constraints=EnsureStr() | EnsureNone()),
+    )
 
-def fs_render(fs_metadata, json=None, **kwargs):
-    """render node based on json option passed renders to file, stdout or deletes json at root
+    _examples_ = [
+        dict(text="Show unsaved changes in a dataset",
+             code_py="diff()",
+             code_cmd="datalad diff"),
+        dict(text="Compare a previous dataset state identified by shasum "
+                  "against current worktree",
+             code_py="diff(fr='SHASUM')",
+             code_cmd="datalad diff --from <SHASUM>"),
+        dict(text="Compare two branches against each other",
+             code_py="diff(fr='branch1', to='branch2')",
+             code_cmd="datalad diff --from branch1 --to branch2"),
+        dict(text="Show unsaved changes in the dataset and potential subdatasets",
+             code_py="diff(recursive=True)",
+             code_cmd="datalad diff -r"),
+        dict(text="Show unsaved changes made to a particular file",
+             code_py="diff(path='path/to/file')",
+             code_cmd="datalad diff <path/to/file>"),
+    ]
+
+    @staticmethod
+    @datasetmethod(name='diff')
+    @eval_results
+    def __call__(
+            path=None,
+            *,
+            fr='HEAD',
+            to=None,
+            dataset=None,
+            annex=None,
+            untracked='normal',
+            recursive=False,
+            recursion_limit=None):
+        yield from diff_dataset(
+            dataset=dataset,
+            fr=ensure_unicode(fr),
+            to=ensure_unicode(to),
+            constant_refs=False,
+            path=path,
+            annex=annex,
+            untracked=untracked,
+            recursive=recursive,
+            recursion_limit=recursion_limit)
+
+    @staticmethod
+    def custom_result_renderer(res, **kwargs):  # pragma: more cover
+        Status.custom_result_renderer(res, **kwargs)
+
+
+def diff_dataset(
+        dataset,
+        fr,
+        to,
+        constant_refs,
+        path=None,
+        annex=None,
+        untracked='normal',
+        recursive=False,
+        recursion_limit=None,
+        reporting_order='depth-first',
+        datasets_only=False,
+):
+    """Internal helper to diff a dataset
 
     Parameters
     ----------
-    fs_metadata: dict
-      Metadata json to be rendered
-    json: str ('file', 'display', 'delete')
-      Render to file, stdout or delete json
+    dataset : Dataset
+      Dataset to perform the diff on. `fr` and `to` parameters are interpreted
+      in the context of this dataset.
+    fr : str
+      Commit-ish to compare from.
+    to : str
+      Commit-ish to compare to.
+    constant_refs : bool
+      If True, `fr` and `to` will be passed on unmodified to diff operations
+      on subdatasets. This can be useful with symbolic references like tags
+      to report subdataset changes independent of superdataset changes.
+      If False, `fr` and `to` will be translated to the subdataset commit-ish
+      that match the given commit-ish in the superdataset.
+    path : Path-like, optional
+      Paths to constrain the diff to (see main diff() command).
+    annex : str, optional
+      Reporting mode for annex properties (see main diff() command).
+    untracked : str, optional
+      Reporting mode for untracked content (see main diff() command).
+    recursive : bool, optional
+      Flag to enable recursive operation (see main diff() command).
+    recursion_limit : int, optional
+      Recursion limit (see main diff() command).
+    reporting_order : {'depth-first', 'breadth-first', 'bottom-up'}, optional
+      By default, subdataset content records are reported after the record
+      on the subdataset's submodule in a superdataset (depth-first).
+      Alternatively, report all superdataset records first, before reporting
+      any subdataset content records (breadth-first). Both 'depth-first'
+      and 'breadth-first' both report dataset content before considering
+      subdatasets. Alternative 'bottom-up' mode is similar to 'depth-first'
+      but dataset content is reported after reporting on subdatasets.
+    datasets_only : bool, optional
+      Consider only changes to (sub)datasets but limiting operation only to
+      paths of subdatasets.
+      Note: ATM incompatible with explicit specification of `path`.
+
+    Yields
+    ------
+    dict
+      DataLad result records.
     """
+    if reporting_order not in ('depth-first', 'breadth-first', 'bottom-up'):
+        raise ValueError('Unknown reporting order: {}'.format(reporting_order))
 
-    metadata_file = metadata_locator(fs_metadata, **kwargs)
+    ds = require_dataset(
+        dataset, check_installed=True, purpose='report difference')
 
-    if json == 'file':
-        # create metadata_root directory if it doesn't exist
-        metadata_dir = dirname(metadata_file)
-        if not exists(metadata_dir):
-            makedirs(metadata_dir)
-        # write directory metadata to json
-        with open(metadata_file, 'w') as f:
-            js.dump(fs_metadata, f)
-
-    # else if json flag set to delete, remove .dir.json of current directory
-    elif json == 'delete' and exists(metadata_file):
-        remove(metadata_file)
-
-    # else dump json to stdout
-    elif json == 'display':
-        safe_print(js.dumps(fs_metadata) + '\n')
-
-
-def fs_traverse(path, repo, parent=None,
-                subdatasets=None,
-                render=True,
-                recurse_datasets=False,
-                recurse_directories=False,
-                json=None, basepath=None):
-    """Traverse path through its nodes and returns a dictionary of relevant
-    attributes attached to each node
-
-    Parameters
-    ----------
-    path: str
-      Path to the directory to be traversed
-    repo: AnnexRepo or GitRepo
-      Repo object the directory belongs too
-    parent: dict
-      Extracted info about parent directory
-    recurse_directories: bool
-      Recurse into subdirectories (note that subdatasets are not traversed)
-    render: bool
-       To render from within function or not. Set to false if results to be
-       manipulated before final render
-
-    Returns
-    -------
-    list of dict
-      extracts and returns a (recursive) list of directory info at path
-      does not traverse into annex, git or hidden directories
-    """
-    subdatasets = subdatasets or []
-    fs = fs_extract(path, repo, basepath=basepath or path)
-    if isdir(path):                     # if node is a directory
-        children = [fs.copy()]          # store its info in its children dict too  (Yarik is not sure why, but I guess for .?)
-        # ATM seems some pieces still rely on having this duplication, so left as is
-        # TODO: strip away
-        for node in listdir(path):
-            nodepath = opj(path, node)
-
-            # Might contain subdatasets, so we should analyze and prepare entries
-            # to pass down... in theory we could just pass full paths may be? strip
-            node_subdatasets = []
-            is_subdataset = False
-            if isdir(nodepath):
-                node_sep = with_pathsep(node)
-                for subds in subdatasets:
-                    if subds == node:
-                        # it is the subdataset
-                        is_subdataset = True
-                    else:
-                        # use path_is_subdir
-                        if subds.startswith(node_sep):
-                            node_subdatasets += [subds[len(node_sep):]]
-
-            # TODO:  it might be a subdir which is non-initialized submodule!
-            # if not ignored, append child node info to current nodes dictionary
-            if is_subdataset:
-                subds = _traverse_handle_subds(
-                    relpath(nodepath, repo.path),
-                    Dataset(repo.path),
-                    recurse_datasets=recurse_datasets,
-                    recurse_directories=recurse_directories,
-                    json=json
+    # we cannot really perform any sorting of paths into subdatasets
+    # or rejecting paths based on the state of the filesystem, as
+    # we need to be able to compare with states that are not represented
+    # in the worktree (anymore)
+    if path:
+        if datasets_only:
+            raise NotImplementedError(
+                "Analysis of provided paths in datasets_only mode is not implemented"
+            )
+
+        ps = []
+        # sort any path argument into the respective subdatasets
+        for p in sorted(ensure_list(path)):
+            # it is important to capture the exact form of the
+            # given path argument, before any normalization happens
+            # distinguish rsync-link syntax to identify
+            # a dataset as whole (e.g. 'ds') vs its
+            # content (e.g. 'ds/')
+            # special case is the root dataset, always report its content
+            # changes
+            orig_path = str(p)
+            resolved_path = resolve_path(p, dataset)
+            p = \
+                resolved_path, \
+                orig_path.endswith(op.sep) or resolved_path == ds.pathobj
+            str_path = str(p[0])
+            root = get_dataset_root(str_path)
+            if root is None:
+                # no root, not possibly underneath the refds
+                yield dict(
+                    action='status',
+                    path=str_path,
+                    refds=ds.path,
+                    status='error',
+                    message='path not underneath this dataset',
+                    logger=lgr)
+                continue
+            if path_under_rev_dataset(ds, str_path) is None:
+                # nothing we support handling any further
+                # there is only a single refds
+                yield dict(
+                    path=str_path,
+                    refds=ds.path,
+                    action='diff',
+                    status='error',
+                    message=(
+                        "dataset containing given paths is not underneath "
+                        "the reference dataset %s: %s",
+                        ds, str_path),
+                    logger=lgr,
                 )
-                children.append(subds)
-            elif not ignored(nodepath):
-                # if recursive, create info dictionary (within) each child node too
-                if recurse_directories:
-                    subdir = fs_traverse(nodepath,
-                                         repo,
-                                         subdatasets=node_subdatasets,
-                                         parent=None,  # children[0],
-                                         recurse_datasets=recurse_datasets,
-                                         recurse_directories=recurse_directories,
-                                         json=json,
-                                         basepath=basepath or path)
-                    subdir.pop('nodes', None)
-                else:
-                    # read child metadata from its metadata file if it exists
-                    subdir_json = metadata_locator(path=node, ds_path=basepath or path)
-                    if exists(subdir_json):
-                        with open(subdir_json) as data_file:
-                            subdir = js.load(data_file)
-                            subdir.pop('nodes', None)
-                    # else extract whatever information you can about the child
-                    else:
-                        # Yarik: this one is way too lean...
-                        subdir = fs_extract(nodepath,
-                                            repo,
-                                            basepath=basepath or path)
-                # append child metadata to list
-                children.extend([subdir])
-
-        # sum sizes of all 1st level children
-        children_size = {}
-        for node in children[1:]:
-            for size_type, child_size in node['size'].items():
-                children_size[size_type] = children_size.get(size_type, 0) + machinesize(child_size)
-
-        # update current node sizes to the humanized aggregate children size
-        fs['size'] = children[0]['size'] = \
-            {size_type: humanize.naturalsize(child_size)
-             for size_type, child_size in children_size.items()}
-
-        children[0]['name'] = '.'       # replace current node name with '.' to emulate unix syntax
-        if parent:
-            parent['name'] = '..'       # replace parent node name with '..' to emulate unix syntax
-            children.insert(1, parent)  # insert parent info after current node info in children dict
-
-        fs['nodes'] = children          # add children info to main fs dictionary
-        if render:                      # render directory node at location(path)
-            fs_render(fs, json=json, ds_path=basepath or path)
-            lgr.info('Directory: %s' % path)
-
-    return fs
-
-
-def ds_traverse(rootds, parent=None, json=None,
-                recurse_datasets=False, recurse_directories=False,
-                long_=False):
-    """Hierarchical dataset traverser
+                continue
 
-    Parameters
-    ----------
-    rootds: Dataset
-      Root dataset to be traversed
-    parent: Dataset
-      Parent dataset of the current rootds
-    recurse_datasets: bool
-      Recurse into subdatasets of the root dataset
-    recurse_directories: bool
-      Recurse into subdirectories of the current dataset
-      In both of above cases, if False, they will not be explicitly
-      recursed but data would be loaded from their meta-data files
-
-    Returns
-    -------
-    list of dict
-      extracts and returns a (recursive) list of dataset(s) info at path
-    """
-    # extract parent info to pass to traverser
-    fsparent = fs_extract(parent.path, parent.repo, basepath=rootds.path) \
-        if parent else None
-
-    # (recursively) traverse file tree of current dataset
-    fs = fs_traverse(
-        rootds.path, rootds.repo,
-        subdatasets=list(rootds.subdatasets(result_xfm='relpaths')),
-        render=False,
-        parent=fsparent,
-        # XXX note that here I kinda flipped the notions!
-        recurse_datasets=recurse_datasets,
-        recurse_directories=recurse_directories,
-        json=json
-    )
+            ps.append(p)
+        path = ps
 
-    # BUT if we are recurse_datasets but not recurse_directories
-    #     we need to handle those subdatasets then somehow since
-    #     otherwise we might not even get to them?!
-
-    fs['nodes'][0]['size'] = fs['size']  # update self's updated size in nodes sublist too!
-
-    # add dataset specific entries to its dict
-    rootds_model = GitModel(rootds.repo)
-    fs['tags'] = rootds_model.describe
-    fs['branch'] = rootds_model.branch
-    index_file = opj(rootds.path, '.git', 'index')
-    fs['index-mtime'] = time.strftime(
-        u"%Y-%m-%d %H:%M:%S",
-        time.localtime(getmtime(index_file))) if exists(index_file) else ''
-
-    # render current dataset
-    lgr.info('Dataset: %s' % rootds.path)
-    fs_render(fs, json=json, ds_path=rootds.path)
-    return fs
-
-
-def _traverse_handle_subds(
-        subds_rpath, rootds,
-        recurse_datasets, recurse_directories, json):
-    """A helper to deal with the subdataset node - recurse or just pick up
-    may be alrady collected in it web meta
-    """
-    subds_path = opj(rootds.path, subds_rpath)
-    subds = Dataset(subds_path)
-    subds_json = metadata_locator(path='.', ds_path=subds_path)
-
-    def handle_not_installed():
-        # for now just traverse as fs
-        lgr.warning("%s is either not installed or lacks meta-data", subds)
-        subfs = fs_extract(subds_path, rootds, basepath=rootds.path)
-        # but add a custom type that it is a not installed subds
-        subfs['type'] = 'uninitialized'
-        # we need to kick it out from 'children'
-        # TODO:  this is inefficient and cruel -- "ignored" should be made
-        # smarted to ignore submodules for the repo
-        #if fs['nodes']:
-        #    fs['nodes'] = [c for c in fs['nodes'] if c['path'] != subds_rpath]
-        return subfs
-
-    if not subds.is_installed():
-        subfs = handle_not_installed()
-    elif recurse_datasets:
-        subfs = ds_traverse(subds,
-                            json=json,
-                            recurse_datasets=recurse_datasets,
-                            recurse_directories=recurse_directories,
-                            parent=rootds)
-        subfs.pop('nodes', None)
-        #size_list.append(subfs['size'])
-    # else just pick the data from metadata_file of each subdataset
+    # TODO we might want to move away from the single-pass+immediate-yield
+    # paradigm for this command. If we gather all information first, we
+    # could do post-processing and detect when a file (same gitsha, or same
+    # key) was copied/moved from another dataset. Another command (e.g.
+    # save) could act on this information and also move/copy
+    # availability information or at least enhance the respective commit
+    # message with cross-dataset provenance info
+
+    # cache to help avoid duplicate status queries
+    content_info_cache = {}
+    for res in _diff_ds(
+            ds,
+            fr,
+            to,
+            constant_refs,
+            recursion_limit
+            if recursion_limit is not None and recursive
+            else -1 if recursive else 0,
+            # TODO recode paths to repo path reference
+            origpaths=None if not path else dict(path),
+            untracked=untracked,
+            annexinfo=annex,
+            cache=content_info_cache,
+            order=reporting_order,
+            datasets_only=datasets_only,
+    ):
+        res.update(
+            refds=ds.path,
+            logger=lgr,
+            action='diff',
+        )
+        yield res
+
+
+def _diff_ds(ds, fr, to, constant_refs, recursion_level, origpaths, untracked,
+             annexinfo, cache, order='depth-first', datasets_only=False):
+    if not ds.is_installed():
+        # asked to query a subdataset that is not available
+        lgr.debug("Skip diff of unavailable subdataset: %s", ds)
+        return
+
+    repo = ds.repo
+    repo_path = repo.pathobj
+    if datasets_only:
+        assert not origpaths  # protected above with NotImplementedError
+        paths = dict(
+            (sds.pathobj.relative_to(ds.pathobj), False)
+            for sds in ds.subdatasets(
+                recursive=False,
+                state='present',
+                result_renderer='disabled',
+                result_xfm='datasets',
+            )
+        )
+        if not paths:
+            # no subdatasets, nothing todo???
+            return
     else:
-        subfs = None
-        lgr.info(subds.path)
-        if exists(subds_json):
-            with open(subds_json) as data_file:
-                subfs = js.load(data_file)
-                subfs.pop('nodes', None)  # remove children
-                subfs['path'] = subds_rpath  # reassign the path
-                #size_list.append(subfs['size'])
+        # filter and normalize paths that match this dataset before passing them
+        # onto the low-level query method
+        paths = None if origpaths is None \
+            else dict(
+                (repo_path / p.relative_to(ds.pathobj), goinside)
+                for p, goinside in origpaths.items()
+                if ds.pathobj in p.parents or (p == ds.pathobj and goinside)
+            )
+    paths_arg = list(paths) if paths else None
+    try:
+        lgr.debug("Diff %s from '%s' to '%s'", ds, fr, to)
+        diff_state = repo.diffstatus(
+            fr,
+            to,
+            paths=paths_arg,
+            untracked=untracked,
+            eval_submodule_state='full' if to is None else 'commit',
+            _cache=cache)
+    except InvalidGitReferenceError as e:
+        yield dict(
+            path=ds.path,
+            status='impossible',
+            message=str(e),
+        )
+        return
+
+    if annexinfo and hasattr(repo, 'get_content_annexinfo'):
+        # this will amend `diff_state`
+        repo.get_content_annexinfo(
+            paths=paths_arg,
+            init=diff_state,
+            eval_availability=annexinfo in ('availability', 'all'),
+            ref=to)
+        # if `fr` is None, we compare against a preinit state, and
+        # a get_content_annexinfo on that state doesn't get us anything new
+        if fr and fr != to:
+            repo.get_content_annexinfo(
+                paths=paths_arg,
+                init=diff_state,
+                eval_availability=annexinfo in ('availability', 'all'),
+                ref=fr,
+                key_prefix="prev_")
+
+    # potentially collect subdataset diff call specs for the end
+    # (if order == 'breadth-first')
+    ds_diffs = []
+    subds_diffcalls = []
+    for path, props in diff_state.items():
+        pathinds = str(ds.pathobj / path.relative_to(repo_path))
+        path_rec = dict(
+            props,
+            path=pathinds,
+            # report the dataset path rather than the repo path to avoid
+            # realpath/symlink issues
+            parentds=ds.path,
+            status='ok',
+        )
+        if order in ('breadth-first', 'depth-first'):
+            yield path_rec
+        elif order == 'bottom-up':
+            ds_diffs.append(path_rec)
         else:
-            # the same drill as if not installed
-            lgr.warning("%s is installed but no meta-data yet", subds)
-            subfs = handle_not_installed()
-    return subfs
-
-
-def _ls_json(loc, fast=False, **kwargs):
-    # hierarchically traverse file tree of (sub-)dataset(s) under path
-    # passed(loc)
-    recurse_datasets = kwargs.pop('recursive', False)
-    recurse_directories = kwargs.pop('all_', False)
-    return ds_traverse(
-        Dataset(loc), parent=None,
-        recurse_directories=recurse_directories,
-        recurse_datasets=recurse_datasets,
-        **kwargs)
+            raise ValueError(order)
+        # for a dataset we need to decide whether to dive in, or not
+        if props.get('type', None) == 'dataset' and (
+                # subdataset path was given in rsync-style 'ds/'
+                (paths and paths.get(path, False))
+                # there is still sufficient recursion level left
+                or recursion_level != 0
+                # no recursion possible anymore, but one of the given
+                # path arguments is in this subdataset
+                or (recursion_level == 0
+                    and paths
+                    and any(path in p.parents for p in paths))):
+            subds_state = props.get('state', None)
+            if subds_state in ('clean', 'deleted'):
+                # no need to look into the subdataset
+                continue
+            elif subds_state in ('added', 'modified'):
+                # dive
+                subds = Dataset(pathinds)
+                call_args = (
+                    subds,
+                    # from before time or from the reported state
+                    fr if constant_refs
+                    else None
+                    if subds_state == 'added'
+                    else props['prev_gitshasum'],
+                    # to the last recorded state, or the worktree
+                    None if to is None
+                    else to if constant_refs
+                    else props['gitshasum'],
+                    constant_refs,
+                )
+                call_kwargs = dict(
+                    # subtract on level on the way down, unless the path
+                    # args instructed to go inside this subdataset
+                    recursion_level=recursion_level
+                    # protect against dropping below zero (would mean unconditional
+                    # recursion)
+                    if not recursion_level or (paths and paths.get(path, False))
+                    else recursion_level - 1,
+                    origpaths=origpaths,
+                    untracked=untracked,
+                    annexinfo=annexinfo,
+                    cache=cache,
+                    order=order,
+                    datasets_only=datasets_only,
+                )
+                if order in ('depth-first', 'bottom-up'):
+                    yield from _diff_ds(*call_args, **call_kwargs)
+                elif order == 'breadth-first':
+                    subds_diffcalls.append((call_args, call_kwargs))
+                else:
+                    raise ValueError(order)
+            else:
+                raise RuntimeError(
+                    "Unexpected subdataset state '{}'. That sucks!".format(
+                        subds_state))
+    # deal with staged ds diffs (for bottom-up)
+    for rec in ds_diffs:
+        yield rec
+    # deal with staged subdataset diffs (for breadth-first)
+    for call_args, call_kwargs in subds_diffcalls:
+        yield from _diff_ds(*call_args, **call_kwargs)
```

### Comparing `datalad-0.9.3/datalad/interface/rerun.py` & `datalad-1.0.0/datalad/local/download_url.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,334 +1,300 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""Rerun commands recorded with `datalad run`"""
+"""Interface to DataLad downloaders
+"""
 
 __docformat__ = 'restructuredtext'
 
+import os.path as op
+from logging import getLogger
 
-import logging
-from collections import namedtuple
-from itertools import dropwhile
-import json
-import re
-
-from datalad.dochelpers import exc_str
-from datalad.interface.base import Interface
-from datalad.interface.utils import eval_results
-from datalad.interface.base import build_doc
+from datalad.core.local.save import Save
+from datalad.distribution.dataset import (
+    Dataset,
+    EnsureDataset,
+    datasetmethod,
+    path_under_rev_dataset,
+    require_dataset,
+    resolve_path,
+)
+from datalad.downloaders.providers import Provider
+from datalad.interface.base import (
+    Interface,
+    build_doc,
+    eval_results,
+)
+from datalad.interface.common_opts import (
+    nosave_opt,
+    save_message_opt,
+)
 from datalad.interface.results import get_status_dict
-from datalad.interface.run import run_command
-
-from datalad.support.constraints import EnsureNone, EnsureStr
-from datalad.support.gitrepo import GitCommandError
+from datalad.support.annexrepo import AnnexRepo
+from datalad.support.constraints import (
+    EnsureNone,
+    EnsureStr,
+)
+from datalad.support.exceptions import (
+    CapturedException,
+    CommandError,
+    NoDatasetFound,
+)
 from datalad.support.param import Parameter
+from datalad.utils import (
+    Path,
+    PurePosixPath,
+    ensure_list_from_str,
+)
 
-from datalad.distribution.dataset import require_dataset
-from datalad.distribution.dataset import EnsureDataset
-from datalad.distribution.dataset import datasetmethod
-
-lgr = logging.getLogger('datalad.interface.rerun')
+lgr = getLogger('datalad.local.download-url')
 
 
 @build_doc
-class Rerun(Interface):
-    """Re-execute previous `datalad run` commands.
-
-    This will unlock any dataset content that is on record to have
-    been modified by the command in the specified revision.  It will
-    then re-execute the command in the recorded path (if it was inside
-    the dataset). Afterwards, all modifications will be saved.
-
-    Examples:
-
-        Re-execute the command from the previous commit.
-
-        $ datalad rerun
-
-        Re-execute any commands in the last five commits.
-
-        $ datalad rerun --since=HEAD~5
+class DownloadURL(Interface):
+    """Download content
 
-        Do the same as above, but re-execute the commands on top of
-        HEAD~5 in a detached state.
-
-        $ datalad rerun --onto= --since=HEAD~5
-
-        Re-execute all previous commands and compare the old and new
-        results.
-
-        $ # on master branch
-        $ datalad rerun --branch=verify --since=
-        $ # now on verify branch
-        $ datalad diff --revision=master..
-        $ git log --oneline --left-right --cherry-pick master...
+    It allows for a uniform download interface to various supported URL
+    schemes (see command help for details), re-using or asking for
+    authentication details maintained by datalad.
     """
+
     _params_ = dict(
-        revision=Parameter(
-            args=("revision",),
-            metavar="REVISION",
-            nargs="?",
-            doc="""rerun command(s) in REVISION. By default, the
-            command from this commit will be executed, but the --since
-            option can be used to construct a revision range.""",
-            default="HEAD",
-            constraints=EnsureStr()),
-        since=Parameter(
-            args=("--since",),
-            doc="""If SINCE is a commit-ish, the commands from all
-            commits that are reachable from REVISION but not SINCE
-            will be re-executed (in other words, the commands in `git
-            log SINCE..REVISION`). If SINCE is an empty string, it is
-            set to the parent of the first commit that contains a
-            recorded command (i.e., all commands in `git log REVISION`
-            will be re-executed).""",
-            constraints=EnsureStr() | EnsureNone()),
-        branch=Parameter(
-            metavar="NAME",
-            args=("-b", "--branch",),
-            doc="create and checkout this branch before rerunning the commands.",
-            constraints=EnsureStr() | EnsureNone()),
-        onto=Parameter(
-            metavar="base",
-            args=("--onto",),
-            doc="""start point for rerunning the commands. If not
-            specified, commands are executed at HEAD. This option can
-            be used to specify an alternative start point, which will
-            be checked out with the branch name specified by --branch
-            or in a detached state otherwise. As a special case, an
-            empty value for this option means to use the commit
-            specified by --since.""",
-            constraints=EnsureStr() | EnsureNone()),
-        message=Parameter(
-            args=("-m", "--message",),
-            metavar="MESSAGE",
-            doc="""use MESSAGE for the reran commit rather than the
-            recorded commit message.  In the case of a multi-commit
-            rerun, all the reran commits will have this message.""",
-            constraints=EnsureStr() | EnsureNone()),
+        urls=Parameter(
+            doc="""URL(s) to be downloaded. Supported protocols: {}""".format(
+                ", ".join(map(repr, sorted(Provider.DOWNLOADERS)))),
+            constraints=EnsureStr(),  # TODO: EnsureURL
+            metavar='url',
+            args=('urls',),
+            nargs='+'),
         dataset=Parameter(
             args=("-d", "--dataset"),
-            doc="""specify the dataset from which to rerun a recorded
-            command. If no dataset is given, an attempt is made to
-            identify the dataset based on the current working
-            directory. If a dataset is given, the command will be
-            executed in the root directory of this dataset.""",
+            metavar='PATH',
+            doc="""specify the dataset to add files to. If no dataset is given,
+            an attempt is made to identify the dataset based on the current
+            working directory. Use [CMD: --nosave CMD][PY: save=False PY] to
+            prevent adding files to the dataset.""",
             constraints=EnsureDataset() | EnsureNone()),
-        # TODO
-        # --list-commands
-        #   go through the history and report any recorded command. this info
-        #   could be used to unlock the associated output files for a rerun
+        overwrite=Parameter(
+            args=("-o", "--overwrite"),
+            action="store_true",
+            doc="""flag to overwrite it if target file exists"""),
+        path=Parameter(
+            args=("-O", "--path"),
+            doc="""target for download. If the path has a trailing separator,
+            it is treated as a directory, and each specified URL is downloaded
+            under that directory to a base name taken from the URL. Without a
+            trailing separator, the value specifies the name of the downloaded
+            file (file name extensions inferred from the URL may be added to it,
+            if they are not yet present) and only a single URL should be given.
+            In both cases, leading directories will be created if needed. This
+            argument defaults to the current directory.""",
+            constraints=EnsureStr() | EnsureNone()),
+        archive=Parameter(
+            args=("--archive",),
+            action="store_true",
+            doc="""pass the downloaded files to [CMD: :command:`datalad
+            add-archive-content --delete` CMD][PY: add_archive_content(...,
+            delete=True) PY]"""),
+        save=nosave_opt,
+        message=save_message_opt
     )
 
+    _examples_ = [
+        dict(text="Download files from an http and S3 URL",
+             code_py="download_url(urls=['http://example.com/file.dat', 's3://bucket/file2.dat'])",
+             code_cmd="datalad download-url http://example.com/file.dat s3://bucket/file2.dat"),
+        dict(text="Download a file to a path and provide a commit message",
+             code_py="download_url(urls='s3://bucket/file2.dat', message='added a file', path='myfile.dat')",
+             code_cmd="""datalad download-url -m 'added a file' -O myfile.dat \\
+                         s3://bucket/file2.dat"""),
+        dict(text="Append a trailing slash to the target path "
+                  "to download into a specified directory",
+             code_py="download_url(['http://example.com/file.dat'], path='data/')",
+             code_cmd="datalad download-url --path=data/ http://example.com/file.dat"),
+        dict(text="Leave off the trailing slash to download into a regular file",
+             code_py="download_url(['http://example.com/file.dat'], path='data')",
+             code_cmd="datalad download-url --path=data http://example.com/file.dat"),
+    ]
+
     @staticmethod
-    @datasetmethod(name='rerun')
+    @datasetmethod(name="download_url")
     @eval_results
-    def __call__(
-            revision="HEAD",
-            since=None,
-            dataset=None,
-            branch=None,
-            message=None,
-            onto=None):
-
-        ds = require_dataset(
-            dataset, check_installed=True,
-            purpose='rerunning a command')
-
-        lgr.debug('rerunning command output underneath %s', ds)
-
-        from datalad.tests.utils import ok_clean_git
-        try:
-            ok_clean_git(ds.path)
-        except AssertionError:
-            yield get_status_dict(
-                'run',
-                ds=ds,
-                status='impossible',
-                message=('unsaved modifications present, '
-                         'cannot detect changes by command'))
-            return
-
-        err_info = get_status_dict('run', ds=ds)
-        if not ds.repo.get_hexsha():
-            yield dict(
-                err_info, status='impossible',
-                message='cannot rerun command, nothing recorded')
-            return
-
-        if branch and branch in ds.repo.get_branches():
-            yield get_status_dict(
-                "run", ds=ds, status="error",
-                message="branch '{}' already exists".format(branch))
-            return
-
-        if not commit_exists(ds, revision + "^"):
-            # Only a single commit is reachable from `revision`.  In
-            # this case, --since has no effect on the range construction.
-            revrange = revision
-        elif since is None:
-            revrange = "{rev}^..{rev}".format(rev=revision)
-        elif since.strip() == "":
-            revrange = revision
-        else:
-            revrange = "{}..{}".format(since, revision)
-
-        if ds.repo.repo.git.rev_list("--merges", revrange, "--"):
-            yield get_status_dict(
-                "run", ds=ds, status="error",
-                message="cannot rerun history with merge commits")
-            return
-
-        Revision = namedtuple("Revision", ["id", "message", "info"])
-
-        def revision_with_info(rev):
-            msg, info = get_commit_runinfo(ds.repo, rev)
-            return Revision(rev, msg, info)
-
-        ids = ds.repo.repo.git.rev_list("--reverse", revrange, "--").split()
-
-        try:
-            revs = list(map(revision_with_info, ids))
-        except ValueError as exc:
-            yield dict(err_info, status='error', message=exc_str(exc))
-            return
-
-        if since is not None and since.strip() == "":
-            # For --since='', drop any leading commits that don't have
-            # a run command.
-            revs = list(dropwhile(lambda r: r.info is None, revs))
-
-        if onto is not None and onto.strip() == "":
-            # Special case: --onto='' is the value of --since.
-            # Because we're currently aborting if the revision list
-            # contains merges, we know that, regardless of if and how
-            # --since is specified, the effective value for --since is
-            # the parent of the first revision.
-            onto = revs[0].id + "^"
-            if not commit_exists(ds, onto):
-                # This is unlikely to happen in the wild because it
-                # means that the first commit is a datalad run commit.
-                # Just abort rather than trying to checkout on orphan
-                # branch or something like that.
+    def __call__(urls,
+                 *,
+                 dataset=None, path=None, overwrite=False,
+                 archive=False, save=True, message=None):
+        from ..downloaders.http import HTTPDownloader
+        from ..downloaders.providers import Providers
+
+        ds = None
+        if save or dataset:
+            try:
+                ds = require_dataset(
+                    dataset, check_installed=True,
+                    purpose='download urls')
+            except NoDatasetFound:
+                pass
+
+        common_report = {"action": "download_url",
+                         "ds": ds}
+
+        got_ds_instance = isinstance(dataset, Dataset)
+        dir_is_target = not path or str(path).endswith(op.sep)
+        path = str(resolve_path(path or op.curdir, ds=dataset))
+        if dir_is_target:
+            # resolve_path() doesn't preserve trailing separators. Add one for
+            # the download() call.
+            path = path + op.sep
+        urls = ensure_list_from_str(urls)
+
+        if not dir_is_target:
+            if len(urls) > 1:
                 yield get_status_dict(
-                    "run", ds=ds, status="error",
-                    message="Commit for --onto does not exist.")
+                    status="error",
+                    message=(
+                        "When specifying multiple urls, --path should point to "
+                        "a directory target (with a trailing separator). Got %r",
+                        path),
+                    type="file",
+                    path=path,
+                    **common_report)
+                return
+            if archive:
+                # make sure the file suffix indicated by a URL is preserved
+                # so that any further archive processing doesn't have to
+                # employ mime type inspection in order to determine the archive
+                # type
+                from datalad.support.network import URL
+                suffixes = PurePosixPath(URL(urls[0]).path).suffixes
+                if not Path(path).suffixes == suffixes:
+                    path += ''.join(suffixes)
+            # we know that we have a single URL
+            # download() would be fine getting an existing directory and
+            # downloading the URL underneath it, but let's enforce a trailing
+            # slash here for consistency.
+            if op.isdir(path):
+                yield get_status_dict(
+                    status="error",
+                    message=(
+                        "Non-directory path given (no trailing separator) "
+                        "but a directory with that name (after adding archive "
+                        "suffix) exists"),
+                    type="file",
+                    path=path,
+                    **common_report)
                 return
 
-        if branch or onto:
-            start_point = onto or "HEAD"
-            if branch:
-                checkout_options = ["-b", branch]
-            else:
-                checkout_options = ["--detach"]
-            ds.repo.checkout(start_point, options=checkout_options)
-
-        for rev in revs:
-            if not rev.info:
-                pick = False
-                try:
-                    ds.repo.repo.git.merge_base("--is-ancestor", rev.id, "HEAD")
-                except GitCommandError:  # Revision is NOT an ancestor of HEAD.
-                    pick = True
-
-                shortrev = ds.repo.repo.git.rev_parse("--short", rev.id)
-                err_msg = "no command for {} found; {}".format(
-                    shortrev,
-                    "cherry picking" if pick else "skipping")
-                yield dict(err_info, status='ok', message=err_msg)
-
-                if pick:
-                    ds.repo.repo.git.cherry_pick(rev.id)
-                continue
-
-            # Keep a "rerun" trail.
-            if "chain" in rev.info:
-                rev.info["chain"].append(rev.id)
+        # TODO setup fancy ui.progressbars doing this in parallel and reporting overall progress
+        # in % of urls which were already downloaded
+        providers = Providers.from_config_files()
+        downloaded_paths = []
+        path_urls = {}
+        need_datalad_remote = False
+        for url in urls:
+            # somewhat "ugly"
+            downloader = providers.get_provider(url).get_downloader(url)
+            try:
+                downloaded_path = downloader.download(url, path=path, overwrite=overwrite)
+            except Exception as e:
+                ce = CapturedException(e)
+                yield get_status_dict(
+                    status="error",
+                    message=str(ce),
+                    type="file",
+                    path=path,
+                    exception=ce,
+                    **common_report)
             else:
-                rev.info["chain"] = [rev.id]
-
-            # now we have to find out what was modified during the
-            # last run, and enable re-modification ideally, we would
-            # bring back the entire state of the tree with #1424, but
-            # we limit ourself to file addition/not-in-place-modification
-            # for now
-            for r in ds.unlock(new_or_modified(ds, rev.id),
-                               return_type='generator', result_xfm=None):
-                yield r
-
-            for r in run_command(rev.info['cmd'], ds, message or rev.message,
-                                 rerun_info=rev.info):
+                if not need_datalad_remote \
+                   and (downloader.authenticator or downloader.credential or
+                        type(downloader) != HTTPDownloader):
+                    need_datalad_remote = True
+                downloaded_paths.append(downloaded_path)
+                path_urls[downloaded_path] = url
+                yield get_status_dict(
+                    status="ok",
+                    type="file",
+                    path=downloaded_path,
+                    **common_report)
+
+        if downloaded_paths and save and ds is not None:
+            msg = message or """\
+[DATALAD] Download URLs
+
+URLs:
+  {}""".format("\n  ".join(urls))
+
+            for r in Save()(downloaded_paths, message=msg,
+                            # ATTN: Pass the original dataset argument to
+                            # preserve relative path handling semantics.
+                            dataset=dataset,
+                            return_type="generator",
+                            result_renderer='disabled',
+                            result_xfm=None,
+                            result_filter=None,
+                            on_failure="ignore"):
                 yield r
 
-
-def get_commit_runinfo(repo, commit="HEAD"):
-    """Return message and run record from a commit message
-
-    If none found - returns None, None; if anything goes wrong - throws
-    ValueError with the message describing the issue
-    """
-    commit_msg = repo.repo.git.show(commit, "--format=%s%n%n%b", "--no-patch")
-    cmdrun_regex = r'\[DATALAD RUNCMD\] (.*)=== Do not change lines below ' \
-                   r'===\n(.*)\n\^\^\^ Do not change lines above \^\^\^'
-    runinfo = re.match(cmdrun_regex, commit_msg,
-                       re.MULTILINE | re.DOTALL)
-    if not runinfo:
-        return None, None
-
-    rec_msg, runinfo = runinfo.groups()
-
-    try:
-        runinfo = json.loads(runinfo)
-    except Exception as e:
-        raise ValueError(
-            'cannot rerun command, command specification is not valid JSON: '
-            '%s' % exc_str(e)
-        )
-    if 'cmd' not in runinfo:
-        raise ValueError(
-            "{} looks like a run commit but does not have a command".format(
-                repo.repo.git.rev_parse("--short", commit)))
-    return rec_msg, runinfo
-
-
-def new_or_modified(dataset, revision="HEAD"):
-    """Yield files that have been added or modified in `revision`.
-
-    Parameters
-    ----------
-    dataset : Dataset
-    revision : string, optional
-        Commit-ish of interest.
-
-    Returns
-    -------
-    Generator that yields AnnotatePaths instances
-    """
-    if commit_exists(dataset, revision + "^"):
-        revrange = "{rev}^..{rev}".format(rev=revision)
-    else:
-        # No other commits are reachable from this revision.  Diff
-        # with an empty tree instead.
-        #             git hash-object -t tree /dev/null
-        empty_tree = "4b825dc642cb6eb9a060e54bf8d69288fbee4904"
-        revrange = "{}..{}".format(empty_tree, revision)
-    diff = dataset.diff(recursive=True,
-                        revision=revrange,
-                        return_type='generator', result_renderer=None)
-    for r in diff:
-        if r.get('type') == 'file' and r.get('state') in ['added', 'modified']:
-            r.pop('status', None)
-            yield r
-
-
-def commit_exists(dataset, commit):
-    try:
-        dataset.repo.repo.git.rev_parse("--verify", commit + "^{commit}")
-    except:
-        return False
-    return True
+            ds_repo = ds.repo
+            if isinstance(ds_repo, AnnexRepo):
+                if need_datalad_remote:
+                    from datalad.customremotes.base import (
+                        ensure_datalad_remote,
+                    )
+                    ensure_datalad_remote(
+                        ds_repo, autoenable=True, encryption=None)
+
+                if got_ds_instance:
+                    # Paths in `downloaded_paths` are already relative to the
+                    # dataset.
+                    rpaths = dict(zip(downloaded_paths, downloaded_paths))
+                else:
+                    # Paths in `downloaded_paths` are already relative to the
+                    # current working directory. Take these relative to the
+                    # dataset for use with the AnnexRepo method calls.
+                    rpaths = {}
+                    for orig_path, resolved in zip(
+                            downloaded_paths,
+                            resolve_path(downloaded_paths, ds=dataset)):
+                        rpath = path_under_rev_dataset(ds, resolved)
+                        if rpath:
+                            rpaths[str(rpath)] = orig_path
+                        else:
+                            lgr.warning("Path %s not under dataset %s",
+                                        orig_path, ds)
+                annex_paths = [p for p, annexed in
+                               zip(rpaths,
+                                   ds_repo.is_under_annex(list(rpaths.keys())))
+                               if annexed]
+                if annex_paths:
+                    for path in annex_paths:
+                        url = path_urls[rpaths[path]]
+                        try:
+                            # The file is already present. This is just to
+                            # register the URL.
+                            ds_repo.add_url_to_file(
+                                path,
+                                url,
+                                # avoid batch mode for single files
+                                # https://github.com/datalad/datalad/issues/2849
+                                batch=len(annex_paths) > 1,
+                                # bypass URL size check, we already have the file
+                                options=['--relaxed'])
+                        except CommandError as exc:
+                            lgr.warning("Registering %s with %s failed: %s",
+                                        path, url, CapturedException(exc))
+
+                    if archive:
+                        for path in annex_paths:
+                            yield from ds.add_archive_content(
+                                path,
+                                delete=True,
+                                on_failure='ignore',
+                                return_type='generator',
+                                result_renderer='disabled'
+                            )
```

### Comparing `datalad-0.9.3/datalad/__main__.py` & `datalad-1.0.0/datalad/__main__.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,20 +1,19 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 """Helper to use datalad as a "runnable" module with  -m datalad"""
 
 import sys
 from . import __version__
-from .auto import AutomagicIO
 from .log import lgr
 
 
 def usage(outfile, executable=sys.argv[0]):
     if '__main__.py' in executable:
         # That was -m datalad way to launch
         executable = "%s -m datalad" % sys.executable
@@ -69,14 +68,29 @@
             usage(sys.stdout, executable=argv[0])
             sys.exit(0)
 
         if opt == "--version":
             sys.stdout.write("datalad %s\n" % __version__)
             sys.exit(0)
 
+    try:
+        from datalad_deprecated.auto import AutomagicIO
+    except Exception as e:
+        # we could just test for ModuleNotFoundError (which should be
+        # all that would happen with PY3.6+, but be a little more robust
+        # and use the pattern from duecredit
+        if type(e).__name__ not in ('ImportError', 'ModuleNotFoundError'):
+            lgr.error("Failed to import datalad_deprecated.auto "
+                      "due to %s", str(e))
+        sys.stderr.write("{}\n".format(
+            "The DataLad AutomagicIO functionality has been moved to an "
+            "extension package. Please install the Python package "
+            "`datalad_deprecated` to be able to use it."))
+        sys.exit(1)
+
     sys.argv = prog_argv
     progname = prog_argv[0]
     sys.path[0] = os.path.split(progname)[0]
 
     try:
         with open(progname) as fp:
             code = compile(fp.read(), progname, 'exec')
@@ -89,14 +103,15 @@
         }
         # Since used explicitly -- activate the beast
         aio = AutomagicIO(activate=True)
         lgr.info("Running code of %s", progname)
         runctx(code, globs, globs)
         # TODO: see if we could hide our presence from the final tracebacks if execution fails
     except IOError as err:
-        lgr.error("Cannot run file %r because: %s" % (sys.argv[0], err))
+        lgr.error("Cannot run file %r because: %s", sys.argv[0], err)
         sys.exit(1)
     except SystemExit:
         pass
 
+
 if __name__ == '__main__':
     main()
```

### Comparing `datalad-0.9.3/datalad/resources/website/assets/images/favicon.ico` & `datalad-1.0.0/docs/source/_static/favicon.ico`

 * *Files identical despite different names*

### Comparing `datalad-0.9.3/datalad/customremotes/base.py` & `datalad-1.0.0/datalad/local/rerun.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,686 +1,724 @@
-# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
-# vi: set ft=python sts=4 ts=4 sw=4 et:
-### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
+# emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
+# ex: set sts=4 ts=4 sw=4 et:
+# ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
-### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""Base classes to custom git-annex remotes (e.g. extraction from archives)"""
-
-from __future__ import absolute_import
+# ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
+"""Rerun commands recorded with `datalad run`"""
 
 __docformat__ = 'restructuredtext'
 
-import inspect
-import errno
-import os
-import sys
-
-from os.path import exists, join as opj, realpath, dirname, lexists
-
-from six.moves import range
-from six.moves.urllib.parse import urlparse
 
+import json
 import logging
-lgr = logging.getLogger('datalad.customremotes')
-lgr.log(5, "Importing datalad.customremotes.main")
-
-from ..ui import ui
-from ..support.protocol import ProtocolInterface
-from ..support.external_versions import external_versions
-from ..support.cache import DictCache
-from ..cmdline.helpers import get_repo_instance
-from ..dochelpers import exc_str
-
-
-URI_PREFIX = "dl"
-SUPPORTED_PROTOCOL = 1
-
-DEFAULT_COST = 100
-DEFAULT_AVAILABILITY = "LOCAL"
-
-from datalad.ui.progressbars import ProgressBarBase
-
-
-class AnnexRemoteQuit(Exception):
-    pass
-
-
-def get_function_nargs(f):
-    while hasattr(f, 'wrapped'):
-        f = f.wrapped
-    argspec = inspect.getargspec(f)
-    assert not argspec.keywords, \
-        "ATM we have none defined with keywords, so disabling having them"
-    if argspec.varargs:
-        # Variable number of arguments
-        return -1
-    else:
-        assert argspec.args, "ATM no static methods"
-        assert argspec.args[0] == "self"
-        return len(argspec.args) - 1
-
-
-class AnnexExchangeProtocol(ProtocolInterface):
-    """A little helper to protocol interactions of custom remote with annex
+import os.path as op
+import re
+import sys
+from copy import copy
+from functools import partial
+from itertools import dropwhile
+
+from datalad.consts import PRE_INIT_COMMIT_SHA
+from datalad.core.local.run import (
+    _format_cmd_shorty,
+    assume_ready_opt,
+    format_command,
+    run_command,
+)
+from datalad.distribution.dataset import (
+    EnsureDataset,
+    datasetmethod,
+    require_dataset,
+)
+from datalad.interface.base import (
+    Interface,
+    build_doc,
+    eval_results,
+)
+from datalad.interface.results import get_status_dict
+from datalad.interface.common_opts import jobs_opt
+
+from datalad.support.constraints import (
+    EnsureNone,
+    EnsureStr,
+)
+from datalad.support.exceptions import CapturedException
+from datalad.support.json_py import load_stream
+from datalad.support.param import Parameter
+
+lgr = logging.getLogger('datalad.local.rerun')
+
+rerun_assume_ready_opt = copy(assume_ready_opt)
+rerun_assume_ready_opt._doc += """
+Note that this option also affects any additional outputs that are
+automatically inferred based on inspecting changed files in the run commit."""
+
+
+@build_doc
+class Rerun(Interface):
+    """Re-execute previous `datalad run` commands.
+
+    This will unlock any dataset content that is on record to have
+    been modified by the command in the specified revision.  It will
+    then re-execute the command in the recorded path (if it was inside
+    the dataset). Afterwards, all modifications will be saved.
+
+    *Report mode*
+
+    || REFLOW >>
+    When called with [CMD: --report CMD][PY: report=True PY], this command
+    reports information about what would be re-executed as a series of records.
+    There will be a record for each revision in the specified revision range.
+    Each of these will have one of the following "rerun_action" values:
+    << REFLOW ||
+
+      - run: the revision has a recorded command that would be re-executed
+      - skip-or-pick: the revision does not have a recorded command and would
+        be either skipped or cherry picked
+      - merge: the revision is a merge commit and a corresponding merge would
+        be made
+
+    The decision to skip rather than cherry pick a revision is based on whether
+    the revision would be reachable from HEAD at the time of execution.
+
+    In addition, when a starting point other than HEAD is specified, there is a
+    rerun_action value "checkout", in which case the record includes
+    information about the revision the would be checked out before rerunning
+    any commands.
+
+    .. note::
+      Currently the "onto" feature only sets the working tree of the current
+      dataset to a previous state. The working trees of any subdatasets remain
+      unchanged.
     """
+    _params_ = dict(
+        revision=Parameter(
+            args=("revision",),
+            metavar="REVISION",
+            nargs="?",
+            doc="""rerun command(s) in `revision`. By default, the command from
+            this commit will be executed, but [CMD: --since CMD][PY: `since`
+            PY] can be used to construct a revision range. The default value is
+            like "HEAD" but resolves to the main branch when on an adjusted
+            branch.""",
+            default=None,
+            constraints=EnsureStr() | EnsureNone()),
+        since=Parameter(
+            args=("--since",),
+            doc="""If `since` is a commit-ish, the commands from all commits
+            that are reachable from `revision` but not `since` will be
+            re-executed (in other words, the commands in :command:`git log
+            SINCE..REVISION`). If SINCE is an empty string, it is set to the
+            parent of the first commit that contains a recorded command (i.e.,
+            all commands in :command:`git log REVISION` will be
+            re-executed).""",
+            constraints=EnsureStr() | EnsureNone()),
+        branch=Parameter(
+            metavar="NAME",
+            args=("-b", "--branch",),
+            doc="create and checkout this branch before rerunning the commands.",
+            constraints=EnsureStr() | EnsureNone()),
+        onto=Parameter(
+            metavar="base",
+            args=("--onto",),
+            doc="""start point for rerunning the commands. If not specified,
+            commands are executed at HEAD. This option can be used to specify
+            an alternative start point, which will be checked out with the
+            branch name specified by [CMD: --branch CMD][PY: `branch` PY] or in
+            a detached state otherwise. As a special case, an empty value for
+            this option means the parent of the first run commit in the
+            specified revision list.""",
+            constraints=EnsureStr() | EnsureNone()),
+        message=Parameter(
+            args=("-m", "--message",),
+            metavar="MESSAGE",
+            doc="""use MESSAGE for the reran commit rather than the
+            recorded commit message.  In the case of a multi-commit
+            rerun, all the reran commits will have this message.""",
+            constraints=EnsureStr() | EnsureNone()),
+        script=Parameter(
+            args=("--script",),
+            metavar="FILE",
+            doc="""extract the commands into [CMD: FILE CMD][PY: this file PY]
+            rather than rerunning. Use - to write to stdout instead. [CMD: This
+            option implies --report. CMD]""",
+            constraints=EnsureStr() | EnsureNone()),
+        dataset=Parameter(
+            args=("-d", "--dataset"),
+            doc="""specify the dataset from which to rerun a recorded
+            command. If no dataset is given, an attempt is made to
+            identify the dataset based on the current working
+            directory. If a dataset is given, the command will be
+            executed in the root directory of this dataset.""",
+            constraints=EnsureDataset() | EnsureNone()),
+        report=Parameter(
+            args=("--report",),
+            action="store_true",
+            doc="""Don't actually re-execute anything, just display what would
+            be done. [CMD: Note: If you give this option, you most likely want
+            to set --output-format to 'json' or 'json_pp'. CMD]"""),
+        assume_ready=rerun_assume_ready_opt,
+        explicit=Parameter(
+            args=("--explicit",),
+            action="store_true",
+            doc="""Consider the specification of inputs and outputs in the run
+            record to be explicit. Don't warn if the repository is dirty, and
+            only save modifications to the outputs from the original record.
+            Note that when several run commits are specified, this applies to
+            every one. Care should also be taken when using [CMD: --onto
+            CMD][PY: `onto` PY] because checking out a new HEAD can easily fail
+            when the working tree has modifications."""),
+        jobs=jobs_opt
+    )
+
+    _examples_ = [
+        dict(text="Re-execute the command from the previous commit",
+             code_py="rerun()",
+             code_cmd="datalad rerun"),
+        dict(text="Re-execute any commands in the last five commits",
+             code_py="rerun(since='HEAD~5')",
+             code_cmd="datalad rerun --since=HEAD~5"),
+        dict(text="Do the same as above, but re-execute the commands on top of "
+                  "HEAD~5 in a detached state",
+             code_py="rerun(onto='', since='HEAD~5')",
+             code_cmd="datalad rerun --onto= --since=HEAD~5"),
+        dict(text="Re-execute all previous commands and compare the old and "
+                  "new results",
+             code_cmd="""% # on master branch
+                % datalad rerun --branch=verify --since=
+                % # now on verify branch
+                % datalad diff --revision=master..
+                % git log --oneline --left-right --cherry-pick master..."""),
+    ]
 
-    HEADER = r"""#!/bin/bash
-
-set -e
-
-# Gets a VALUE response and stores it in $RET
-report () {
-    echo "$@" >&2
-}
-
-recv () {
-    read resp
-    #resp=${resp%\n}
-    target="$@"
-    if [ "$resp" != "$target" ]; then
-        report "! exp $target"
-        report "  got $resp"
-    else
-        report "+ got $resp"
-    fi
-}
-
-send () {
-    echo "$@"
-    report "sent $@"
-}
-
-"""
-
-    def __init__(self, repopath, custom_remote_name=None):
-        super(AnnexExchangeProtocol, self).__init__()
-        self.repopath = repopath
-        self.custom_remote_name = custom_remote_name
-        self._file = None
-        self._initiated = False
 
-    def initiate(self):
-        if self._initiated:
+    @staticmethod
+    @datasetmethod(name='rerun')
+    @eval_results
+    def __call__(
+            revision=None,
+            *,
+            since=None,
+            dataset=None,
+            branch=None,
+            message=None,
+            onto=None,
+            script=None,
+            report=False,
+            assume_ready=None,
+            explicit=False,
+            jobs=None):
+
+        ds = require_dataset(
+            dataset, check_installed=True,
+            purpose='rerun a command')
+        ds_repo = ds.repo
+
+        lgr.debug('rerunning command output underneath %s', ds)
+
+        if script is None and not (report or explicit) and ds_repo.dirty:
+            yield get_status_dict(
+                'run',
+                ds=ds,
+                status='impossible',
+                message=(
+                    'clean dataset required to detect changes from command; '
+                    'use `datalad status` to inspect unsaved changes'))
             return
-        self._initiated = True
-        d = opj(self.repopath, '.git', 'bin')
-        if not exists(d):
-            os.makedirs(d)
-
-        suf = '-' + self.custom_remote_name.rstrip(':') if self.custom_remote_name else ''
-        self._file = _file = opj(d, 'git-annex-remote-datalad' + suf)
-
-        if exists(_file):
-            lgr.debug("Commenting out previous entries")
-            # comment out all the past entries
-            with open(_file) as f:
-                entries = f.readlines()
-            for i in range(len(self.HEADER.split(os.linesep)), len(entries)):
-                e = entries[i]
-                if e.startswith('recv ') or e.startswith('send '):
-                    entries[i] = '#' + e
-            with open(_file, 'w') as f:
-                f.write(''.join(entries))
-            return  # nothing else to be done
-
-        lgr.debug("Initiating protocoling."
-                  "cd %s; vim %s"
-                  % (realpath(self.repopath),
-                     _file[len(self.repopath) + 1:]))
-        with open(_file, 'a') as f:
-            f.write(self.HEADER)
-        os.chmod(_file, 0o755)
-
-    def write_section(self, cmd):
-        self.initiate()
-        with open(self._file, 'a') as f:
-            f.write('%s### %s%s' % (os.linesep, cmd, os.linesep))
-        lgr.debug("New section in the protocol: "
-                  "cd %s; PATH=%s:$PATH %s"
-                  % (realpath(self.repopath),
-                     dirname(self._file),
-                     cmd))
-
-    def write_entries(self, entries):
-        self.initiate()
-        with open(self._file, 'a') as f:
-            f.write(os.linesep.join(entries + ['']))
-
-    def __iadd__(self, entry):
-        self.initiate()
-        with open(self._file, 'a') as f:
-            f.write(entry + os.linesep)
-        return self
-
-    def start_section(self, cmd):
-        self._sections.append({'command': cmd})
-        self.write_section(cmd)
-        return len(self._sections) - 1
-
-    def end_section(self, id_, exception):
-        # raise exception in case of invalid id_ for consistency:
-        self._sections.__getitem__(id_)
-
-    def add_section(self, cmd, exception):
-        self.start_section(cmd)
-
-    @property
-    def records_callables(self):
-        return False
-
-    @property
-    def records_ext_commands(self):
-        return True
-
-    @property
-    def do_execute_ext_commands(self):
-        return True
-
-    @property
-    def do_execute_callables(self):
-        return True
-
-
-class AnnexCustomRemote(object):
-    """Base class to provide custom special remotes for git-annex
-
-    Implements git-annex special custom remotes protocol described
-    at
-    http://git-annex.branchable.com/design/external_special_remote_protocol/
-    """
-
-    # Must be defined in subclasses.  There is no classlevel properties, so leaving as this for now
 
-    CUSTOM_REMOTE_NAME = None  # if None -- no additional custom remote name
-    SUPPORTED_SCHEMES = ()
+        if not ds_repo.get_hexsha():
+            yield get_status_dict(
+                'run', ds=ds,
+                status='impossible',
+                message='cannot rerun command, nothing recorded')
+            return
 
-    COST = DEFAULT_COST
-    AVAILABILITY = DEFAULT_AVAILABILITY
+        # ATTN: Use get_corresponding_branch() rather than is_managed_branch()
+        # for compatibility with a plain GitRepo.
+        if (onto is not None or branch is not None) and \
+           ds_repo.get_corresponding_branch():
+            yield get_status_dict(
+                "run", ds=ds, status="impossible",
+                message=("--%s is incompatible with adjusted branch",
+                         "branch" if onto is None else "onto"))
+            return
 
-    def __init__(self, path=None, cost=None, fin=None, fout=None):  # , availability=DEFAULT_AVAILABILITY):
-        """
-        Parameters
-        ----------
-        path : string, optional
-            Path to the repository for which this custom remote is serving.
-            Usually this class is instantiated by a script which runs already
-            within that directory, so the default is to point to current
-            directory, i.e. '.'
-        fin:
-        fout:
-            input/output streams.  If not specified, stdin, stdout used
-        """
-        # TODO: probably we shouldn't have runner here but rather delegate
-        # to AnnexRepo's functionality
-        from ..support.annexrepo import AnnexRepo
-        from ..cmd import GitRunner
-
-        self.runner = GitRunner()
-
-        # Custom remotes correspond to annex via stdin/stdout
-        self.fin = fin or sys.stdin
-        self.fout = fout or sys.stdout
-
-        self.repo = get_repo_instance(class_=AnnexRepo) \
-            if not path \
-            else AnnexRepo(path, create=False, init=False)
-
-        self.path = self.repo.path
-
-        self._progress = 0  # transmission to be reported back if available
-        if cost is None:
-            cost = self.COST
-        self.cost = cost
-        #self.availability = availability.upper()
-        assert(self.AVAILABILITY.upper() in ("LOCAL", "GLOBAL"))
-
-        # To signal either we are in the loop and e.g. could correspond to annex
-        self._in_the_loop = False
-        self._protocol = \
-            AnnexExchangeProtocol(self.path, self.CUSTOM_REMOTE_NAME) \
-            if os.environ.get('DATALAD_TESTS_PROTOCOLREMOTE') else None
-
-        self._contentlocations = DictCache(size_limit=100)  # TODO: config ?
-
-        # instruct annex backend UI to use this remote
-        if ui.backend == 'annex':
-            ui.set_specialremote(self)
-
-        # Delay introspection until the first instance gets born
-        # could in principle be done once in the metaclass I guess
-        self.__class__._introspect_req_signatures()
-        self._annex_supports_info = \
-            external_versions['cmd:annex'] >= '6.20180206'
-
-    @classmethod
-    def _introspect_req_signatures(cls):
-        """
-        Check req_ methods to figure out expected number of arguments
-        See https://github.com/datalad/datalad/issues/1727
-        """
-        if hasattr(cls, '_req_nargs'):
-            # We have already figured it out for this class
+        if branch and branch in ds_repo.get_branches():
+            yield get_status_dict(
+                "run", ds=ds, status="error",
+                message="branch '{}' already exists".format(branch))
             return
-        cls._req_nargs = {
-            m[4:]: get_function_nargs(getattr(cls, m))
-            for m in dir(cls)
-            if m.startswith('req_')
-        }
-
-    @classmethod
-    def _get_custom_scheme(cls, prefix):
-        """Helper to generate custom datalad URL prefixes
-        """
-        # prefix which will be used in all URLs supported by this custom remote
-        # https://tools.ietf.org/html/rfc2718 dictates "URL Schemes" standard
-        # 2.1.2   suggests that we do use // since all of our URLs will define
-        #         some hierarchical structure.  But actually since we might encode
-        #         additional information (such as size) into the URL, it will not be
-        #         strictly conforming it. Thus we will not use //
-        return "%s+%s" % (URI_PREFIX, prefix)  # if .PREFIX else '')
-
-    # Helpers functionality
-
-    def get_contentlocation(self, key, absolute=False, verify_exists=True):
-        """Return (relative to top or absolute) path to the file containing the key
-
-        This is a wrapper around AnnexRepo.get_contentlocation which provides caching
-        of the result (we are asking the location for the same archive key often)
-        """
-        if key not in self._contentlocations:
-            fpath = self.repo.get_contentlocation(key, batch=True)
-            if fpath:  # shouldn't store empty ones
-                self._contentlocations[key] = fpath
-        else:
-            fpath = self._contentlocations[key]
-            # but verify that it exists
-            if verify_exists and not lexists(opj(self.path, fpath)):
-                # prune from cache
-                del self._contentlocations[key]
-                fpath = ''
 
-        if absolute and fpath:
-            return opj(self.path, fpath)
+        if revision is None:
+            revision = ds_repo.get_corresponding_branch() or \
+                ds_repo.get_active_branch() or "HEAD"
+
+        if not ds_repo.commit_exists(revision + "^"):
+            # Only a single commit is reachable from `revision`.  In
+            # this case, --since has no effect on the range construction.
+            revrange = revision
+        elif since is None:
+            revrange = "{rev}^..{rev}".format(rev=revision)
+        elif since.strip() == "":
+            revrange = revision
         else:
-            return fpath
+            revrange = "{}..{}".format(since, revision)
 
-    #
-    # Communication with git-annex
-    #
-    def send(self, *args):
-        """Send a message to git-annex
-
-        Parameters
-        ----------
-        `*args`: list of strings
-           arguments to be joined by a space and passed to git-annex
-        """
-        msg = " ".join(map(str, args))
-        if not self._in_the_loop:
-            lgr.debug("We are not yet in the loop, thus should not send to annex"
-                      " anything.  Got: %s" % msg.encode())
-            return
-        try:
-            self.heavydebug("Sending %r" % msg)
-            self.fout.write(msg + "\n")  # .encode())
-            self.fout.flush()
-            if self._protocol is not None:
-                self._protocol += "send %s" % msg
-        except IOError as exc:
-            lgr.debug("Failed to send due to %s" % str(exc))
-            if exc.errno == errno.EPIPE:
-                self.stop()
-            else:
-                raise exc
+        results = _rerun_as_results(ds, revrange, since, branch, onto, message)
+        if script:
+            handler = _get_script_handler(script, since, revision)
+        elif report:
+            handler = _report
+        else:
+            handler = partial(_rerun, assume_ready=assume_ready,
+                              explicit=explicit, jobs=jobs)
 
-    def send_unsupported(self, msg=None):
-        """Send UNSUPPORTED-REQUEST to annex and log optional message in our log
-        """
-        if msg:
-            lgr.debug(msg)
-        self.send("UNSUPPORTED-REQUEST")
-
-    def read(self, req=None, n=1):
-        """Read a message from git-annex
-
-        Parameters
-        ----------
-
-        req : string, optional
-           Expected request - first msg of the response
-        n : int
-           Number of response elements after first msg
-        """
-        # TODO: should we strip or should we not? verify how annex would deal
-        # with filenames starting/ending with spaces - encoded?
-        # Split right away
-        l = self.fin.readline().rstrip(os.linesep)
-        if self._protocol is not None:
-            self._protocol += "recv %s" % l
-        msg = l.split(None, n)
-        if req and ((not msg) or (req != msg[0])):
-            # verify correct response was given
-            self.send_unsupported(
-                "Expected %r, got a line %r.  Ignoring" % (req, l)
-            )
-            return None
-        self.heavydebug("Received %r" % (msg,))
-        return msg
-
-    # TODO: see if we could adjust the "originating" file:line, because
-    # otherwise they are all reported from main.py:117 etc
-    def heavydebug(self, msg, *args, **kwargs):
-        lgr.log(4, msg, *args, **kwargs)
-
-    # Since protocol allows for some messaging back, let's duplicate to lgr
-    def debug(self, msg):
-        lgr.debug(msg)
-        self.send("DEBUG", msg)
-
-    def error(self, msg, annex_err="ERROR"):
-        lgr.error(msg)
-        self.send(annex_err, msg)
-
-    def info(self, msg):
-        lgr.info(msg)
-        if self._annex_supports_info:
-            self.send('INFO', msg)
-
-    def progress(self, bytes):
-        bytes = int(bytes)
-        if self._progress != bytes:
-            self.send("PROGRESS", bytes)
+        for res in handler(ds, results):
+            yield res
 
-    def main(self):
-        """Interface to the command line tool"""
 
+def _revrange_as_results(dset, revrange):
+    ds_repo = dset.repo
+    rev_lines = ds_repo.get_revisions(
+        revrange, fmt="%H %P", options=["--reverse", "--topo-order"])
+    if not rev_lines:
+        return
+
+    for rev_line in rev_lines:
+        # The strip() below is necessary because, with the format above, a
+        # commit without any parent has a trailing space. (We could also use a
+        # custom `rev-list --parents ...` call to avoid this.)
+        fields = rev_line.strip().split(" ")
+        rev, parents = fields[0], fields[1:]
+        res = get_status_dict("run", ds=dset, commit=rev, parents=parents)
+        full_msg = ds_repo.format_commit("%B", rev)
         try:
-            self._in_the_loop = True
-            self._loop()
-        except AnnexRemoteQuit:
-            pass  # no harm
-        except KeyboardInterrupt:
-            self.stop("Interrupted by user")
-        except Exception as e:
-            self.stop(str(e))
-        finally:
-            self._in_the_loop = False
-
-    def stop(self, msg=None):
-        lgr.debug("Stopping communications of %s%s" %
-                 (self, ": %s" % msg if msg else ""))
-        raise AnnexRemoteQuit(msg)
-
-    def _loop(self):
-        """The main loop
-        """
-
-        self.send("VERSION", SUPPORTED_PROTOCOL)
-
-        while True:
-            l = self.read(n=1)
-
-            if l is not None and not l:
-                # empty line: exit
-                self.stop()
-                return
-
-            req, req_load = l[0], l[1:]
-            method = getattr(self, "req_%s" % req, None)
-            if not method:
-                self.send_unsupported(
-                    "We have no support for %s request, part of %s response"
-                    % (req, l)
-                )
+            msg, info = get_run_info(dset, full_msg)
+        except ValueError as exc:
+            # Recast the error so the message includes the revision.
+            raise ValueError(
+                "Error on {}'s message".format(rev)) from exc
+
+        if info is not None:
+            if len(parents) != 1:
+                lgr.warning(
+                    "%s has run information but is a %s commit; "
+                    "it will not be re-executed",
+                    rev,
+                    "merge" if len(parents) > 1 else "root")
                 continue
+            res["run_info"] = info
+            res["run_message"] = msg
+        yield dict(res, status="ok")
 
-            req_nargs = self._req_nargs[req]
-            if req_load and req_nargs > 1:
-                assert len(req_load) == 1, "Could be only one due to n=1"
-                # but now we need to slice it according to the respective req
-                # We assume that at least it shouldn't start with a space
-                # since str.split would get rid of it as well, and then we should
-                # have used re.split(" ", ...)
-                req_load = req_load[0].split(None, req_nargs - 1)
-
-            try:
-                method(*req_load)
-            except Exception as e:
-                self.error("Problem processing %r with parameters %r: %r"
-                           % (req, req_load, e))
-                from traceback import format_exc
-                lgr.error("Caught exception detail: %s" % format_exc())
 
-    def req_INITREMOTE(self, *args):
-        """Initialize this remote. Provides high level abstraction.
+def _rerun_as_results(dset, revrange, since, branch, onto, message):
+    """Represent the rerun as result records.
 
-        Specific implementation should go to _initialize
-        """
+    In the standard case, the information in these results will be used to
+    actually re-execute the commands.
+    """
 
-        try:
-            self._initremote(*args)
-        except Exception as e:
-            self.error("Failed to initialize %s due to %s" % (self, e),
-                       "INITREMOTE-FAILURE")
+    try:
+        results = _revrange_as_results(dset, revrange)
+    except ValueError as exc:
+        ce = CapturedException(exc)
+        yield get_status_dict("run", status="error", message=str(ce),
+                              exception=ce)
+        return
+
+    ds_repo = dset.repo
+    # Drop any leading commits that don't have a run command. These would be
+    # skipped anyways.
+    results = list(dropwhile(lambda r: "run_info" not in r, results))
+    if not results:
+        yield get_status_dict(
+            "run", status="impossible", ds=dset,
+            message=("No run commits found in range %s", revrange))
+        return
+
+    if onto is not None and onto.strip() == "":
+        onto = results[0]["commit"] + "^"
+
+    if onto and not ds_repo.commit_exists(onto):
+        yield get_status_dict(
+            "run", ds=dset, status="error",
+            message=("Revision specified for --onto (%s) does not exist.",
+                     onto))
+        return
+
+    start_point = onto or "HEAD"
+    if branch or onto:
+        yield get_status_dict(
+            "run",
+            ds=dset,
+            # Resolve this to the full hexsha so downstream code gets a
+            # predictable form.
+            commit=ds_repo.get_hexsha(start_point),
+            branch=branch,
+            rerun_action="checkout",
+            status="ok")
+
+    def skip_or_pick(hexsha, result, msg):
+        result["rerun_action"] = "skip-or-pick"
+        shortrev = ds_repo.get_hexsha(hexsha, short=True)
+        result["message"] = (
+            "%s %s; %s",
+            shortrev, msg, "skipping or cherry picking")
+
+    for res in results:
+        hexsha = res["commit"]
+        if "run_info" in res:
+            rerun_dsid = res["run_info"].get("dsid")
+            if rerun_dsid is not None and rerun_dsid != dset.id:
+                skip_or_pick(hexsha, res, "was ran from a different dataset")
+                res["status"] = "impossible"
+            else:
+                res["rerun_action"] = "run"
+                res["diff"] = diff_revision(dset, hexsha)
+                # This is the overriding message, if any, passed to this rerun.
+                res["rerun_message"] = message
         else:
-            self.send("INITREMOTE-SUCCESS")
+            if len(res["parents"]) > 1:
+                res["rerun_action"] = "merge"
+            else:
+                skip_or_pick(hexsha, res, "does not have a command")
+        yield res
 
-    def req_PREPARE(self, *args):
-        """Prepare "to deliver". Provides high level abstraction
 
-         Specific implementation should go to _prepare
-         """
-        try:
-            self._prepare(*args)
-        except Exception as e:
-            self.error("Failed to prepare %s due to %s" % (self, e),
-                       "PREPARE-FAILURE")
+def _mark_nonrun_result(result, which):
+    msg = dict(skip="skipping", pick="cherry picking")[which]
+    result["rerun_action"] = which
+    result["message"] = result["message"][:-1] + (msg,)
+    return result
+
+
+def _rerun(dset, results, assume_ready=None, explicit=False, jobs=None):
+    ds_repo = dset.repo
+    # Keep a map from an original hexsha to a new hexsha created by the rerun
+    # (i.e. a reran, cherry-picked, or merged commit).
+    new_bases = {}  # original hexsha => reran hexsha
+    branch_to_restore = ds_repo.get_active_branch()
+    head = onto = ds_repo.get_hexsha()
+    for res in results:
+        lgr.info(_get_rerun_log_msg(res))
+        rerun_action = res.get("rerun_action")
+        if not rerun_action:
+            yield res
+            continue
+
+        res_hexsha = res["commit"]
+        if rerun_action == "checkout":
+            if res.get("branch"):
+                branch = res["branch"]
+                checkout_options = ["-b", branch]
+                branch_to_restore = branch
+            else:
+                checkout_options = ["--detach"]
+                branch_to_restore = None
+            ds_repo.checkout(res_hexsha,
+                             options=checkout_options)
+            head = onto = res_hexsha
+            continue
+
+        # First handle the two cases that don't require additional steps to
+        # identify the base, a root commit or a merge commit.
+
+        if not res["parents"]:
+            _mark_nonrun_result(res, "skip")
+            yield res
+            continue
+
+        if rerun_action == "merge":
+            old_parents = res["parents"]
+            new_parents = [new_bases.get(p, p) for p in old_parents]
+            if old_parents == new_parents:
+                if not ds_repo.is_ancestor(res_hexsha, head):
+                    ds_repo.checkout(res_hexsha)
+            elif res_hexsha != head:
+                if ds_repo.is_ancestor(res_hexsha, onto):
+                    new_parents = [p for p in new_parents
+                                   if not ds_repo.is_ancestor(p, onto)]
+                if new_parents:
+                    if new_parents[0] != head:
+                        # Keep the direction of the original merge.
+                        ds_repo.checkout(new_parents[0])
+                    if len(new_parents) > 1:
+                        msg = ds_repo.format_commit("%B", res_hexsha)
+                        ds_repo.call_git(
+                            ["merge", "-m", msg,
+                             "--no-ff", "--allow-unrelated-histories"] +
+                            new_parents[1:])
+                    head = ds_repo.get_hexsha()
+                    new_bases[res_hexsha] = head
+            yield res
+            continue
+
+        # For all the remaining actions, first make sure we're on the
+        # appropriate base.
+
+        parent = res["parents"][0]
+        new_base = new_bases.get(parent)
+        head_to_restore = None  # ... to find our way back if we skip.
+
+        if new_base:
+            if new_base != head:
+                ds_repo.checkout(new_base)
+                head_to_restore, head = head, new_base
+        elif parent != head and ds_repo.is_ancestor(onto, parent):
+            if rerun_action == "run":
+                ds_repo.checkout(parent)
+                head = parent
+            else:
+                _mark_nonrun_result(res, "skip")
+                yield res
+                continue
         else:
-            self.send("PREPARE-SUCCESS")
+            if parent != head:
+                new_bases[parent] = head
 
-    def req_EXPORTSUPPORTED(self):
-        self.send(
-            'EXPORTSUPPORTED-SUCCESS'
-            if hasattr(self, 'req_EXPORT')
-            else 'EXPORTSUPPORTED-FAILURE'
-        )
-
-    ## define in subclass if EXPORT is supported
-    # def req_EXPORT(self, name):
-    #   pass
-
-    def req_GETCOST(self):
-        self.send("COST", self.cost)
-
-    def req_GETAVAILABILITY(self):
-        self.send("AVAILABILITY", self.AVAILABILITY.upper())
-
-    def req_CLAIMURL(self, url):
-        scheme = urlparse(url).scheme
-        if scheme in self.SUPPORTED_SCHEMES:
-            self.debug("Claiming url %r" % url)
-            self.send("CLAIMURL-SUCCESS")
-        else:
-            self.debug("Not claiming url %s" % url)
-            self.send("CLAIMURL-FAILURE")
+        # We've adjusted base. Now skip, pick, or run the commit.
 
-    # TODO: we should unify what to be overriden and some will provide CHECKURL
+        if rerun_action == "skip-or-pick":
+            if ds_repo.is_ancestor(res_hexsha, head):
+                _mark_nonrun_result(res, "skip")
+                if head_to_restore:
+                    ds_repo.checkout(head_to_restore)
+                    head, head_to_restore = head_to_restore, None
+                yield res
+                continue
+            else:
+                ds_repo.cherry_pick(res_hexsha)
+                _mark_nonrun_result(res, "pick")
+                yield res
+        elif rerun_action == "run":
+            run_info = res["run_info"]
+            # Keep a "rerun" trail.
+            if "chain" in run_info:
+                run_info["chain"].append(res_hexsha)
+            else:
+                run_info["chain"] = [res_hexsha]
 
-    def req_TRANSFER(self, cmd, key, file):
-        if cmd in ("RETRIEVE",):
-            lgr.debug("%s key %s into/from %s" % (cmd, key, file))  # was INFO level
-            try:
-                self._transfer(cmd, key, file)
-            except Exception as exc:
-                self.send(
-                    "TRANSFER-FAILURE %s %s %s" % (cmd, key, exc_str(exc))
-                )
-        else:
-            self.send_unsupported(
-                "Received unsupported by our TRANSFER command %s" % cmd
-            )
-
-    # Specific implementations to be provided in derived classes when necessary
-
-    def req_CHECKURL(self, url):
-        """
-        The remote replies with one of CHECKURL-FAILURE, CHECKURL-CONTENTS, or CHECKURL-MULTI.
-
-        CHECKURL-CONTENTS Size|UNKNOWN Filename
-            Indicates that the requested url has been verified to exist.
-            The Size is the size in bytes, or use "UNKNOWN" if the size could not be determined.
-            The Filename can be empty (in which case a default is used), or can specify a filename that is suggested to be used for this url.
-
-        CHECKURL-MULTI Url Size|UNKNOWN Filename ...
-            Indicates that the requested url has been verified to exist, and contains multiple files, which can each be accessed using their own url.
-            Note that since a list is returned, neither the Url nor the Filename can contain spaces.
-
-        CHECKURL-FAILURE
-            Indicates that the requested url could not be accessed.
-        """
-        self.send_unsupported()
-
-    def req_CHECKPRESENT(self, key):
-        """
-        CHECKPRESENT-SUCCESS Key
-            Indicates that a key has been positively verified to be present in the remote.
-        CHECKPRESENT-FAILURE Key
-            Indicates that a key has been positively verified to not be present in the remote.
-        CHECKPRESENT-UNKNOWN Key ErrorMsg
-            Indicates that it is not currently possible to verify if the key is present in the remote. (Perhaps the remote cannot be contacted.)
-        """
-        raise NotImplementedError()
-
-    def req_REMOVE(self, key):
-        """
-        REMOVE-SUCCESS Key
-            Indicates the key has been removed from the remote. May be returned if the remote didn't have the key at the point removal was requested.
-        REMOVE-FAILURE Key ErrorMsg
-            Indicates that the key was unable to be removed from the remote.
-        """
-        raise NotImplementedError()
-
-    def req_WHEREIS(self, key):
-        """Added in 5.20150812-17-g6bc46e3
-
-        provide any information about ways to access the content of a key stored in it,
-        such as eg, public urls. This will be displayed to the user by eg,
-        git annex whereis. The remote replies with WHEREIS-SUCCESS or WHEREIS-FAILURE.
-        Note that users expect git annex whereis to run fast, without eg, network access.
-        This is not needed when SETURIPRESENT is used, since such uris are automatically
-        displayed by git annex whereis.
-
-        WHEREIS-SUCCESS String
-            Indicates a location of a key. Typically an url, the string can be anything
-            that it makes sense to display to the user about content stored in the special
-            remote.
-        WHEREIS-FAILURE
-            Indicates that no location is known for a key.
-        """
-        raise NotImplementedError()
-
-    def _transfer(self, cmd, key, file):
-        raise NotImplementedError()
-
-    def _initremote(self, *args):
-        """Custom initialization of the special custom remote."""
-        pass
-
-    def _prepare(self, *args):
-        """Prepare special custom remote.  To be overridden"""
-        pass
-
-    # some requests we can send out
-    def get_DIRHASH(self, key, full=False):
-        """Gets a two level hash associated with a Key.
-
-        Parameters
-        ----------
-        full: bool, optional
-          If True, would spit out full DIRHASH path, i.e. with a KEY/ directory
-
-        Something like "abc/def". This is always the same for any given Key, so
-        can be used for eg, creating hash directory structures to store Keys in.
-        """
-        self.send("DIRHASH", key)
-        val = self.read("VALUE", 1)[1]
-        if full:
-            return opj(self.path, val, key)
-        else:
-            return val
+            # now we have to find out what was modified during the last run,
+            # and enable re-modification ideally, we would bring back the
+            # entire state of the tree with #1424, but we limit ourself to file
+            # addition/not-in-place-modification for now
+            auto_outputs = (ap["path"] for ap in new_or_modified(res["diff"]))
+            outputs = run_info.get("outputs", [])
+            outputs_dir = op.join(dset.path, run_info["pwd"])
+            auto_outputs = [p for p in auto_outputs
+                            # run records outputs relative to the "pwd" field.
+                            if op.relpath(p, outputs_dir) not in outputs]
+
+            message = res["rerun_message"] or res["run_message"]
+            for r in run_command(run_info['cmd'],
+                                 dataset=dset,
+                                 inputs=run_info.get("inputs", []),
+                                 extra_inputs=run_info.get("extra_inputs", []),
+                                 outputs=outputs,
+                                 assume_ready=assume_ready,
+                                 explicit=explicit,
+                                 rerun_outputs=auto_outputs,
+                                 message=message,
+                                 jobs=jobs,
+                                 rerun_info=run_info):
+                yield r
+        new_head = ds_repo.get_hexsha()
+        if new_head not in [head, res_hexsha]:
+            new_bases[res_hexsha] = new_head
+        head = new_head
+
+    if branch_to_restore:
+        # The user asked us to replay the sequence onto a branch, but the
+        # history had merges, so we're in a detached state.
+        ds_repo.update_ref("refs/heads/" + branch_to_restore,
+                           "HEAD")
+        ds_repo.checkout(branch_to_restore)
+
+
+def _get_rerun_log_msg(res):
+    "Prepare log message for a rerun to summarize an action about to happen"
+    msg = ''
+    rerun_action = res.get("rerun_action")
+    if rerun_action:
+        msg += rerun_action
+    if res.get('commit'):
+        msg += " commit %s;" % res.get('commit')[:7]
+    rerun_run_message = res.get("run_message")
+    if rerun_run_message:
+        if len(rerun_run_message) > 20:
+            rerun_run_message = rerun_run_message[:17] + '...'
+        msg += " (%s)" % rerun_run_message
+    rerun_message = res.get("message")
+    if rerun_message:
+        msg += " " + rerun_message[0] % rerun_message[1:]
+    msg = msg.lstrip()
+    return msg
+
+
+def _report(dset, results):
+    ds_repo = dset.repo
+    for res in results:
+        if "run_info" in res:
+            if res["status"] != "impossible":
+                res["diff"] = list(res["diff"])
+                # Add extra information that is useful in the report but not
+                # needed for the rerun.
+                out = ds_repo.format_commit("%an%x00%aI", res["commit"])
+                res["author"], res["date"] = out.split("\0")
+        yield res
+
+
+def _get_script_handler(script, since, revision):
+    ofh = sys.stdout if script.strip() == "-" else open(script, "w")
+
+    def fn(dset, results):
+        ds_repo = dset.repo
+        header = """\
+#!/bin/sh
+#
+# This file was generated by running (the equivalent of)
+#
+#   datalad rerun --script={script}{since} {revision}
+#
+# in {ds}{path}\n"""
+        ofh.write(header.format(
+            script=script,
+            since="" if since is None else " --since=" + since,
+            revision=ds_repo.get_hexsha(revision),
+            ds='dataset {} at '.format(dset.id) if dset.id else '',
+            path=dset.path))
+
+        for res in results:
+            if res["status"] != "ok":
+                yield res
+                return
 
-    def get_URLS(self, key):
-        """Gets URL(s) associated with a Key.
+            if "run_info" not in res:
+                continue
 
-        """
-        urls = []
-        for scheme in self.SUPPORTED_SCHEMES:
-            scheme_ = scheme + ":"
-            self.send("GETURLS", key, scheme_)
-            while True:
-                url = self.read("VALUE", 1)
-                if not url or len(url) <= 1:
-                    # so there were no URL output, we must be done
-                    break
-                url = url[1:]
-                if url:
-                    assert(len(url) == 1)
-                    urls.append(url[0])
-                else:
-                    break
-
-        self.heavydebug("Got %d URL(s) for key %s: %s", len(urls), key, urls)
-
-        #if not urls:
-        #    raise ValueError("Did not get any URLs for %s which we support" % key)
-
-        return urls
-
-    def _get_key_path(self, key):
-        """Return path to the KEY file
-        """
-        # TODO: should actually be implemented by AnnexRepo
-        #       Command is available in annex >= 20140410
-        (out, err) = \
-            self.runner(['git-annex', 'contentlocation', key], cwd=self.path)
-        # TODO: it would exit with non-0 if key is not present locally.
-        # we need to catch and throw our exception
-        return opj(self.path, out.rstrip(os.linesep))
-
-    # TODO: test on annex'es generated with those new options e.g.-c annex.tune.objecthash1=true
-    #def get_GETCONFIG SETCONFIG  SETCREDS  GETCREDS  GETUUID  GETGITDIR  SETWANTED  GETWANTED
-    #SETSTATE GETSTATE SETURLPRESENT  SETURLMISSING
-
-
-def generate_uuids():
-    """Generate UUIDs for our remotes. Even though quick, for consistency pre-generated and recorded in consts.py"""
-    import uuid
-    return {
-        remote: str(uuid.uuid5(uuid.NAMESPACE_URL, 'http://datalad.org/specialremotes/%s' % remote))
-        for remote in {'datalad', 'datalad-archives'}
-    }
-
-
-def init_datalad_remote(repo, remote, encryption=None, autoenable=False, opts=[]):
-    """Initialize datalad special remote"""
-    from datalad.support.external_versions import external_versions
-    from datalad.consts import DATALAD_SPECIAL_REMOTES_UUIDS
-    lgr.info("Initiating special remote %s" % remote)
-    remote_opts = [
-        'encryption=%s' % str(encryption).lower(),
-        'type=external',
-        'autoenable=%s' % str(bool(autoenable)).lower(),
-        'externaltype=%s' % remote
-    ]
-    if external_versions['cmd:annex'] >= '6.20170208':
-        # use unique uuid for our remotes
-        # This should help with merges of disconnected repos etc
-        # ATM only datalad/datalad-archives is expected,
-        # so on purpose getitem
-        remote_opts.append('uuid=%s' % DATALAD_SPECIAL_REMOTES_UUIDS[remote])
-    return repo.init_remote(remote, remote_opts + opts)
+            run_info = res["run_info"]
+            cmd = run_info["cmd"]
 
+            expanded_cmd = format_command(
+                dset, cmd,
+                **dict(run_info,
+                       dspath=dset.path,
+                       pwd=op.join(dset.path, run_info["pwd"])))
+
+            msg = res["run_message"]
+            if msg == _format_cmd_shorty(expanded_cmd):
+                msg = ''
+
+            ofh.write(
+                "\n" + "".join("# " + ln
+                               for ln in msg.splitlines(True)) +
+                "\n")
+            commit_descr = ds_repo.describe(res["commit"])
+            ofh.write('# (record: {})\n'.format(
+                commit_descr if commit_descr else res["commit"]))
+
+            ofh.write(expanded_cmd + "\n")
+        if ofh is not sys.stdout:
+            ofh.close()
 
-lgr.log(5, "Done importing datalad.customremotes.main")
+        if ofh is sys.stdout:
+            yield None
+        else:
+            yield get_status_dict(
+                "run", ds=dset, status="ok",
+                path=script,
+                message=("Script written to %s", script))
+
+    return fn
+
+
+def get_run_info(dset, message):
+    """Extract run information from `message`
+
+    Parameters
+    ----------
+    message : str
+        A commit message.
+
+    Returns
+    -------
+    A tuple with the command's message and a dict with run information. Both
+    these values are None if `message` doesn't have a run command.
+
+    Raises
+    ------
+    A ValueError if the information in `message` is invalid.
+    """
+    cmdrun_regex = r'\[DATALAD RUNCMD\] (.*)=== Do not change lines below ' \
+                   r'===\n(.*)\n\^\^\^ Do not change lines above \^\^\^'
+    runinfo = re.match(cmdrun_regex, message, re.MULTILINE | re.DOTALL)
+    if not runinfo:
+        return None, None
+
+    rec_msg, runinfo = runinfo.groups()
+
+    try:
+        runinfo = json.loads(runinfo)
+    except Exception as e:
+        raise ValueError(
+            'cannot rerun command, command specification is not valid JSON'
+        ) from e
+    if not isinstance(runinfo, (list, dict)):
+        # this is a run record ID -> load the beast
+        record_dir = dset.config.get(
+            'datalad.run.record-directory',
+            default=op.join('.datalad', 'runinfo'))
+        record_path = op.join(dset.path, record_dir, runinfo)
+        if not op.lexists(record_path):
+            raise ValueError("Run record sidecar file not found: {}".format(record_path))
+        # TODO `get` the file
+        recs = load_stream(record_path, compressed=True)
+        # TODO check if there is a record
+        runinfo = next(recs)
+    if 'cmd' not in runinfo:
+        raise ValueError("Looks like a run commit but does not have a command")
+    return rec_msg.rstrip(), runinfo
+
+
+def diff_revision(dataset, revision="HEAD"):
+    """Yield files that have been added or modified in `revision`.
+
+    Parameters
+    ----------
+    dataset : Dataset
+    revision : string, optional
+        Commit-ish of interest.
+
+    Returns
+    -------
+    Generator that yields AnnotatePaths instances
+    """
+    if dataset.repo.commit_exists(revision + "^"):
+        fr = revision + "^"
+    else:
+        # No other commits are reachable from this revision.  Diff
+        # with an empty tree instead.
+        fr = PRE_INIT_COMMIT_SHA
+
+    def changed(res):
+        return res.get("action") == "diff" and res.get("state") != "clean"
+
+    diff = dataset.diff(recursive=True,
+                        fr=fr, to=revision,
+                        result_filter=changed,
+                        return_type='generator', result_renderer='disabled')
+    for r in diff:
+        yield r
+
+
+def new_or_modified(diff_results):
+    """Filter diff result records to those for new or modified files.
+    """
+    for r in diff_results:
+        if r.get('type') in ('file', 'symlink') \
+                and r.get('state') in ['added', 'modified']:
+            r.pop('status', None)
+            yield r
```

### Comparing `datalad-0.9.3/datalad/customremotes/tests/test_archives.py` & `datalad-1.0.0/datalad/customremotes/tests/test_archives.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,64 +1,96 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 """Tests for customremotes archives providing dl+archive URLs handling"""
 
-from datalad.tests.utils import known_failure_v6
-from datalad.tests.utils import known_failure_direct_mode
-
+import glob
+import logging
+import os
+import os.path as op
+import sys
+from time import sleep
+from unittest.mock import patch
+
+from datalad.api import Dataset
+from datalad.cmd import (
+    GitWitlessRunner,
+    KillOutput,
+    StdOutErrCapture,
+    WitlessRunner,
+)
+from datalad.support.exceptions import CommandError
 
-from ..archives import ArchiveAnnexCustomRemote
-from ..base import AnnexExchangeProtocol
-from ...support.annexrepo import AnnexRepo
 from ...consts import ARCHIVES_SPECIAL_REMOTE
-from ...tests.utils import *
-from ...cmd import Runner, GitRunner
-from ...utils import _path_
-
-from . import _get_custom_runner
-
-# both files will have the same content
-# fn_inarchive_obscure = 'test.dat'
-# fn_extracted_obscure = 'test2.dat'
-fn_inarchive_obscure = get_most_obscure_supported_name()
-fn_archive_obscure = fn_inarchive_obscure.replace('a', 'b') + '.tar.gz'
-fn_extracted_obscure = fn_inarchive_obscure.replace('a', 'z')
+from ...support.annexrepo import AnnexRepo
+from ...tests.test_archives import (
+    fn_archive_obscure,
+    fn_archive_obscure_ext,
+    fn_in_archive_obscure,
+)
+from ...tests.utils_pytest import (
+    abspath,
+    assert_equal,
+    assert_false,
+    assert_not_equal,
+    assert_not_in,
+    assert_raises,
+    assert_true,
+    chpwd,
+    eq_,
+    in_,
+    known_failure_githubci_win,
+    ok_,
+    serve_path_via_http,
+    swallow_logs,
+    with_tempfile,
+    with_tree,
+)
+from ...utils import unlink
+from ..archives import (
+    ArchiveAnnexCustomRemote,
+    link_file_load,
+)
 
-#import line_profiler
-#prof = line_profiler.LineProfiler()
 
 # TODO: with_tree ATM for archives creates this nested top directory
 # matching archive name, so it will be a/d/test.dat ... we don't want that probably
+@known_failure_githubci_win
 @with_tree(
-    tree=(('a.tar.gz', {'d': {fn_inarchive_obscure: '123'}}),
+    tree=(('a.tar.gz', {'d': {fn_in_archive_obscure: '123'}}),
           ('simple.txt', '123'),
-          (fn_archive_obscure, (('d', ((fn_inarchive_obscure, '123'),)),)),
-          (fn_extracted_obscure, '123')))
+          (fn_archive_obscure_ext, (('d', ((fn_in_archive_obscure, '123'),)),)),
+          (fn_archive_obscure, '123')))
 @with_tempfile()
-def check_basic_scenario(fn_archive, fn_extracted, direct, d, d2):
-    annex = AnnexRepo(d, runner=_get_custom_runner(d), direct=direct)
+def test_basic_scenario(d=None, d2=None):
+    fn_archive, fn_extracted = fn_archive_obscure_ext, fn_archive_obscure
+    annex = AnnexRepo(d, backend='MD5E')
     annex.init_remote(
         ARCHIVES_SPECIAL_REMOTE,
         ['encryption=none', 'type=external', 'externaltype=%s' % ARCHIVES_SPECIAL_REMOTE,
          'autoenable=true'
          ])
     assert annex.is_special_annex_remote(ARCHIVES_SPECIAL_REMOTE)
     # We want two maximally obscure names, which are also different
-    assert(fn_extracted != fn_inarchive_obscure)
-    annex.add(fn_archive, commit=True, msg="Added tarball")
-    annex.add(fn_extracted, commit=True, msg="Added the load file")
+    assert(fn_extracted != fn_in_archive_obscure)
+    annex.add(fn_archive)
+    annex.commit(msg="Added tarball")
+    annex.add(fn_extracted)
+    annex.commit(msg="Added the load file")
 
     # Operations with archive remote URL
-    annexcr = ArchiveAnnexCustomRemote(path=d)
+    # this is not using this class for its actual purpose
+    # being a special remote implementation
+    # likely all this functionality should be elsewhere
+    annexcr = ArchiveAnnexCustomRemote(annex=None, path=d)
     # few quick tests for get_file_url
 
     eq_(annexcr.get_file_url(archive_key="xyz", file="a.dat"), "dl+archive:xyz#path=a.dat")
     eq_(annexcr.get_file_url(archive_key="xyz", file="a.dat", size=999), "dl+archive:xyz#path=a.dat&size=999")
 
     # see https://github.com/datalad/datalad/issues/441#issuecomment-223376906
     # old style
@@ -66,192 +98,199 @@
     eq_(annexcr._parse_url("dl+archive:xyz/a.dat"), ("xyz", "a.dat", {}))  # old format without size
     # new style
     eq_(annexcr._parse_url("dl+archive:xyz#path=a.dat&size=999"), ("xyz", "a.dat", {'size': 999}))
     eq_(annexcr._parse_url("dl+archive:xyz#path=a.dat"), ("xyz", "a.dat", {}))  # old format without size
 
     file_url = annexcr.get_file_url(
         archive_file=fn_archive,
-        file=fn_archive.replace('.tar.gz', '') + '/d/'+fn_inarchive_obscure)
+        file=fn_archive.replace('.tar.gz', '') + '/d/' + fn_in_archive_obscure)
 
     annex.add_url_to_file(fn_extracted, file_url, ['--relaxed'])
     annex.drop(fn_extracted)
 
     list_of_remotes = annex.whereis(fn_extracted, output='descriptions')
     in_('[%s]' % ARCHIVES_SPECIAL_REMOTE, list_of_remotes)
 
     assert_false(annex.file_has_content(fn_extracted))
-    annex.get(fn_extracted)
+
+    with swallow_logs(new_level=logging.INFO) as cml:
+        annex.get(fn_extracted)
+        # Hint users to the extraction cache (and to datalad clean)
+        cml.assert_logged(msg="datalad-archives special remote is using an "
+                              "extraction", level="INFO", regex=False)
     assert_true(annex.file_has_content(fn_extracted))
 
     annex.rm_url(fn_extracted, file_url)
-    assert_false(annex.drop(fn_extracted)['success'])
+    assert_raises(CommandError, annex.drop, fn_extracted)
 
     annex.add_url_to_file(fn_extracted, file_url)
     annex.drop(fn_extracted)
     annex.get(fn_extracted)
     annex.drop(fn_extracted)  # so we don't get from this one next
 
     # Let's create a clone and verify chain of getting file through the tarball
-    cloned_annex = AnnexRepo.clone(d, d2,
-                                   runner=_get_custom_runner(d2),
-                                   direct=direct)
+    cloned_annex = AnnexRepo.clone(d, d2)
     # we still need to enable manually atm that special remote for archives
     # cloned_annex.enable_remote('annexed-archives')
 
     assert_false(cloned_annex.file_has_content(fn_archive))
     assert_false(cloned_annex.file_has_content(fn_extracted))
     cloned_annex.get(fn_extracted)
     assert_true(cloned_annex.file_has_content(fn_extracted))
     # as a result it would also fetch tarball
     assert_true(cloned_annex.file_has_content(fn_archive))
 
-    # Check if protocol was collected
-    if os.environ.get('DATALAD_TESTS_PROTOCOLREMOTE'):
-        assert_is_instance(annex.cmd_call_wrapper.protocol, AnnexExchangeProtocol)
-        protocol_file = _path_(annex.path,
-                               '.git/bin/git-annex-remote-datalad-archive')
-        ok_file_has_content(protocol_file, "VERSION 1", re_=True, match=False)
-        ok_file_has_content(protocol_file, "GETAVAILABILITY", re_=True, match=False)
-        ok_file_has_content(protocol_file, "#!/bin/bash", re_=True, match=False)
-    else:
-        assert_false(isinstance(annex.cmd_call_wrapper.protocol, AnnexExchangeProtocol))
-
     # verify that we can drop if original archive gets dropped but available online:
     #  -- done as part of the test_add_archive_content.py
     # verify that we can't drop a file if archive key was dropped and online archive was removed or changed size! ;)
 
 
+@known_failure_githubci_win
 @with_tree(
-    tree={'a.tar.gz': {'d': {fn_inarchive_obscure: '123'}}}
+    tree={'a.tar.gz': {'d': {fn_in_archive_obscure: '123'}}}
 )
-@known_failure_direct_mode  #FIXME
-def test_annex_get_from_subdir(topdir):
-    from datalad.api import add_archive_content
-    annex = AnnexRepo(topdir, init=True)
-    annex.add('a.tar.gz', commit=True)
-    add_archive_content('a.tar.gz', annex=annex, delete=True)
-    fpath = opj(topdir, 'a', 'd', fn_inarchive_obscure)
-
-    with chpwd(opj(topdir, 'a', 'd')):
-        runner = Runner()
-        runner(['git', 'annex', 'drop', '--', fn_inarchive_obscure])  # run git annex drop
-        assert_false(annex.file_has_content(fpath))             # and verify if file deleted from directory
-        runner(['git', 'annex', 'get', '--', fn_inarchive_obscure])   # run git annex get
-        assert_true(annex.file_has_content(fpath))              # and verify if file got into directory
+def test_annex_get_from_subdir(topdir=None):
+    ds = Dataset(topdir)
+    ds.create(force=True)
+    ds.save('a.tar.gz')
+    ds.add_archive_content('a.tar.gz', delete=True)
+    fpath = op.join(topdir, 'a', 'd', fn_in_archive_obscure)
+
+    with chpwd(op.join(topdir, 'a', 'd')):
+        runner = WitlessRunner()
+        runner.run(
+            ['git', 'annex', 'drop', '--', fn_in_archive_obscure],
+            protocol=KillOutput)  # run git annex drop
+        assert_false(ds.repo.file_has_content(fpath))             # and verify if file deleted from directory
+        runner.run(
+            ['git', 'annex', 'get', '--', fn_in_archive_obscure],
+            protocol=KillOutput)   # run git annex get
+        assert_true(ds.repo.file_has_content(fpath))              # and verify if file got into directory
 
 
 def test_get_git_environ_adjusted():
-    gitrunner = GitRunner()
+    gitrunner = GitWitlessRunner()
     env = {"GIT_DIR": "../../.git", "GIT_WORK_TREE": "../../", "TEST_VAR": "Exists"}
 
     # test conversion of relevant env vars from relative_path to correct absolute_path
     adj_env = gitrunner.get_git_environ_adjusted(env)
     assert_equal(adj_env["GIT_DIR"], abspath(env["GIT_DIR"]))
     assert_equal(adj_env["GIT_WORK_TREE"], abspath(env["GIT_WORK_TREE"]))
 
     # test if other environment variables passed to function returned unaltered
     assert_equal(adj_env["TEST_VAR"], env["TEST_VAR"])
 
     # test import of sys_env if no environment passed to function
-    sys_env = gitrunner.get_git_environ_adjusted()
-    assert_equal(sys_env["PWD"], os.environ.get("PWD"))
-
-
-def test_basic_scenario():
-    yield check_basic_scenario, 'a.tar.gz', 'simple.txt', False
-    if not on_windows:
-        yield check_basic_scenario, 'a.tar.gz', 'simple.txt', True
-    #yield check_basic_scenario, 'a.tar.gz', fn_extracted_obscure, False
-    #yield check_basic_scenario, fn_archive_obscure, 'simple.txt', False
-    yield check_basic_scenario, fn_archive_obscure, fn_extracted_obscure, False
+    with patch.dict('os.environ', {'BOGUS': '123'}):
+        sys_env = gitrunner.get_git_environ_adjusted()
+        assert_equal(sys_env["BOGUS"], "123")
 
 
 def test_no_rdflib_loaded():
     # rely on rdflib polluting stdout to see that it is not loaded whenever we load this remote
     # since that adds 300ms delay for no immediate use
-    from ...cmd import Runner
-    runner = Runner()
-    with swallow_outputs() as cmo:
-        runner.run([sys.executable, '-c', 'import datalad.customremotes.archives, sys; print([k for k in sys.modules if k.startswith("rdflib")])'],
-               log_stdout=False, log_stderr=False)
-        # print cmo.out
-        assert_not_in("rdflib", cmo.out)
-        assert_not_in("rdflib", cmo.err)
-
-
-from .test_base import BASE_INTERACTION_SCENARIOS, check_interaction_scenario
-
-
-@with_tree(tree={'archive.tar.gz': {'f1.txt': 'content'}})
-def test_interactions(tdir):
-    # Just a placeholder since constructor expects a repo
-    repo = AnnexRepo(tdir, create=True, init=True)
-    repo.add('archive.tar.gz')
-    repo.commit('added')
-    for scenario in BASE_INTERACTION_SCENARIOS + [
-        [
-            ('GETCOST', 'COST %d' % ArchiveAnnexCustomRemote.COST),
-        ],
-        [
-            # by default we do not require any fancy init
-            # no urls supported by default
-            ('CLAIMURL http://example.com', 'CLAIMURL-FAILURE'),
-            # we know that is just a single option, url, is expected so full
-            # one would be passed
-            ('CLAIMURL http://example.com roguearg', 'CLAIMURL-FAILURE'),
-        ],
-        # basic interaction failing to fetch content from archive
-        [
-            ('TRANSFER RETRIEVE somekey somefile', 'GETURLS somekey dl+archive:'),
-            ('VALUE dl+archive://somekey2#path', None),
-            ('VALUE dl+archive://somekey3#path', None),
-            ('VALUE',
-             re.compile(
-                 'TRANSFER-FAILURE RETRIEVE somekey Failed to fetch any '
-                 'archive containing somekey. Tried: \[\]')
-             )
-        ],
-        # # incorrect response received from annex -- something isn't right but ... later
-        # [
-        #     ('TRANSFER RETRIEVE somekey somefile', 'GETURLS somekey dl+archive:'),
-        #     # We reply with UNSUPPORTED-REQUEST in these cases
-        #     ('GETCOST', 'UNSUPPORTED-REQUEST'),
-        # ],
-    ]:
-        check_interaction_scenario(ArchiveAnnexCustomRemote, tdir, scenario)
+    runner = WitlessRunner()
+    out = runner.run(
+        [sys.executable,
+         '-c',
+         'import datalad.customremotes.archives, sys; '
+         'print([k for k in sys.modules if k.startswith("rdflib")])'],
+        protocol=StdOutErrCapture)
+    # print cmo.out
+    assert_not_in("rdflib", out['stdout'])
+    assert_not_in("rdflib", out['stderr'])
 
 
-from datalad.tests.utils import serve_path_via_http
 @with_tree(tree=
     {'1.tar.gz':
          {
-             'bu.dat': '52055957098986598349795121365535'*10000,
-             'bu3.dat': '8236397048205454767887168342849275422'*10000
+             'bu.dat': '52055957098986598349795121365535' * 10000,
+             'bu3.dat': '8236397048205454767887168342849275422' * 10000
           },
     '2.tar.gz':
          {
-             'bu2.dat': '17470674346319559612580175475351973007892815102'*10000
+             'bu2.dat': '17470674346319559612580175475351973007892815102' * 10000
           },
     }
 )
 @serve_path_via_http()
 @with_tempfile
-def check_observe_tqdm(topdir, topurl, outdir):
+def check_observe_tqdm(topdir=None, topurl=None, outdir=None):
     # just a helper to enable/use when want quickly to get some
     # repository with archives and observe tqdm
-    from datalad.api import create, download_url, add_archive_content
+    from datalad.api import (
+        add_archive_content,
+        create,
+    )
     ds = create(outdir)
     for f in '1.tar.gz', '2.tar.gz':
         with chpwd(outdir):
             ds.repo.add_url_to_file(f, topurl + f)
-            ds.add(f)
+            ds.save(f)
             add_archive_content(f, delete=True, drop_after=True)
-    files = glob.glob(opj(outdir, '*'))
+    files = glob.glob(op.join(outdir, '*'))
     ds.drop(files) # will not drop tarballs
     ds.repo.drop([], options=['--all', '--fast'])
     ds.get(files)
     ds.repo.drop([], options=['--all', '--fast'])
     # now loop so we could play with it outside
     print(outdir)
     # import pdb; pdb.set_trace()
     while True:
-       sleep(0.1)
+        sleep(0.1)
+
+
+@known_failure_githubci_win
+@with_tempfile
+def test_link_file_load(tempfile=None):
+    tempfile2 = tempfile + '_'
+
+    with open(tempfile, 'w') as f:
+        f.write("LOAD")
+
+    link_file_load(tempfile, tempfile2)  # this should work in general
+
+    ok_(os.path.exists(tempfile2))
+
+    with open(tempfile2, 'r') as f:
+        assert_equal(f.read(), "LOAD")
+
+    def inode(fname):
+        with open(fname) as fd:
+            return os.fstat(fd.fileno()).st_ino
+
+    def stats(fname, times=True):
+        """Return stats on the file which should have been preserved"""
+        with open(fname) as fd:
+            st = os.fstat(fd.fileno())
+            stats = (st.st_mode, st.st_uid, st.st_gid, st.st_size)
+            if times:
+                return stats + (st.st_atime, st.st_mtime)
+            else:
+                return stats
+            # despite copystat mtime is not copied. TODO
+            #        st.st_mtime)
+
+    # TODO: fix up the test to not rely on OS assumptions but rather
+    # first sense filesystem about linking support.
+    # For Yarik's Windows 10 VM test was failing under assumption that
+    # linking is not supported at all, but I guess it does.
+    if True:  # on_linux or on_osx:
+        # above call should result in the hardlink
+        assert_equal(inode(tempfile), inode(tempfile2))
+        assert_equal(stats(tempfile), stats(tempfile2))
+
+        # and if we mock absence of .link
+        def raise_AttributeError(*args):
+            raise AttributeError("TEST")
+
+        with patch('os.link', raise_AttributeError):
+            with swallow_logs(logging.WARNING) as cm:
+                link_file_load(tempfile, tempfile2)  # should still work
+                ok_("failed (TEST), copying file" in cm.out)
+
+        # should be a copy (after mocked call)
+        assert_not_equal(inode(tempfile), inode(tempfile2))
+    with open(tempfile2, 'r') as f:
+        assert_equal(f.read(), "LOAD")
+    assert_equal(stats(tempfile, times=False), stats(tempfile2, times=False))
+    unlink(tempfile2)  # TODO: next two with_tempfile
```

### Comparing `datalad-0.9.3/datalad/customremotes/archives.py` & `datalad-1.0.0/datalad/customremotes/archives.py`

 * *Files 13% similar despite different names*

```diff
@@ -2,70 +2,125 @@
 # vi: set ft=python sts=4 ts=4 sw=4 et:
 ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""Custom remote to support getting the load from archives present under annex"""
+"""Custom remote to get the load from archives present under annex"""
 
 __docformat__ = 'restructuredtext'
 
+import logging
 import os
-from os.path import exists, join as opj
-from collections import OrderedDict
+import os.path as op
+import shutil
 from operator import itemgetter
+from pathlib import Path
+from urllib.parse import urlparse
 
-import logging
-lgr = logging.getLogger('datalad.customremotes.archive')
-lgr.log(5, "Importing datalad.customremotes.archive")
+from annexremote import UnsupportedRequest
+
+from datalad.consts import ARCHIVES_SPECIAL_REMOTE
+from datalad.customremotes import RemoteError
+from datalad.customremotes.main import main as super_main
+from datalad.distribution.dataset import Dataset
+from datalad.support.annexrepo import AnnexRepo
+from datalad.support.archives import ArchivesCache
+from datalad.support.cache import DictCache
+from datalad.support.exceptions import CapturedException
+from datalad.support.locking import lock_if_check_fails
+from datalad.support.network import URL
+from datalad.utils import (
+    ensure_bytes,
+    get_dataset_root,
+    getpwd,
+    unique,
+    unlink,
+)
 
-from ..dochelpers import exc_str
-from ..cmd import link_file_load
-from ..support.archives import ArchivesCache
-from ..support.network import URL
-from ..support.locking import lock_if_check_fails
-from ..utils import getpwd
-from ..utils import unique
 from .base import AnnexCustomRemote
-from .main import main as super_main
+
+lgr = logging.getLogger('datalad.customremotes.archive')
+
+
+# ####
+# Preserve from previous version
+# TODO: document intention
+# ####
+# this one might get under Runner for better output/control
+def link_file_load(src, dst, dry_run=False):
+    """Just a little helper to hardlink files's load
+    """
+    dst_dir = op.dirname(dst)
+    if not op.exists(dst_dir):
+        os.makedirs(dst_dir)
+    if op.lexists(dst):
+        lgr.log(9, "Destination file %(dst)s exists. Removing it first",
+                locals())
+        # TODO: how would it interact with git/git-annex
+        unlink(dst)
+    lgr.log(9, "Hardlinking %(src)s under %(dst)s", locals())
+    src_realpath = op.realpath(src)
+
+    try:
+        os.link(src_realpath, dst)
+    except (OSError, AttributeError) as e:
+        # we need to catch OSError too, because Python's own logic
+        # of not providing link() where it is known to be unsupported
+        # (e.g. Windows) will not cover scenarios where a particular
+        # filesystem simply does not implement it on an otherwise
+        # sane platform (e.g. exfat on Linux)
+        lgr.warning("Linking of %s failed (%s), copying file", src, e)
+        shutil.copyfile(src_realpath, dst)
+        shutil.copystat(src_realpath, dst)
+    else:
+        lgr.log(2, "Hardlinking finished")
 
 
 # TODO: RF functionality not specific to being a custom remote (loop etc)
 #       into a separate class
 class ArchiveAnnexCustomRemote(AnnexCustomRemote):
     """Special custom remote allowing to obtain files from archives
 
-     Archives should also be under annex control.
+     Archives must be under annex'ed themselves.
     """
-
     CUSTOM_REMOTE_NAME = "archive"
-    SUPPORTED_SCHEMES = (AnnexCustomRemote._get_custom_scheme(CUSTOM_REMOTE_NAME),)
+    SUPPORTED_SCHEMES = (
+        AnnexCustomRemote._get_custom_scheme(CUSTOM_REMOTE_NAME),)
     # Since we support only 1 scheme here
     URL_SCHEME = SUPPORTED_SCHEMES[0]
     URL_PREFIX = URL_SCHEME + ":"
 
-    AVAILABILITY = "local"
     COST = 500
 
-    def __init__(self, persistent_cache=True, **kwargs):
-        super(ArchiveAnnexCustomRemote, self).__init__(**kwargs)
+    def __init__(self, annex, path=None, persistent_cache=True, **kwargs):
+        super().__init__(annex)
+
+        # MIH figure out what the following is all about
+        # in particular path==None
+        self.repo = Dataset(get_dataset_root(Path.cwd())).repo \
+            if not path \
+            else AnnexRepo(path, create=False, init=False)
+
+        self.path = self.repo.path
         # annex requests load by KEY not but URL which it originally asked
         # about.  So for a key we might get back multiple URLs and as a
         # heuristic let's use the most recently asked one
 
         self._last_url = None  # for heuristic to choose among multiple URLs
         self._cache = ArchivesCache(self.path, persistent=persistent_cache)
+        self._contentlocations = DictCache(size_limit=100)  # TODO: config ?
 
     def stop(self, *args):
         """Stop communication with annex"""
         self._cache.clean()
-        super(ArchiveAnnexCustomRemote, self).stop(*args)
 
-    def get_file_url(self, archive_file=None, archive_key=None, file=None, size=None):
+    def get_file_url(self, archive_file=None, archive_key=None, file=None,
+                     size=None):
         """Given archive (file or a key) and a file -- compose URL for access
 
         Examples
         --------
 
         dl+archive:SHA256E-s176--69...3e.tar.gz#path=1/d2/2d&size=123
             when size of file within archive was known to be 123
@@ -76,31 +131,34 @@
         ----------
         size: int, optional
           Size of the file.  If not provided, will simply be empty
         """
         assert(file is not None)
         if archive_file is not None:
             if archive_key is not None:
-                raise ValueError("Provide archive_file or archive_key - not both")
-            archive_key = self.repo.get_file_key(archive_file)
+                raise ValueError(
+                    "Provide archive_file or archive_key - not both")
+            archive_key = self.repo.get_file_annexinfo(archive_file)['key']
         assert(archive_key is not None)
-        attrs = OrderedDict()  # looking forward for more
+        attrs = dict()  # looking forward for more
         if file:
             attrs['path'] = file.lstrip('/')
         if size is not None:
             attrs['size'] = size
-        return str(URL(scheme=self.URL_SCHEME, path=archive_key, fragment=attrs))
+        return str(URL(scheme=self.URL_SCHEME,
+                       path=archive_key,
+                       fragment=attrs))
 
     @property
     def cache(self):
         return self._cache
 
     def _parse_url(self, url):
-        """Parse url and return archive key, file within archive and additional attributes (such as size)
-        """
+        """Parse url and return archive key, file within archive and
+        additional attributes (such as size)"""
         url = URL(url)
         assert(url.scheme == self.URL_SCHEME)
         fdict = url.fragment_dict
         if 'path' not in fdict:
             # must be old-style key/path#size=
             assert '/' in url.path, "must be of key/path format"
             key, path = url.path.split('/', 1)
@@ -109,23 +167,26 @@
         if 'size' in fdict:
             fdict['size'] = int(fdict['size'])
         return key, path, fdict
 
     def _gen_akey_afiles(self, key, sorted=False, unique_akeys=True):
         """Given a key, yield akey, afile pairs
 
-        if `sorted`, then first those which have extracted version in local cache
-        will be yielded
+        if `sorted`, then first those which have extracted version in local
+        cache will be yielded
 
         Gets determined based on urls for datalad archives
 
         Made "generators all the way" as an exercise but also to delay any
         checks etc until really necessary.
         """
-        urls = self.get_URLS(key)
+        # we will need all URLs anyways later on ATM, so lets list() them
+        # Anyways here we have a single scheme (archive) so there is not
+        # much optimization possible
+        urls = list(self.gen_URLS(key))
 
         akey_afiles = [
             self._parse_url(url)[:2]  # skip size
             for url in urls
         ]
 
         if unique_akeys:
@@ -145,265 +206,277 @@
                 akey,
                 absolute=True, verify_exists=False
             ))
             for akey, afile in akey_afiles
         )
 
         # by default get_contentlocation would return empty result for a key
-        # which is not available locally.  But we could still have extracted archive
-        # in the cache.  So we need pretty much get first all possible and then
-        # only remove those which aren't present locally.  So verify_exists was added
+        # which is not available locally.  But we could still have extracted
+        # archive in the cache.  So we need pretty much get first all possible
+        # and then only remove those which aren't present locally.  So
+        # verify_exists was added
         yielded = set()
         akey_afile_paths_ = []
 
-        # utilize cache to check which archives might already be present in the cache
+        # utilize cache to check which archives might already be present in the
+        # cache
         for akey_afile, akey_path in akey_afile_paths:
             if akey_path and self.cache[akey_path].is_extracted:
                 yield akey_afile
                 yielded.add(akey_afile)
             akey_afile_paths_.append((akey_afile, akey_path))
 
-        # replace generators with already collected ones into a list.
-        # The idea that in many cases we don't even need to create a full list
-        # and that initial single yield would be enough, thus we don't need to check
+        # replace generators with already collected ones into a list.  The idea
+        # that in many cases we don't even need to create a full list and that
+        # initial single yield would be enough, thus we don't need to check
         # locations etc for every possible hit
         akey_afile_paths = akey_afile_paths_
 
         # if not present in the cache -- check which are present
         # locally and choose that one to use, so it would get extracted
         for akey_afile, akey_path in akey_afile_paths:
-            if akey_path and exists(akey_path):
+            if akey_path and op.exists(akey_path):
                 yielded.add(akey_afile)
                 yield akey_afile
 
-        # So no archive is present either in the cache or originally under annex
-        # XXX some kind of a heuristic I guess is to use last_url ;-)
-        if self._last_url and self._last_url in urls and (len(urls) == len(akey_afiles)):
+        # So no archive is present either in the cache or originally under
+        # annex XXX some kind of a heuristic I guess is to use last_url ;-)
+        if self._last_url and self._last_url in urls \
+                and (len(urls) == len(akey_afiles)):
             akey_afile, _ = akey_afile_paths[urls.index(self._last_url)]
             yielded.add(akey_afile)
             yield akey_afile
 
         for akey_afile, _ in akey_afile_paths:
             if akey_afile not in yielded:
                 yield akey_afile
 
-    # Protocol implementation
-    def req_CHECKURL(self, url):
+    def get_contentlocation(self, key, absolute=False, verify_exists=True):
+        """Return (relative to top or absolute) path to the file containing the key
+
+        This is a wrapper around AnnexRepo.get_contentlocation which provides
+        caching of the result (we are asking the location for the same archive
+        key often)
         """
+        if key not in self._contentlocations:
+            fpath = self.repo.get_contentlocation(key, batch=True)
+            if fpath:  # shouldn't store empty ones
+                self._contentlocations[key] = fpath
+        else:
+            fpath = self._contentlocations[key]
+            # but verify that it exists
+            if verify_exists and not op.lexists(op.join(self.path, fpath)):
+                # prune from cache
+                del self._contentlocations[key]
+                fpath = ''
 
-        Replies
+        if absolute and fpath:
+            return op.join(self.path, fpath)
+        else:
+            return fpath
 
-        CHECKURL-CONTENTS Size|UNKNOWN Filename
-            Indicates that the requested url has been verified to exist.
-            The Size is the size in bytes, or use "UNKNOWN" if the size could
-            not be determined.
-            The Filename can be empty (in which case a default is used), or can
-            specify a filename that is suggested to be used for this url.
-        CHECKURL-MULTI Url Size|UNKNOWN Filename ...
-            Indicates that the requested url has been verified to exist, and
-            contains multiple files, which can each be accessed using their own
-            url.
-            Note that since a list is returned, neither the Url nor the Filename
-            can contain spaces.
-        CHECKURL-FAILURE
-            Indicates that the requested url could not be accessed.
-        """
+    # Protocol implementation
+    def checkurl(self, url):
         # TODO:  what about those MULTI and list to be returned?
         #  should we return all filenames or keys within archive?
         #  might be way too many?
         #  only if just archive portion of url is given or the one pointing
         #  to specific file?
-        lgr.debug("Current directory: %s, url: %s" % (os.getcwd(), url))
+        lgr.debug("Current directory: %s, url: %s", os.getcwd(), url)
         akey, afile, attrs = self._parse_url(url)
         size = attrs.get('size', None)
 
         # But reply that present only if archive is present
-        # TODO: this would throw exception if not present, so this statement is kinda bogus
+        # TODO: this would throw exception if not present, so this statement is
+        # kinda bogus
         akey_path = self.get_contentlocation(akey, absolute=True)
         if akey_path:
             # Extract via cache only if size is not yet known
             if size is None:
                 # if for testing we want to force getting the archive extracted
-                # _ = self.cache.assure_extracted(self._get_key_path(akey)) # TEMP
                 efile = self.cache[akey_path].get_extracted_filename(afile)
+                efile = ensure_bytes(efile)
 
-                if exists(efile):
+                if op.exists(efile):
                     size = os.stat(efile).st_size
 
-            if size is None:
-                size = 'UNKNOWN'
-
-            # FIXME: providing filename causes annex to not even talk to ask
-            # upon drop :-/
-            self.send("CHECKURL-CONTENTS", size)  # , basename(afile))
-
             # so it was a good successful one -- record
             self._last_url = url
+
+            if size is None:
+                return True
+            else:
+                # FIXME: providing filename causes annex to not even talk to
+                # ask upon drop :-/
+                return [dict(size=size)]  # , basename(afile))
+
         else:
-            # TODO: theoretically we should first check if key is available from
-            # any remote to know if file is available
-            self.send("CHECKURL-FAILURE")
-
-    def req_CHECKPRESENT(self, key):
-        """Check if copy is available
-
-        TODO: just proxy the call to annex for underlying tarball
-
-        Replies
-
-        CHECKPRESENT-SUCCESS Key
-            Indicates that a key has been positively verified to be present in
-            the remote.
-        CHECKPRESENT-FAILURE Key
-            Indicates that a key has been positively verified to not be present
-            in the remote.
-        CHECKPRESENT-UNKNOWN Key ErrorMsg
-            Indicates that it is not currently possible to verify if the key is
-            present in the remote. (Perhaps the remote cannot be contacted.)
-        """
+            # TODO: theoretically we should first check if key is available
+            # from any remote to know if file is available
+            return False
+
+    def checkpresent(self, key):
         # TODO: so we need to maintain mapping from urls to keys.  Then
         # we could even store the filename within archive
         # Otherwise it is unrealistic to even require to recompute key if we
         # knew the backend etc
-        lgr.debug("VERIFYING key %s" % key)
-        # The same content could be available from multiple locations within the same
-        # archive, so let's not ask it twice since here we don't care about "afile"
+        # The same content could be available from multiple locations within
+        # the same archive, so let's not ask it twice since here we don't care
+        # about "afile"
         for akey, _ in self._gen_akey_afiles(key, unique_akeys=True):
-            if self.get_contentlocation(akey) or self.repo.is_available(akey, batch=True, key=True):
-                self.send("CHECKPRESENT-SUCCESS", key)
-                return
-        self.send("CHECKPRESENT-UNKNOWN", key)
+            if self.get_contentlocation(akey) \
+                    or self.repo.is_available(akey, batch=True, key=True):
+                return True
+        # it is unclear to MIH why this must be UNKNOWN rather than FALSE
+        # but this is how I found it
+        raise RemoteError('Key not present')
 
-    def req_REMOVE(self, key):
-        """
-        REMOVE-SUCCESS Key
-            Indicates the key has been removed from the remote. May be returned
-            if the remote didn't have the key at the point removal was requested
-        REMOVE-FAILURE Key ErrorMsg
-            Indicates that the key was unable to be removed from the remote.
-        """
-        self.send("REMOVE-FAILURE", key,
-                  "Removal from file archives is not supported")
-        return
+    def remove(self, key):
+        raise UnsupportedRequest('This special remote cannot remove content')
         # # TODO: proxy query to the underlying tarball under annex that if
         # # tarball was removed (not available at all) -- report success,
         # # otherwise failure (current the only one)
         # akey, afile = self._get_akey_afile(key)
         # if False:
-        #     # TODO: proxy, checking present of local tarball is not sufficient
+        #     # TODO: proxy, checking present of local tarball is not
+        #     # sufficient
         #     #  not exists(self.get_key_path(key)):
         #     self.send("REMOVE-SUCCESS", akey)
         # else:
         #     self.send("REMOVE-FAILURE", akey,
         #               "Removal from file archives is not supported")
 
-    def req_WHEREIS(self, key):
-        """
-        WHEREIS-SUCCESS String
-            Indicates a location of a key. Typically an url, the string can be anything
-            that it makes sense to display to the user about content stored in the special
-            remote.
-        WHEREIS-FAILURE
-            Indicates that no location is known for a key.
-        """
-        self.send("WHEREIS-FAILURE")
-        # although more logical is to report back success, it leads to imho more confusing
-        # duplication. See
+    def whereis(self, key):
+        return False
+        # although more logical is to report back success, it leads to imho
+        # more confusing duplication. See
         # http://git-annex.branchable.com/design/external_special_remote_protocol/#comment-3f9588f6a972ae566347b6f467b53b54
-
         # try:
         #     key, file = self._get_akey_afile(key)
         #     self.send("WHEREIS-SUCCESS", "file %s within archive %s" % (file, key))
         # except ValueError:
         #     self.send("WHEREIS-FAILURE")
 
-    def _transfer(self, cmd, key, path):
-
+    def transfer_retrieve(self, key, file):
         akeys_tried = []
         # the same file could come from multiple files within the same archive
         # So far it doesn't make sense to "try all" of them since if one fails
         # it means the others would fail too, so it makes sense to immediately
         # prune the list so we keep only the ones from unique akeys.
         # May be whenever we support extraction directly from the tarballs
         # we should go through all and choose the one easiest to get or smth.
-        for akey, afile in self._gen_akey_afiles(key, sorted=True, unique_akeys=True):
+        for akey, afile in self._gen_akey_afiles(
+                key, sorted=True, unique_akeys=True):
             if not akey:
                 lgr.warning("Got an empty archive key %r for key %s. Skipping",
                             akey, key)
                 continue
             akeys_tried.append(akey)
             try:
                 with lock_if_check_fails(
                     check=(self.get_contentlocation, (akey,)),
-                    lock_path=(lambda k: opj(self.repo.path, '.git', 'datalad-archives-%s' % k), (akey,)),
+                    lock_path=(
+                        lambda k: op.join(self.repo.path,
+                                          '.git',
+                                          'datalad-archives-%s' % k),
+                        (akey,)),
                     operation="annex-get"
                 ) as (akey_fpath, lock):
                     if lock:
                         assert not akey_fpath
                         self._annex_get_archive_by_key(akey)
                         akey_fpath = self.get_contentlocation(akey)
 
                 if not akey_fpath:
                     raise RuntimeError(
                         "We were reported to fetch it alright but now can't "
                         "get its location.  Check logic"
-                )
+                    )
 
-                akey_path = opj(self.repo.path, akey_fpath)
-                assert exists(akey_path), "Key file %s is not present" % akey_path
+                akey_path = op.join(self.repo.path, akey_fpath)
+                assert op.exists(akey_path), \
+                       "Key file %s is not present" % akey_path
 
                 # Extract that bloody file from the bloody archive
                 # TODO: implement/use caching, for now a simple one
                 #  actually patool doesn't support extraction of a single file
                 #  https://github.com/wummel/patool/issues/20
                 # so
                 pwd = getpwd()
-                lgr.debug("Getting file {afile} from {akey_path} while PWD={pwd}".format(**locals()))
+                lgr.debug(
+                    "Getting file %s from %s while PWD=%s",
+                    afile, akey_path, pwd)
+                was_extracted = self.cache[akey_path].is_extracted
                 apath = self.cache[akey_path].get_extracted_file(afile)
-                link_file_load(apath, path)
-                self.send('TRANSFER-SUCCESS', cmd, key)
+                link_file_load(apath, file)
+                if not was_extracted and self.cache[akey_path].is_extracted:
+                    self.message(
+                        "%s special remote is using an extraction cache "
+                        "under %s. Remove it with DataLad's 'clean' "
+                        "command to save disk space." %
+                        (ARCHIVES_SPECIAL_REMOTE,
+                         self.cache[akey_path].path),
+                        type='info',
+                    )
                 return
             except Exception as exc:
-                # from celery.contrib import rdb
-                # rdb.set_trace()
-                exc_ = exc_str(exc)
-                self.debug("Failed to fetch {akey} containing {key}: {exc_}".format(**locals()))
+                ce = CapturedException(exc)
+                self.message(
+                    "Failed to fetch {akey} containing {key}: {msg}".format(
+                        akey=akey,
+                        key=key,
+                        # we need to get rid of any newlines, or we might
+                        # break the special remote protocol
+                        msg=str(ce).replace('\n', '|')
+                    ))
                 continue
 
-        raise RuntimeError(
+        raise RemoteError(
             "Failed to fetch any archive containing {key}. "
             "Tried: {akeys_tried}".format(**locals())
         )
 
+    def claimurl(self, url):
+        scheme = urlparse(url).scheme
+        if scheme in self.SUPPORTED_SCHEMES:
+            return True
+        else:
+            return False
+
     def _annex_get_archive_by_key(self, akey):
         # TODO: make it more stringent?
         # Command could have fail to run if key was not present locally yet
         # Thus retrieve the key using annex
         # TODO: we need to report user somehow about this happening and
         # progress on the download
         from humanize import naturalsize
+
+        from datalad.support.annexrepo import AnnexJsonProtocol
+
         akey_size = self.repo.get_size_from_key(akey)
-        self.info(
+        self.message(
             "To obtain some keys we need to fetch an archive "
             "of size %s"
-            % (naturalsize(akey_size) if akey_size else "unknown")
+            % (naturalsize(akey_size) if akey_size else "unknown"),
+            type='info',
         )
 
-        def progress_indicators(l):
-            self.info("PROGRESS-JSON: " + l.rstrip(os.linesep))
-
-        self.runner(["git-annex", "get",
-                     "--json", "--json-progress",
-                     "--key", akey
-                     ],
-                    log_stdout=progress_indicators,
-                    log_stderr='offline',
-                    # False, # to avoid lock down
-                    log_online=True,
-                    cwd=self.path, expect_stderr=True)
+        try:
+            self.repo._call_annex(
+                ["get", "--json", "--json-progress", "--key", akey],
+                protocol=AnnexJsonProtocol,
+            )
+        except Exception:
+            self.message(f'Failed to fetch archive with key {akey}')
+            raise
 
 
 def main():
     """cmdline entry point"""
-    super_main(backend="archive")
-
-lgr.log(5, "Done importing datalad.customremotes.archive")
+    super_main(
+        cls=ArchiveAnnexCustomRemote,
+        remote_name='datalad-archives',
+        description=\
+        "extract content from archives (.tar{,.gz}, .zip, etc) which are "
+        "in turn managed by git-annex.  See `datalad add-archive-content` "
+        "command",
+    )
```

### Comparing `datalad-0.9.3/datalad/plugin/tests/test_plugins.py` & `datalad-1.0.0/datalad/cli/main.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,228 +1,238 @@
-# emacs: -*- mode: python-mode; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# -*- coding: utf-8 -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""Test plugin interface mechanics"""
+"""This is the main() CLI entryproint"""
 
+# It should start-up and run as fast as possible for a responsive CLI.
 
-from datalad.tests.utils import known_failure_direct_mode
+# Imports are done inline and as late as possible to avoid paying for
+# an unconditional commulative overhead that is only actually needed
+# in some special cases.
+
+__docformat__ = 'restructuredtext'
 
-import os
 import logging
-from os.path import join as opj
-from os.path import exists
-from mock import patch
-
-from datalad.api import plugin
-from datalad.api import create
-from datalad import cfg
-
-from datalad.tests.utils import swallow_logs
-from datalad.tests.utils import swallow_outputs
-from datalad.tests.utils import with_tempfile
-from datalad.tests.utils import chpwd
-from datalad.tests.utils import create_tree
-from datalad.tests.utils import assert_raises
-from datalad.tests.utils import assert_status
-from datalad.tests.utils import assert_in
-from datalad.tests.utils import assert_not_in
-from datalad.tests.utils import eq_
-from datalad.tests.utils import ok_clean_git
-
-broken_plugin = """garbage"""
-
-nodocs_plugin = """\
-def dlplugin():
-    pass
-"""
-
-# functioning plugin dummy
-dummy_plugin = '''\
-"""real dummy"""
-
-def dlplugin(dataset, noval, withval='test'):
-    "mydocstring"
-    yield dict(
-        status='ok',
-        action='dummy',
-        args=dict(
-            dataset=dataset,
-            noval=noval,
-            withval=withval))
-'''
-
-
-@with_tempfile()
-@with_tempfile(mkdir=True)
-def test_plugin_call(path, dspath):
-    # make plugins
-    create_tree(
-        path,
-        {
-            'dlplugin_dummy.py': dummy_plugin,
-            'dlplugin_nodocs.py': nodocs_plugin,
-            'dlplugin_broken.py': broken_plugin,
-        })
-    fake_dummy_spec = {
-        'dummy': {'file': opj(path, 'dlplugin_dummy.py')},
-        'nodocs': {'file': opj(path, 'dlplugin_nodocs.py')},
-        'broken': {'file': opj(path, 'dlplugin_broken.py')},
-    }
-
-    with patch('datalad.plugin._get_plugins', return_value=fake_dummy_spec):
-        with swallow_outputs() as cmo:
-            plugin(showplugininfo=True)
-            # hyphen spacing depends on the longest plugin name!
-            # sorted
-            # summary list generation doesn't actually load plugins for speed,
-            # hence broken is not known to be broken here
-            eq_(cmo.out,
-                "broken [no synopsis] ({})\ndummy  - real dummy ({})\nnodocs [no synopsis] ({})\n".format(
-                    fake_dummy_spec['broken']['file'],
-                    fake_dummy_spec['dummy']['file'],
-                    fake_dummy_spec['nodocs']['file']))
-        with swallow_outputs() as cmo:
-            plugin(['dummy'], showpluginhelp=True)
-            eq_(cmo.out.rstrip(), "mydocstring")
-        with swallow_outputs() as cmo:
-            plugin(['nodocs'], showpluginhelp=True)
-            eq_(cmo.out.rstrip(), "This plugin has no documentation")
-        # loading fails, no docs
-        assert_raises(ValueError, plugin, ['broken'], showpluginhelp=True)
-
-    # assume this most obscure plugin name is not used
-    assert_raises(ValueError, plugin, '32sdfhvz984--^^')
-
-    # broken plugin argument, must match Python keyword arg
-    # specs
-    assert_raises(ValueError, plugin, ['dummy', '1245'])
-
-    def fake_is_installed(*args, **kwargs):
-        return True
-    with patch('datalad.plugin._get_plugins', return_value=fake_dummy_spec), \
-        patch('datalad.distribution.dataset.Dataset.is_installed', return_value=True):
-        # does not trip over unsupported argument, they get filtered out, because
-        # we carry all kinds of stuff
-        with swallow_logs(new_level=logging.WARNING) as cml:
-            res = list(plugin(['dummy', 'noval=one', 'obscure=some']))
-            assert_status('ok', res)
-            cml.assert_logged(
-                msg=".*ignoring plugin argument\\(s\\).*obscure.*, not supported by plugin.*",
-                regex=True, level='WARNING')
-        # fails on missing positional arg
-        assert_raises(TypeError, plugin, ['dummy'])
-        # positional and kwargs actually make it into the plugin
-        res = list(plugin(['dummy', 'noval=one', 'withval=two']))[0]
-        eq_('one', res['args']['noval'])
-        eq_('two', res['args']['withval'])
-        # kwarg defaults are preserved
-        res = list(plugin(['dummy', 'noval=one']))[0]
-        eq_('test', res['args']['withval'])
-        # repeated specification yields list input
-        res = list(plugin(['dummy', 'noval=one', 'noval=two']))[0]
-        eq_(['one', 'two'], res['args']['noval'])
-        # can do the same thing  while bypassing argument parsing for calls
-        # from within python, and even preserve native python dtypes
-        res = list(plugin(['dummy', ('noval', 1), ('noval', 'two')]))[0]
-        eq_([1, 'two'], res['args']['noval'])
-        # and we can further simplify in this case by passing lists right
-        # away
-        res = list(plugin(['dummy', ('noval', [1, 'two'])]))[0]
-        eq_([1, 'two'], res['args']['noval'])
-
-    # dataset arg handling
-    # run plugin that needs a dataset where there is none
-    with patch('datalad.plugin._get_plugins', return_value=fake_dummy_spec):
-        ds = None
-        with chpwd(dspath):
-            assert_raises(ValueError, plugin, ['dummy', 'noval=one'])
-            # create a dataset here, fixes the error
-            ds = create()
-            res = list(plugin(['dummy', 'noval=one']))[0]
-            # gives dataset instance
-            eq_(ds, res['args']['dataset'])
-        # no do again, giving the dataset path
-        # but careful, `dataset` is a proper argument
-        res = list(plugin(['dummy', 'noval=one'], dataset=dspath))[0]
-        eq_(ds, res['args']['dataset'])
-        # however, if passed alongside the plugins args it also works
-        res = list(plugin(['dummy', 'dataset={}'.format(dspath), 'noval=one']))[0]
-        eq_(ds, res['args']['dataset'])
-        # but if both are given, the proper args takes precedence
-        assert_raises(ValueError, plugin, ['dummy', 'dataset={}'.format(dspath), 'noval=one'],
-                      dataset='rubbish')
-
-
-@with_tempfile(mkdir=True)
-def test_plugin_config(path):
-    # baseline behavior, empty datasets on create
-    ds = create(dataset=opj(path, 'ds1'))
-    eq_(sorted(os.listdir(ds.path)), ['.datalad', '.git', '.gitattributes'])
-    # now we configure a plugin to run twice after `create`
-    cfg.add('datalad.create.run-after',
-            'add_readme filename=after1.txt',
-            where='global')
-    cfg.add('datalad.create.run-after',
-            'add_readme filename=after2.txt',
-            where='global')
-    # force reload to pick up newly populated .gitconfig
-    cfg.reload(force=True)
-    assert_in('datalad.create.run-after', cfg)
-    # and now we create a dataset and expect the two readme files
-    # to be part of it
-    ds = create(dataset=opj(path, 'ds'))
-    ok_clean_git(ds.path)
-    assert(exists(opj(ds.path, 'after1.txt')))
-    assert(exists(opj(ds.path, 'after2.txt')))
-    # cleanup
-    cfg.unset(
-        'datalad.create.run-after',
-        where='global')
-    assert_not_in('datalad.create.run-after', cfg)
-
-
-@with_tempfile(mkdir=True)
-def test_wtf(path):
-    # smoke test for now
-    with swallow_outputs() as cmo:
-        plugin(['wtf'], dataset=path)
-        assert_not_in('Dataset information', cmo.out)
-        assert_in('Configuration', cmo.out)
-    with chpwd(path):
-        with swallow_outputs() as cmo:
-            plugin(['wtf'])
-            assert_not_in('Dataset information', cmo.out)
-            assert_in('Configuration', cmo.out)
-    # now with a dataset
-    ds = create(path)
-    with swallow_outputs() as cmo:
-        plugin(['wtf'], dataset=ds.path)
-        assert_in('Configuration', cmo.out)
-        assert_in('Dataset information', cmo.out)
-        assert_in('path: {}'.format(ds.path), cmo.out)
-
-
-@with_tempfile(mkdir=True)
-@known_failure_direct_mode  #FIXME
-def test_no_annex(path):
-    ds = create(path)
-    ok_clean_git(ds.path)
-    create_tree(
-        ds.path,
-        {'code': {
-            'inannex': 'content',
-            'notinannex': 'othercontent'}})
-    # add two files, pre and post configuration
-    ds.add(opj('code', 'inannex'))
-    plugin(['no_annex', 'pattern=code/**'], dataset=ds)
-    ds.add(opj('code', 'notinannex'))
-    ok_clean_git(ds.path)
-    # one is annex'ed, the other is not, despite no change in add call
-    # importantly, also .gitattribute is not annexed
-    eq_([opj('code', 'inannex')],
-        ds.repo.get_annexed_files())
+
+lgr = logging.getLogger('datalad.cli')
+
+lgr.log(5, "Importing cli.main")
+
+import os
+import sys
+
+import datalad
+
+from .parser import setup_parser
+
+# TODO cross-check with unconditional imports in .parser
+# special case imports
+#   from .helpers import _fix_datalad_ri
+#   import platform
+#   from .helpers import _parse_overrides_from_cmdline
+#   from datalad.utils import chpwd
+#   from .utils import setup_exceptionhook
+#   from datalad.support.exceptions import ...
+
+# unconditional imports, no meaningful functionality without them
+#   from .parser import setup_parser
+
+
+def _on_msys_tainted_paths():
+    """This duplicates datalad.utils.on_msys_tainted_paths
+
+    But it does it while minimizing runtime penalties on all irrelevant
+    systems.
+    """
+    if os.environ.get('MSYSTEM', '')[:4] not in ('MSYS', 'MING'):
+        return False
+    if 'MSYS_NO_PATHCONV' in os.environ:
+        return False
+    import platform
+    if platform.system().lower() != 'windows':
+        return False
+    return True
+
+
+def main(args=sys.argv):
+    """Main CLI entrypoint"""
+    lgr.log(5, "Starting main(%r)", args)
+    # record that we came in via the cmdline
+    datalad.__api = 'cmdline'
+    completing = "_ARGCOMPLETE" in os.environ
+    if completing and 'COMP_LINE' in os.environ:
+        import shlex
+
+        # TODO support posix=False too?
+        args = shlex.split(os.environ['COMP_LINE']) or args
+
+    if _on_msys_tainted_paths():
+        # Possibly present DataLadRIs were stripped of a leading /
+        from .helpers import _fix_datalad_ri
+        args = [_fix_datalad_ri(s) for s in args]
+
+    from datalad.support.entrypoints import load_extensions
+
+    # load extensions requested by configuration
+    # analog to what coreapi is doing for a Python session
+    # importantly, load them prior to parser construction, such
+    # that CLI tuning is also within reach for extensions
+    load_extensions()
+
+    # PYTHON_ARGCOMPLETE_OK
+    # TODO possibly construct a dedicated parser just for autocompletion
+    # rather than lobotomizing the normal one
+    parser = setup_parser(args, completing=completing)
+    try:
+        import argcomplete
+        argcomplete.autocomplete(parser)
+    except ImportError:
+        pass
+
+    # parse cmd args
+    lgr.debug("Parsing known args among %r", args)
+    cmdlineargs, unparsed_args = parser.parse_known_args(args[1:])
+    # did the parser tell us what command to run?
+    has_func = hasattr(cmdlineargs, 'func') and cmdlineargs.func is not None
+    if unparsed_args:
+        if has_func:
+            lgr.error('unknown argument%s: %s',
+                's' if len(unparsed_args) > 1 else '',
+                unparsed_args if len(unparsed_args) > 1 else unparsed_args[0],
+            )
+            cmdlineargs.subparser.print_usage()
+            sys.exit(1)
+        else:
+            # store all unparsed arguments
+            cmdlineargs.datalad_unparsed_args = unparsed_args
+
+    # pull config overrides from cmdline args and put in effect
+    if cmdlineargs.cfg_overrides is not None:
+        from .helpers import _parse_overrides_from_cmdline
+        datalad.cfg.overrides.update(
+            _parse_overrides_from_cmdline(cmdlineargs)
+        )
+        # enable overrides
+        datalad.cfg.reload(force=True)
+        # try loading extensions again, in case the configuration
+        # added new ones to consider
+        load_extensions()
+
+    if 'datalad.runtime.librarymode' in datalad.cfg:
+        datalad.enable_librarymode()
+
+    if cmdlineargs.change_path is not None:
+        from datalad.utils import chpwd
+        for path in cmdlineargs.change_path:
+            chpwd(path)
+
+    # check argparse could determine what commands needs to be executed
+    if not has_func:
+        # just let argparser spit out its error, since there is smth wrong
+        parser.parse_args(args)
+        # if that one didn't puke -- we should
+        parser.print_usage()
+        lgr.error("Please specify the command")
+        # matches exit code for InsufficientArgumentsError
+        sys.exit(2)
+
+    _run(cmdlineargs)
+
+
+def _run(namespace):
+    """Execute a CLI operation
+
+    Depending on CLI debugging options the CLI operation is executed
+    in a debug harness or an exception handler.
+
+    Parameters
+    ----------
+    namespace: Namespace
+      Object returned by `ArgumentParser.parse_args()` with fully
+      populated and validated CLI command and arguments.
+
+    Raises
+    ------
+    SystemExit
+      When the CLI completed without error (exit 0).
+    """
+    # execute the command, either with a debugger catching
+    # a crash, or with a simplistic exception handler.
+    # note that result rendering is happening in the
+    # execution handler, when the command-generator is unwound
+    ret = _run_with_debugger(namespace) \
+        if namespace.common_debug or namespace.common_idebug \
+        else _run_with_exception_handler(namespace)
+
+    # all good, not strictly needed, but makes internal testing easier
+    sys.exit(0)
+
+
+def _run_with_debugger(cmdlineargs):
+    """Execute the command and drop into debugger if it crashes"""
+    from .utils import setup_exceptionhook
+
+    # so we could see/stop clearly at the point of failure
+    setup_exceptionhook(ipython=cmdlineargs.common_idebug)
+    return cmdlineargs.func(cmdlineargs)
+
+
+def _run_with_exception_handler(cmdlineargs):
+    """Execute the command and perform some reporting
+    normalization if it crashes, but otherwise just let it go"""
+    # otherwise - guard and only log the summary. Postmortem is not
+    # as convenient if being caught in this ultimate except
+    try:
+        return cmdlineargs.func(cmdlineargs)
+    # catch BaseException for KeyboardInterrupt
+    except BaseException as exc:
+        from datalad.support.exceptions import (
+            CapturedException,
+            CommandError,
+            IncompleteResultsError,
+            InsufficientArgumentsError,
+        )
+        ce = CapturedException(exc)
+        # we crashed, it has got to be non-zero for starters
+        exit_code = 1
+        if isinstance(exc, InsufficientArgumentsError):
+            # if the func reports inappropriate usage, give help output
+            lgr.error('%s (%s)', ce, exc.__class__.__name__)
+            cmdlineargs.subparser.print_usage(sys.stderr)
+            exit_code = 2
+        elif isinstance(exc, IncompleteResultsError):
+            # in general we do not want to see the error again, but
+            # present in debug output
+            lgr.debug('could not perform all requested actions: %s', ce)
+        elif isinstance(exc, CommandError):
+            exit_code = _communicate_commanderror(exc) or exit_code
+        elif isinstance(exc, KeyboardInterrupt):
+            from datalad.ui import ui
+            ui.error("\nInterrupted by user while doing magic: %s" % ce)
+            exit_code = 3
+        else:
+            # some unforeseen problem
+            lgr.error('%s', ce.format_with_cause())
+        sys.exit(exit_code)
+
+
+def _communicate_commanderror(exc):
+    """Behave as if the command ran directly"""
+    exc_msg = exc.to_str(include_output=False)
+    if exc_msg:
+        msg = exc_msg.encode() if isinstance(exc_msg, str) else exc_msg
+        os.write(2, msg + b"\n")
+    # push any captured output to the respective streams
+    for out, stream in ((exc.stdout, 1), (exc.stderr, 2)):
+        if out:
+            os.write(stream,
+                     out.encode() if isinstance(out, str) else out)
+    # pass on exit code
+    return exc.code
+
+
+lgr.log(5, "Done importing cli.main")
```

### Comparing `datalad-0.9.3/datalad/plugin/export_tarball.py` & `datalad-1.0.0/datalad/distributed/tests/ria_utils.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,84 +1,108 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""export a dataset to a tarball"""
 
-__docformat__ = 'restructuredtext'
+import inspect
+import os
+from functools import wraps
+from glob import glob
+
+from datalad.tests.utils_pytest import (
+    SkipTest,
+    attr,
+    create_tree,
+)
+from datalad.utils import Path
+
+common_init_opts = ["encryption=none", "type=external", "externaltype=ora",
+                    "autoenable=true"]
+
+example_payload = {
+    'one.txt': 'content1',
+    'subdir': {
+        'two': 'content2',
+    },
+}
+
+
+example_payload2 = {
+    'three.txt': 'content3',
+    'subdir': {
+        'four': 'content4',
+    },
+}
+
+
+def get_all_files(path):
+    return sorted([
+        Path(p).relative_to(path)
+        for p in glob(str(Path(path) / '**'), recursive=True)
+        if not Path(p).is_dir()
+    ])
+
+
+def initremote(repo, name, encryption=None, config=None):
+    cfg = dict(config) if config else {}
+    cfg['encryption'] = encryption if encryption else 'none'
+    args = ['{}={}'.format(k, v) for k, v in cfg.items()]
+    repo.init_remote(name, args)
+
+
+def initexternalremote(repo, name, type, encryption=None, config=None):
+    config = dict(
+        config if config else {},
+        type='external',
+        externaltype=type,
+    )
+    return initremote(repo, name, encryption=encryption, config=config)
+
+
+def setup_archive_remote(repo, archive_path):
+
+    # for integration in a URL, we need POSIX version of the path
+    archive_path = Path(archive_path)
+
+    if 'DATALAD_TESTS_SSH' in os.environ:
+        cfg = {'url': 'ria+ssh://datalad-test{}'
+                      ''.format(archive_path.as_posix())}
+    else:
+        cfg = {'url': 'ria+{}'.format(archive_path.as_uri())}
+    initexternalremote(repo, 'archive', 'ora', config=cfg)
 
 
-# PLUGIN API
-def dlplugin(dataset, output=None):
-    import os
-    import tarfile
-    from mock import patch
-    from os.path import join as opj, dirname, normpath, isabs
-    from datalad.utils import file_basename
-    from datalad.support.annexrepo import AnnexRepo
-
-    import logging
-    lgr = logging.getLogger('datalad.plugin.tarball')
-
-    repo = dataset.repo
-    committed_date = repo.get_commit_date()
-
-    # could be used later on to filter files by some criterion
-    def _filter_tarinfo(ti):
-        # Reset the date to match the one of the last commit, not from the
-        # filesystem since git doesn't track those at all
-        # TODO: use the date of the last commit when any particular
-        # file was changed -- would be the most kosher yoh thinks to the
-        # degree of our abilities
-        ti.mtime = committed_date
-        return ti
+def populate_dataset(ds):
+    # create 2 commits
+    for pl in [example_payload, example_payload2]:
+        create_tree(ds.path, pl)
+        ds.save()
 
-    if output is None:
-        output = "datalad_{}.tar.gz".format(dataset.id)
-    else:
-        if not output.endswith('.tar.gz'):
-            output += '.tar.gz'
 
-    root = dataset.path
-    # use dir inside matching the output filename
-    # TODO: could be an option to the export plugin allowing empty value
-    # for no leading dir
-    leading_dir = file_basename(output)
-
-    # workaround for inability to pass down the time stamp
-    with patch('time.time', return_value=committed_date), \
-            tarfile.open(output, "w:gz") as tar:
-        repo_files = sorted(repo.get_indexed_files())
-        if isinstance(repo, AnnexRepo):
-            annexed = repo.is_under_annex(
-                repo_files, allow_quick=True, batch=True)
-        else:
-            annexed = [False] * len(repo_files)
-        for i, rpath in enumerate(repo_files):
-            fpath = opj(root, rpath)
-            if annexed[i]:
-                # resolve to possible link target
-                link_target = os.readlink(fpath)
-                if not isabs(link_target):
-                    link_target = normpath(opj(dirname(fpath), link_target))
-                fpath = link_target
-            # name in the tarball
-            aname = normpath(opj(leading_dir, rpath))
-            tar.add(
-                fpath,
-                arcname=aname,
-                recursive=False,
-                filter=_filter_tarinfo)
-
-    if not isabs(output):
-        output = opj(os.getcwd(), output)
-
-    yield dict(
-        status='ok',
-        path=output,
-        type='file',
-        action='export_tarball',
-        logger=lgr)
+def check_not_generatorfunction(func):
+    """Internal helper to verify that we are not decorating generator tests"""
+    if inspect.isgeneratorfunction(func):
+        raise RuntimeError("{}: must not be decorated, is a generator test"
+                           .format(func.__name__))
+
+
+def skip_non_ssh(func):
+    """Skips non-SSH-based tests if environment variable DATALAD_TESTS_SSH was
+    set
+
+    This is for test alternatives in order to blow runtime of SSH testing with
+    tests that ran in other test builds.
+    """
+
+    check_not_generatorfunction(func)
+
+    @wraps(func)
+    @attr('skip_ssh')
+    def  _wrap_skip_non_ssh(*args, **kwargs):
+        if 'DATALAD_TESTS_SSH' in os.environ:
+            raise SkipTest("Disabled, since DATALAD_TESTS_SSH is set")
+        return func(*args, **kwargs)
+    return  _wrap_skip_non_ssh
```

### Comparing `datalad-0.9.3/datalad/config.py` & `datalad-1.0.0/datalad/local/run_procedure.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,629 +1,523 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""
-"""
+"""Run prepared procedures (DataLad scripts) on a dataset"""
 
-import datalad
-from datalad.cmd import GitRunner
-from datalad.dochelpers import exc_str
-from distutils.version import LooseVersion
+__docformat__ = 'restructuredtext'
 
-import re
+
+import logging
 import os
-from os.path import join as opj, exists
-from os.path import getmtime
-from os.path import abspath
-from time import time
-
-cfg_kv_regex = re.compile(r'(^.*)\n(.*)$', flags=re.MULTILINE)
-cfg_section_regex = re.compile(r'(.*)\.[^.]+')
-cfg_sectionoption_regex = re.compile(r'(.*)\.([^.]+)')
-
-
-_where_reload_doc = """
-        where : {'dataset', 'local', 'global'}, optional
-          Indicator which configuration file to modify. 'dataset' indicates the
-          persistent configuration in .datalad/config of a dataset; 'local'
-          the configuration of a dataset's Git repository in .git/config;
-          'global' refers to the general configuration that is not specific to
-          a single repository (usually in $USER/.gitconfig).
-        reload : bool
-          Flag whether to reload the configuration from file(s) after
-          modification. This can be disable to make multiple sequential
-          modifications slightly more efficient.""".lstrip()
-
-
-# we cannot import external_versions here, as the cfg comes before anything
-# and we would have circular imports
-def get_git_version(runner):
-    """Return version of available git"""
-    return runner.run('git version'.split())[0].split()[2]
-
-
-def _where_reload(obj):
-    """Helper decorator to simplify providing repetitive docstring"""
-    obj.__doc__ = obj.__doc__ % _where_reload_doc
-    return obj
-
-
-def _parse_gitconfig_dump(dump, store, fileset, replace):
-    if replace:
-        # if we want to replace existing values in the store
-        # collect into a new dict and `update` the store at the
-        # end. This way we get the desired behavior of multi-value
-        # keys, but only for the current source
-        dct = {}
-        fileset = set()
-    else:
-        # if we don't want to replace value, perform the multi-value
-        # preserving addition on the existing store right away
-        dct = store
-    for line in dump.split('\0'):
-        if not line:
-            continue
-        if line.startswith('file:'):
-            # origin line
-            fileset.add(abspath(line[5:]))
-            continue
-        if line.startswith('command line:'):
-            # nothing we could handle
-            continue
-        k, v = cfg_kv_regex.match(line).groups()
-        present_v = dct.get(k, None)
-        if present_v is None:
-            dct[k] = v
+import os.path as op
+import stat
+import sys
+from argparse import REMAINDER
+from glob import iglob
+
+import datalad.support.ansi_colors as ac
+from datalad import cfg
+from datalad.core.local.run import Run
+from datalad.distribution.dataset import (
+    Dataset,
+    EnsureDataset,
+    datasetmethod,
+    require_dataset,
+)
+from datalad.interface.base import (
+    Interface,
+    build_doc,
+    eval_results,
+)
+from datalad.interface.results import get_status_dict
+from datalad.support.constraints import EnsureNone
+from datalad.support.exceptions import (
+    InsufficientArgumentsError,
+    NoDatasetFound,
+)
+from datalad.support.param import Parameter
+from datalad.utils import (
+    ensure_list,
+    guard_for_format,
+    join_cmdline,
+    quote_cmdlinearg,
+    split_cmdline,
+)
+
+if sys.version_info < (3, 9):
+    from importlib_resources import (
+        as_file,
+        files,
+    )
+else:
+    from importlib.resources import (
+        as_file,
+        files,
+    )
+
+lgr = logging.getLogger('datalad.local.run_procedures')
+
+
+def _get_file_match(dir, name='*'):
+    targets = (name, ('[!_]*.py'), ('[!_]*.sh'))
+    lgr.debug("Looking for procedure '%s' in '%s'", name, dir)
+    for target in targets:
+        for m in iglob(op.join(dir, target)):
+            m_bn = op.basename(m)
+            if name == '*':
+                report_name = m_bn[:-3] if m_bn.endswith('.py') or \
+                                           m_bn.endswith('.sh') \
+                                        else m_bn
+                yield m, report_name
+            elif m_bn == name or m_bn.startswith('{}.'.format(name)):
+                yield m, name
+
+
+def _get_proc_config(name, ds=None):
+    """get configuration of named procedure
+
+    Figures call format string and help message for a given procedure name,
+    based on dataset.
+
+    Returns
+    -------
+    tuple
+      (call format string, help string) or possibly None for either value,
+      if there's nothing configured
+    """
+    # figure what ConfigManager to ask
+    cm = cfg if ds is None else ds.config
+    # ConfigManager may not be up-to-date, particularly if we are in a
+    # subdataset due to recursion in `_get_procedure_implementation` where
+    # outside caller operates (and reloads) on superdataset only. With
+    # force=False, this shouldn't be expensive.
+    cm.reload()
+    # ConfigManager might return a tuple for different reasons.
+    # The config might have been defined multiple times in the same location
+    # (within .datalad/config for example) or there are multiple values for
+    # it on different levels of git-config (system, user, repo). git-config
+    # in turn does report such things ordered from most general to most
+    # specific configuration. We do want the most specific one here, so
+    # config.get(), which returns the last entry, works here.
+    # TODO: At this point we cannot determine whether it was actually
+    # configured to yield several values by the very same config, in which
+    # case we should actually issue a warning, since we then have no idea
+    # of a priority. But ConfigManager isn't able yet to tell us or to
+    # restrict the possibility to define multiple values to particular items
+    v = cm.get('datalad.procedures.{}.call-format'.format(name), None)
+    h = cm.get('datalad.procedures.{}.help'.format(name), None)
+    return v, h
+
+
+def _get_procedure_implementation(name='*', ds=None):
+    """get potential procedures: path, name, configuration, and a help message
+
+    The order of consideration is user-level, system-level, extra locations, dataset,
+    datalad extensions, datalad. Therefore local definitions/configurations take
+    precedence over ones, that come from outside (via a datalad-extension or a
+    dataset with its .datalad/config). If a dataset had precedence (as it was
+    before), the addition (or just an update) of a (sub-)dataset would otherwise
+    surprisingly cause you to execute code different from what you defined
+    within ~/.gitconfig or your local repository's .git/config.
+    So, local definitions take precedence over remote ones and more specific
+    ones over more general ones.
+
+    Yields
+    ------
+    tuple
+      path, name, format string, help message
+    """
+
+    ds = ds if isinstance(ds, Dataset) else Dataset(ds) if ds else None
+
+    # 1. check system and user account for procedure
+    for loc in (cfg.obtain('datalad.locations.user-procedures'),
+                cfg.obtain('datalad.locations.system-procedures'),
+                cfg.get('datalad.locations.extra-procedures', get_all=True)):
+        for dir in ensure_list(loc):
+            for m, n in _get_file_match(dir, name):
+                yield (m, n,) + _get_proc_config(n)
+    # 2. check dataset for procedure
+    if ds is not None and ds.is_installed():
+        # could be more than one
+        dirs = ensure_list(
+                ds.config.obtain('datalad.locations.dataset-procedures'))
+        for dir in dirs:
+            # TODO `get` dirs if necessary
+            for m, n in _get_file_match(op.join(ds.path, dir), name):
+                yield (m, n,) + _get_proc_config(n, ds=ds)
+        # 2.1. check subdatasets recursively
+        for subds in ds.subdatasets(return_type='generator',
+                                    result_xfm='datasets',
+                                    result_renderer='disabled'):
+            for m, n, f, h in _get_procedure_implementation(name=name, ds=subds):
+                yield m, n, f, h
+
+    # 3. check extensions for procedure
+    from datalad.support.entrypoints import iter_entrypoints
+
+    for epname, epmodule, _ in iter_entrypoints('datalad.extensions'):
+        res = files(epmodule) / "resources" / "procedures"
+        if res.is_dir():
+            with as_file(res) as p:
+                for m, n in _get_file_match(p, name):
+                    yield (m, n,) + _get_proc_config(n)
+    # 4. at last check datalad itself for procedure
+    with as_file(files("datalad") / "resources" / "procedures") as p:
+        for m, n in _get_file_match(p, name):
+            yield (m, n,) + _get_proc_config(n)
+
+
+def _guess_exec(script_file):
+    try:
+        is_exec = os.stat(script_file).st_mode & stat.S_IEXEC
+    except OSError as e:
+        from errno import ENOENT
+        if e.errno == ENOENT and op.islink(script_file):
+            # broken symlink
+            # does not exist; there's nothing to detect at all
+            return {'type': None, 'template': None, 'state': 'absent'}
         else:
-            if isinstance(present_v, tuple):
-                dct[k] = present_v + (v,)
-            else:
-                dct[k] = (present_v, v)
-    if replace:
-        store.update(dct)
-    return store, fileset
-
-
-def _parse_env(store):
-    dct = {}
-    for k in os.environ:
-        if not k.startswith('DATALAD_'):
-            continue
-        dct[k.replace('_', '.').lower()] = os.environ[k]
-    store.update(dct)
-    return store
-
-
-def anything2bool(val):
-    if hasattr(val, 'lower'):
-        val = val.lower()
-    if val in {"off", "no", "false", "0"} or not bool(val):
-        return False
-    elif val in {"on", "yes", "true", True} \
-            or (hasattr(val, 'isdigit') and val.isdigit() and int(val)) \
-            or isinstance(val, int) and val:
-        return True
+            raise e
+
+    # on some FS the executable bit might not be all that reliable
+    # but a procedure might nevertheless be supported.
+    # go by extension with "known" interpreters first, and only then
+    # try to execute something that looks executable
+    if script_file.endswith('.sh'):
+        return {'type': u'bash_script',
+                'template': u'bash {script} {ds} {args}',
+                'state': 'executable'}
+    elif script_file.endswith('.py'):
+        ex = quote_cmdlinearg(sys.executable)
+        return {'type': u'python_script',
+                'template': u'%s {script} {ds} {args}' % ex,
+                'state': 'executable'}
+    elif is_exec and not os.path.isdir(script_file):
+        return {'type': u'executable',
+                'template': u'{script} {ds} {args}',
+                'state': 'executable'}
     else:
-        raise TypeError(
-            "Got value %s which could not be interpreted as a boolean"
-            % repr(val))
-
-
-class ConfigManager(object):
-    """Thin wrapper around `git-config` with support for a dataset configuration.
-
-    The general idea is to have an object that is primarily used to read/query
-    configuration option.  Upon creation, current configuration is read via one
-    (or max two, in the case of the presence of dataset-specific configuration)
-    calls to `git config`.  If this class is initialized with a Dataset
-    instance, it supports reading and writing configuration from
-    ``.datalad/config`` inside a dataset too. This file is committed to Git and
-    hence useful to ship certain configuration items with a dataset.
-
-    The API aims to provide the most significant read-access API of a
-    dictionary, the Python ConfigParser, and GitPython's config parser
-    implementations.
-
-    This class is presently not capable of efficiently writing multiple
-    configurations items at once.  Instead, each modification results in a
-    dedicated call to `git config`. This author thinks this is OK, as he
-    cannot think of a situation where a large number of items need to be
-    written during normal operation. If such need arises, various solutions are
-    possible (via GitPython, or an independent writer).
-
-    Each instance carries a public `overrides` attribute. This dictionary
-    contains variables that override any setting read from a file. The overrides
-    are persistent across reloads, and are not modified by any of the
-    manipulation methods, such as `set` or `unset`.
-
-    Any DATALAD_* environment variable is also presented as a configuration
-    item. Settings read from environment variables are not stored in any of the
-    configuration file, but are read dynamically from the environment at each
-    `reload()` call. Their values take precedence over any specification in
-    configuration files, and even overrides.
-
-    Parameters
-    ----------
-    dataset : Dataset, optional
-      If provided, all `git config` calls are executed in this dataset's
-      directory. Moreover, any modifications are, by default, directed to
-      this dataset's configuration file (which will be created on demand)
-    dataset_only : bool
-      If True, configuration items are only read from a datasets persistent
-      configuration file, if any present (the one in ``.datalad/config``, not
-      ``.git/config``).
-    overrides : dict, optional
-      Variable overrides, see general class documentation for details.
+        return {'type': None, 'template': None, 'state': None}
+
+
+@build_doc
+class RunProcedure(Interface):
+    """Run prepared procedures (DataLad scripts) on a dataset
+
+    *Concept*
+
+    A "procedure" is an algorithm with the purpose to process a dataset in a
+    particular way. Procedures can be useful in a wide range of scenarios,
+    like adjusting dataset configuration in a uniform fashion, populating
+    a dataset with particular content, or automating other routine tasks,
+    such as synchronizing dataset content with certain siblings.
+
+    Implementations of some procedures are shipped together with DataLad,
+    but additional procedures can be provided by 1) any DataLad extension,
+    2) any (sub-)dataset, 3) a local user, or 4) a local system administrator.
+    DataLad will look for procedures in the following locations and order:
+
+    Directories identified by the configuration settings
+
+    - 'datalad.locations.user-procedures' (determined by
+      platformdirs.user_config_dir; defaults to '$HOME/.config/datalad/procedures'
+      on GNU/Linux systems)
+    - 'datalad.locations.system-procedures' (determined by
+      platformdirs.site_config_dir; defaults to '/etc/xdg/datalad/procedures' on
+      GNU/Linux systems)
+    - 'datalad.locations.dataset-procedures'
+
+    and subsequently in the 'resources/procedures/' directories of any
+    installed extension, and, lastly, of the DataLad installation itself.
+
+    Please note that a dataset that defines
+    'datalad.locations.dataset-procedures' provides its procedures to
+    any dataset it is a subdataset of. That way you can have a collection of
+    such procedures in a dedicated dataset and install it as a subdataset into
+    any dataset you want to use those procedures with. In case of a naming
+    conflict with such a dataset hierarchy, the dataset you're calling
+    run-procedures on will take precedence over its subdatasets and so on.
+
+    Each configuration setting can occur multiple times to indicate multiple
+    directories to be searched. If a procedure matching a given name is found
+    (filename without a possible extension), the search is aborted and this
+    implementation will be executed. This makes it possible for individual
+    datasets, users, or machines to override externally provided procedures
+    (enabling the implementation of customizable processing "hooks").
+
+
+    *Procedure implementation*
+
+    A procedure can be any executable. Executables must have the appropriate
+    permissions and, in the case of a script, must contain an appropriate
+    "shebang" line. If a procedure is not executable, but its filename ends
+    with '.py', it is automatically executed by the 'python' interpreter
+    (whichever version is available in the present environment). Likewise,
+    procedure implementations ending on '.sh' are executed via 'bash'.
+
+    Procedures can implement any argument handling, but must be capable
+    of taking at least one positional argument (the absolute path to the
+    dataset they shall operate on).
+
+    For further customization there are two configuration settings per procedure
+    available:
+
+    - 'datalad.procedures.<NAME>.call-format'
+      fully customizable format string to determine how to execute procedure
+      NAME (see also datalad-run).
+      It currently requires to include the following placeholders:
+
+      - '{script}': will be replaced by the path to the procedure
+      - '{ds}': will be replaced by the absolute path to the dataset the
+        procedure shall operate on
+      - '{args}': (not actually required) will be replaced by
+        [CMD: all additional arguments passed into run-procedure after NAME CMD]
+        [PY: all but the first element of `spec` if `spec` is a list or tuple PY]
+        As an example the default format string for a call to a python script is:
+        "python {script} {ds} {args}"
+    - 'datalad.procedures.<NAME>.help'
+      will be shown on `datalad run-procedure --help-proc NAME` to provide a
+      description and/or usage info for procedure NAME
     """
-    def __init__(self, dataset=None, dataset_only=False, overrides=None):
-        # store in a simple dict
-        # no subclassing, because we want to be largely read-only, and implement
-        # config writing separately
-        self._store = {}
-        self._cfgfiles = set()
-        self._cfgmtimes = None
-        # public dict to store variables that always override any setting
-        # read from a file
-        # `hasattr()` is needed because `datalad.cfg` is generated upon first module
-        # import, hence when this code runs first, there cannot be any config manager
-        # to inherit from
-        self.overrides = datalad.cfg.overrides.copy() if hasattr(datalad, 'cfg') else {}
-        if overrides is not None:
-            self.overrides.update(overrides)
-        if dataset is None:
-            self._dataset_path = None
-            self._dataset_cfgfname = None
-            self._repo_cfgfname = None
-        else:
-            self._dataset_path = dataset.path
-            self._dataset_cfgfname = opj(self._dataset_path, '.datalad', 'config')
-            if not dataset_only:
-                self._repo_cfgfname = opj(self._dataset_path, '.git', 'config')
-        self._dataset_only = dataset_only
-        # Since configs could contain sensitive information, to prevent
-        # any "facilitated" leakage -- just disable logging of outputs for
-        # this runner
-        run_kwargs = dict(log_outputs=False)
-        if dataset is not None:
-            # make sure we run the git config calls in the dataset
-            # to pick up the right config files
-            run_kwargs['cwd'] = dataset.path
-        self._runner = GitRunner(**run_kwargs)
+    _params_ = dict(
+        spec=Parameter(
+            args=("spec",),
+            metavar='NAME [ARGS]',
+            nargs=REMAINDER,
+            doc="""Name and possibly additional arguments of the to-be-executed
+            procedure. [PY: Can also be a dictionary coming from
+            run-procedure(discover=True).][CMD: Note, that all options
+            to run-procedure need to be put before NAME, since all
+            ARGS get assigned to NAME CMD]"""),
+        dataset=Parameter(
+            args=("-d", "--dataset"),
+            metavar="PATH",
+            doc="""specify the dataset to run the procedure on.
+            An attempt is made to identify the dataset based on the current
+            working directory.""",
+            constraints=EnsureDataset() | EnsureNone()),
+        discover=Parameter(
+            args=('--discover',),
+            action='store_true',
+            doc="""if given, all configured paths are searched for procedures
+            and one result record per discovered procedure is yielded, but
+            no procedure is executed"""),
+        help_proc=Parameter(
+            args=('--help-proc',),
+            action='store_true',
+            doc="""if given, get a help message for procedure NAME from config
+            setting datalad.procedures.NAME.help"""
+        )
+    )
+
+    _examples_ = [
+        dict(text="Find out which procedures are available on the current system",
+             code_py="run_procedure(discover=True)",
+             code_cmd="datalad run-procedure --discover"),
+        dict(text="Run the 'yoda' procedure in the current dataset",
+             code_py="run_procedure(spec='cfg_yoda', recursive=True)",
+             code_cmd="datalad run-procedure cfg_yoda"),
+    ]
+
+    result_renderer = 'tailored'
+
+    @staticmethod
+    @datasetmethod(name='run_procedure')
+    @eval_results
+    def __call__(
+            spec=None,
+            *,
+            dataset=None,
+            discover=False,
+            help_proc=False):
+        if not spec and not discover:
+            raise InsufficientArgumentsError('requires at least a procedure name')
+        if help_proc and not spec:
+            raise InsufficientArgumentsError('requires a procedure name')
+
         try:
-            self._gitconfig_has_showorgin = \
-                LooseVersion(get_git_version(self._runner)) >= '2.8.0'
-        except:
-            # no git something else broken, assume git is present anyway
-            # to not delay this, but assume it is old
-            self._gitconfig_has_showorgin = False
-
-        self.reload(force=True)
-
-    def reload(self, force=False):
-        """Reload all configuration items from the configured sources
-
-        If `force` is False, all files configuration was previously read from
-        are checked for differences in the modification times. If no difference
-        is found for any file no reload is performed. This mechanism will not
-        detect newly created global configuration files, use `force` in this case.
-        """
-        if not force and self._cfgmtimes:
-            # we aren't forcing and we have read files before
-            # check if any file we read from has changed
-            current_time = time()
-            curmtimes = {c: getmtime(c) for c in self._cfgfiles if exists(c)}
-            if all(curmtimes[c] == self._cfgmtimes.get(c) and
-                   # protect against low-res mtimes (FAT32 has 2s, EXT3 has 1s!)
-                   # if mtime age is less than worst resolution assume modified
-                   (current_time - curmtimes[c]) > 2.0
-                   for c in curmtimes):
-                # all the same, nothing to do, except for
-                # superimpose overrides, could have changed in the meantime
-                self._store.update(self.overrides)
-                # reread env, is quick
-                self._store = _parse_env(self._store)
-                return
-
-        self._store = {}
-        # 2-step strategy:
-        #   - load datalad dataset config from dataset
-        #   - load git config from all supported by git sources
-        # in doing so we always stay compatible with where Git gets its
-        # config from, but also allow to override persistent information
-        # from dataset locally or globally
-        run_args = ['-z', '-l']
-        if self._gitconfig_has_showorgin:
-            run_args.append('--show-origin')
-
-        if self._dataset_cfgfname:
-            if exists(self._dataset_cfgfname):
-                stdout, stderr = self._run(
-                    run_args + ['--file', self._dataset_cfgfname],
-                    log_stderr=True
-                )
-                # overwrite existing value, do not amend to get multi-line
-                # values
-                self._store, self._cfgfiles = _parse_gitconfig_dump(
-                    stdout, self._store, self._cfgfiles, replace=False)
-
-        if self._dataset_only:
-            # superimpose overrides
-            self._store.update(self.overrides)
+            ds = require_dataset(
+                dataset, check_installed=False,
+                purpose='run a procedure')
+        except NoDatasetFound:
+            ds = None
+
+        if discover:
+            # specific path of procedures that were already reported
+            reported = set()
+            # specific names of procedure for which an active one has been
+            # found
+            active = set()
+            for m, cmd_name, cmd_tmpl, cmd_help in \
+                    _get_procedure_implementation('*', ds=ds):
+                if m in reported:
+                    continue
+                ex = _guess_exec(m)
+                # configured template (call-format string) takes precedence:
+                if cmd_tmpl:
+                    ex['template'] = cmd_tmpl
+                if ex['state'] is None:
+                    # doesn't seem like a match
+                    lgr.debug("%s does not look like a procedure, ignored.", m)
+                    continue
+                state = 'overridden' if cmd_name in active else ex['state']
+                message = ex['type'] if ex['type'] else 'unknown type'
+                message += ' ({})'.format(state) if state != 'executable' else ''
+                res = get_status_dict(
+                    action='discover_procedure',
+                    path=m,
+                    type='file',
+                    logger=lgr,
+                    refds=ds.path if ds else None,
+                    status='ok',
+                    state=state,
+                    procedure_name=cmd_name,
+                    procedure_type=ex['type'],
+                    procedure_callfmt=ex['template'],
+                    procedure_help=cmd_help,
+                    message=message)
+                reported.add(m)
+                if state == 'executable':
+                    active.add(cmd_name)
+                yield res
             return
 
-        stdout, stderr = self._run(run_args, log_stderr=True)
-        self._store, self._cfgfiles = _parse_gitconfig_dump(
-            stdout, self._store, self._cfgfiles, replace=True)
-
-        # always monitor the dataset cfg location, we know where it is in all cases
-        if self._dataset_cfgfname:
-            self._cfgfiles.add(self._dataset_cfgfname)
-            self._cfgfiles.add(self._repo_cfgfname)
-        self._cfgmtimes = {c: getmtime(c) for c in self._cfgfiles if exists(c)}
-
-        # superimpose overrides
-        self._store.update(self.overrides)
-
-        # override with environment variables
-        self._store = _parse_env(self._store)
-
-    @_where_reload
-    def obtain(self, var, default=None, dialog_type=None, valtype=None,
-               store=False, where=None, reload=True, **kwargs):
-        """
-        Convenience method to obtain settings interactively, if needed
-
-        A UI will be used to ask for user input in interactive sessions.
-        Questions to ask, and additional explanations can be passed directly
-        as arguments, or retrieved from a list of pre-configured items.
-
-        Additionally, this method allows for type conversion and storage
-        of obtained settings. Both aspects can also be pre-configured.
-
-        Parameters
-        ----------
-        var : str
-          Variable name including any section like `git config` expects them,
-          e.g. 'core.editor'
-        default : any type
-          In interactive sessions and if `store` is True, this default value
-          will be presented to the user for confirmation (or modification).
-          In all other cases, this value will be silently assigned unless
-          there is an existing configuration setting.
-        dialog_type : {'question', 'yesno', None}
-          Which dialog type to use in interactive sessions. If `None`,
-          pre-configured UI options are used.
-        store : bool
-          Whether to store the obtained value (or default)
-        %s
-        `**kwargs`
-          Additional arguments for the UI function call, such as a question
-          `text`.
-        """
-        # do local import, as this module is import prominently and the
-        # could theroetically import all kind of weired things for type
-        # conversion
-        from datalad.interface.common_cfg import definitions as cfg_defs
-        # fetch what we know about this variable
-        cdef = cfg_defs.get(var, {})
-        # type conversion setup
-        if valtype is None and 'type' in cdef:
-            valtype = cdef['type']
-        if valtype is None:
-            valtype = lambda x: x
-
-        # any default?
-        if default is None and 'default' in cdef:
-            default = cdef['default']
-
-        _value = None
-        if var in self:
-            # nothing needs to be obtained, it is all here already
-            _value = self[var]
-        elif store is False and default is not None:
-            # nothing will be stored, and we have a default -> no user confirmation
-            # we cannot use logging, because we want to use the config to confiugre
-            # the logging
-            #lgr.debug('using default {} for config setting {}'.format(default, var))
-            _value = default
 
-        if _value is not None:
-            # we got everything we need and can exit early
-            try:
-                return valtype(_value)
-            except Exception as e:
-                raise ValueError(
-                    "value '{}' of existing configuration for '{}' cannot be "
-                    "converted to the desired type '{}' ({})".format(
-                        _value, var, valtype, exc_str(e)))
+        if isinstance(spec, dict):
+            # Skip getting procedure implementation if called with a
+            # dictionary (presumably coming from --discover)
+            procedure_file = spec['path']
+            cmd_name = spec['procedure_name']
+            cmd_tmpl = spec['procedure_callfmt']
+            cmd_help = spec['procedure_help']
 
-        # now we need to try to obtain something from the user
-        from datalad.ui import ui
+            name = cmd_name
+            args = []
 
-        # configure UI
-        dialog_opts = kwargs
-        if dialog_type is None:  # no override
-            # check for common knowledge on how to obtain a value
-            if 'ui' in cdef:
-                dialog_type = cdef['ui'][0]
-                # pull standard dialog settings
-                dialog_opts = cdef['ui'][1]
-                # update with input
-                dialog_opts.update(kwargs)
-
-        if (not ui.is_interactive or dialog_type is None) and default is None:
-            raise RuntimeError(
-                "cannot obtain value for configuration item '{}', "
-                "not preconfigured, no default, no UI available".format(var))
-
-        if not hasattr(ui, dialog_type):
-            raise ValueError("UI '{}' does not support dialog type '{}'".format(
-                ui, dialog_type))
-
-        # configure storage destination, if needed
-        if store:
-            if where is None and 'destination' in cdef:
-                where = cdef['destination']
-            if where is None:
-                raise ValueError(
-                    "request to store configuration item '{}', but no "
-                    "storage destination specified".format(var))
-
-        # obtain via UI
-        dialog = getattr(ui, dialog_type)
-        _value = dialog(default=default, **dialog_opts)
-
-        if _value is None:
-            # we got nothing
-            if default is None:
-                raise RuntimeError(
-                    "could not obtain value for configuration item '{}', "
-                    "not preconfigured, no default".format(var))
-            # XXX maybe we should return default here, even it was returned
-            # from the UI -- if that is even possible
+        else:
 
-        # execute type conversion before storing to check that we got
-        # something that looks like what we want
-        try:
-            value = valtype(_value)
-        except Exception as e:
-            raise ValueError(
-                "cannot convert user input `{}` to desired type ({})".format(
-                    _value, exc_str(e)))
-            # XXX we could consider "looping" until we have a value of proper
-            # type in case of a user typo...
-
-        if store:
-            # store value as it was before any conversion, needs to be str
-            # anyway
-            # needs string conversion nevertheless, because default could come
-            # in as something else
-            self.add(var, '{}'.format(_value), where=where, reload=reload)
-        return value
-
-    #
-    # Compatibility with dict API
-    #
-    def __len__(self):
-        return len(self._store)
-
-    def __getitem__(self, key):
-        return self._store.__getitem__(key)
-
-    def __contains__(self, key):
-        return self._store.__contains__(key)
-
-    def keys(self):
-        """Returns list of configuration item names"""
-        return self._store.keys()
-
-    def get(self, key, default=None):
-        """D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None."""
-        return self._store.get(key, default)
-
-    #
-    # Compatibility with ConfigParser API
-    #
-    def sections(self):
-        """Returns a list of the sections available"""
-        return list(set([cfg_section_regex.match(k).group(1) for k in self._store]))
-
-    def options(self, section):
-        """Returns a list of options available in the specified section."""
-        opts = []
-        for k in self._store:
-            sec, opt = cfg_sectionoption_regex.match(k).groups()
-            if sec == section:
-                opts.append(opt)
-        return opts
-
-    def has_section(self, section):
-        """Indicates whether a section is present in the configuration"""
-        for k in self._store:
-            if k.startswith(section):
-                return True
-        return False
-
-    def has_option(self, section, option):
-        """If the given section exists, and contains the given option"""
-        for k in self._store:
-            sec, opt = cfg_sectionoption_regex.match(k).groups()
-            if sec == section and opt == option:
-                return True
-        return False
-
-    def getint(self, section, option):
-        """A convenience method which coerces the option value to an integer"""
-        return int(self.get_value(section, option))
-
-    def getbool(self, section, option, default=None):
-        """A convenience method which coerces the option value to a bool
-
-        Values "on", "yes", "true" and any int!=0 are considered True
-        Values which evaluate to bool False, "off", "no", "false" are considered
-        False
-        TypeError is raised for other values.
-        """
-        val = self.get_value(section, option, default=default)
-        return anything2bool(val)
-
-    def getfloat(self, section, option):
-        """A convenience method which coerces the option value to a float"""
-        return float(self.get_value(section, option))
-
-    # this is a hybrid of ConfigParser and dict API
-    def items(self, section=None):
-        """Return a list of (name, value) pairs for each option
-
-        Optionally limited to a given section.
-        """
-        if section is None:
-            return self._store.items()
-        return [(k, v) for k, v in self._store.items()
-                if cfg_section_regex.match(k).group(1) == section]
-
-    #
-    # Compatibility with GitPython's ConfigParser
-    #
-    def get_value(self, section, option, default=None):
-        """Like `get()`, but with an optional default value
-
-        If the default is not None, the given default value will be returned in
-        case the option did not exist. This behavior imitates GitPython's
-        config parser.
-        """
-        try:
-            return self['.'.join((section, option))]
-        except KeyError as e:
-            # this strange dance is needed because gitpython does it this way
-            if default is not None:
-                return default
+            if not isinstance(spec, (tuple, list)):
+                # maybe coming from config
+                spec = split_cmdline(spec)
+            name = spec[0]
+            args = spec[1:]
+
+            try:
+                # get the first match an run with it
+                procedure_file, cmd_name, cmd_tmpl, cmd_help = \
+                    next(_get_procedure_implementation(name, ds=ds))
+            except StopIteration:
+                raise ValueError("Cannot find procedure with name '%s'" % name)
+
+        ex = _guess_exec(procedure_file)
+        # configured template (call-format string) takes precedence:
+        if cmd_tmpl:
+            ex['template'] = cmd_tmpl
+
+        if help_proc:
+            if cmd_help:
+                res = get_status_dict(
+                        action='procedure_help',
+                        path=procedure_file,
+                        type='file',
+                        logger=lgr,
+                        refds=ds.path if ds else None,
+                        status='ok',
+                        state=ex['state'],
+                        procedure_name=cmd_name,
+                        procedure_type=ex['type'],
+                        procedure_callfmt=ex['template'],
+                        message=cmd_help)
             else:
-                raise e
+                res = get_status_dict(
+                        action='procedure_help',
+                        path=procedure_file,
+                        type='file',
+                        logger=lgr,
+                        refds=ds.path if ds else None,
+                        status='impossible',
+                        state=ex['state'],
+                        procedure_name=cmd_name,
+                        procedure_type=ex['type'],
+                        procedure_callfmt=ex['template'],
+                        message="No help available for '%s'" % name)
+
+            yield res
+            return
 
-    #
-    # Modify configuration (proxy respective git-config call)
-    #
-    @_where_reload
-    def _run(self, args, where=None, reload=False, **kwargs):
-        """Centralized helper to run "git config" calls
-
-        Parameters
-        ----------
-        args : list
-          Arguments to pass for git config
-        %s
-        **kwargs
-          Keywords arguments for Runner's call
-        """
-        if where:
-            args = self._get_location_args(where) + args
-        out = self._runner.run(['git', 'config'] + args, **kwargs)
-        if reload:
-            self.reload()
-        return out
+        if not ex['template']:
+            raise ValueError("No idea how to execute procedure %s. "
+                             "Missing 'execute' permissions?" % procedure_file)
+
+        cmd = ex['template'].format(
+            script=guard_for_format(quote_cmdlinearg(procedure_file)),
+            ds=guard_for_format(quote_cmdlinearg(ds.path)) if ds else '',
+            args=join_cmdline(args) if args else '')
+        lgr.info(u"Running procedure %s", name)
+        lgr.debug(u'Full procedure command: %r', cmd)
+        for r in Run.__call__(
+                cmd=cmd,
+                dataset=ds,
+                explicit=True,
+                inputs=None,
+                outputs=None,
+                # pass through here
+                on_failure='ignore',
+                return_type='generator',
+                result_renderer='disabled'
+        ):
+            yield r
+
+        if ds:
+            # the procedure ran and we have to anticipate that it might have
+            # changed the dataset config, so we need to trigger an unforced
+            # reload.
+            # we have to do this despite "being done here", because
+            # run_procedure() runs in the same process and reuses dataset (config
+            # manager) instances, and the next interaction with a dataset should
+            # be able to count on an up-to-date config
+            ds.config.reload()
+
+    @staticmethod
+    def custom_result_renderer(res, **kwargs):
+        from datalad.interface.utils import generic_result_renderer
+        from datalad.ui import ui
 
-    def _get_location_args(self, where, args=None):
-        if args is None:
-            args = []
-        cfg_labels = ('dataset', 'local', 'global')
-        if where not in cfg_labels:
-            raise ValueError(
-                "unknown configuration label '{}' (not in {})".format(
-                    where, cfg_labels))
-        if where == 'dataset':
-            if not self._dataset_cfgfname:
-                raise ValueError(
-                    'ConfigManager cannot store to configuration to dataset, '
-                    'none specified')
-            # create an empty config file if none exists, `git config` will
-            # fail otherwise
-            dscfg_dirname = opj(self._dataset_path, '.datalad')
-            if not exists(dscfg_dirname):
-                os.makedirs(dscfg_dirname)
-            if not exists(self._dataset_cfgfname):
-                open(self._dataset_cfgfname, 'w').close()
-            args.extend(['--file', self._dataset_cfgfname])
-        elif where == 'global':
-            args.append('--global')
-        elif where == 'local':
-            args.append('--local')
-        return args
-
-    @_where_reload
-    def add(self, var, value, where='dataset', reload=True):
-        """Add a configuration variable and value
-
-        Parameters
-        ----------
-        var : str
-          Variable name including any section like `git config` expects them, e.g.
-          'core.editor'
-        value : str
-          Variable value
-        %s"""
-        self._run(['--add', var, value], where=where, reload=reload, log_stderr=True)
-
-    @_where_reload
-    def set(self, var, value, where='dataset', reload=True, force=False):
-        """Set a variable to a value.
-
-        In opposition to `add`, this replaces the value of `var` if there is
-        one already.
-
-        Parameters
-        ----------
-        var : str
-          Variable name including any section like `git config` expects them, e.g.
-          'core.editor'
-        value : str
-          Variable value
-        force: bool
-          if set, replaces all occurrences of `var` by a single one with the
-          given `value`. Otherwise raise if multiple entries for `var` exist
-          already
-        %s"""
-        from datalad.support.gitrepo import to_options
-
-        self._run(to_options(replace_all=force) + [var, value],
-                  where=where, reload=reload, log_stderr=True)
-
-    @_where_reload
-    def rename_section(self, old, new, where='dataset', reload=True):
-        """Rename a configuration section
-
-        Parameters
-        ----------
-        old : str
-          Name of the section to rename.
-        new : str
-          Name of the section to rename to.
-        %s"""
-        self._run(['--rename-section', old, new], where=where, reload=reload)
-
-    @_where_reload
-    def remove_section(self, sec, where='dataset', reload=True):
-        """Rename a configuration section
-
-        Parameters
-        ----------
-        sec : str
-          Name of the section to remove.
-        %s"""
-        self._run(['--remove-section', sec], where=where, reload=reload)
-
-    @_where_reload
-    def unset(self, var, where='dataset', reload=True):
-        """Remove all occurrences of a variable
-
-        Parameters
-        ----------
-        var : str
-          Name of the variable to remove
-        %s"""
-        # use unset all as it is simpler for now
-        self._run(['--unset-all', var], where=where, reload=reload)
+        if res['status'] != 'ok' or 'procedure' not in res.get('action', ''):
+            # it's not our business
+            generic_result_renderer(res)
+            return
+
+        if kwargs.get('discover', None):
+            ui.message('{name} ({path}){msg}'.format(
+                # bold-faced name, if active
+                name=ac.color_word(res['procedure_name'], ac.BOLD)
+                if res['state'] == 'executable' else res['procedure_name'],
+                path=res['path'],
+                msg=' [{}]'.format(
+                    res['message'][0] % res['message'][1:]
+                    if isinstance(res['message'], tuple) else res['message'])
+                if 'message' in res else ''
+            ))
+
+        elif kwargs.get('help_proc', None):
+            ui.message('{name} ({path}){help}'.format(
+                name=ac.color_word(res['procedure_name'], ac.BOLD),
+                path=op.relpath(
+                    res['path'],
+                    res['refds'])
+                if res.get('refds', None) else res['path'],
+                help='{nl}{msg}'.format(
+                    nl=os.linesep,
+                    msg=res['message'][0] % res['message'][1:]
+                    if isinstance(res['message'], tuple) else res['message'])
+                if 'message' in res else ''
+            ))
+
+        else:
+            generic_result_renderer(res)
```

### Comparing `datalad-0.9.3/datalad/support/tests/test_cache.py` & `datalad-1.0.0/datalad/support/tests/test_cache.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 
+from ...tests.utils_pytest import assert_equal
 from ..cache import DictCache
-from ...tests.utils import assert_equal
 
 
 def test_DictCache():
     d = DictCache(size_limit=2)
 
     assert_equal(d, {})
     d['a'] = 2
```

### Comparing `datalad-0.9.3/datalad/support/tests/test_annexrepo.py` & `datalad-1.0.0/datalad/support/tests/test_annexrepo.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,330 +1,369 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 """Test implementation of class AnnexRepo
 
 """
 
-from datalad.tests.utils import known_failure_v6
-
+import gc
+import json
 import logging
-from functools import partial
 import os
+import re
+import sys
+import unittest.mock
+from functools import partial
+from glob import glob
 from os import mkdir
+from os.path import (
+    basename,
+    curdir,
+    exists,
+)
 from os.path import join as opj
-from os.path import basename
-from os.path import realpath
-from os.path import relpath
-from os.path import curdir
-from os.path import pardir
-from os.path import exists
+from os.path import (
+    pardir,
+    relpath,
+)
+from queue import Queue
 from shutil import copyfile
-from nose.tools import assert_not_is_instance
-
-from six.moves.urllib.parse import urljoin
-from six.moves.urllib.parse import urlsplit
-
-import git
-from git import GitCommandError
-from mock import patch
-import gc
+from unittest.mock import patch
+from urllib.parse import (
+    urljoin,
+    urlsplit,
+)
 
-from datalad.cmd import Runner
+import pytest
 
+from datalad import cfg as dl_cfg
+from datalad.api import clone
+from datalad.cmd import GitWitlessRunner
+from datalad.cmd import WitlessRunner as Runner
+from datalad.consts import (
+    DATALAD_SPECIAL_REMOTE,
+    DATALAD_SPECIAL_REMOTES_UUIDS,
+    WEB_SPECIAL_REMOTE_UUID,
+)
+from datalad.distribution.dataset import Dataset
+from datalad.runner.gitrunner import GitWitlessRunner
+from datalad.support import path as op
+# imports from same module:
+from datalad.support.annexrepo import (
+    AnnexJsonProtocol,
+    AnnexRepo,
+    GeneratorAnnexJsonNoStderrProtocol,
+    GeneratorAnnexJsonProtocol,
+)
+from datalad.support.exceptions import (
+    AnnexBatchCommandError,
+    CommandError,
+    FileInGitError,
+    FileNotInAnnexError,
+    FileNotInRepositoryError,
+    IncompleteResultsError,
+    InsufficientArgumentsError,
+    MissingExternalDependency,
+    OutdatedExternalDependency,
+    OutOfSpaceError,
+    RemoteNotAvailableError,
+)
 from datalad.support.external_versions import external_versions
-
+from datalad.support.gitrepo import GitRepo
 from datalad.support.sshconnector import get_connection_hash
+from datalad.tests.utils_pytest import (
+    DEFAULT_BRANCH,
+    DEFAULT_REMOTE,
+    OBSCURE_FILENAME,
+    SkipTest,
+    assert_cwd_unchanged,
+)
+from datalad.tests.utils_pytest import assert_dict_equal as deq_
+from datalad.tests.utils_pytest import (
+    assert_equal,
+    assert_false,
+    assert_in,
+    assert_is_instance,
+    assert_not_equal,
+    assert_not_in,
+    assert_not_is_instance,
+    assert_raises,
+    assert_re_in,
+    assert_repo_status,
+    assert_result_count,
+    assert_true,
+    create_tree,
+    eq_,
+    find_files,
+    get_most_obscure_supported_name,
+    known_failure_githubci_win,
+    known_failure_windows,
+    maybe_adjust_repo,
+    ok_,
+    ok_annex_get,
+    ok_file_has_content,
+    ok_file_under_git,
+    ok_git_config_not_empty,
+    on_nfs,
+    on_travis,
+    serve_path_via_http,
+    set_annex_version,
+    skip_if,
+    skip_if_adjusted_branch,
+    skip_if_on_windows,
+    skip_if_root,
+    skip_nomultiplex_ssh,
+    slow,
+    swallow_logs,
+    swallow_outputs,
+    with_parametric_batch,
+    with_sameas_remote,
+    with_tempfile,
+    with_tree,
+    xfail_buggy_annex_info,
+)
+from datalad.utils import (
+    Path,
+    chpwd,
+    get_linux_distribution,
+    on_windows,
+    quote_cmdlinearg,
+    rmtree,
+    unlink,
+)
 
-from datalad.utils import on_windows
-from datalad.utils import chpwd
-from datalad.utils import rmtree
-from datalad.utils import linux_distribution_name
-
-from datalad.tests.utils import ignore_nose_capturing_stdout
-from datalad.tests.utils import assert_cwd_unchanged
-from datalad.tests.utils import with_testrepos
-from datalad.tests.utils import with_tempfile
-from datalad.tests.utils import with_tree
-from datalad.tests.utils import create_tree
-from datalad.tests.utils import with_batch_direct
-from datalad.tests.utils import assert_dict_equal as deq_
-from datalad.tests.utils import assert_is_instance
-from datalad.tests.utils import assert_false
-from datalad.tests.utils import assert_in
-from datalad.tests.utils import assert_is
-from datalad.tests.utils import assert_not_in
-from datalad.tests.utils import assert_re_in
-from datalad.tests.utils import assert_raises
-from datalad.tests.utils import assert_not_equal
-from datalad.tests.utils import assert_equal
-from datalad.tests.utils import assert_true
-from datalad.tests.utils import eq_
-from datalad.tests.utils import ok_
-from datalad.tests.utils import ok_git_config_not_empty
-from datalad.tests.utils import ok_annex_get
-from datalad.tests.utils import ok_clean_git
-from datalad.tests.utils import ok_file_has_content
-from datalad.tests.utils import swallow_logs
-from datalad.tests.utils import swallow_outputs
-from datalad.tests.utils import local_testrepo_flavors
-from datalad.tests.utils import serve_path_via_http
-from datalad.tests.utils import get_most_obscure_supported_name
-from datalad.tests.utils import SkipTest
-from datalad.tests.utils import skip_ssh
-from datalad.tests.utils import find_files
-
-from datalad.support.exceptions import CommandError
-from datalad.support.exceptions import CommandNotAvailableError
-from datalad.support.exceptions import FileNotInRepositoryError
-from datalad.support.exceptions import FileNotInAnnexError
-from datalad.support.exceptions import FileInGitError
-from datalad.support.exceptions import OutOfSpaceError
-from datalad.support.exceptions import RemoteNotAvailableError
-from datalad.support.exceptions import OutdatedExternalDependency
-from datalad.support.exceptions import MissingExternalDependency
-from datalad.support.exceptions import InsufficientArgumentsError
-from datalad.support.exceptions import AnnexBatchCommandError
-from datalad.support.exceptions import IncompleteResultsError
-
-from datalad.support.gitrepo import GitRepo
 
-# imports from same module:
-from datalad.support.annexrepo import AnnexRepo
-from datalad.support.annexrepo import ProcessAnnexProgressIndicators
-from .utils import check_repo_deals_with_inode_change
+_GIT_ANNEX_VERSIONS_INFO = AnnexRepo.check_repository_versions()
 
 
-@ignore_nose_capturing_stdout
 @assert_cwd_unchanged
-@with_testrepos('.*annex.*')
 @with_tempfile
-def test_AnnexRepo_instance_from_clone(src, dst):
+@with_tempfile
+def test_AnnexRepo_instance_from_clone(src=None, dst=None):
 
+    origin = AnnexRepo(src, create=True)
     ar = AnnexRepo.clone(src, dst)
     assert_is_instance(ar, AnnexRepo, "AnnexRepo was not created.")
     ok_(os.path.exists(os.path.join(dst, '.git', 'annex')))
 
-    # do it again should raise GitCommandError since git will notice
+    # do it again should raise ValueError since git will notice
     # there's already a git-repo at that path and therefore can't clone to `dst`
     with swallow_logs(new_level=logging.WARN) as cm:
-        assert_raises(GitCommandError, AnnexRepo.clone, src, dst)
-        if git.__version__ != "1.0.2" and git.__version__ != "2.0.5":
-            assert("already exists" in cm.out)
+        assert_raises(ValueError, AnnexRepo.clone, src, dst)
 
 
-@ignore_nose_capturing_stdout
 @assert_cwd_unchanged
-@with_testrepos('.*annex.*', flavors=local_testrepo_flavors)
-def test_AnnexRepo_instance_from_existing(path):
+@with_tempfile
+def test_AnnexRepo_instance_from_existing(path=None):
+    AnnexRepo(path, create=True)
 
     ar = AnnexRepo(path)
     assert_is_instance(ar, AnnexRepo, "AnnexRepo was not created.")
     ok_(os.path.exists(os.path.join(path, '.git')))
 
 
-@ignore_nose_capturing_stdout
 @assert_cwd_unchanged
 @with_tempfile
-def test_AnnexRepo_instance_brand_new(path):
+def test_AnnexRepo_instance_brand_new(path=None):
 
     GitRepo(path)
     assert_raises(RuntimeError, AnnexRepo, path, create=False)
 
     ar = AnnexRepo(path)
     assert_is_instance(ar, AnnexRepo, "AnnexRepo was not created.")
     ok_(os.path.exists(os.path.join(path, '.git')))
 
 
 @assert_cwd_unchanged
-@with_testrepos('.*annex.*')
 @with_tempfile
-def test_AnnexRepo_crippled_filesystem(src, dst):
+def test_AnnexRepo_crippled_filesystem(dst=None):
 
-    ar = AnnexRepo.clone(src, dst)
+    ar = AnnexRepo(dst)
 
     # fake git-annex entries in .git/config:
-    writer = ar.repo.config_writer()
-    writer.set_value("annex", "crippledfilesystem", True)
-    writer.release()
+    ar.config.set(
+        "annex.crippledfilesystem",
+        'true',
+        scope='local')
     ok_(ar.is_crippled_fs())
-    writer.set_value("annex", "crippledfilesystem", False)
-    writer.release()
+    ar.config.set(
+        "annex.crippledfilesystem",
+        'false',
+        scope='local')
     assert_false(ar.is_crippled_fs())
     # since we can't remove the entry, just rename it to fake its absence:
-    writer.rename_section("annex", "removed")
-    writer.set_value("annex", "something", "value")
-    writer.release()
+    ar.config.rename_section("annex", "removed", scope='local')
+    ar.config.set("annex.something", "value", scope='local')
     assert_false(ar.is_crippled_fs())
 
 
+@known_failure_githubci_win
+@with_tempfile
 @assert_cwd_unchanged
-@with_testrepos('.*annex.*', flavors=local_testrepo_flavors)
-def test_AnnexRepo_is_direct_mode(path):
+def test_AnnexRepo_is_direct_mode(path=None):
 
     ar = AnnexRepo(path)
     eq_(ar.config.getbool("annex", "direct", False),
         ar.is_direct_mode())
 
 
+@known_failure_githubci_win
 @with_tempfile()
-def test_AnnexRepo_is_direct_mode_gitrepo(path):
+def test_AnnexRepo_is_direct_mode_gitrepo(path=None):
     repo = GitRepo(path, create=True)
     # artificially make .git/annex so no annex section gets initialized
     # in .git/config.  We did manage somehow to make this happen (via publish)
     # but didn't reproduce yet, so just creating manually
     mkdir(opj(repo.path, '.git', 'annex'))
     ar = AnnexRepo(path, init=False, create=False)
     # It is unlikely though that annex would be in direct mode (requires explicit)
     # annex magic, without having annex section under .git/config
     dm = ar.is_direct_mode()
-
-    if ar.is_crippled_fs() or on_windows:
-        ok_(dm)
-    else:
-        assert_false(dm)
-
-
-@assert_cwd_unchanged
-@with_testrepos('.*annex.*')
-@with_tempfile
-def test_AnnexRepo_set_direct_mode(src, dst):
-
-    ar = AnnexRepo.clone(src, dst)
-
-    if ar.config.getint("annex", "version") >= 6:
-        # there's no direct mode available:
-        assert_raises(CommandError, ar.set_direct_mode, True)
-        raise SkipTest("Test not applicable in repository version >= 6")
-
-    ar.set_direct_mode(True)
-    ok_(ar.is_direct_mode(), "Switching to direct mode failed.")
-    if ar.is_crippled_fs():
-        assert_raises(CommandNotAvailableError, ar.set_direct_mode, False)
-        ok_(
-            ar.is_direct_mode(),
-            "Indirect mode on crippled fs detected. Shouldn't be possible.")
-    else:
-        ar.set_direct_mode(False)
-        assert_false(ar.is_direct_mode(), "Switching to indirect mode failed.")
-
-
-@assert_cwd_unchanged
-@with_testrepos('.*annex.*', flavors=local_testrepo_flavors)
-@with_tempfile
-def test_AnnexRepo_annex_proxy(src, annex_path):
-    ar = AnnexRepo.clone(src, annex_path)
-    if ar.config.getint("annex", "version") >= 6:
-        # there's no direct mode available and therefore no 'annex proxy':
-        assert_raises(CommandError, ar.proxy, ['git', 'status'])
-        raise SkipTest("Test not applicable in repository version >= 6")
-    ar.set_direct_mode(True)
-
-    # annex proxy raises in indirect mode:
-    try:
-        ar.set_direct_mode(False)
-        assert_raises(CommandNotAvailableError, ar.proxy, ['git', 'status'])
-    except CommandNotAvailableError:
-        # we can't switch to indirect
-        pass
+    # no direct mode, ever
+    assert_false(dm)
 
 
+# ignore warning since we are testing that function here. Remove upon full deprecation
+@pytest.mark.filterwarnings(r"ignore: AnnexRepo.get_file_key\(\) is deprecated")
 @assert_cwd_unchanged
-@with_testrepos('.*annex.*', flavors=local_testrepo_flavors)
 @with_tempfile
-def test_AnnexRepo_get_file_key(src, annex_path):
+def test_AnnexRepo_get_file_key(annex_path=None):
 
-    ar = AnnexRepo.clone(src, annex_path)
+    ar = AnnexRepo(annex_path)
+    (ar.pathobj / 'test.dat').write_text('123\n')
+    ar.save('test.dat', git=True)
+    (ar.pathobj / 'test-annex.dat').write_text(
+        "content to be annex-addurl'd")
+    ar.save('some')
 
     # test-annex.dat should return the correct key:
-    eq_(
-        ar.get_file_key("test-annex.dat"),
-        'SHA256E-s28--2795fb26981c5a687b9bf44930cc220029223f472cea0f0b17274f4473181e7b.dat')
+    test_annex_key = \
+        'SHA256E-s28' \
+        '--2795fb26981c5a687b9bf44930cc220029223f472cea0f0b17274f4473181e7b.dat'
+    eq_(ar.get_file_key("test-annex.dat"), test_annex_key)
 
     # and should take a list with an empty string as result, if a file wasn't
     # in annex:
     eq_(
         ar.get_file_key(["filenotpresent.wtf", "test-annex.dat"]),
-        ['', 'SHA256E-s28--2795fb26981c5a687b9bf44930cc220029223f472cea0f0b17274f4473181e7b.dat']
+        ['', test_annex_key]
     )
 
     # test.dat is actually in git
     # should raise Exception; also test for polymorphism
     assert_raises(IOError, ar.get_file_key, "test.dat")
     assert_raises(FileNotInAnnexError, ar.get_file_key, "test.dat")
     assert_raises(FileInGitError, ar.get_file_key, "test.dat")
 
     # filenotpresent.wtf doesn't even exist
     assert_raises(IOError, ar.get_file_key, "filenotpresent.wtf")
 
+    # if we force batch mode, no failure for not present or not annexed files
+    eq_(ar.get_file_key("filenotpresent.wtf", batch=True), '')
+    eq_(ar.get_file_key("test.dat", batch=True), '')
+    eq_(ar.get_file_key("test-annex.dat", batch=True), test_annex_key)
+
 
 @with_tempfile(mkdir=True)
-def test_AnnexRepo_get_outofspace(annex_path):
+def test_AnnexRepo_get_outofspace(annex_path=None):
     ar = AnnexRepo(annex_path, create=True)
 
     def raise_cmderror(*args, **kwargs):
         raise CommandError(
             cmd="whatever",
             stderr="junk around not enough free space, need 905.6 MB more and after"
         )
 
-    with patch.object(AnnexRepo, '_run_annex_command', raise_cmderror) as cma, \
+    with patch.object(GitWitlessRunner, 'run_on_filelist_chunks', raise_cmderror) as cma, \
             assert_raises(OutOfSpaceError) as cme:
         ar.get("file")
-    exc = cme.exception
+    exc = cme.value
     eq_(exc.sizemore_msg, '905.6 MB')
-    assert_re_in(".*annex (find|get). needs 905.6 MB more", str(exc))
+    assert_re_in(".*annex.*(find|get).*needs 905.6 MB more", str(exc), re.DOTALL)
 
 
-@with_testrepos('basic_annex', flavors=['local'])
-def test_AnnexRepo_get_remote_na(path):
-    ar = AnnexRepo(path)
+@with_tempfile
+@with_tempfile
+def test_AnnexRepo_get_remote_na(src=None, path=None):
+    origin = AnnexRepo(src, create=True)
+    (origin.pathobj / 'test-annex.dat').write_text("content")
+    origin.save()
+    ar = AnnexRepo.clone(src, path)
 
     with assert_raises(RemoteNotAvailableError) as cme:
         ar.get('test-annex.dat', options=["--from=NotExistingRemote"])
-    eq_(cme.exception.remote, "NotExistingRemote")
+    eq_(cme.value.remote, "NotExistingRemote")
 
     # and similar one whenever invoking with remote parameter
     with assert_raises(RemoteNotAvailableError) as cme:
         ar.get('test-annex.dat', remote="NotExistingRemote")
-    eq_(cme.exception.remote, "NotExistingRemote")
+    eq_(cme.value.remote, "NotExistingRemote")
+
+
+@with_sameas_remote
+def test_annex_repo_sameas_special(repo=None):
+    remotes = repo.get_special_remotes()
+    eq_(len(remotes), 2)
+    rsync_info = [v for v in remotes.values()
+                  if v.get("sameas-name") == "r_rsync"]
+    eq_(len(rsync_info), 1)
+    # r_rsync is a sameas remote that points to r_dir. Its sameas-name value
+    # has been copied under "name".
+    eq_(rsync_info[0]["name"], rsync_info[0]["sameas-name"])
 
 
 # 1 is enough to test file_has_content
-@with_batch_direct
-@with_testrepos('.*annex.*', flavors=['local'], count=1)
+@with_parametric_batch
 @with_tempfile
-def test_AnnexRepo_file_has_content(batch, direct, src, annex_path):
-    ar = AnnexRepo.clone(src, annex_path, direct=direct)
+@with_tempfile
+def test_AnnexRepo_file_has_content(src=None, annex_path=None, *, batch):
+    origin = AnnexRepo(src)
+    (origin.pathobj / 'test.dat').write_text('123\n')
+    origin.save('test.dat', git=True)
+    (origin.pathobj / 'test-annex.dat').write_text("content")
+    origin.save('some')
+    ar = AnnexRepo.clone(src, annex_path)
     testfiles = ["test-annex.dat", "test.dat"]
 
     eq_(ar.file_has_content(testfiles), [False, False])
 
     ok_annex_get(ar, "test-annex.dat")
     eq_(ar.file_has_content(testfiles, batch=batch), [True, False])
     eq_(ar.file_has_content(testfiles[:1], batch=batch), [True])
 
     eq_(ar.file_has_content(testfiles + ["bogus.txt"], batch=batch),
         [True, False, False])
 
     assert_false(ar.file_has_content("bogus.txt", batch=batch))
     ok_(ar.file_has_content("test-annex.dat", batch=batch))
 
+    ar.unlock(["test-annex.dat"])
+    eq_(ar.file_has_content(["test-annex.dat"], batch=batch),
+        [True])
+    with open(opj(annex_path, "test-annex.dat"), "a") as ofh:
+        ofh.write("more")
+    eq_(ar.file_has_content(["test-annex.dat"], batch=batch),
+        [False])
+
 
 # 1 is enough to test
-@with_batch_direct
-@with_testrepos('.*annex.*', flavors=['local'], count=1)
+@xfail_buggy_annex_info
+@with_parametric_batch
 @with_tempfile
-def test_AnnexRepo_is_under_annex(batch, direct, src, annex_path):
-    ar = AnnexRepo.clone(src, annex_path, direct=direct)
+@with_tempfile
+def test_AnnexRepo_is_under_annex(src=None, annex_path=None, *, batch):
+    origin = AnnexRepo(src)
+    (origin.pathobj / 'test-annex.dat').write_text("content")
+    origin.save('some')
+    ar = AnnexRepo.clone(src, annex_path)
 
     with open(opj(annex_path, 'not-committed.txt'), 'w') as f:
         f.write("aaa")
 
     testfiles = ["test-annex.dat", "not-committed.txt", "INFO.txt"]
     # wouldn't change
     target_value = [True, False, False]
@@ -337,50 +376,60 @@
 
     eq_(ar.is_under_annex(testfiles + ["bogus.txt"], batch=batch),
                  target_value + [False])
 
     assert_false(ar.is_under_annex("bogus.txt", batch=batch))
     ok_(ar.is_under_annex("test-annex.dat", batch=batch))
 
+    ar.unlock(["test-annex.dat"])
+    eq_(ar.is_under_annex(["test-annex.dat"], batch=batch),
+        [True])
+    with open(opj(annex_path, "test-annex.dat"), "a") as ofh:
+        ofh.write("more")
+    eq_(ar.is_under_annex(["test-annex.dat"], batch=batch),
+        [False])
+
 
+@xfail_buggy_annex_info
 @with_tree(tree=(('about.txt', 'Lots of abouts'),
                  ('about2.txt', 'more abouts'),
                  ('d', {'sub.txt': 'more stuff'})))
 @serve_path_via_http()
 @with_tempfile
-def test_AnnexRepo_web_remote(sitepath, siteurl, dst):
+def test_AnnexRepo_web_remote(sitepath=None, siteurl=None, dst=None):
 
     ar = AnnexRepo(dst, create=True)
     testurl = urljoin(siteurl, 'about.txt')
     testurl2 = urljoin(siteurl, 'about2.txt')
     testurl3 = urljoin(siteurl, 'd/sub.txt')
     url_file_prefix = urlsplit(testurl).netloc.split(':')[0]
     testfile = '%s_about.txt' % url_file_prefix
     testfile2 = '%s_about2.txt' % url_file_prefix
     testfile3 = opj('d', 'sub.txt')
 
     # get the file from remote
     with swallow_outputs() as cmo:
-        ar.add_urls([testurl])
+        ar.add_url_to_file(testfile, testurl)
     l = ar.whereis(testfile)
-    assert_in(ar.WEB_UUID, l)
+    assert_in(WEB_SPECIAL_REMOTE_UUID, l)
     eq_(len(l), 2)
     ok_(ar.file_has_content(testfile))
 
     # output='full'
     lfull = ar.whereis(testfile, output='full')
     eq_(set(lfull), set(l))  # the same entries
-    non_web_remote = l[1 - l.index(ar.WEB_UUID)]
+    non_web_remote = l[1 - l.index(WEB_SPECIAL_REMOTE_UUID)]
     assert_in('urls', lfull[non_web_remote])
     eq_(lfull[non_web_remote]['urls'], [])
-    assert_not_in('uuid', lfull[ar.WEB_UUID])  # no uuid in the records
-    eq_(lfull[ar.WEB_UUID]['urls'], [testurl])
+    assert_not_in('uuid', lfull[WEB_SPECIAL_REMOTE_UUID])  # no uuid in the records
+    eq_(lfull[WEB_SPECIAL_REMOTE_UUID]['urls'], [testurl])
+    assert_equal(lfull[WEB_SPECIAL_REMOTE_UUID]['description'], 'web')
 
     # --all and --key are incompatible
-    assert_raises(CommandError, ar.whereis, [], options='--all', output='full', key=True)
+    assert_raises(CommandError, ar.whereis, [testfile], options='--all', output='full', key=True)
 
     # output='descriptions'
     ldesc = ar.whereis(testfile, output='descriptions')
     eq_(set(ldesc), set([v['description'] for v in lfull.values()]))
 
     # info w/ and w/o fast mode
     for fast in [True, False]:
@@ -391,57 +440,65 @@
         eq_(info, info_batched)
         # while at it ;)
         with swallow_outputs() as cmo:
             eq_(ar.info('nonexistent', batch=False), None)
             eq_(ar.info('nonexistent-batch', batch=True), None)
             eq_(cmo.out, '')
             eq_(cmo.err, '')
+            ar.precommit()  # to stop all the batched processes for swallow_outputs
 
     # annex repo info
     repo_info = ar.repo_info(fast=False)
     eq_(repo_info['local annex size'], 14)
     eq_(repo_info['backend usage'], {'SHA256E': 1})
     # annex repo info in fast mode
     repo_info_fast = ar.repo_info(fast=True)
     # doesn't give much testable info, so just comparing a subset for match with repo_info info
     eq_(repo_info_fast['semitrusted repositories'], repo_info['semitrusted repositories'])
     #import pprint; pprint.pprint(repo_info)
 
     # remove the remote
     ar.rm_url(testfile, testurl)
     l = ar.whereis(testfile)
-    assert_not_in(ar.WEB_UUID, l)
+    assert_not_in(WEB_SPECIAL_REMOTE_UUID, l)
     eq_(len(l), 1)
 
     # now only 1 copy; drop should fail
-    res = ar.drop(testfile)
-    eq_(res['command'], 'drop')
-    eq_(res['success'], False)
-    assert_in('adjust numcopies', res['note'])
+    try:
+        res = ar.drop(testfile)
+    except CommandError as e:
+        # there should be at least one result that was captured
+        # TODO think about a more standard way of accessing such
+        # records in a CommandError, maybe having a more specialized
+        # exception derived from CommandError
+        res = e.kwargs['stdout_json'][0]
+        eq_(res['command'], 'drop')
+        eq_(res['success'], False)
+        assert_in('adjust numcopies', res['note'])
 
     # read the url using different method
     ar.add_url_to_file(testfile, testurl)
     l = ar.whereis(testfile)
-    assert_in(ar.WEB_UUID, l)
+    assert_in(WEB_SPECIAL_REMOTE_UUID, l)
     eq_(len(l), 2)
     ok_(ar.file_has_content(testfile))
 
     # 2 known copies now; drop should succeed
     ar.drop(testfile)
     l = ar.whereis(testfile)
-    assert_in(ar.WEB_UUID, l)
+    assert_in(WEB_SPECIAL_REMOTE_UUID, l)
     eq_(len(l), 1)
     assert_false(ar.file_has_content(testfile))
     lfull = ar.whereis(testfile, output='full')
     assert_not_in(non_web_remote, lfull) # not present -- so not even listed
 
     # multiple files/urls
     # get the file from remote
     with swallow_outputs() as cmo:
-        ar.add_urls([testurl2])
+        ar.add_url_to_file(testfile2, testurl2)
 
     # TODO: if we ask for whereis on all files, we should get for all files
     lall = ar.whereis('.')
     eq_(len(lall), 2)
     for e in lall:
         assert(isinstance(e, list))
     # but we don't know which one for which file. need a 'full' one for that
@@ -451,49 +508,111 @@
     eq_(set(lall_full), {testfile, testfile2})
 
     # add a bogus 2nd url to testfile
 
     someurl = "http://example.com/someurl"
     ar.add_url_to_file(testfile, someurl, options=['--relaxed'])
     lfull = ar.whereis(testfile, output='full')
-    eq_(set(lfull[ar.WEB_UUID]['urls']), {testurl, someurl})
+    eq_(set(lfull[WEB_SPECIAL_REMOTE_UUID]['urls']), {testurl, someurl})
 
     # and now test with a file in subdirectory
     subdir = opj(dst, 'd')
     os.mkdir(subdir)
     with swallow_outputs() as cmo:
         ar.add_url_to_file(testfile3, url=testurl3)
     ok_file_has_content(opj(dst, testfile3), 'more stuff')
-    eq_(set(ar.whereis(testfile3)), {ar.WEB_UUID, non_web_remote})
-    eq_(set(ar.whereis(testfile3, output='full').keys()), {ar.WEB_UUID, non_web_remote})
+    eq_(set(ar.whereis(testfile3)), {WEB_SPECIAL_REMOTE_UUID, non_web_remote})
+    eq_(set(ar.whereis(testfile3, output='full').keys()), {WEB_SPECIAL_REMOTE_UUID, non_web_remote})
 
     # and if we ask for both files
     info2 = ar.info([testfile, testfile3])
     eq_(set(info2), {testfile, testfile3})
     eq_(info2[testfile3]['size'], 10)
 
     full = ar.whereis([], options='--all', output='full')
     eq_(len(full.keys()), 3)  # we asked for all files -- got 3 keys
-    assert_in(ar.WEB_UUID, full['SHA256E-s10--a978713ea759207f7a6f9ebc9eaebd1b40a69ae408410ddf544463f6d33a30e1.txt'])
+    assert_in(WEB_SPECIAL_REMOTE_UUID, full['SHA256E-s10--a978713ea759207f7a6f9ebc9eaebd1b40a69ae408410ddf544463f6d33a30e1.txt'])
 
     # which would work even if we cd to that subdir, but then we should use explicit curdir
     with chpwd(subdir):
         cur_subfile = opj(curdir, 'sub.txt')
-        eq_(set(ar.whereis(cur_subfile)), {ar.WEB_UUID, non_web_remote})
-        eq_(set(ar.whereis(cur_subfile, output='full').keys()), {ar.WEB_UUID, non_web_remote})
+        eq_(set(ar.whereis(cur_subfile)), {WEB_SPECIAL_REMOTE_UUID, non_web_remote})
+        eq_(set(ar.whereis(cur_subfile, output='full').keys()), {WEB_SPECIAL_REMOTE_UUID, non_web_remote})
         testfiles = [cur_subfile, opj(pardir, testfile)]
         info2_ = ar.info(testfiles)
         # Should maintain original relative file names
         eq_(set(info2_), set(testfiles))
         eq_(info2_[cur_subfile]['size'], 10)
 
 
-@with_testrepos('.*annex.*', flavors=['local', 'network'])
+@with_tree(tree={"a.txt": "a",
+                 "b": "b",
+                 OBSCURE_FILENAME: "c",
+                 "subdir": {"d": "d", "e": "e"}})
+def test_find_batch_equivalence(path=None):
+    ar = AnnexRepo(path)
+    files = ["a.txt", "b", OBSCURE_FILENAME]
+    ar.add(files + ["subdir"])
+    ar.commit("add files")
+    query = ["not-there"] + files
+    expected = {f: f for f in files}
+    expected.update({"not-there": ""})
+    eq_(expected, ar.find(query, batch=True))
+    eq_(expected, ar.find(query))
+    # If we give a subdirectory, we split that output.
+    eq_(set(ar.find(["subdir"])["subdir"]), {"subdir/d", "subdir/e"})
+    eq_(ar.find(["subdir"]), ar.find(["subdir"], batch=True))
+    # manually ensure that no annex batch processes are around anymore
+    # that make the test cleanup break on windows.
+    # story at https://github.com/datalad/datalad/issues/4190
+    # even an explicit `del ar` does not get it done
+    ar._batched.close()
+
+
+@with_tempfile(mkdir=True)
+def test_repo_info(path=None):
+    repo = AnnexRepo(path)
+    info = repo.repo_info()  # works in empty repo without crashing
+    eq_(info['local annex size'], 0)
+    eq_(info['size of annexed files in working tree'], 0)
+
+    def get_custom(custom={}):
+        """Need a helper since repo_info modifies in place so we should generate
+        new each time
+        """
+        custom_json = {
+            'available local disk space': 'unknown',
+            'size of annexed files in working tree': "0",
+            'success': True,
+            'command': 'info',
+        }
+        if custom:
+            custom_json.update(custom)
+        return [custom_json]
+
+    with patch.object(
+            repo, '_call_annex_records',
+            return_value=get_custom()):
+        info = repo.repo_info()
+        eq_(info['available local disk space'], None)
+
+    with patch.object(
+        repo, '_call_annex_records',
+        return_value=get_custom({
+            "available local disk space": "19193986496 (+100000 reserved)"})):
+        info = repo.repo_info()
+        eq_(info['available local disk space'], 19193986496)
+
+
+@with_tempfile
 @with_tempfile
-def test_AnnexRepo_migrating_backends(src, dst):
+def test_AnnexRepo_migrating_backends(src=None, dst=None):
+    origin = AnnexRepo(src)
+    (origin.pathobj / 'test-annex.dat').write_text("content")
+    origin.save('some')
     ar = AnnexRepo.clone(src, dst, backend='MD5')
     eq_(ar.default_backends, ['MD5'])
     # GitPython has a bug which causes .git/config being wiped out
     # under Python3, triggered by collecting its config instance I guess
     gc.collect()
     ok_git_config_not_empty(ar)  # Must not blow, see https://github.com/gitpython-developers/GitPython/issues/333
 
@@ -506,26 +625,21 @@
     ar.add(filename, backend='MD5')
     eq_(ar.get_file_backend(filename), 'MD5')
     eq_(ar.get_file_backend('test-annex.dat'), 'SHA256E')
 
     # migrating will only do, if file is present
     ok_annex_get(ar, 'test-annex.dat')
 
-    if ar.is_direct_mode():
-        # No migration in direct mode
-        assert_raises(CommandNotAvailableError, ar.migrate_backend,
-                      'test-annex.dat')
-    else:
-        eq_(ar.get_file_backend('test-annex.dat'), 'SHA256E')
-        ar.migrate_backend('test-annex.dat')
-        eq_(ar.get_file_backend('test-annex.dat'), 'MD5')
-
-        ar.migrate_backend('', backend='SHA1')
-        eq_(ar.get_file_backend(filename), 'SHA1')
-        eq_(ar.get_file_backend('test-annex.dat'), 'SHA1')
+    eq_(ar.get_file_backend('test-annex.dat'), 'SHA256E')
+    ar.migrate_backend('test-annex.dat')
+    eq_(ar.get_file_backend('test-annex.dat'), 'MD5')
+
+    ar.migrate_backend('', backend='SHA1')
+    eq_(ar.get_file_backend(filename), 'SHA1')
+    eq_(ar.get_file_backend('test-annex.dat'), 'SHA1')
 
 
 tree1args = dict(
     tree=(
         ('firstfile', 'whatever'),
         ('secondfile', 'something else'),
         ('remotefile', 'pretends to be remote'),
@@ -536,403 +650,400 @@
 tree1_md5e_keys = {
     'firstfile': 'MD5E-s8--008c5926ca861023c1d2a36653fd88e2',
     'faraway': 'MD5E-s17--5b849ed02f914d3bbb5038fe4e3fead9',
     'secondfile': 'MD5E-s14--6c7ba9c5a141421e1c03cb9807c97c74',
     'remotefile': 'MD5E-s21--bf7654b3de20d5926d407ea7d913deb0'
 }
 
-
-@with_tree(**tree1args)
-def __test_get_md5s(path):
-    # was used just to generate above dict
-    annex = AnnexRepo(path, init=True, backend='MD5E')
-    files = [basename(f) for f in find_files('.*', path)]
-    annex.add(files, commit=True)
-    print({f: annex.get_file_key(f) for f in files})
+# this code is only here for documentation purposes
+# @with_tree(**tree1args)
+# def __test_get_md5s(path):
+#     # was used just to generate above dict
+#     annex = AnnexRepo(path, init=True, backend='MD5E')
+#     files = [basename(f) for f in find_files('.*', path)]
+#     annex.add(files)
+#     annex.commit()
+#     print({f: p['key'] for f, p in annex.get_content_annexinfo(files)})
 
 
-@with_batch_direct
+@with_parametric_batch
 @with_tree(**tree1args)
-def test_dropkey(batch, direct, path):
+def test_dropkey(path=None, *, batch):
     kw = {'batch': batch}
-    annex = AnnexRepo(path, init=True, backend='MD5E', direct=direct)
+    annex = AnnexRepo(path, init=True, backend='MD5E')
     files = list(tree1_md5e_keys)
-    annex.add(files, commit=True)
+    annex.add(files)
+    annex.commit()
     # drop one key
     annex.drop_key(tree1_md5e_keys[files[0]], **kw)
     # drop multiple
     annex.drop_key([tree1_md5e_keys[f] for f in files[1:3]], **kw)
     # drop already dropped -- should work as well atm
     # https://git-annex.branchable.com/bugs/dropkey_--batch_--json_--force_is_always_succesfull
     annex.drop_key(tree1_md5e_keys[files[0]], **kw)
     # and a mix with already dropped or not
     annex.drop_key(list(tree1_md5e_keys.values()), **kw)
+    # AnnexRepo is not able to guarantee that all batched processes are
+    # terminated when test cleanup code runs, avoid a crash (i.e. resource busy)
+    annex._batched.close()
 
 
 @with_tree(**tree1args)
 @serve_path_via_http()
-def test_AnnexRepo_backend_option(path, url):
+def test_AnnexRepo_backend_option(path=None, url=None):
     ar = AnnexRepo(path, backend='MD5')
 
     # backend recorded in .gitattributes
-    eq_(ar.get_git_attributes()['annex.backend'], 'MD5')
+    eq_(ar.get_gitattributes('.')['.']['annex.backend'], 'MD5')
 
     ar.add('firstfile', backend='SHA1')
     ar.add('secondfile')
     eq_(ar.get_file_backend('firstfile'), 'SHA1')
     eq_(ar.get_file_backend('secondfile'), 'MD5')
 
     with swallow_outputs() as cmo:
         # must be added under different name since annex 20160114
         ar.add_url_to_file('remotefile2', url + 'remotefile', backend='SHA1')
     eq_(ar.get_file_backend('remotefile2'), 'SHA1')
 
     with swallow_outputs() as cmo:
-        ar.add_urls([url + 'faraway'], backend='SHA1')
-    # TODO: what's the annex-generated name of this?
-    # For now, workaround:
-    ok_(ar.get_file_backend(f) == 'SHA1'
-        for f in ar.get_indexed_files() if 'faraway' in f)
+        ar.add_url_to_file('from_faraway', url + 'faraway', backend='SHA1')
+    eq_(ar.get_file_backend('from_faraway'), 'SHA1')
 
 
-@with_testrepos('.*annex.*', flavors=local_testrepo_flavors)
-@with_tempfile
-def test_AnnexRepo_get_file_backend(src, dst):
-    #init local test-annex before cloning:
-    AnnexRepo(src)
+@with_tempfile
+@with_tempfile
+def test_AnnexRepo_get_file_backend(src=None, dst=None):
+    origin = AnnexRepo(src, create=True)
+    (origin.pathobj / 'test-annex.dat').write_text("content")
+    origin.save()
 
     ar = AnnexRepo.clone(src, dst)
 
     eq_(ar.get_file_backend('test-annex.dat'), 'SHA256E')
-    if not ar.is_direct_mode():
-        # no migration in direct mode
-        ok_annex_get(ar, 'test-annex.dat', network=False)
-        ar.migrate_backend('test-annex.dat', backend='SHA1')
-        eq_(ar.get_file_backend('test-annex.dat'), 'SHA1')
-    else:
-        assert_raises(CommandNotAvailableError, ar.migrate_backend,
-                      'test-annex.dat', backend='SHA1')
+    # no migration
+    ok_annex_get(ar, 'test-annex.dat', network=False)
+    ar.migrate_backend('test-annex.dat', backend='SHA1')
+    eq_(ar.get_file_backend('test-annex.dat'), 'SHA1')
 
 
+@skip_if_adjusted_branch
 @with_tempfile
-def test_AnnexRepo_always_commit(path):
+def test_AnnexRepo_always_commit(path=None):
 
     repo = AnnexRepo(path)
-    runner = Runner(cwd=path)
+
+    def get_annex_commit_counts():
+        return len(repo.get_revisions("git-annex"))
+
+    n_annex_commits_initial = get_annex_commit_counts()
+
     file1 = get_most_obscure_supported_name() + "_1"
     file2 = get_most_obscure_supported_name() + "_2"
     with open(opj(path, file1), 'w') as f:
         f.write("First file.")
     with open(opj(path, file2), 'w') as f:
         f.write("Second file.")
 
     # always_commit == True is expected to be default
     repo.add(file1)
 
     # Now git-annex log should show the addition:
-    out, err = repo._run_annex_command('log')
-    out_list = out.rstrip(os.linesep).splitlines()
+    out_list = list(repo.call_annex_items_(['log']))
     eq_(len(out_list), 1)
-    assert_in(file1, out_list[0])
+
+    quote = lambda s: s.replace('"', r'\"')
+    def assert_in_out(filename, out):
+        filename_quoted = quote(filename)
+        if repo._check_version_kludges('quotepath-respected') == "no":
+            assert_in(filename, out)
+        elif repo._check_version_kludges('quotepath-respected') == "maybe":
+            assert filename in out or filename_quoted in out
+        else:
+            assert_in(filename_quoted, out)
+    assert_in_out(file1, out_list[0])
+
     # check git log of git-annex branch:
     # expected: initial creation, update (by annex add) and another
     # update (by annex log)
-    out, err = runner.run(['git', 'log', 'git-annex'])
-    num_commits = len([commit
-                       for commit in out.rstrip(os.linesep).split('\n')
-                       if commit.startswith('commit')])
-    eq_(num_commits, 3)
-
-    repo.always_commit = False
-    repo.add(file2)
-
-    # No additional git commit:
-    out, err = runner.run(['git', 'log', 'git-annex'])
-    num_commits = len([commit
-                       for commit in out.rstrip(os.linesep).split('\n')
-                       if commit.startswith('commit')])
-    eq_(num_commits, 3)
-
-    repo.always_commit = True
-
-    # Still one commit only in git-annex log,
-    # but 'git annex log' was called when always_commit was true again,
-    # so it should commit the addition at the end. Calling it again should then
-    # show two commits.
-    out, err = repo._run_annex_command('log')
-    out_list = out.rstrip(os.linesep).splitlines()
-    eq_(len(out_list), 2, "Output:\n%s" % out_list)
-    assert_in(file1, out_list[0])
-    assert_in("recording state in git", out_list[1])
-
-    out, err = repo._run_annex_command('log')
-    out_list = out.rstrip(os.linesep).splitlines()
-    eq_(len(out_list), 2, "Output:\n%s" % out_list)
-    assert_in(file1, out_list[0])
-    assert_in(file2, out_list[1])
+    eq_(get_annex_commit_counts(), n_annex_commits_initial + 1)
+
+    with patch.object(repo, "always_commit", False):
+        repo.add(file2)
+
+        # No additional git commit:
+        eq_(get_annex_commit_counts(), n_annex_commits_initial + 1)
+
+        out = repo.call_annex(['log'])
+
+        # And we see only the file before always_commit was set to false:
+        assert_in_out(file1, out)
+        assert_not_in(file2, out)
+        assert_not_in(quote(file2), out)
+
+    # With always_commit back to True, do something that will trigger a commit
+    # on the annex branches.
+    repo.call_annex(['sync'])
+
+    out = repo.call_annex(['log'])
+    assert_in_out(file1, out)
+    assert_in_out(file2, out)
 
     # Now git knows as well:
-    out, err = runner.run(['git', 'log', 'git-annex'])
-    num_commits = len([commit
-                       for commit in out.rstrip(os.linesep).split('\n')
-                       if commit.startswith('commit')])
-    eq_(num_commits, 4)
+    eq_(get_annex_commit_counts(), n_annex_commits_initial + 2)
 
 
-@with_testrepos('basic_annex', flavors=['local'])
 @with_tempfile
-def test_AnnexRepo_on_uninited_annex(origin, path):
+@with_tempfile
+def test_AnnexRepo_on_uninited_annex(src=None, path=None):
+    origin = AnnexRepo(src, create=True)
+    (origin.pathobj / 'test-annex.dat').write_text("content")
+    origin.save()
     # "Manually" clone to avoid initialization:
-    from datalad.cmd import Runner
     runner = Runner()
-    _ = runner(["git", "clone", origin, path], expect_stderr=True)
+    runner.run(["git", "clone", origin.path, path])
 
     assert_false(exists(opj(path, '.git', 'annex'))) # must not be there for this test to be valid
     annex = AnnexRepo(path, create=False, init=False)  # so we can initialize without
     # and still can get our things
     assert_false(annex.file_has_content('test-annex.dat'))
-    with swallow_outputs():
-        annex.get('test-annex.dat')
-        ok_(annex.file_has_content('test-annex.dat'))
+    annex.get('test-annex.dat')
+    ok_(annex.file_has_content('test-annex.dat'))
 
 
 @assert_cwd_unchanged
 @with_tempfile
-def test_AnnexRepo_commit(path):
+def test_AnnexRepo_commit(path=None):
 
     ds = AnnexRepo(path, create=True)
     filename = opj(path, get_most_obscure_supported_name())
     with open(filename, 'w') as f:
         f.write("File to add to git")
     ds.add(filename, git=True)
 
-    assert_raises(AssertionError, ok_clean_git, path, annex=True)
+    assert_raises(AssertionError, assert_repo_status, path, annex=True)
 
     ds.commit("test _commit")
-    ok_clean_git(path, annex=True)
+    assert_repo_status(path, annex=True)
 
     # nothing to commit doesn't raise by default:
     ds.commit()
     # but does with careless=False:
     assert_raises(CommandError, ds.commit, careless=False)
 
     # committing untracked file raises:
     with open(opj(path, "untracked"), "w") as f:
         f.write("some")
     assert_raises(FileNotInRepositoryError, ds.commit, files="untracked")
     # not existing file as well:
     assert_raises(FileNotInRepositoryError, ds.commit, files="not-existing")
 
 
-@with_testrepos('.*annex.*', flavors=['clone'])
-@known_failure_v6  #FIXME
-def test_AnnexRepo_add_to_annex(path):
-
-    # Note: Some test repos appears to not be initialized.
-    #       Therefore: 'init=True'
-    # TODO: Fix these repos finally!
-    # clone as provided by with_testrepos:
-    repo = AnnexRepo(path, create=False, init=True)
+@with_tempfile
+def test_AnnexRepo_add_to_annex(path=None):
+    repo = AnnexRepo(path)
 
-    ok_clean_git(repo, annex=True, ignore_submodules=True)
+    assert_repo_status(repo, annex=True)
     filename = get_most_obscure_supported_name()
     filename_abs = opj(repo.path, filename)
     with open(filename_abs, "w") as f:
         f.write("some")
 
     out_json = repo.add(filename)
     # file is known to annex:
-    if not repo.is_direct_mode():
-        assert_true(os.path.islink(filename_abs),
-                    "Annexed file is not a link.")
-    else:
-        assert_false(os.path.islink(filename_abs),
-                     "Annexed file is link in direct mode.")
+    ok_(repo.is_under_annex(filename_abs),
+        "Annexed file is not a link.")
     assert_in('key', out_json)
-    key = repo.get_file_key(filename)
+    key = repo.get_file_annexinfo(filename)['key']
     assert_false(key == '')
     assert_equal(key, out_json['key'])
     ok_(repo.file_has_content(filename))
 
     # uncommitted:
-    # but not in direct mode branch
-    if repo.is_direct_mode():
-        ok_(not repo.is_dirty(submodules=False))
-    else:
-        ok_(repo.is_dirty(submodules=False))
+    ok_(repo.dirty)
 
     repo.commit("Added file to annex.")
-    ok_clean_git(repo, annex=True, ignore_submodules=True)
+    assert_repo_status(repo, annex=True)
 
     # now using commit/msg options:
     filename = "another.txt"
     with open(opj(repo.path, filename), "w") as f:
         f.write("something else")
 
-    repo.add(filename, commit=True, msg="Added another file to annex.")
+    repo.add(filename)
+    repo.commit(msg="Added another file to annex.")
     # known to annex:
-    ok_(repo.get_file_key(filename))
-    ok_(repo.file_has_content(filename))
+    fileprops = repo.get_file_annexinfo(filename, eval_availability=True)
+    ok_(fileprops['key'])
+    ok_(fileprops['has_content'])
 
     # and committed:
-    ok_clean_git(repo, annex=True, ignore_submodules=True)
-
+    assert_repo_status(repo, annex=True)
 
-@with_testrepos('.*annex.*', flavors=['clone'])
-@known_failure_v6  #FIXME
-def test_AnnexRepo_add_to_git(path):
 
-    # Note: Some test repos appears to not be initialized.
-    #       Therefore: 'init=True'
-    # TODO: Fix these repos finally!
-
-    # clone as provided by with_testrepos:
-    repo = AnnexRepo(path, create=False, init=True)
+@with_tempfile
+def test_AnnexRepo_add_to_git(path=None):
+    repo = AnnexRepo(path)
 
-    ok_clean_git(repo, annex=True, ignore_submodules=True)
+    assert_repo_status(repo, annex=True)
     filename = get_most_obscure_supported_name()
     with open(opj(repo.path, filename), "w") as f:
         f.write("some")
     repo.add(filename, git=True)
 
     # not in annex, but in git:
-    assert_raises(FileInGitError, repo.get_file_key, filename)
+    eq_(repo.get_file_annexinfo(filename), {})
     # uncommitted:
-    ok_(repo.is_dirty(submodules=False))
+    ok_(repo.dirty)
     repo.commit("Added file to annex.")
-    ok_clean_git(repo, annex=True, ignore_submodules=True)
+    assert_repo_status(repo, annex=True)
 
     # now using commit/msg options:
     filename = "another.txt"
     with open(opj(repo.path, filename), "w") as f:
         f.write("something else")
 
-    repo.add(filename, git=True, commit=True,
-             msg="Added another file to annex.")
+    repo.add(filename, git=True)
+    repo.commit(msg="Added another file to annex.")
     # not in annex, but in git:
-    assert_raises(FileInGitError, repo.get_file_key, filename)
+    eq_(repo.get_file_annexinfo(filename), {})
 
     # and committed:
-    ok_clean_git(repo, annex=True, ignore_submodules=True)
-
+    assert_repo_status(repo, annex=True)
 
-@with_testrepos('submodule_annex', flavors=['clone'])
-def test_AnnexRepo_add_unexpected_direct_mode(path):
-    # tests a special case where a submodule is in direct mode, while it's
-    # superproject is not.
-    # There is no point in this test, if direct mode was enforced in the
-    # superproject already (either by test run configuration or FS) or if the
-    # repositories are in V6 by default (where there is no direct mode)
 
-    top = AnnexRepo(path)
-
-    if top.is_direct_mode() or top.config.get("annex.version") == '6':
-        raise SkipTest("Nothing to test for")
-
-    top.update_submodule('subm 1', init=True)
-    sub = AnnexRepo(opj(path, 'subm 1'))
-    sub.set_direct_mode(True)
-    with swallow_logs(new_level=logging.WARNING) as cml:
-        top.add('.')
-        cml.assert_logged(msg="Known bug in direct mode.",
-                          level="WARNING",
-                          regex=False)
-
-
-
-@ignore_nose_capturing_stdout
-@with_testrepos('.*annex.*', flavors=['local'])
-# TODO: flavor 'network' has wrong content for test-annex.dat!
 @with_tempfile
-def test_AnnexRepo_get(src, dst):
+@with_tempfile
+def test_AnnexRepo_get(src=None, dst=None):
+    ar = AnnexRepo(src)
+    (ar.pathobj / 'test-annex.dat').write_text(
+        "content to be annex-addurl'd")
+    ar.save('some')
 
     annex = AnnexRepo.clone(src, dst)
     assert_is_instance(annex, AnnexRepo, "AnnexRepo was not created.")
     testfile = 'test-annex.dat'
     testfile_abs = opj(dst, testfile)
     assert_false(annex.file_has_content("test-annex.dat"))
     with swallow_outputs():
         annex.get(testfile)
     ok_(annex.file_has_content("test-annex.dat"))
     ok_file_has_content(testfile_abs, "content to be annex-addurl'd", strip=True)
 
     called = []
     # for some reason yoh failed mock to properly just call original func
-    orig_run = annex._run_annex_command
+    orig_run = annex._git_runner.run_on_filelist_chunks
 
-    def check_run(cmd, annex_options, **kwargs):
-        called.append(cmd)
-        if cmd == 'find':
-            assert_not_in('-J5', annex_options)
-        elif cmd == 'get':
-            assert_in('-J5', annex_options)
+    def check_run(cmd, files, **kwargs):
+        cmd_name = cmd[cmd.index('annex') + 1]
+        called.append(cmd_name)
+        if cmd_name == 'find':
+            assert_not_in('-J5', cmd)
+        elif cmd_name == 'get':
+            assert_in('-J5', cmd)
         else:
             raise AssertionError(
-                "no other commands so far should be ran. Got %s, %s" %
-                (cmd, annex_options)
+                "no other commands so far should be ran. Got %s" % cmd
             )
-        return orig_run(cmd, annex_options=annex_options, **kwargs)
+        return orig_run(cmd, files, **kwargs)
 
     annex.drop(testfile)
-    with patch.object(AnnexRepo, '_run_annex_command',
-                      side_effect=check_run, auto_spec=True), \
+    with patch.object(GitWitlessRunner, 'run_on_filelist_chunks',
+                      side_effect=check_run), \
             swallow_outputs():
         annex.get(testfile, jobs=5)
     eq_(called, ['find', 'get'])
     ok_file_has_content(testfile_abs, "content to be annex-addurl'd", strip=True)
 
 
+@with_tree(tree={'file.dat': 'content'})
+@with_tempfile
+def test_v7_detached_get(opath=None, path=None):
+    # http://git-annex.branchable.com/bugs/get_fails_to_place_v7_unlocked_file_content_into_the_file_tree_in_v7_in_repo_with_detached_HEAD/
+    origin = AnnexRepo(opath, create=True, version=7)
+    GitRepo.add(origin, 'file.dat')  # force direct `git add` invocation
+    origin.commit('added')
+
+    AnnexRepo.clone(opath, path)
+    repo = AnnexRepo(path)
+    # test getting in a detached HEAD
+    repo.checkout('HEAD^{}')
+    repo.call_annex(['upgrade'])  # TODO: .upgrade ?
+
+    repo.get('file.dat')
+    ok_file_has_content(op.join(repo.path, 'file.dat'), "content")
+
+
 # TODO:
 #def init_remote(self, name, options):
 #def enable_remote(self, name):
 
-@with_testrepos('basic_annex$', flavors=['clone'])
+@pytest.mark.parametrize("batch", [False, True])
+@with_tempfile
+@with_tempfile
 @with_tempfile
-def _test_AnnexRepo_get_contentlocation(batch, path, work_dir_outside):
-    annex = AnnexRepo(path, create=False, init=False)
+def test_AnnexRepo_get_contentlocation(src=None, path=None, work_dir_outside=None, *, batch):
+    ar = AnnexRepo(src)
+    (ar.pathobj / 'test-annex.dat').write_text(
+        "content to be annex-addurl'd")
+    ar.save('some')
+
+    annex = AnnexRepo.clone(src, path)
     fname = 'test-annex.dat'
-    key = annex.get_file_key(fname)
+    key = annex.get_file_annexinfo(fname)['key']
+    # MIH at this point the whole test and get_contentlocation() itself
+    # is somewhat moot. The above call already has properties like
+    # 'hashdirmixed', 'hashdirlower', and 'key' from which the location
+    # could be built.
+    # with eval_availability=True, it also has 'objloc' with a absolute
+    # path to a verified annex key location
+
     # TODO: see if we can avoid this or specify custom exception
     eq_(annex.get_contentlocation(key, batch=batch), '')
 
     with swallow_outputs() as cmo:
         annex.get(fname)
     key_location = annex.get_contentlocation(key, batch=batch)
     assert(key_location)
+
+    if annex.is_managed_branch():
+        # the rest of the test assumes annexed files being symlinks
+        return
+
     # they both should point to the same location eventually
-    eq_(os.path.realpath(opj(annex.path, fname)),
-        os.path.realpath(opj(annex.path, key_location)))
+    eq_((annex.pathobj / fname).resolve(),
+        (annex.pathobj / key_location).resolve())
 
     # test how it would look if done under a subdir of the annex:
     with chpwd(opj(annex.path, 'subdir'), mkdir=True):
         key_location = annex.get_contentlocation(key, batch=batch)
         # they both should point to the same location eventually
-        eq_(os.path.realpath(opj(annex.path, fname)),
-            os.path.realpath(opj(annex.path, key_location)))
+        eq_((annex.pathobj / fname).resolve(),
+            (annex.pathobj / key_location).resolve())
 
     # test how it would look if done under a dir outside of the annex:
     with chpwd(work_dir_outside, mkdir=True):
         key_location = annex.get_contentlocation(key, batch=batch)
         # they both should point to the same location eventually
-        eq_(os.path.realpath(opj(annex.path, fname)),
-            os.path.realpath(opj(annex.path, key_location)))
-
-
-def test_AnnexRepo_get_contentlocation():
-    for batch in (False, True):
-        yield _test_AnnexRepo_get_contentlocation, batch
+        eq_((annex.pathobj / fname).resolve(),
+            (annex.pathobj / key_location).resolve())
 
 
+@known_failure_windows
 @with_tree(tree=(('about.txt', 'Lots of abouts'),
                  ('about2.txt', 'more abouts'),
                  ('about2_.txt', 'more abouts_'),
                  ('d', {'sub.txt': 'more stuff'})))
 @serve_path_via_http()
 @with_tempfile
-def test_AnnexRepo_addurl_to_file_batched(sitepath, siteurl, dst):
+def test_AnnexRepo_addurl_to_file_batched(sitepath=None, siteurl=None, dst=None):
+
+    if dl_cfg.get('datalad.fake-dates'):
+        raise SkipTest(
+            "Faked dates are enabled; skipping batched addurl tests")
 
     ar = AnnexRepo(dst, create=True)
     testurl = urljoin(siteurl, 'about.txt')
     testurl2 = urljoin(siteurl, 'about2.txt')
     testurl2_ = urljoin(siteurl, 'about2_.txt')
     testurl3 = urljoin(siteurl, 'd/sub.txt')
     url_file_prefix = urlsplit(testurl).netloc.split(':')[0]
@@ -945,51 +1056,47 @@
     # TODO: __call__ of the BatchedAnnex must be checked to be called
     copyfile(opj(sitepath, 'about.txt'), opj(dst, testfile))
     # must crash sensibly since file exists, we shouldn't addurl to non-annexed files
     with assert_raises(AnnexBatchCommandError):
         ar.add_url_to_file(testfile, testurl, batch=True)
 
     # Remove it and re-add
-    os.unlink(opj(dst, testfile))
+    unlink(opj(dst, testfile))
     ar.add_url_to_file(testfile, testurl, batch=True)
 
     info = ar.info(testfile)
     eq_(info['size'], 14)
     assert(info['key'])
     # not even added to index yet since we this repo is with default batch_size
-    # but: in direct mode it is added!
-    if ar.is_direct_mode():
-        assert_in(ar.WEB_UUID, ar.whereis(testfile))
-    else:
-        assert_not_in(ar.WEB_UUID, ar.whereis(testfile))
+    assert_not_in(WEB_SPECIAL_REMOTE_UUID, ar.whereis(testfile))
 
     # TODO: none of the below should re-initiate the batch process
 
     # add to an existing and staged annex file
     copyfile(opj(sitepath, 'about2.txt'), opj(dst, testfile2))
     ar.add(testfile2)
     ar.add_url_to_file(testfile2, testurl2, batch=True)
     assert(ar.info(testfile2))
     # not committed yet
-    # assert_in(ar.WEB_UUID, ar.whereis(testfile2))
+    # assert_in(WEB_SPECIAL_REMOTE_UUID, ar.whereis(testfile2))
 
     # add to an existing and committed annex file
     copyfile(opj(sitepath, 'about2_.txt'), opj(dst, testfile2_))
     ar.add(testfile2_)
     if ar.is_direct_mode():
-        assert_in(ar.WEB_UUID, ar.whereis(testfile))
+        assert_in(WEB_SPECIAL_REMOTE_UUID, ar.whereis(testfile))
     else:
-        assert_not_in(ar.WEB_UUID, ar.whereis(testfile))
+        assert_not_in(WEB_SPECIAL_REMOTE_UUID, ar.whereis(testfile))
     ar.commit("added about2_.txt and there was about2.txt lingering around")
     # commit causes closing all batched annexes, so testfile gets committed
-    assert_in(ar.WEB_UUID, ar.whereis(testfile))
+    assert_in(WEB_SPECIAL_REMOTE_UUID, ar.whereis(testfile))
     assert(not ar.dirty)
     ar.add_url_to_file(testfile2_, testurl2_, batch=True)
     assert(ar.info(testfile2_))
-    assert_in(ar.WEB_UUID, ar.whereis(testfile2_))
+    assert_in(WEB_SPECIAL_REMOTE_UUID, ar.whereis(testfile2_))
 
     # add into a new file
     # filename = 'newfile.dat'
     filename = get_most_obscure_supported_name()
 
     # Note: The following line was necessary, since the test setup just
     # doesn't work with singletons
@@ -997,295 +1104,411 @@
     AnnexRepo._unique_instances.clear()
     ar2 = AnnexRepo(dst, batch_size=1)
 
     with swallow_outputs():
         eq_(len(ar2._batched), 0)
         ar2.add_url_to_file(filename, testurl, batch=True)
         eq_(len(ar2._batched), 1)  # we added one more with batch_size=1
+        ar2.precommit()  # to possibly stop batch process occupying the stdout
     ar2.commit("added new file")  # would do nothing ATM, but also doesn't fail
     assert_in(filename, ar2.get_files())
-    assert_in(ar.WEB_UUID, ar2.whereis(filename))
+    assert_in(WEB_SPECIAL_REMOTE_UUID, ar2.whereis(filename))
 
-    if not ar.is_direct_mode():
-        # in direct mode there's nothing to commit
-        ar.commit("actually committing new files")
+    ar.commit("actually committing new files")
     assert_in(filename, ar.get_files())
-    assert_in(ar.WEB_UUID, ar.whereis(filename))
+    assert_in(WEB_SPECIAL_REMOTE_UUID, ar.whereis(filename))
     # this poor bugger still wasn't added since we used default batch_size=0 on him
 
-    # and closing the pipes now shoudn't anyhow affect things
+    # and closing the pipes now shouldn't anyhow affect things
     eq_(len(ar._batched), 1)
     ar._batched.close()
     eq_(len(ar._batched), 1)  # doesn't remove them, just closes
     assert(not ar.dirty)
 
     ar._batched.clear()
     eq_(len(ar._batched), 0)  # .clear also removes
 
     raise SkipTest("TODO: more, e.g. add with a custom backend")
     # TODO: also with different modes (relaxed, fast)
     # TODO: verify that file is added with that backend and that we got a new batched process
 
 
-@with_tempfile(mkdir=True)
-def test_annex_backends(path):
-    repo = AnnexRepo(path)
-    eq_(repo.default_backends, None)
+@with_tree(tree={"foo": "foo content"})
+@serve_path_via_http()
+@with_tree(tree={"bar": "bar content"})
+def test_annexrepo_fake_dates_disables_batched(sitepath=None, siteurl=None, dst=None):
+    ar = AnnexRepo(dst, create=True, fake_dates=True)
+
+    with swallow_logs(new_level=logging.DEBUG) as cml:
+        ar.add_url_to_file("foo-dst", urljoin(siteurl, "foo"), batch=True)
+        cml.assert_logged(
+            msg="Not batching addurl call because fake dates are enabled",
+            level="DEBUG",
+            regex=False)
+
+    ar.add("bar")
+    ar.commit("add bar")
+    key = ar.get_content_annexinfo(["bar"]).popitem()[1]['key']
+
+    with swallow_logs(new_level=logging.DEBUG) as cml:
+        ar.drop_key(key, batch=True)
+        cml.assert_logged(
+            msg="Not batching drop_key call because fake dates are enabled",
+            level="DEBUG",
+            regex=False)
 
-    rmtree(path)
 
-    repo = AnnexRepo(path, backend='MD5E')
-    eq_(repo.default_backends, ['MD5E'])
+@with_tempfile(mkdir=True)
+def test_annex_backends(path=None):
+    path = Path(path)
+    repo_default = AnnexRepo(path / "r_default")
+    eq_(repo_default.default_backends, None)
+
+    repo_kw = AnnexRepo(path / "repo_kw", backend='MD5E')
+    eq_(repo_kw.default_backends, ['MD5E'])
 
     # persists
-    repo = AnnexRepo(path)
-    eq_(repo.default_backends, ['MD5E'])
+    repo_kw = AnnexRepo(path / "repo_kw")
+    eq_(repo_kw.default_backends, ['MD5E'])
 
+    repo_config = AnnexRepo(path / "repo_config")
+    repo_config.config.set("annex.backend", "MD5E", reload=True)
+    eq_(repo_config.default_backends, ["MD5E"])
+
+    repo_compat = AnnexRepo(path / "repo_compat")
+    repo_compat.config.set("annex.backends", "MD5E WORM", reload=True)
+    eq_(repo_compat.default_backends, ["MD5E", "WORM"])
+
+
+# ignore deprecation warnings since here we should not use high level
+# interface like push
+@pytest.mark.filterwarnings(r"ignore: AnnexRepo.copy_to\(\) is deprecated")
+@skip_nomultiplex_ssh  # too much of "multiplex" testing
+@with_tempfile(mkdir=True)
+def test_annex_ssh(topdir=None):
+    # On Xenial, this hangs with a recent git-annex. It bisects to git-annex's
+    # 7.20191230-142-g75059c9f3. This is likely due to an interaction with an
+    # older openssh version. See
+    # https://git-annex.branchable.com/bugs/SSH-based_git-annex-init_hang_on_older_systems___40__Xenial__44___Jessie__41__/
+    if external_versions['cmd:system-ssh'] < '7.4' and \
+       external_versions['cmd:annex'] <= '8.20200720.1':
+        raise SkipTest("Test known to hang")
+
+    topdir = Path(topdir)
+    rm1 = AnnexRepo(topdir / "remote1", create=True)
+    rm2 = AnnexRepo.clone(rm1.path, str(topdir / "remote2"))
+    rm2.remove_remote(DEFAULT_REMOTE)
+
+    main_tmp = AnnexRepo.clone(rm1.path, str(topdir / "main"))
+    main_tmp.remove_remote(DEFAULT_REMOTE)
+    repo_path = main_tmp.path
+    del main_tmp
+    remote_1_path = rm1.path
+    remote_2_path = rm2.path
 
-@skip_ssh
-@with_tempfile
-@with_testrepos('basic_annex', flavors=['local'])
-@with_testrepos('basic_annex', flavors=['local'])
-def test_annex_ssh(repo_path, remote_1_path, remote_2_path):
     from datalad import ssh_manager
-    # create remotes:
-    rm1 = AnnexRepo(remote_1_path, create=False)
-    rm2 = AnnexRepo(remote_2_path, create=False)
 
     # check whether we are the first to use these sockets:
-    socket_1 = opj(ssh_manager.socket_dir, get_connection_hash('datalad-test'))
-    socket_2 = opj(ssh_manager.socket_dir, get_connection_hash('localhost'))
+    hash_1 = get_connection_hash('datalad-test')
+    socket_1 = opj(str(ssh_manager.socket_dir), hash_1)
+    hash_2 = get_connection_hash('datalad-test2')
+    socket_2 = opj(str(ssh_manager.socket_dir), hash_2)
     datalad_test_was_open = exists(socket_1)
-    localhost_was_open = exists(socket_2)
+    datalad_test2_was_open = exists(socket_2)
 
     # repo to test:AnnexRepo(repo_path)
     # At first, directly use git to add the remote, which should be recognized
     # by AnnexRepo's constructor
     gr = GitRepo(repo_path, create=True)
-    AnnexRepo(repo_path)
     gr.add_remote("ssh-remote-1", "ssh://datalad-test" + remote_1_path)
 
-    # Now, make it an annex:
     ar = AnnexRepo(repo_path, create=False)
 
-    # connection to 'datalad-test' should be known to ssh manager:
-    assert_in(socket_1, ssh_manager._connections)
-    # but socket was not touched:
+    # socket was not touched:
     if datalad_test_was_open:
         ok_(exists(socket_1))
     else:
         ok_(not exists(socket_1))
 
-    from datalad import lgr
-    lgr.debug("HERE")
     # remote interaction causes socket to be created:
-    try:
-        # Note: For some reason, it hangs if log_stdout/err True
-        # TODO: Figure out what's going on
-        #  yoh: I think it is because of what is "TODOed" within cmd.py --
-        #       trying to log/obtain both through PIPE could lead to lock
-        #       downs.
-        # here we use our swallow_logs to overcome a problem of running under
-        # nosetests without -s, when nose then tries to swallow stdout by
-        # mocking it with StringIO, which is not fully compatible with Popen
-        # which needs its .fileno()
-        with swallow_outputs():
-            ar._run_annex_command('sync',
-                                  expect_stderr=True,
-                                  log_stdout=False,
-                                  log_stderr=False,
-                                  expect_fail=True)
-    # sync should return exit code 1, since it can not merge
-    # doesn't matter for the purpose of this test
-    except CommandError as e:
-        if e.code == 1:
-            pass
+    (ar.pathobj / "foo").write_text("foo")
+    (ar.pathobj / "bar").write_text("bar")
+    ar.add("foo")
+    ar.add("bar")
+    ar.commit("add files")
 
+    ar.copy_to(["foo"], remote="ssh-remote-1")
+    # copy_to() opens it if needed.
+    #
+    # Note: This isn't racy because datalad-sshrun should not close this itself
+    # because the connection was either already open before this test or
+    # copy_to(), not the underlying git-annex/datalad-sshrun call, opens it.
     ok_(exists(socket_1))
 
     # add another remote:
-    ar.add_remote('ssh-remote-2', "ssh://localhost" + remote_2_path)
+    ar.add_remote('ssh-remote-2', "ssh://datalad-test2" + remote_2_path)
 
-    # now, this connection to localhost was requested:
-    assert_in(socket_2, ssh_manager._connections)
-    # but socket was not touched:
-    if localhost_was_open:
+    # socket was not touched:
+    if datalad_test2_was_open:
         # FIXME: occasionally(?) fails in V6:
-        if not ar.config.getint("annex", "version") == 6:
-            ok_(exists(socket_2))
+        # ok_(exists(socket_2))
+        pass
     else:
         ok_(not exists(socket_2))
 
-    # sync with the new remote:
-    try:
-        with swallow_outputs():
-            ar._run_annex_command('sync', annex_options=['ssh-remote-2'],
-                                  expect_stderr=True,
-                                  log_stdout=False,
-                                  log_stderr=False,
-                                  expect_fail=True)
-    # sync should return exit code 1, since it can not merge
-    # doesn't matter for the purpose of this test
-    except CommandError as e:
-        if e.code == 1:
-            pass
+    # copy to the new remote:
+    #
+    # Same racy note as the copy_to() call above.
+    ar.copy_to(["foo"], remote="ssh-remote-2")
 
-    ok_(exists(socket_2))
+    if not exists(socket_2):  # pragma: no cover
+        # @known_failure (marked for grep)
+        raise SkipTest("test_annex_ssh hit known failure (gh-4781)")
 
+    # Check that git-annex is actually using datalad-sshrun.
+    fail_cmd = quote_cmdlinearg(sys.executable) + "-c 'assert 0'"
+    with patch.dict('os.environ', {'GIT_SSH_COMMAND': fail_cmd}):
+        with assert_raises(CommandError):
+            ar.copy_to(["bar"], remote="ssh-remote-2")
+    ar.copy_to(["bar"], remote="ssh-remote-2")
 
-@with_testrepos('basic_annex', flavors=['clone'])
-@with_tempfile(mkdir=True)
-def test_annex_remove(path1, path2):
-    ar1 = AnnexRepo(path1, create=False)
-    ar2 = AnnexRepo.clone(path1, path2, create=True, direct=True)
+    ssh_manager.close(ctrl_path=[socket_1, socket_2])
 
-    for repo in (ar1, ar2):
-        file_list = repo.get_annexed_files()
-        assert len(file_list) >= 1
-        # remove a single file
-        out = repo.remove(file_list[0])
-        assert_not_in(file_list[0], repo.get_annexed_files())
-        eq_(out[0], file_list[0])
 
-        with open(opj(repo.path, "rm-test.dat"), "w") as f:
-            f.write("whatever")
+@with_tempfile
+def test_annex_remove(path=None):
+    ar = AnnexRepo(path)
+    (ar.pathobj / 'test-annex.dat').write_text(
+        "content to be annex-addurl'd")
+    ar.save('some')
+
+    repo = AnnexRepo(path, create=False)
 
-        # add it
-        repo.add("rm-test.dat")
+    file_list = list(repo.get_content_annexinfo(init=None))
+    assert len(file_list) >= 1
+    # remove a single file
+    out = repo.remove(str(file_list[0]))
+    assert_not_in(file_list[0], repo.get_content_annexinfo(init=None))
+    eq_(out[0], str(file_list[0].relative_to(repo.pathobj)))
 
-        # remove without '--force' should fail, due to staged changes:
-        if repo.is_direct_mode():
-            assert_raises(CommandError, repo.remove, "rm-test.dat")
-        else:
-            assert_raises(GitCommandError, repo.remove, "rm-test.dat")
-        assert_in("rm-test.dat", repo.get_annexed_files())
+    with open(opj(repo.path, "rm-test.dat"), "w") as f:
+        f.write("whatever")
 
-        # now force:
-        out = repo.remove("rm-test.dat", force=True)
-        assert_not_in("rm-test.dat", repo.get_annexed_files())
-        eq_(out[0], "rm-test.dat")
+    # add it
+    repo.add("rm-test.dat")
+
+    # remove without '--force' should fail, due to staged changes:
+    assert_raises(CommandError, repo.remove, "rm-test.dat")
+    assert_in("rm-test.dat", repo.get_annexed_files())
+
+    # now force:
+    out = repo.remove("rm-test.dat", force=True)
+    assert_not_in("rm-test.dat", repo.get_annexed_files())
+    eq_(out[0], "rm-test.dat")
 
 
 @with_tempfile
 @with_tempfile
 @with_tempfile
-def test_repo_version(path1, path2, path3):
-    annex = AnnexRepo(path1, create=True, version=6)
-    ok_clean_git(path1, annex=True)
-    version = annex.repo.config_reader().get_value('annex', 'version')
-    eq_(version, 6)
+def test_repo_version_upgrade(path1=None, path2=None, path3=None):
+    with swallow_logs(new_level=logging.INFO) as cm:
+        # Since git-annex 7.20181031, v6 repos upgrade to v7.
+        # Future proofing: We will test on v6 as long as it is upgradeable,
+        # but would switch to first upgradeable after
+        Uversion = 6 if 6 in _GIT_ANNEX_VERSIONS_INFO["upgradable"] \
+            else _GIT_ANNEX_VERSIONS_INFO["upgradeable"][0]
+        v_first_supported = next(i for i in _GIT_ANNEX_VERSIONS_INFO["supported"] if i >= Uversion)
+        annex = AnnexRepo(path1, create=True, version=Uversion)
+        assert_repo_status(path1, annex=True)
+        v_upgraded_to = int(annex.config.get('annex.version'))
+
+        if external_versions['cmd:annex'] <= '10.20220724':
+            eq_(v_upgraded_to, v_first_supported)
+            assert_in("will be upgraded to 8", cm.out)
+        else:
+            # 10.20220724-5-g63cef2ae0 started to auto-upgrade to 10, although 8 was the
+            # lowest supported. In general we can only assert that we upgrade into one
+            # of the supported
+            assert_in(v_upgraded_to, _GIT_ANNEX_VERSIONS_INFO["supported"])
+            assert_in("will be upgraded to %s or later version" % v_first_supported, cm.out)
 
     # default from config item (via env var):
-    with patch.dict('os.environ', {'DATALAD_REPO_VERSION': '6'}):
+    with patch.dict('os.environ', {'DATALAD_REPO_VERSION': str(Uversion)}):
+        # and check consistency of upgrading to the default version:
         annex = AnnexRepo(path2, create=True)
-        version = annex.repo.config_reader().get_value('annex', 'version')
-        eq_(version, 6)
+        version = int(annex.config.get('annex.version'))
+        eq_(version, v_upgraded_to)
 
-        # parameter `version` still has priority over default config:
-        annex = AnnexRepo(path3, create=True, version=5)
-        version = annex.repo.config_reader().get_value('annex', 'version')
-        eq_(version, 5)
 
+@pytest.mark.parametrize("version", _GIT_ANNEX_VERSIONS_INFO["supported"])
+def test_repo_version_supported(version, tmp_path):
+        # default from config item (via env var):
+        Uversion = _GIT_ANNEX_VERSIONS_INFO["upgradable"][0]
+        with patch.dict('os.environ', {'DATALAD_REPO_VERSION': str(Uversion)}):
+            # ...parameter `version` still has priority over default config:
+            annex = AnnexRepo(str(tmp_path), create=True, version=version)
+            annex_version = int(annex.config.get('annex.version'))
+            if not annex.is_managed_branch():
+                # There is no "upgrade" for any of the supported versions.
+                # if we are not in adjusted branch
+                eq_(annex_version, version)
+            else:
+                print("HERE")
+                # some annex command might have ran to trigger the update
+                assert annex_version in {v for v in _GIT_ANNEX_VERSIONS_INFO["supported"] if v >= version}
+
+
+@skip_if(external_versions['cmd:annex'] > '8.20210428', "Stopped showing if too quick")
+@with_tempfile
+def test_init_scanning_message(path=None):
+    with swallow_logs(new_level=logging.INFO) as cml:
+        AnnexRepo(path, create=True, version=7)
+        # somewhere around 8.20210428-186-g428c91606 git annex changed
+        # handling of scanning for unlocked files upon init and started to report
+        # "scanning for annexed" instead of "scanning for unlocked".
+        # Could be a line among many (as on Windows) so match=False so we search
+        assert_re_in(".*scanning for .* files", cml.out, flags=re.IGNORECASE, match=False)
+
+
+# ignore deprecation warnings since that is the test testing that functionality
+@pytest.mark.filterwarnings(r"ignore: AnnexRepo.copy_to\(\) is deprecated")
+@with_tempfile
+@with_tempfile
+@with_tempfile
+def test_annex_copy_to(src=None, origin=None, clone=None):
+    ar = AnnexRepo(src)
+    (ar.pathobj / 'test.dat').write_text("123\n")
+    ar.save('some', git=True)
+    (ar.pathobj / 'test-annex.dat').write_text("content")
+    ar.save('some')
 
-@with_testrepos('.*annex.*', flavors=['clone'])
-@with_tempfile(mkdir=True)
-def test_annex_copy_to(origin, clone):
-    repo = AnnexRepo(origin, create=False)
-    remote = AnnexRepo.clone(origin, clone, create=True)
+    repo = AnnexRepo.clone(src, origin)
+    remote = AnnexRepo.clone(origin, clone)
     repo.add_remote("target", clone)
 
     assert_raises(IOError, repo.copy_to, "doesnt_exist.dat", "target")
-    assert_raises(FileInGitError, repo.copy_to, "INFO.txt", "target")
+    assert_raises(FileInGitError, repo.copy_to, "test.dat", "target")
     assert_raises(ValueError, repo.copy_to, "test-annex.dat", "invalid_target")
 
+    # see #3102
+    # "copying" a dir shouldn't do anything and not raise.
+    os.mkdir(opj(repo.path, "subdir"))
+    repo.copy_to("subdir", "target")
+
     # test-annex.dat has no content to copy yet:
     eq_(repo.copy_to("test-annex.dat", "target"), [])
 
     repo.get("test-annex.dat")
     # now it has:
     eq_(repo.copy_to("test-annex.dat", "target"), ["test-annex.dat"])
     # and will not be copied again since it was already copied
-    eq_(repo.copy_to(["INFO.txt", "test-annex.dat"], "target"), [])
+    eq_(repo.copy_to(["test.dat", "test-annex.dat"], "target"), [])
 
     # Test that if we pass a list of items and annex processes them nicely,
     # we would obtain a list back. To not stress our tests even more -- let's mock
     def ok_copy(command, **kwargs):
         # Check that we do pass to annex call only the list of files which we
         #  asked to be copied
-        assert_in('copied1', kwargs['annex_options'])
-        assert_in('copied2', kwargs['annex_options'])
-        assert_in('existed', kwargs['annex_options'])
-        return """
-{"command":"copy","note":"to target ...", "success":true, "key":"akey1", "file":"copied1"}
-{"command":"copy","note":"to target ...", "success":true, "key":"akey2", "file":"copied2"}
-{"command":"copy","note":"checking target ...", "success":true, "key":"akey3", "file":"existed"}
-""", ""
-    with patch.object(repo, '_run_annex_command', ok_copy):
+        assert_in('copied1', kwargs['files'])
+        assert_in('copied2', kwargs['files'])
+        assert_in('existed', kwargs['files'])
+        return [
+                {"command":"copy","note":"to target ...", "success":True,
+                 "key":"akey1", "file":"copied1"},
+                {"command":"copy","note":"to target ...", "success":True,
+                 "key":"akey2", "file":"copied2"},
+                {"command":"copy","note":"checking target ...", "success":True,
+                 "key":"akey3", "file":"existed"},
+        ]
+    # Note that we patch _call_annex_records,
+    # which is in turn invoked first by copy_to for "find" operation.
+    # TODO: provide a dedicated handling within above ok_copy for 'find' command
+    with patch.object(repo, '_call_annex_records', ok_copy):
         eq_(repo.copy_to(["copied2", "copied1", "existed"], "target"),
             ["copied1", "copied2"])
 
     # now let's test that we are correctly raising the exception in case if
     # git-annex execution fails
-    orig_run = repo._run_annex_command
+    orig_run = repo._call_annex
 
     # Kinda a bit off the reality since no nonex* would not be returned/handled
     # by _get_expected_files, so in real life -- wouldn't get report about Incomplete!?
     def fail_to_copy(command, **kwargs):
-        if command == 'copy':
+        if command[0] == 'copy':
             # That is not how annex behaves
             # http://git-annex.branchable.com/bugs/copy_does_not_reflect_some_failed_copies_in_--json_output/
             # for non-existing files output goes into stderr
+            #
+            # stderr output depends on config+version of annex, though:
+            if not dl_cfg.getbool(
+                    section="annex", option="skipunknown",
+                    # git-annex switched default for this config:
+                    default=bool(
+                        external_versions['cmd:annex'] < '10.20220222')):
+
+                stderr = "error: pathspec 'nonex1' did not match any file(s) " \
+                         "known to git\n" \
+                         "error: pathspec 'nonex2' did not match any file(s) " \
+                         "known to git\n"
+            else:
+                stderr = "git-annex: nonex1 not found\n" \
+                         "git-annex: nonex2 not found\n"
+
             raise CommandError(
                 "Failed to run ...",
-                stdout=
-                    '{"command":"copy","note":"to target ...", "success":true, "key":"akey1", "file":"copied"}\n'
-                    '{"command":"copy","note":"checking target ...", "success":true, "key":"akey2", "file":"existed"}\n',
-                stderr=
-                    'git-annex: nonex1 not found\n'
-                    'git-annex: nonex2 not found\n'
+                stdout_json=[
+                    {"command":"copy","note":"to target ...", "success":True,
+                     "key":"akey1", "file":"copied"},
+                    {"command":"copy","note":"checking target ...",
+                     "success":True, "key":"akey2", "file":"existed"},
+                ],
+                stderr=stderr
             )
         else:
             return orig_run(command, **kwargs)
 
     def fail_to_copy_get_expected(files, expr):
         assert files == ["copied", "existed", "nonex1", "nonex2"]
         return {'akey1': 10}, ["copied"]
 
-    with patch.object(repo, '_run_annex_command', fail_to_copy), \
+    with patch.object(repo, '_call_annex', fail_to_copy), \
             patch.object(repo, '_get_expected_files', fail_to_copy_get_expected):
         with assert_raises(IncompleteResultsError) as cme:
             repo.copy_to(["copied", "existed", "nonex1", "nonex2"], "target")
-    eq_(cme.exception.results, ["copied"])
-    eq_(cme.exception.failed, ['nonex1', 'nonex2'])
+    eq_(cme.value.results, ["copied"])
+    eq_(cme.value.failed, ['nonex1', 'nonex2'])
 
 
-
-@with_testrepos('.*annex.*', flavors=['local'])
-# TODO: flavor 'network' has wrong content for test-annex.dat!
 @with_tempfile
-def test_annex_drop(src, dst):
+@with_tempfile
+def test_annex_drop(src=None, dst=None):
+    ar = AnnexRepo(src)
+    (ar.pathobj / 'test-annex.dat').write_text("content")
+    ar.save('some')
+
     ar = AnnexRepo.clone(src, dst)
     testfile = 'test-annex.dat'
     assert_false(ar.file_has_content(testfile))
     ar.get(testfile)
     ok_(ar.file_has_content(testfile))
+    eq_(len([f for f in ar.fsck(fast=True) if f['file'] == testfile]), 1)
 
     # drop file by name:
     result = ar.drop([testfile])
     assert_false(ar.file_has_content(testfile))
     ok_(isinstance(result, list))
     eq_(len(result), 1)
     eq_(result[0]['command'], 'drop')
     eq_(result[0]['success'], True)
     eq_(result[0]['file'], testfile)
 
     ar.get(testfile)
 
     # drop file by key:
-    testkey = ar.get_file_key(testfile)
+    testkey = ar.get_file_annexinfo(testfile)['key']
     result = ar.drop([testkey], key=True)
     assert_false(ar.file_has_content(testfile))
     ok_(isinstance(result, list))
     eq_(len(result), 1)
     eq_(result[0]['command'], 'drop')
     eq_(result[0]['success'], True)
     eq_(result[0]['key'], testkey)
@@ -1294,96 +1517,137 @@
     assert_raises(TypeError, ar.drop)
     assert_raises(InsufficientArgumentsError, ar.drop, [], options=["--jobs=5"])
     assert_raises(InsufficientArgumentsError, ar.drop, [])
 
     # too much arguments:
     assert_raises(CommandError, ar.drop, ['.'], options=['--all'])
 
+    (ar.pathobj / 'somefile.txt').write_text('this')
+    ar.save()
+    with assert_raises(CommandError) as e:
+        ar.drop('somefile.txt')
+    # CommandError has to pull the errors from the JSON record 'note'
+    assert_in('necessary cop', str(e.value))
+
+    with assert_raises(CommandError) as e:
+        ar._call_annex_records(['fsck', '-N', '3'])
+    # CommandError has to pull the errors from the JSON record 'error-messages'
+    assert_in('1 of 3 trustworthy copies', str(e.value))
 
-@with_testrepos('basic_annex', flavors=['clone'])
-def test_annex_remove(path):
-    repo = AnnexRepo(path, create=False)
 
-    file_list = repo.get_annexed_files()
-    assert len(file_list) >= 1
-    # remove a single file
-    out = repo.remove(file_list[0])
-    assert_not_in(file_list[0], repo.get_annexed_files())
-    eq_(out[0], file_list[0])
+@with_tree({"a.txt": "a", "b.txt": "b", "c.py": "c", "d": "d"})
+def test_annex_get_annexed_files(path=None):
+    repo = AnnexRepo(path)
+    repo.add(".")
+    repo.commit()
+    eq_(set(repo.get_annexed_files()), {"a.txt", "b.txt", "c.py", "d"})
 
-    with open(opj(repo.path, "rm-test.dat"), "w") as f:
-        f.write("whatever")
+    repo.drop("a.txt", options=["--force"])
+    eq_(set(repo.get_annexed_files()), {"a.txt", "b.txt", "c.py", "d"})
+    eq_(set(repo.get_annexed_files(with_content_only=True)),
+        {"b.txt", "c.py", "d"})
 
-    # add it
-    repo.add("rm-test.dat")
+    eq_(set(repo.get_annexed_files(patterns=["*.txt"])),
+        {"a.txt", "b.txt"})
+    eq_(set(repo.get_annexed_files(with_content_only=True,
+                                   patterns=["*.txt"])),
+        {"b.txt"})
 
-    # remove without '--force' should fail, due to staged changes:
-    assert_raises(CommandError, repo.remove, "rm-test.dat")
-    assert_in("rm-test.dat", repo.get_annexed_files())
+    eq_(set(repo.get_annexed_files(patterns=["*.txt", "*.py"])),
+        {"a.txt", "b.txt", "c.py"})
 
-    # now force:
-    out = repo.remove("rm-test.dat", force=True)
-    assert_not_in("rm-test.dat", repo.get_annexed_files())
-    eq_(out[0], "rm-test.dat")
+    eq_(set(repo.get_annexed_files()),
+        set(repo.get_annexed_files(patterns=["*"])))
 
+    eq_(set(repo.get_annexed_files(with_content_only=True)),
+        set(repo.get_annexed_files(with_content_only=True, patterns=["*"])))
 
-@with_batch_direct
-@with_testrepos('basic_annex', flavors=['clone'], count=1)
-def test_is_available(batch, direct, p):
-    annex = AnnexRepo(p)
+
+@pytest.mark.parametrize("batch", [True, False])
+@with_tree(tree={"test-annex.dat": "content"})
+@serve_path_via_http()
+@with_tempfile()
+@with_tempfile()
+def test_is_available(_=None, content_url=None, origpath=None, path=None, *,
+                      batch):
+
+    fname = "test-annex.dat"
+    content_url += "/" + fname
+    origds = Dataset(origpath).create()
+    origds.repo.add_url_to_file(fname, content_url)
+    origds.save()
+    origds.drop(fname)
+    annex = clone(origpath, path).repo
 
     # bkw = {'batch': batch}
     if batch:
         is_available = partial(annex.is_available, batch=batch)
     else:
         is_available = annex.is_available
 
-    fname = 'test-annex.dat'
-    key = annex.get_file_key(fname)
+    key = annex.get_content_annexinfo([fname]).popitem()[1]['key']
 
     # explicit is to verify data type etc
     assert is_available(key, key=True) is True
     assert is_available(fname) is True
 
     # known remote but doesn't have it
-    assert is_available(fname, remote='origin') is False
+    assert is_available(fname, remote=DEFAULT_REMOTE) is False
+
+    # If the 'datalad' special remote is present, it will claim fname's URL.
+    if DATALAD_SPECIAL_REMOTE in annex.get_remotes():
+        remote = DATALAD_SPECIAL_REMOTE
+        uuid = DATALAD_SPECIAL_REMOTES_UUIDS[DATALAD_SPECIAL_REMOTE]
+    else:
+        remote = "web"
+        uuid = WEB_SPECIAL_REMOTE_UUID
+
     # it is on the 'web'
-    assert is_available(fname, remote='web') is True
+    assert is_available(fname, remote=remote) is True
     # not effective somehow :-/  may be the process already running or smth
     # with swallow_logs(), swallow_outputs():  # it will complain!
     assert is_available(fname, remote='unknown') is False
     assert_false(is_available("boguskey", key=True))
 
     # remove url
-    urls = annex.get_urls(fname) #, **bkw)
+    urls = annex.whereis(fname, output="full").get(uuid, {}).get("urls", [])
+
     assert(len(urls) == 1)
+    eq_(urls,
+        annex.whereis(key, key=True, output="full")
+        .get(uuid, {}).get("urls"))
     annex.rm_url(fname, urls[0])
 
     assert is_available(key, key=True) is False
     assert is_available(fname) is False
-    assert is_available(fname, remote='web') is False
+    assert is_available(fname, remote=remote) is False
 
 
 @with_tempfile(mkdir=True)
-def test_annex_add_no_dotfiles(path):
+def test_get_urls_none(path=None):
+    ar = AnnexRepo(path, create=True)
+    with open(opj(ar.path, "afile"), "w") as f:
+        f.write("content")
+    eq_(ar.get_urls("afile"), [])
+
+
+@xfail_buggy_annex_info
+@with_tempfile(mkdir=True)
+def test_annex_add_no_dotfiles(path=None):
     ar = AnnexRepo(path, create=True)
     print(ar.path)
     assert_true(os.path.exists(ar.path))
     assert_false(ar.dirty)
     os.makedirs(opj(ar.path, '.datalad'))
     # we don't care about empty directories
     assert_false(ar.dirty)
     with open(opj(ar.path, '.datalad', 'somefile'), 'w') as f:
         f.write('some content')
     # make sure the repo is considered dirty now
     assert_true(ar.dirty)  # TODO: has been more detailed assertion (untracked file)
-    # no file is being added, as dotfiles/directories are ignored by default
-    ar.add('.', git=False)
-    # double check, still dirty
-    assert_true(ar.dirty)  # TODO: has been more detailed assertion (untracked file)
     # now add to git, and it should work
     ar.add('.', git=True)
     # all in index
     assert_true(ar.dirty)
     # TODO: has been more specific:
     # assert_false(ar.repo.is_dirty(
     #     index=False, working_tree=True, untracked_files=True, submodules=True))
@@ -1391,22 +1655,19 @@
     # all committed
     assert_false(ar.dirty)
     # not known to annex
     assert_false(ar.is_under_annex(opj(ar.path, '.datalad', 'somefile')))
 
 
 @with_tempfile
-def test_annex_version_handling(path):
-    with patch.object(AnnexRepo, 'git_annex_version', None) as cmpov, \
-         patch.object(AnnexRepo, '_check_git_annex_version',
-                      auto_spec=True,
-                      side_effect=AnnexRepo._check_git_annex_version) \
-            as cmpc, \
-         patch.object(external_versions, '_versions',
-                      {'cmd:annex': AnnexRepo.GIT_ANNEX_MIN_VERSION}):
+def test_annex_version_handling_at_min_version(path=None):
+    with set_annex_version(AnnexRepo.GIT_ANNEX_MIN_VERSION):
+        po = patch.object(AnnexRepo, '_check_git_annex_version',
+                          side_effect=AnnexRepo._check_git_annex_version)
+        with po as cmpc:
             eq_(AnnexRepo.git_annex_version, None)
             ar1 = AnnexRepo(path, create=True)
             assert(ar1)
             eq_(AnnexRepo.git_annex_version, AnnexRepo.GIT_ANNEX_MIN_VERSION)
             eq_(cmpc.call_count, 1)
             # 2nd time must not be called
             try:
@@ -1414,126 +1675,49 @@
                 rmtree(path)
             except OSError:
                 pass
             ar2 = AnnexRepo(path)
             assert(ar2)
             eq_(AnnexRepo.git_annex_version, AnnexRepo.GIT_ANNEX_MIN_VERSION)
             eq_(cmpc.call_count, 1)
-    with patch.object(AnnexRepo, 'git_annex_version', None) as cmpov, \
-            patch.object(AnnexRepo, '_check_git_annex_version',
-                         auto_spec=True,
-                         side_effect=AnnexRepo._check_git_annex_version):
-        # no git-annex at all
-        with patch.object(
-                external_versions, '_versions', {'cmd:annex': None}):
-            eq_(AnnexRepo.git_annex_version, None)
-            with assert_raises(MissingExternalDependency) as cme:
-                try:
-                    # Note: Remove to cause creation of a new instance
-                    rmtree(path)
-                except OSError:
-                    pass
-                AnnexRepo(path)
-            if linux_distribution_name == 'debian':
-                assert_in("http://neuro.debian.net", str(cme.exception))
-            eq_(AnnexRepo.git_annex_version, None)
-
-        # outdated git-annex at all
-        with patch.object(
-                external_versions, '_versions', {'cmd:annex': '6.20160505'}):
-            eq_(AnnexRepo.git_annex_version, None)
-            try:
-                # Note: Remove to cause creation of a new instance
-                rmtree(path)
-            except OSError:
-                pass
-            assert_raises(OutdatedExternalDependency, AnnexRepo, path)
-            # and we don't assign it
-            eq_(AnnexRepo.git_annex_version, None)
-            # so we could still fail
-            try:
-                # Note: Remove to cause creation of a new instance
-                rmtree(path)
-            except OSError:
-                pass
-            assert_raises(OutdatedExternalDependency, AnnexRepo, path)
 
 
-def test_ProcessAnnexProgressIndicators():
-    irrelevant_lines = (
-        'abra',
-        '{"some_json": "sure thing"}'
-    )
-    # regular lines, without completion for known downloads
-    success_lines = (
-        '{"command":"get","note":"","success":true,"key":"key1","file":"file1"}',
-        '{"command":"comm","note":"","success":true,"key":"backend-s10--key2"}',
-    )
-    progress_lines = (
-        '{"byte-progress":10,"action":{"command":"get","note":"from web...",'
-            '"key":"key1","file":"file1"},"percent-progress":"10%"}',
-    )
+@with_tempfile
+def test_annex_version_handling_bad_git_annex(path=None):
+    with set_annex_version(None):
+        eq_(AnnexRepo.git_annex_version, None)
+        with assert_raises(MissingExternalDependency) as cme:
+            AnnexRepo(path)
+        linux_distribution_name = get_linux_distribution()[0]
+        if linux_distribution_name == 'debian':
+            assert_in("handbook.datalad.org", str(cme.value))
+        eq_(AnnexRepo.git_annex_version, None)
 
-    # without providing expected entries
-    proc = ProcessAnnexProgressIndicators()
-    # when without any target downloads, there is no total_pbar
-    assert_is(proc.total_pbar, None)
-    # for regular lines -- should just return them without side-effects
-    for l in irrelevant_lines + success_lines:
-        with swallow_outputs() as cmo:
-            eq_(proc(l), l)
-            eq_(proc.pbars, {})
-            eq_(cmo.out, '')
-            eq_(cmo.err, '')
-    # should process progress lines
-    eq_(proc(progress_lines[0]), None)
-    eq_(len(proc.pbars), 1)
-    # but when we finish download -- should get cleared
-    eq_(proc(success_lines[0]), success_lines[0])
-    eq_(proc.pbars, {})
-    # and no side-effect of any kind in finish
-    eq_(proc.finish(), None)
-
-    proc = ProcessAnnexProgressIndicators(expected={'key1': 100, 'key2': None})
-    # when without any target downloads, there is no total_pbar
-    assert(proc.total_pbar is not None)
-    eq_(proc.total_pbar.total, 100)  # as much as it knows at this point
-    eq_(proc.total_pbar.current, 0)
-    # for regular lines -- should still just return them without side-effects
-    for l in irrelevant_lines:
-        with swallow_outputs() as cmo:
-            eq_(proc(l), l)
-            eq_(proc.pbars, {})
-            eq_(cmo.out, '')
-            eq_(cmo.err, '')
-    # should process progress lines
-    # it doesn't swallow everything -- so there will be side-effects in output
-    with swallow_outputs() as cmo:
-        eq_(proc(progress_lines[0]), None)
-        eq_(len(proc.pbars), 1)
-        # but when we finish download -- should get cleared
-        eq_(proc(success_lines[0]), success_lines[0])
-        eq_(proc.pbars, {})
-        out = cmo.out
-
-    from datalad.ui import ui
-    from datalad.ui.dialog import SilentConsoleLog
-
-    assert out \
-        if not isinstance(ui.ui, SilentConsoleLog) else not out
-    assert proc.total_pbar is not None
-    # and no side-effect of any kind in finish
-    with swallow_outputs() as cmo:
-        eq_(proc.finish(), None)
-        eq_(proc.total_pbar, None)
+    with set_annex_version('6.20160505'):
+        eq_(AnnexRepo.git_annex_version, None)
+        try:
+            # Note: Remove to cause creation of a new instance
+            rmtree(path)
+        except OSError:
+            pass
+        assert_raises(OutdatedExternalDependency, AnnexRepo, path)
+        # and we don't assign it
+        eq_(AnnexRepo.git_annex_version, None)
+        # so we could still fail
+        try:
+            # Note: Remove to cause creation of a new instance
+            rmtree(path)
+        except OSError:
+            pass
+        assert_raises(OutdatedExternalDependency, AnnexRepo, path)
 
 
 @with_tempfile
 @with_tempfile
-def test_get_description(path1, path2):
+def test_get_description(path1=None, path2=None):
     annex1 = AnnexRepo(path1, create=True)
     # some content for git-annex branch
     create_tree(path1, {'1.dat': 'content'})
     annex1.add('1.dat', git=False)
     annex1.commit("msg")
     annex1_description = annex1.get_description()
     assert_not_equal(annex1_description, path1)
@@ -1547,27 +1731,47 @@
     annex2.fetch('annex1')
     # it will match the remote name
     eq_(annex2.get_description(uuid=annex1.uuid),
         annex1_description + ' [annex1]')
     # add a little probe file to make sure it stays untracked
     create_tree(path1, {'probe': 'probe'})
     assert_not_in('probe', annex2.get_indexed_files())
-    annex2.merge_annex('annex1')
+    annex2.localsync('annex1')
     assert_not_in('probe', annex2.get_indexed_files())
     # but let's remove the remote
     annex2.remove_remote('annex1')
     eq_(annex2.get_description(uuid=annex1.uuid), annex1_description)
 
 
 @with_tempfile(mkdir=True)
 @with_tempfile(mkdir=True)
-def test_AnnexRepo_flyweight(path1, path2):
+def test_AnnexRepo_flyweight(path1=None, path2=None):
+
+    import sys
 
     repo1 = AnnexRepo(path1, create=True)
     assert_is_instance(repo1, AnnexRepo)
+
+    # Due to issue 4862, we currently still require gc.collect() under unclear
+    # circumstances to get rid of an exception traceback when creating in an
+    # existing directory. That traceback references the respective function
+    # frames which in turn reference the repo instance (they are methods).
+    # Doesn't happen on all systems, though. Eventually we need to figure that
+    # out.
+    # However, still test for the refcount after gc.collect() to ensure we don't
+    # introduce new circular references and make the issue worse!
+    gc.collect()
+
+    # As long as we don't reintroduce any circular references or produce
+    # garbage during instantiation that isn't picked up immediately, `repo1`
+    # should be the only counted reference to this instance.
+    # Note, that sys.getrefcount reports its own argument and therefore one
+    # reference too much.
+    assert_equal(1, sys.getrefcount(repo1) - 1)
+
     # instantiate again:
     repo2 = AnnexRepo(path1, create=False)
     assert_is_instance(repo2, AnnexRepo)
     # the very same object:
     ok_(repo1 is repo2)
 
     # reference the same in an different way:
@@ -1582,59 +1786,103 @@
 
     # Now, let's try to get a GitRepo instance from a path, we already have an
     # AnnexRepo of
     repo4 = GitRepo(path1)
     assert_is_instance(repo4, GitRepo)
     assert_not_is_instance(repo4, AnnexRepo)
 
+    orig_id = id(repo1)
 
-@with_testrepos(flavors=local_testrepo_flavors)
+    # Be sure we have exactly one object in memory:
+    assert_equal(1, len([o for o in gc.get_objects()
+                         if isinstance(o, AnnexRepo) and o.path == path1]))
+
+
+    # But we have two GitRepos in memory (the AnnexRepo and repo4):
+    assert_equal(2, len([o for o in gc.get_objects()
+                         if isinstance(o, GitRepo) and o.path == path1]))
+
+    # deleting one reference doesn't change anything - we still get the same
+    # thing:
+    del repo1
+    gc.collect()  # TODO: see first comment above
+    ok_(repo2 is not None)
+    ok_(repo2 is repo3)
+    ok_(repo2 == repo3)
+
+    repo1 = AnnexRepo(path1)
+    eq_(orig_id, id(repo1))
+
+    del repo1
+    del repo2
+
+    # for testing that destroying the object calls close() on BatchedAnnex:
+    class Dummy:
+        def __init__(self, *args, **kwargs):
+            self.close_called = False
+
+        def close(self):
+            self.close_called = True
+
+    fake_batch = Dummy()
+
+    # Killing last reference will lead to garbage collection which will call
+    # AnnexRepo's finalizer:
+    with patch.object(repo3._batched, 'close', fake_batch.close):
+        with swallow_logs(new_level=1) as cml:
+            del repo3
+            gc.collect()  # TODO: see first comment above
+            cml.assert_logged(msg="Finalizer called on: AnnexRepo(%s)" % path1,
+                              level="Level 1",
+                              regex=False)
+            # finalizer called close() on BatchedAnnex:
+            assert_true(fake_batch.close_called)
+
+    # Flyweight is gone:
+    assert_not_in(path1, AnnexRepo._unique_instances.keys())
+
+    # gc doesn't know any instance anymore:
+    assert_equal([], [o for o in gc.get_objects()
+                      if isinstance(o, AnnexRepo) and o.path == path1])
+    # GitRepo is unaffected:
+    assert_equal(1, len([o for o in gc.get_objects()
+                         if isinstance(o, GitRepo) and o.path == path1]))
+
+    # new object is created on re-request:
+    repo1 = AnnexRepo(path1)
+    assert_equal(1, len([o for o in gc.get_objects()
+                         if isinstance(o, AnnexRepo) and o.path == path1]))
+
+
+@with_tempfile
 @with_tempfile(mkdir=True)
 @with_tempfile
-def test_AnnexRepo_get_toppath(repo, tempdir, repo2):
+def test_AnnexRepo_get_toppath(repo=None, tempdir=None, repo2=None):
+    AnnexRepo(repo, create=True)
 
-    reporeal = realpath(repo)
+    reporeal = str(Path(repo).resolve())
     eq_(AnnexRepo.get_toppath(repo, follow_up=False), reporeal)
     eq_(AnnexRepo.get_toppath(repo), repo)
     # Generate some nested directory
     AnnexRepo(repo2, create=True)
-    repo2real = realpath(repo2)
+    repo2real = str(Path(repo2).resolve())
     nested = opj(repo2, "d1", "d2")
     os.makedirs(nested)
     eq_(AnnexRepo.get_toppath(nested, follow_up=False), repo2real)
     eq_(AnnexRepo.get_toppath(nested), repo2)
     # and if not under git, should return None
     eq_(AnnexRepo.get_toppath(tempdir), None)
 
 
-@with_testrepos(".*basic.*", flavors=['local'])
-@with_tempfile(mkdir=True)
-def test_AnnexRepo_add_submodule(source, path):
-
-    top_repo = AnnexRepo(path, create=True)
-
-    top_repo.add_submodule('sub', name='sub', url=source)
-    top_repo.commit('submodule added')
-    eq_([s.name for s in top_repo.get_submodules()], ['sub'])
-
-    ok_clean_git(top_repo, annex=True)
-    ok_clean_git(opj(path, 'sub'), annex=False)
-
-
-def test_AnnexRepo_update_submodule():
-    raise SkipTest("TODO")
-
-
-@known_failure_v6  #FIXME
 def test_AnnexRepo_get_submodules():
     raise SkipTest("TODO")
 
 
 @with_tempfile(mkdir=True)
-def test_AnnexRepo_dirty(path):
+def test_AnnexRepo_dirty(path=None):
 
     repo = AnnexRepo(path, create=True)
     ok_(not repo.dirty)
 
     # pure git operations:
     # untracked file
     with open(opj(path, 'file1.txt'), 'w') as f:
@@ -1645,16 +1893,15 @@
     ok_(repo.dirty)
     # clean again
     repo.commit("file1.txt added")
     ok_(not repo.dirty)
     # modify to be the same
     with open(opj(path, 'file1.txt'), 'w') as f:
         f.write('whatever')
-    if not repo.config.getint("annex", "version") == 6:
-        ok_(not repo.dirty)
+    ok_(not repo.dirty)
     # modified file
     with open(opj(path, 'file1.txt'), 'w') as f:
         f.write('something else')
     ok_(repo.dirty)
     # clean again
     repo.add('file1.txt', git=True)
     repo.commit("file1.txt modified")
@@ -1663,342 +1910,40 @@
     # annex operations:
     # untracked file
     with open(opj(path, 'file2.txt'), 'w') as f:
         f.write('different content')
     ok_(repo.dirty)
     # annexed file
     repo.add('file2.txt', git=False)
-    if not repo.is_direct_mode():
-        # in direct mode 'annex add' results in a clean repo
-        ok_(repo.dirty)
-        # commit
-        repo.commit("file2.txt annexed")
-    ok_(not repo.dirty)
-
-    # TODO: unlock/modify
-
-    # TODO: submodules
-
-
-def _test_status(ar):
-    # TODO: plain git submodule and even deeper hierarchy?
-    #       => complete recursion to work if started from within plain git;
-    #       But this is then relevant for Dataset.status() - not herein
-
-    def sync_wrapper(push=False, pull=False, commit=False):
-        # wraps common annex-sync call, since it currently fails under
-        # mysterious circumstances in V6 adjusted branch setups
-        try:
-            ar.sync(push=push, pull=pull, commit=commit)
-        except CommandError as e:
-            if "fatal: entry 'submod' object type (blob) doesn't match mode type " \
-               "(commit)" in e.stderr:
-                # some bug in adjusted branch(?) + submodule
-                # TODO: figure out and probably report
-                # stdout:
-                # commit
-                # [adjusted/master(unlocked) ae3e9a7] git-annex in ben@tree:/tmp/datalad_temp_test_AnnexRepo_status7qKhRQ
-                #  4 files changed, 4 insertions(+), 2 deletions(-)
-                #  create mode 100644 fifth
-                #  create mode 100644 sub/third
-                # ok
-                #
-                # failed
-                # stderr:
-                # fatal: entry 'submod' object type (blob) doesn't match mode type (commit)
-                # git-annex: user error (git ["--git-dir=.git","--work-tree=.","--literal-pathspecs","mktree","--batch","-z"] exited 128)
-                # git-annex: sync: 1 failed
-
-                # But it almost works - so apperently nothing to do
-                import logging
-                lgr = logging.getLogger("datalad.support.tests.test-status")
-                lgr.warning("DEBUG: v6 sync failure")
-
-    stat = {'untracked': [],
-            'deleted': [],
-            'modified': [],
-            'added': [],
-            'type_changed': []}
-    eq_(stat, ar.get_status())
-
-    # untracked files:
-    with open(opj(ar.path, 'first'), 'w') as f:
-        f.write("it's huge!")
-    with open(opj(ar.path, 'second'), 'w') as f:
-        f.write("looser")
-
-    stat['untracked'].append('first')
-    stat['untracked'].append('second')
-    eq_(stat, ar.get_status())
-
-    # add a file to git
-    ar.add('first', git=True)
-    sync_wrapper()
-    stat['untracked'].remove('first')
-    stat['added'].append('first')
-    eq_(stat, ar.get_status())
-
-    # add a file to annex
-    ar.add('second')
-    sync_wrapper()
-    stat['untracked'].remove('second')
-    if not ar.is_direct_mode():
-        # in direct mode annex-status doesn't report an added file 'added'
-        stat['added'].append('second')
-    eq_(stat, ar.get_status())
-
-    # commit to be clean again:
-    ar.commit("added first and second")
-    stat = {'untracked': [],
-            'deleted': [],
-            'modified': [],
-            'added': [],
-            'type_changed': []}
-    eq_(stat, ar.get_status())
-    # create a file to be unannexed:
-    with open(opj(ar.path, 'fifth'), 'w') as f:
-        f.write("total disaster")
-
-    ar.add('fifth')
-    sync_wrapper()
-    # TODO:
-    # Note: For some reason this seems to be the only place, where we actually
-    # need to call commit via annex-proxy. If called via '-c core.bare=False'
-    # and/or '--work-tree=.' the file ends up in git instead of annex.
-    # Note 2: This is only if we explicitly pass a path. Otherwise it works
-    # without annex-proxy.
-    ar.commit(msg="fifth to be unannexed", files='fifth',
-              proxy=ar.is_direct_mode())
-    eq_(stat, ar.get_status())
-
-    ar.unannex('fifth')
-
-    sync_wrapper(pull=False, push=False, commit=True)
-    stat['untracked'].append('fifth')
-    eq_(stat, ar.get_status())
-
-    # modify a file in git:
-    with open(opj(ar.path, 'first'), 'w') as f:
-        f.write("increased tremendousness")
-    stat['modified'].append('first')
-    eq_(stat, ar.get_status())
-
-    # modify an annexed file:
-    if not ar.is_direct_mode():
-        # actually: if 'second' isn't locked, which is the case in direct mode
-        ar.unlock('second')
-        if not ar.get_active_branch().endswith('(unlocked)'):
-            stat['type_changed'].append('second')
-        eq_(stat, ar.get_status())
-    with open(opj(ar.path, 'second'), 'w') as f:
-        f.write("Needed to unlock first. Sad!")
-    if not ar.is_direct_mode():
-        ar.add('second')  # => modified
-        if not ar.get_active_branch().endswith('(unlocked)'):
-            stat['type_changed'].remove('second')
-    stat['modified'].append('second')
-    sync_wrapper()
-    eq_(stat, ar.get_status())
-
-    # create something in a subdir
-    os.mkdir(opj(ar.path, 'sub'))
-    with open(opj(ar.path, 'sub', 'third'), 'w') as f:
-        f.write("tired of winning")
-
-    # Note, that this is different from 'git status',
-    # which would just say 'sub/':
-    stat['untracked'].append(opj('sub', 'third'))
-    eq_(stat, ar.get_status())
-
-    # test parameters for status to restrict results:
-    # limit requested states:
-    limited_status = ar.get_status(untracked=True, deleted=False, modified=True,
-                                   added=True, type_changed=False)
-    eq_(len(limited_status), 3)
-    ok_(all([k in ('untracked', 'modified', 'added') for k in limited_status]))
-    eq_(stat['untracked'], limited_status['untracked'])
-    eq_(stat['modified'], limited_status['modified'])
-    eq_(stat['added'], limited_status['added'])
-    # limit requested files:
-    limited_status = ar.get_status(path=opj('sub', 'third'))
-    eq_(limited_status['untracked'], [opj('sub', 'third')])
-    ok_(all([len(limited_status[l]) == 0 for l in ('modified', 'added',
-                                                   'deleted', 'type_changed')]))
-    # again, with a list:
-    limited_status = ar.get_status(path=[opj('sub', 'third'), 'second'])
-    eq_(limited_status['untracked'], [opj('sub', 'third')])
-    eq_(limited_status['modified'], ['second'])
-    ok_(all([len(limited_status[l]) == 0 for l in ('added', 'deleted',
-                                                   'type_changed')]))
-
-    # create a subrepo:
-    sub = AnnexRepo(opj(ar.path, 'submod'), create=True)
-    # nothing changed, it's empty besides .git, which is ignored
-    eq_(stat, ar.get_status())
-
-    # file in subrepo
-    with open(opj(ar.path, 'submod', 'fourth'), 'w') as f:
-        f.write("this is a birth certificate")
-    stat['untracked'].append(opj('submod', 'fourth'))
-    eq_(stat, ar.get_status())
-
-    # add to subrepo
-    sub.add('fourth', commit=True, msg="birther mod init'ed")
-    stat['untracked'].remove(opj('submod', 'fourth'))
-
-    if ar.get_active_branch().endswith('(unlocked)') and \
-       'adjusted' in ar.get_active_branch():
-        # we are running on adjusted branch => do it in submodule, too
-        sub.adjust()
-
-    # Note, that now the non-empty repo is untracked
-    stat['untracked'].append('submod/')
-    eq_(stat, ar.get_status())
-
-    # add the submodule
-    ar.add_submodule('submod', url=opj(curdir, 'submod'))
-
-    stat['untracked'].remove('submod/')
-    stat['added'].append('.gitmodules')
-
-    # 'submod/' might either be reported as 'added' or 'modified'.
-    # Therefore more complex assertions at this point:
-    reported_stat = ar.get_status()
-    eq_(stat['untracked'], reported_stat['untracked'])
-    eq_(stat['deleted'], reported_stat['deleted'])
-    eq_(stat['type_changed'], reported_stat['type_changed'])
-    ok_(stat['added'] == reported_stat['added'] or
-        stat['added'] + ['submod/'] == reported_stat['added'])
-    ok_(stat['modified'] == reported_stat['modified'] or
-        stat['modified'] + ['submod/'] == reported_stat['modified'])
-
-    # simpler assertion if we ignore submodules:
-    eq_(stat, ar.get_status(submodules=False))
-
-    # commit the submodule
-    # in direct mode, commit of a removed submodule fails with:
-    #  error: unable to index file submod
-    #  fatal: updating files failed
-    #
-    # - this happens, when commit is called with -c core.bare=False
-    # - it works when called via annex proxy
-    # - if we add a submodule instead of removing one, it's vice versa with
-    #   the very same error message
-
-    ar.commit(msg="submodule added", files=['.gitmodules', 'submod'])
-
-    stat['added'].remove('.gitmodules')
-    eq_(stat, ar.get_status())
-
-    # add another file to submodule
-    with open(opj(ar.path, 'submod', 'not_tracked'), 'w') as f:
-        f.write("#LastNightInSweden")
-    stat['modified'].append('submod/')
-    eq_(stat, ar.get_status())
-
-    # add the untracked file:
-    sub.add('not_tracked')
-    if sub.is_direct_mode():
-        # 'sub' is now considered to be clean; therefore it's not reported as
-        # modified upwards
-        # This is consistent in a way, but surprising in another ...
-        pass
-    else:
-        eq_(stat, ar.get_status())
-    sub.commit(msg="added file not_tracked")
-    # 'submod/' still modified when looked at from above:
-    eq_(stat, ar.get_status())
-
-    ar.add('submod', git=True)
-    ar.commit(msg="submodule modified", files='submod')
-    stat['modified'].remove('submod/')
-    eq_(stat, ar.get_status())
-
-    # clean again:
-    stat = {'untracked': [],
-            'deleted': [],
-            'modified': [],
-            'added': [],
-            'type_changed': []}
-
-    ar.add('first', git=True)
-    ar.add(opj('sub', 'third'))
-    ar.add('fifth')
-    sync_wrapper()
-
-    if ar.config.getint("annex", "version") == 6:
-        # mixed annexed/not-annexed files ATm can't be committed with explicitly
-        # given paths in v6
-        # See:
-        # http://git-annex.branchable.com/bugs/committing_files_into_git_doesn__39__t_work_with_explicitly_given_paths_in_V6
-        ar.commit()
-    else:
-        super(AnnexRepo, ar).commit(files=['first', 'fifth', opj('sub', 'third'), 'second'])
-    eq_(stat, ar.get_status())
-
-    # remove a file in annex:
-    ar.remove('fifth')
-    stat['deleted'].append('fifth')
-    eq_(stat, ar.get_status())
-
-    # remove a file in git:
-    ar.remove('first')
-    stat['deleted'].append('first')
-    eq_(stat, ar.get_status())
-
-    # remove a submodule:
-    # rm; git rm; git commit
-    from datalad.utils import rmtree
-    rmtree(opj(ar.path, 'submod'))
-    stat['deleted'].append('submod')
-    eq_(stat, ar.get_status())
-    # recreate an empty mountpoint, since we currently do it in uninstall:
-    os.makedirs(opj(ar.path, 'submod'))
-    stat['deleted'].remove('submod')
-    eq_(stat, ar.get_status())
-
-    ar.remove('submod')
-    # TODO: Why the difference?
-    stat['deleted'].append('submod')
-    stat['modified'].append('.gitmodules')
-    eq_(stat, ar.get_status())
-
-    # Note: Here again we need to use annex-proxy; This contradicts the addition
-    # of the very same submodule, which we needed to commit via
-    # -c core.bare=False instead. Otherwise the very same failure happens.
-    # Just vice versa. See above where 'submod' is added.
-    ar.commit("submod removed", files=['submod', '.gitmodules'])
-    stat['modified'].remove('.gitmodules')
-    stat['deleted'].remove('submod')
-    eq_(stat, ar.get_status())
-
-
-@with_tempfile(mkdir=True)
-@with_tempfile(mkdir=True)
-def test_AnnexRepo_status(path, path2):
-
-    ar = AnnexRepo(path, create=True)
-    _test_status(ar)
-    if ar.config.getint("annex", "version") == 6:
-        # in case of v6 have a second run with adjusted branch feature:
-        ar2 = AnnexRepo(path2, create=True)
-        ar2.commit(msg="empty commit to create branch 'master'",
-                   options=['--allow-empty'])
-        ar2._run_annex_command('adjust', annex_options=['--unlock'])
-        _test_status(ar2)
+    ok_(repo.dirty)
+    # commit
+    repo.commit("file2.txt annexed")
 
+    ok_(not repo.dirty)
 
+    repo.unlock("file2.txt")
+    # Unlocking the file is seen as a modification when we're not already in an
+    # adjusted branch (for this test, that would be the case if we're on a
+    # crippled filesystem).
+    ok_(repo.dirty ^ repo.is_managed_branch())
+    repo.save()
+    ok_(not repo.dirty)
 
-# TODO: test dirty
-# TODO: GitRep.dirty
-# TODO: test/utils ok_clean_git
+    subm = AnnexRepo(repo.pathobj / "subm", create=True)
+    (subm.pathobj / "foo").write_text("foo")
+    subm.save()
+    ok_(repo.dirty)
+    repo.save()
+    assert_false(repo.dirty)
+    maybe_adjust_repo(subm)
+    assert_false(repo.dirty)
 
 
 @with_tempfile(mkdir=True)
-def test_AnnexRepo_set_remote_url(path):
+def test_AnnexRepo_set_remote_url(path=None):
 
     ar = AnnexRepo(path, create=True)
     ar.add_remote('some', 'http://example.com/.git')
     assert_equal(ar.config['remote.some.url'],
                  'http://example.com/.git')
     assert_not_in('remote.some.annexurl', ar.config.keys())
     # change url:
@@ -2013,15 +1958,15 @@
                  'ssh://whatever.ru')
     assert_in('remote.some.annexurl', ar.config.keys())
     assert_equal(ar.config['remote.some.annexurl'],
                  'ssh://whatever.ru')
 
 
 @with_tempfile(mkdir=True)
-def test_wanted(path):
+def test_wanted(path=None):
     ar = AnnexRepo(path, create=True)
     eq_(ar.get_preferred_content('wanted'), None)
     # test samples with increasing "trickiness"
     for v in ("standard",
               "include=*.nii.gz or include=*.nii",
               "exclude=archive/* and (include=*.dat or smallerthan=2b)"
               ):
@@ -2034,160 +1979,778 @@
     # make a clone and see if all cool there
     # intentionally clone as pure Git and do not annex init so to see if we
     # are ignoring crummy log msgs
     ar1_path = ar.path + '_1'
     GitRepo.clone(ar.path, ar1_path)
     ar1 = AnnexRepo(ar1_path, init=False)
     eq_(ar1.get_preferred_content('wanted'), None)
-    eq_(ar1.get_preferred_content('wanted', 'origin'), v)
+    eq_(ar1.get_preferred_content('wanted', DEFAULT_REMOTE), v)
     ar1.set_preferred_content('wanted', expr='standard')
     eq_(ar1.get_preferred_content('wanted'), 'standard')
 
 
 @with_tempfile(mkdir=True)
-def test_AnnexRepo_metadata(path):
+def test_AnnexRepo_metadata(path=None):
     # prelude
+    obscure_name = get_most_obscure_supported_name()
+
     ar = AnnexRepo(path, create=True)
     create_tree(
         path,
         {
             'up.dat': 'content',
-            'd o"w n': {
-                'd o w n.dat': 'lowcontent'
+            obscure_name: {
+                obscure_name + '.dat': 'lowcontent'
             }
         })
     ar.add('.', git=False)
     ar.commit('content')
-    ok_clean_git(path)
+    assert_repo_status(path)
     # fugue
     # doesn't do anything if there is nothing to do
     ar.set_metadata('up.dat')
     eq_([], list(ar.get_metadata(None)))
     eq_([], list(ar.get_metadata('')))
     eq_([], list(ar.get_metadata([])))
     eq_({'up.dat': {}}, dict(ar.get_metadata('up.dat')))
     # basic invocation
-    eq_(1, len(list(ar.set_metadata(
+    eq_(1, len(ar.set_metadata(
         'up.dat',
         reset={'mike': 'awesome'},
         add={'tag': 'awesome'},
         remove={'tag': 'awesome'},  # cancels prev, just to use it
         init={'virgin': 'true'},
-        purge=['nothere']))))
+        purge=['nothere'])))
     # no timestamps by default
     md = dict(ar.get_metadata('up.dat'))
     deq_({'up.dat': {
         'virgin': ['true'],
         'mike': ['awesome']}},
         md)
     # matching timestamp entries for all keys
     md_ts = dict(ar.get_metadata('up.dat', timestamps=True))
     for k in md['up.dat']:
         assert_in('{}-lastchanged'.format(k), md_ts['up.dat'])
     assert_in('lastchanged', md_ts['up.dat'])
     # recursive needs a flag
-    assert_raises(CommandError, list, ar.set_metadata('.', purge=['virgin']))
-    list(ar.set_metadata('.', purge=['virgin'], recursive=True))
+    assert_raises(CommandError, ar.set_metadata, '.', purge=['virgin'])
+    ar.set_metadata('.', purge=['virgin'], recursive=True)
     deq_({'up.dat': {
         'mike': ['awesome']}},
         dict(ar.get_metadata('up.dat')))
     # Use trickier tags (spaces, =)
-    list(ar.set_metadata('.', reset={'tag': 'one and= '}, purge=['mike'], recursive=True))
-    playfile = opj('d o"w n', 'd o w n.dat')
+    ar.set_metadata('.', reset={'tag': 'one and= '}, purge=['mike'], recursive=True)
+    playfile = opj(obscure_name, obscure_name + '.dat')
     target = {
         'up.dat': {
             'tag': ['one and= ']},
         playfile: {
             'tag': ['one and= ']}}
     deq_(target, dict(ar.get_metadata('.')))
+    for batch in (True, False):
+        # no difference in reporting between modes
+        deq_(target, dict(ar.get_metadata(['up.dat', playfile], batch=batch)))
     # incremental work like a set
-    list(ar.set_metadata(playfile, add={'tag': 'one and= '}))
+    ar.set_metadata(playfile, add={'tag': 'one and= '})
     deq_(target, dict(ar.get_metadata('.')))
-    list(ar.set_metadata(playfile, add={'tag': ' two'}))
+    ar.set_metadata(playfile, add={'tag': ' two'})
     # returned values are sorted
     eq_([' two', 'one and= '], dict(ar.get_metadata(playfile))[playfile]['tag'])
     # init honor prior values
-    list(ar.set_metadata(playfile, init={'tag': 'three'}))
+    ar.set_metadata(playfile, init={'tag': 'three'})
     eq_([' two', 'one and= '], dict(ar.get_metadata(playfile))[playfile]['tag'])
-    list(ar.set_metadata(playfile, remove={'tag': ' two'}))
+    ar.set_metadata(playfile, remove={'tag': ' two'})
     deq_(target, dict(ar.get_metadata('.')))
     # remove non-existing doesn't error and doesn't change anything
-    list(ar.set_metadata(playfile, remove={'ether': 'best'}))
+    ar.set_metadata(playfile, remove={'ether': 'best'})
     deq_(target, dict(ar.get_metadata('.')))
     # add works without prior existence
-    list(ar.set_metadata(playfile, add={'novel': 'best'}))
+    ar.set_metadata(playfile, add={'novel': 'best'})
     eq_(['best'], dict(ar.get_metadata(playfile))[playfile]['novel'])
 
 
+@with_tree(tree={'file.txt': 'content'})
+@serve_path_via_http()
+@with_tempfile
+def test_AnnexRepo_addurl_batched_and_set_metadata(path=None, url=None, dest=None):
+    ar = AnnexRepo(dest, create=True)
+    fname = "file.txt"
+    ar.add_url_to_file(fname, urljoin(url, fname), batch=True)
+    ar.set_metadata(fname, init={"number": "one"})
+    eq_(["one"], dict(ar.get_metadata(fname))[fname]["number"])
+
+
 @with_tempfile(mkdir=True)
-def test_change_description(path):
+def test_change_description(path=None):
     # prelude
     ar = AnnexRepo(path, create=True, description='some')
     eq_(ar.get_description(), 'some')
     # try change it
     ar = AnnexRepo(path, create=False, init=True, description='someother')
     # this doesn't cut the mustard, still old
     eq_(ar.get_description(), 'some')
     # need to resort to "internal" helper
     ar._init(description='someother')
     eq_(ar.get_description(), 'someother')
 
 
-@with_testrepos('basic_annex', flavors=['clone'])
-def test_AnnexRepo_get_corresponding_branch(path):
+@with_tempfile
+@with_tempfile
+def test_AnnexRepo_get_corresponding_branch(src_path=None, path=None):
+    src = AnnexRepo(src_path, create=True)
+    (src.pathobj / 'test-annex.dat').write_text("content")
+    src.save('some')
+
+    ar = AnnexRepo.clone(src_path, path)
+
+    # we should be on the default branch.
+    eq_(DEFAULT_BRANCH,
+        ar.get_corresponding_branch() or ar.get_active_branch())
 
-    ar = AnnexRepo(path)
+    # special case v6 adjusted branch is not provided by a dedicated build:
+    ar.adjust()
+    # as above, we still want to get the default branch, while being on
+    # 'adjusted/<default branch>(unlocked)'
+    eq_('adjusted/{}(unlocked)'.format(DEFAULT_BRANCH),
+        ar.get_active_branch())
+    eq_(DEFAULT_BRANCH, ar.get_corresponding_branch())
 
-    # we should be on master or a corresponding branch like annex/direct/master
-    # respectively if ran in direct mode build.
-    # We want to get 'master' in any case
-    eq_('master', ar.get_corresponding_branch())
 
-    # special case v6 adjusted branch is not provided by a dedicated build:
-    if ar.config.getint("annex", "version") == 6:
-        ar.adjust()
-        # as above, we still want to get 'master', while being on
-        # 'adjusted/master(unlocked)'
-        eq_('adjusted/master(unlocked)', ar.get_active_branch())
-        eq_('master', ar.get_corresponding_branch())
+@with_tempfile
+@with_tempfile
+def test_AnnexRepo_get_tracking_branch(src_path=None, path=None):
+    src = AnnexRepo(src_path, create=True)
+    (src.pathobj / 'test-annex.dat').write_text("content")
+    src.save('some')
 
+    ar = AnnexRepo.clone(src_path, path)
 
-@with_testrepos('basic_annex', flavors=['clone'])
-def test_AnnexRepo_get_tracking_branch(path):
+    # we want the relation to original branch, e.g. in v6+ adjusted branch
+    eq_((DEFAULT_REMOTE, 'refs/heads/' + DEFAULT_BRANCH),
+        ar.get_tracking_branch())
 
-    ar = AnnexRepo(path)
 
-    # we want the relation to original branch, especially in direct mode
-    # or even in v6 adjusted branch
-    eq_(('origin', 'refs/heads/master'), ar.get_tracking_branch())
+@skip_if_adjusted_branch
+@with_tempfile
+def test_AnnexRepo_is_managed_branch(path=None):
+    ar = AnnexRepo(path, create=True)
+    (ar.pathobj / 'test-annex.dat').write_text("content")
+    ar.save('some')
 
+    ar.adjust()
+    ok_(ar.is_managed_branch())
 
-@with_testrepos('basic_annex', flavors=['clone'])
-def test_AnnexRepo_is_managed_branch(path):
 
-    ar = AnnexRepo(path)
+@with_tempfile(mkdir=True)
+def test_fake_is_not_special(path=None):
+    ar = AnnexRepo(path, create=True)
+    # doesn't exist -- we fail by default
+    assert_raises(RemoteNotAvailableError, ar.is_special_annex_remote, "fake")
+    assert_false(ar.is_special_annex_remote("fake", check_if_known=False))
 
-    if ar.is_direct_mode():
-        ok_(ar.is_managed_branch())
+
+@with_tree(tree={"remote": {}, "main": {}, "special": {}})
+def test_is_special(path=None):
+    rem = AnnexRepo(op.join(path, "remote"), create=True)
+    dir_arg = "directory={}".format(op.join(path, "special"))
+    rem.init_remote("imspecial",
+                    ["type=directory", "encryption=none", dir_arg])
+    ok_(rem.is_special_annex_remote("imspecial"))
+
+    ar = AnnexRepo.clone(rem.path, op.join(path, "main"))
+    assert_false(ar.is_special_annex_remote(DEFAULT_REMOTE))
+
+    assert_false(ar.is_special_annex_remote("imspecial",
+                                            check_if_known=False))
+    ar.enable_remote("imspecial", options=[dir_arg])
+    ok_(ar.is_special_annex_remote("imspecial"))
+
+    # With a mis-configured remote, give warning and return false.
+    ar.config.unset(f"remote.{DEFAULT_REMOTE}.url", scope="local")
+    with swallow_logs(new_level=logging.WARNING) as cml:
+        assert_false(ar.is_special_annex_remote(DEFAULT_REMOTE))
+        cml.assert_logged(msg=".*no URL.*", level="WARNING", regex=True)
+
+
+@with_tempfile(mkdir=True)
+def test_fake_dates(path=None):
+    ar = AnnexRepo(path, create=True, fake_dates=True)
+    timestamp = ar.config.obtain("datalad.fake-dates-start") + 1
+    # Commits from the "git annex init" call are one second ahead.
+    for commit in ar.get_branch_commits_("git-annex"):
+        eq_(timestamp, int(ar.format_commit('%ct', commit)))
+    assert_in("timestamp={}s".format(timestamp),
+              ar.call_git(["cat-file", "blob", "git-annex:uuid.log"], read_only=True))
+
+
+# to prevent regression
+# http://git-annex.branchable.com/bugs/v6_-_under_subdir__58___git_add___34__whines__34____44___git_commit___34__blows__34__/
+# It is disabled because is not per se relevant to DataLad since we do not
+# Since we invoke from the top of the repo, we do not hit it,
+# but thought to leave it around if we want to enforce/test system-wide git being
+# compatible with annex for v6 mode
+@with_tempfile(mkdir=True)
+def _test_add_under_subdir(path):
+    ar = AnnexRepo(path, create=True, version=6)
+    gr = GitRepo(path)  # "Git" view over the repository, so we force "git add"
+    subdir = opj(path, 'sub')
+    subfile = opj('sub', 'empty')
+    # os.mkdir(subdir)
+    create_tree(subdir, {'empty': ''})
+    runner = Runner(cwd=subdir)
+    with chpwd(subdir):
+        runner.run(['git', 'add', 'empty'])  # should add successfully
+        # gr.commit('important') #
+        runner.run(['git', 'commit', '-m', 'important'])
+        ar.is_under_annex(subfile)
+
+
+# https://github.com/datalad/datalad/issues/2892
+@with_tempfile(mkdir=True)
+def test_error_reporting(path=None):
+    ar = AnnexRepo(path, create=True)
+    res = ar.call_annex_records(['add'], files='gl\\orious BS')
+    target = {
+        'command': 'add',
+        # whole thing, despite space, properly quotes backslash
+        'file': 'gl\\orious BS',
+        'note': 'not found',
+        'success': False
+    }
+    assert len(res) >= 1
+    if 'message-id' in res[0]:
+        # new since ~ 10.20230407-99-gbe36e208c2
+        target['message-id'] = 'FileNotFound'
+        target['input'] = ['gl\\orious BS']
+        target['error-messages'] = ['git-annex: gl\\orious BS not found']
     else:
-        # ATM only direct mode and v6 adjusted branches should return True.
-        # Adjusted branch requires a call of git-annex-adjust and shouldn't
-        # be the state of a fresh clone
-        ok_(not ar.is_managed_branch())
-    if ar.config.getint("annex", "version") == 6:
-        ar.adjust()
-        ok_(ar.is_managed_branch())
+        # our own produced record
+        target['error-messages'] = ['File unknown to git']
+    eq_(res, [target])
+
+
+@with_tree(tree={
+    'file1': "content1",
+    'dir1': {'file2': 'content2'},
+})
+def test_annexjson_protocol(path=None):
+    ar = AnnexRepo(path, create=True)
+    ar.save()
+    assert_repo_status(path)
+    # first an orderly execution
+    res = ar._call_annex(
+        ['find', '.', '--json'],
+        protocol=AnnexJsonProtocol)
+    for k in ('stdout', 'stdout_json', 'stderr'):
+        assert_in(k, res)
+    orig_j = res['stdout_json']
+    eq_(len(orig_j), 2)
+    # not meant as an exhaustive check for output structure,
+    # just some assurance that it is not totally alien
+    ok_(all(j['file'] for j in orig_j))
+    # no complaints, unless git-annex is triggered to run in debug mode
+    if logging.getLogger('datalad.annex').getEffectiveLevel() > 8:
+        eq_(res['stderr'], '')
+
+    # Note: git-annex-find <non-existent-path> does not error with all annex
+    # versions. Fixed in annex commit
+    # ce91f10132805d11448896304821b0aa9c6d9845 (Feb 28, 2022).
+    if '10.20220222' < external_versions['cmd:annex'] < '10.20220322':
+        raise SkipTest("zero-exit annex-find bug")
+
+    # now the same, but with a forced error
+    with assert_raises(CommandError) as e:
+        ar._call_annex(['find', '.', 'error', '--json'],
+                       protocol=AnnexJsonProtocol)
+    # normal operation is not impaired
+    eq_(e.value.kwargs['stdout_json'], orig_j)
+    # we get a clue what went wrong,
+    # but reporting depends on config + version (default changed):
+    msg = "pathspec 'error' did not match" if not dl_cfg.getbool(
+        section="annex", option="skipunknown",
+        # git-annex switched default for this config:
+        default=bool(external_versions['cmd:annex'] < '10.20220222')) else \
+        "error not found"
+    assert_in(msg, e.value.stderr)
+    # there should be no errors reported in an individual records
+    # hence also no pointless statement in the str()
+    assert_not_in('errors from JSON records', str(e.value))
+
+
+@with_tempfile
+def test_annexjson_protocol_long(path=None, *, caplog):
+    records = [
+        {"k": "v" * 20},
+        # Value based off of
+        # Lib.asyncio.unix_events._UnixReadPipeTransport.max_size.
+        {"k": "v" * 256 * 1024},
+        # and tiny ones in between should not be lost
+        {"k": "v"},
+        # even a much larger one - we should handle as well
+        {"k": "v" * 256 * 1024 * 5},
+    ]
+    with open(path, 'w') as f:
+        for record in records:
+            print("print(%r);" % json.dumps(record), file=f)
+    runner = GitWitlessRunner()
+    with caplog.at_level(logging.ERROR), \
+        swallow_logs(new_level=logging.ERROR):
+        res = runner.run(
+            [sys.executable, path],
+            protocol=AnnexJsonProtocol
+        )
+    eq_(res['stdout'], '')
+    eq_(res['stderr'], '')
+    eq_(res['stdout_json'], records)
+
+
+@pytest.mark.parametrize("print_opt", ['', ', end=""'])
+@with_tempfile
+def test_annexjson_protocol_incorrect(path=None, *, print_opt, caplog):
+    # Test that we still log some incorrectly formed JSON record
+    bad_json = '{"I": "am wrong,}'
+    with open(path, 'w') as f:
+        print("print(%r%s);" % (bad_json, print_opt), file=f)
+    runner = GitWitlessRunner()
+    # caplog only to not cause memory error in case of heavy debugging
+    # Unfortunately it lacks similar .assert_logged with a regex matching
+    # to be just used instead
+    with caplog.at_level(logging.ERROR), \
+        swallow_logs(new_level=logging.ERROR) as cml:
+        res = runner.run(
+            [sys.executable, path],
+            protocol=AnnexJsonProtocol
+        )
+        cml.assert_logged(
+            msg=".*[rR]eceived undecodable JSON output",
+            level="ERROR",
+            regex=True)
+    # only error logged and nothing returned
+    eq_(res['stdout'], '')
+    eq_(res['stderr'], '')
+    eq_(res['stdout_json'], [])
+
+# see https://github.com/datalad/datalad/pull/5400 for troubleshooting
+# for stalling with unlock=False, and then with unlock=True it took >= 300 sec
+# https://github.com/datalad/datalad/pull/5433#issuecomment-784470028
+@skip_if(on_travis and on_nfs)  # TODO. stalls
+# http://git-annex.branchable.com/bugs/cannot_commit___34__annex_add__34__ed_modified_file_which_switched_its_largefile_status_to_be_committed_to_git_now/#comment-bf70dd0071de1bfdae9fd4f736fd1ec
+# https://github.com/datalad/datalad/issues/1651
+@known_failure_githubci_win
+@pytest.mark.parametrize("unlock", [True, False])
+@with_tree(tree={
+    '.gitattributes': "** annex.largefiles=(largerthan=4b)",
+    'alwaysbig': 'a'*10,
+    'willgetshort': 'b'*10,
+    'tobechanged-git': 'a',
+    'tobechanged-annex': 'a'*10,
+})
+def test_commit_annex_commit_changed(path=None, *, unlock):
+    # Here we test commit working correctly if file was just removed
+    # (not unlocked), edited and committed back
+
+    # TODO: an additional possible interaction to check/solidify - if files
+    # first get unannexed (after being optionally unlocked first)
+    unannex = False
+
+    ar = AnnexRepo(path, create=True)
+    ar.save(paths=[".gitattributes"], git=True)
+    ar.save("initial commit")
+    assert_repo_status(path)
+    # Now let's change all but commit only some
+    files = [op.basename(p) for p in glob(op.join(path, '*'))]
+    if unlock:
+        ar.unlock(files)
+    if unannex:
+        ar.unannex(files)
+    create_tree(
+        path
+        , {
+            'alwaysbig': 'a'*11,
+            'willgetshort': 'b',
+            'tobechanged-git': 'aa',
+            'tobechanged-annex': 'a'*11,
+            'untracked': 'unique'
+        }
+        , remove_existing=True
+    )
+    assert_repo_status(
+        path
+        , modified=files if not unannex else ['tobechanged-git']
+        , untracked=['untracked'] if not unannex else
+          # all but the one in git now
+          ['alwaysbig', 'tobechanged-annex', 'untracked', 'willgetshort']
+    )
+
+    ar.save("message", paths=['alwaysbig', 'willgetshort'])
+    assert_repo_status(
+        path
+        , modified=['tobechanged-git', 'tobechanged-annex']
+        , untracked=['untracked']
+    )
+    ok_file_under_git(path, 'alwaysbig', annexed=True)
+    ok_file_under_git(path, 'willgetshort', annexed=False)
+
+    ar.save("message2", untracked='no') # commit all changed
+    assert_repo_status(
+        path
+        , untracked=['untracked']
+    )
+    ok_file_under_git(path, 'tobechanged-git', annexed=False)
+    ok_file_under_git(path, 'tobechanged-annex', annexed=True)
+
+
+_test_unannex_tree = {
+    OBSCURE_FILENAME: 'content1',
+    OBSCURE_FILENAME + ".dat": 'content2',
+}
+if not on_windows and (
+        external_versions['cmd:annex'] <= '10.20230407' or external_versions['cmd:annex'] >= '10.20230408'
+):
+    # Only whenever we are not within the development versions of the 10.20230407
+    # where we cannot do version comparison relibalye,
+    # the case where we have entire filename within ""
+    _test_unannex_tree[f'"{OBSCURE_FILENAME}"'] = 'content3'
+
+
+@with_tree(tree=_test_unannex_tree)
+def test_unannex_etc(path=None):
+    # Primarily to test if quote/unquote/not-quote'ing work for tricky
+    # filenames. Ref: https://github.com/datalad/datalad/pull/7372
+    repo = AnnexRepo(path)
+    files = list(_test_unannex_tree)
+    # here it is through json so kinda guaranteed to work but let's check too
+    assert files == [x['file'] for x in repo.add(files)]
+    assert sorted(files) == sorted(repo.get_annexed_files())
+    assert files == repo.unannex(files)
 
 
+@slow  # 15 + 17sec on travis
+@pytest.mark.parametrize("cls", [GitRepo, AnnexRepo])
 @with_tempfile(mkdir=True)
-@with_tempfile()
-def test_AnnexRepo_flyweight_monitoring_inode(path, store):
-    # testing for issue #1512
-    check_repo_deals_with_inode_change(AnnexRepo, path, store)
+def test_files_split_exc(topdir=None, *, cls):
+    r = cls(topdir)
+    # absent files -- should not crash with "too long" but some other more
+    # meaningful exception
+    files = ["f" * 100 + "%04d" % f for f in range(100000)]
+    if isinstance(r, AnnexRepo):
+        # Annex'es add first checks for what is being added and does not fail
+        # for non existing files either ATM :-/  TODO: make consistent etc
+        r.add(files)
+    else:
+        with assert_raises(Exception) as ecm:
+            r.add(files)
+        assert_not_in('too long', str(ecm.value))
+        assert_not_in('too many', str(ecm.value))
+
+
+# with 204  (/ + (98+3)*2 + /) chars guaranteed, we hit "filename too long" quickly on windows
+# so we are doomed to shorten the filepath for testing on windows. Since the limits are smaller
+# on windows (16k vs e.g. 1m on linux in CMD_MAX_ARG), it would already be a "struggle" for it,
+# we also reduce number of dirs/files
+_ht_len, _ht_n = (48, 20) if on_windows else (98, 100)
+
+_HEAVY_TREE = {
+    # might already run into 'filename too long' on windows probably
+    "d" * _ht_len + '%03d' % d: {
+        # populate with not entirely unique but still not all identical (empty) keys.
+        # With content unique to that filename we would still get 100 identical
+        # files for each key, thus possibly hitting regressions in annex like
+        # https://git-annex.branchable.com/bugs/significant_performance_regression_impacting_datal/
+        # but also would not hit filesystem as hard as if we had all the keys unique.
+        'f' * _ht_len + '%03d' % f: str(f)
+        for f in range(_ht_n)
+    }
+    for d in range(_ht_n)
+}
+
+# @known_failure_windows  # might fail with some older annex `cp` failing to set permissions
+@slow  # 313s  well -- if errors out - only 3 sec
+@pytest.mark.parametrize("cls", [GitRepo, AnnexRepo])
+@with_tree(tree=_HEAVY_TREE)
+def test_files_split(topdir=None, *, cls):
+    from glob import glob
+    r = cls(topdir)
+    dirs = glob(op.join(topdir, '*'))
+    files = glob(op.join(topdir, '*', '*'))
+
+    r.add(files)
+    r.commit(files=files)
+
+    # Let's modify and do dl.add for even a heavier test
+    # Now do for real on some heavy directory
+    import datalad.api as dl
+    for f in files:
+        os.unlink(f)
+        with open(f, 'w') as f:
+            f.write('1')
+    dl.save(dataset=r.path, path=dirs, result_renderer="disabled")
+
+
+@skip_if_on_windows
+@skip_if_root
+@with_tree({
+    'repo': {
+        'file1': 'file1',
+        'file2': 'file2'
+    }
+})
+def test_ro_operations(path=None):
+    # This test would function only if there is a way to run sudo
+    # non-interactively, e.g. on Travis or on your local (watchout!) system
+    # after you ran sudo command recently.
+    run = Runner().run
+    sudochown = lambda cmd: run(['sudo', '-n', 'chown'] + cmd)
+
+    repo = AnnexRepo(op.join(path, 'repo'), init=True)
+    repo.add('file1')
+    repo.commit()
+
+    # make a clone
+    repo2 = repo.clone(repo.path, op.join(path, 'clone'))
+    repo2.get('file1')
+
+    # progress forward original repo and fetch (but nothing else) it into repo2
+    repo.add('file2')
+    repo.commit()
+    repo2.fetch(DEFAULT_REMOTE)
+
+    # Assure that regardless of umask everyone could read it all
+    run(['chmod', '-R', 'a+rX', repo2.path])
+    try:
+        # To assure that git/git-annex really cannot acquire a lock and do
+        # any changes (e.g. merge git-annex branch), we make this repo owned by root
+        sudochown(['-R', 'root', repo2.path])
+    except Exception as exc:
+        # Exception could be CommandError or IOError when there is no sudo
+        raise SkipTest("Cannot run sudo chown non-interactively: %s" % exc)
+
+    # recent git would refuse to run  git status  in repository owned by someone else
+    # which could lead to odd git-annex errors before 10.20220504-55-gaf0d85446 AKA 10.20220525~13
+    # see https://github.com/datalad/datalad/issues/5665 and after an informative error
+    # https://github.com/datalad/datalad/issues/6708
+    # To overcome - explicitly add the path into allowed
+    dl_cfg.add('safe.directory', repo2.path, scope='global')
+
+    try:
+        assert not repo2.get('file1')  # should work since file is here already
+        repo2.status()  # should be Ok as well
+        # and we should get info on the file just fine
+        assert repo2.info('file1')
+        # The tricky part is the repo_info which might need to update
+        # remotes UUID -- by default it should fail!
+        # Oh well -- not raised on travis... whatever for now
+        #with assert_raises(CommandError):
+        #    repo2.repo_info()
+        # but should succeed if we disallow merges
+        repo2.repo_info(merge_annex_branches=False)
+        # and ultimately the ls which uses it
+        try:
+            from datalad.api import ls
+            ls(repo2.path, all_=True, long_=True)
+        except ImportError:
+            raise SkipTest(
+                "No `ls` command available (provided by -deprecated extension)")
+    finally:
+        sudochown(['-R', str(os.geteuid()), repo2.path])
+
+    # just check that all is good again
+    repo2.repo_info()
+
+
+@skip_if_on_windows
+@skip_if_root
+@with_tree({
+    'file1': 'file1',
+})
+def test_save_noperms(path=None):
+    # check that we do report annex error messages
+
+    # This test would function only if there is a way to run sudo
+    # non-interactively, e.g. on Travis or on your local (watchout!) system
+    # after you ran sudo command recently.
+    repo = AnnexRepo(path, init=True)
+
+    run = Runner().run
+    sudochown = lambda cmd: run(['sudo', '-n', 'chown'] + cmd)
+
+    try:
+        # To assure that git/git-annex really cannot acquire a lock and do
+        # any changes (e.g. merge git-annex branch), we make this repo owned by root
+        sudochown(['-R', 'root:root', str(repo.pathobj / 'file1')])
+    except Exception as exc:
+        # Exception could be CommandError or IOError when there is no sudo
+        raise SkipTest("Cannot run sudo chown non-interactively: %s" % exc)
+
+    try:
+        repo.save(paths=['file1'])
+    except CommandError as exc:
+        res = exc.kwargs["stdout_json"]
+        assert_result_count(res, 1)
+        assert_result_count(res, 1, file='file1',
+                            command='add', success=False)
+        assert_in('permission denied', res[0]['error-messages'][0])
+    finally:
+        sudochown(['-R', str(os.geteuid()), repo.path])
+
+
+def test_get_size_from_key():
+
+    # see https://git-annex.branchable.com/internals/key_format/
+    # BACKEND[-sNNNN][-mNNNN][-SNNNN-CNNNN]--NAME
+
+    test_keys = {"ANYBACKEND--NAME": None,
+                 "ANYBACKEND-s123-m1234--NAME-WITH-DASHES.ext": 123,
+                 "MD5E-s100-S10-C1--somen.ame": 10,
+                 "SHA256-s99-S10-C10--name": 9,
+                 "SHA256E-sNaN--name": None,  # debatable: None or raise?
+                 }
+
+    invalid = ["ANYBACKEND-S10-C30--missing-total",
+               "s99-S10-C10--NOBACKEND",
+               "MD5-s100-S5--no-chunk-number"]
+
+    for key in invalid:
+        assert_raises(ValueError, AnnexRepo.get_size_from_key, key)
+
+    for key, value in test_keys.items():
+        eq_(AnnexRepo.get_size_from_key(key), value)
 
 
 @with_tempfile(mkdir=True)
-def test_fake_is_not_special(path):
+def test_call_annex(path=None):
     ar = AnnexRepo(path, create=True)
-    # doesn't exist -- we fail by default
-    assert_raises(RemoteNotAvailableError, ar.is_special_annex_remote, "fake")
-    assert_false(ar.is_special_annex_remote("fake", check_if_known=False))
+    # we raise on mistakes
+    with assert_raises(CommandError):
+        ar._call_annex(['not-an-annex-command'])
+    # and we get to know why
+    try:
+        ar._call_annex(['not-an-annex-command'])
+    except CommandError as e:
+        assert_in('Invalid argument', e.stderr)
+
+
+@with_tempfile
+def test_whereis_zero_copies(path=None):
+    repo = AnnexRepo(path, create=True)
+    (repo.pathobj / "foo").write_text("foo")
+    repo.save()
+    repo.drop(["foo"], options=["--force"])
+
+    for output in "full", "uuids", "descriptions":
+        res = repo.whereis(files=["foo"], output=output)
+        if output == "full":
+            assert_equal(res["foo"], {})
+        else:
+            assert_equal(res, [[]])
+
+
+@with_tempfile(mkdir=True)
+def test_whereis_batch_eqv(path=None):
+    path = Path(path)
+
+    repo_a = AnnexRepo(path / "a", create=True)
+    (repo_a.pathobj / "foo").write_text("foo")
+    (repo_a.pathobj / "bar").write_text("bar")
+    (repo_a.pathobj / "baz").write_text("baz")
+    repo_a.save()
+
+    repo_b = repo_a.clone(repo_a.path, str(path / "b"))
+    repo_b.drop(["bar"])
+    repo_b.drop(["baz"])
+    repo_b.drop(["baz"], options=["--from=" + DEFAULT_REMOTE, "--force"])
+
+    files = ["foo", "bar", "baz"]
+    info = repo_b.get_content_annexinfo(files)
+    keys = [info[repo_b.pathobj / f]['key'] for f in files]
+
+    for output in "full", "uuids", "descriptions":
+        out_non_batch = repo_b.whereis(files=files, batch=False, output=output)
+        assert_equal(out_non_batch,
+                     repo_b.whereis(files=files, batch=True, output=output))
+        out_non_batch_keys = repo_b.whereis(files=keys, batch=False, key=True, output=output)
+        # should be identical
+        if output == 'full':
+            # we need to map files to keys though
+            assert_equal(out_non_batch_keys,
+                         {k: out_non_batch[f] for f, k in zip(files, keys)})
+        else:
+            assert_equal(out_non_batch, out_non_batch_keys)
+
+        if external_versions['cmd:annex'] >= '8.20210903':
+            # --batch-keys support was introduced
+            assert_equal(out_non_batch_keys,
+                         repo_b.whereis(files=keys, batch=True, key=True, output=output))
+
+    if external_versions['cmd:annex'] < '8.20210903':
+        # --key= and --batch are incompatible.
+        with assert_raises(ValueError):
+            repo_b.whereis(files=files, batch=True, key=True)
+
+
+def test_done_deprecation():
+    with unittest.mock.patch("datalad.cmd.warnings.warn") as warn_mock:
+        _ = AnnexJsonProtocol("done")
+        warn_mock.assert_called_once()
+
+    with unittest.mock.patch("datalad.cmd.warnings.warn") as warn_mock:
+        _ = AnnexJsonProtocol()
+        warn_mock.assert_not_called()
+
+
+def test_generator_annex_json_protocol():
+
+    runner = Runner()
+    stdin_queue = Queue()
+
+    def json_object(count: int):
+        json_template = '{{"id": "some-id", "count": {count}}}'
+        return json_template.format(count=count).encode()
+
+    count = 123
+    stdin_queue.put(json_object(count=count))
+    for result in runner.run(cmd="cat", protocol=GeneratorAnnexJsonProtocol, stdin=stdin_queue):
+        assert_equal(
+            result,
+            {
+                "id": "some-id",
+                "count": count,
+            }
+        )
+        if count == 133:
+            break
+        count += 1
+        stdin_queue.put(json_object(count=count))
+
+
+def test_captured_exception():
+    class RaiseMock:
+        def add_(self, *args, **kwargs):
+            raise CommandError("RaiseMock.add_")
+
+    with patch("datalad.support.annexrepo.super") as repl_super:
+        repl_super.return_value = RaiseMock()
+        gen = AnnexRepo.add_(object(), [])
+        assert_raises(CommandError, gen.send, None)
+
+
+@skip_if_on_windows
+def test_stderr_rejecting_protocol_trigger():
+    result_generator = GitWitlessRunner().run(
+        "echo ssss >&2",
+        protocol=GeneratorAnnexJsonNoStderrProtocol)
+
+    try:
+        tuple(result_generator)
+    except CommandError as e:
+        assert_in("ssss", e.stderr)
+        return
+    assert_true(False)
+
+
+@skip_if_on_windows
+def test_stderr_rejecting_protocol_ignore():
+
+    result_generator = GitWitlessRunner().run(
+        ['echo', '{"status": "ok"}'],
+        protocol=GeneratorAnnexJsonNoStderrProtocol)
+    assert_equal(tuple(result_generator), ({"status": "ok"},))
```

### Comparing `datalad-0.9.3/datalad/support/tests/test_vcr_.py` & `datalad-1.0.0/datalad/support/tests/test_vcr_.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,20 +1,22 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 """Tests for vcr adapter"""
 
+from ...tests.utils_pytest import (
+    SkipTest,
+    eq_,
+)
 from ..vcr_ import use_cassette
-from ...tests.utils import SkipTest
-from ...tests.utils import eq_
 
 
 def test_use_cassette_if_no_vcr():
     # just test that our do nothing decorator does the right thing if vcr is not present
     skip = False
     try:
         import vcr
```

### Comparing `datalad-0.9.3/datalad/support/tests/test_gitrepo.py` & `datalad-1.0.0/datalad/core/local/tests/test_save.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,1271 +1,1125 @@
-# emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# -*- coding: utf-8 -*-
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""Test implementation of class GitRepo
-
-"""
-
-from nose.tools import assert_is_instance
+"""Test save command"""
 
+import itertools
+import logging
 import os
-from datalad.tests.utils import *
-from datalad.tests.utils_testrepos import BasicAnnexTestRepo
-from datalad.utils import getpwd, chpwd
-
-from datalad.support.sshconnector import get_connection_hash
-
-# imports from same module:
-# we want to test everything in gitrepo:
-from ..gitrepo import *
-from ..gitrepo import _normalize_path
-from ..exceptions import FileNotInRepositoryError
-from .utils import check_repo_deals_with_inode_change
-
-
-@with_tempfile(mkdir=True)
-def test_GitRepo_invalid_path(path):
-    with chpwd(path):
-        assert_raises(ValueError, GitRepo, path="git://some/url", create=True)
-        ok_(not exists(opj(path, "git:")))
-        assert_raises(ValueError, GitRepo, path="file://some/relative/path", create=True)
-        ok_(not exists(opj(path, "file:")))
-
-
-@assert_cwd_unchanged
-@with_testrepos(flavors=local_testrepo_flavors)
-@with_tempfile
-def test_GitRepo_instance_from_clone(src, dst):
-
-    gr = GitRepo.clone(src, dst)
-    assert_is_instance(gr, GitRepo, "GitRepo was not created.")
-    assert_is_instance(gr.repo, gitpy.Repo,
-                       "Failed to instantiate GitPython Repo object.")
-    assert_true(exists(opj(dst, '.git')))
-
-    # do it again should raise GitCommandError since git will notice there's
-    # already a git-repo at that path and therefore can't clone to `dst`
-    # Note: Since GitRepo is now a WeakSingletonRepo, this is prevented from
-    # happening atm. Disabling for now:
-#    raise SkipTest("Disabled for RF: WeakSingletonRepo")
-    with swallow_logs() as logs:
-        assert_raises(GitCommandError, GitRepo.clone, src, dst)
-
-
-@assert_cwd_unchanged
-@with_testrepos(flavors=local_testrepo_flavors)
-def test_GitRepo_instance_from_existing(path):
-
-    gr = GitRepo(path)
-    assert_is_instance(gr, GitRepo, "GitRepo was not created.")
-    assert_true(exists(opj(path, '.git')))
-
-
-@assert_cwd_unchanged
-@with_tempfile
-@with_tempfile
-def test_GitRepo_instance_from_not_existing(path, path2):
-    # 1. create=False and path doesn't exist:
-    assert_raises(NoSuchPathError, GitRepo, path, create=False)
-    assert_false(exists(path))
-
-    # 2. create=False, path exists, but no git repo:
-    os.mkdir(path)
-    assert_true(exists(path))
-    assert_raises(InvalidGitRepositoryError, GitRepo, path, create=False)
-    assert_false(exists(opj(path, '.git')))
-
-    # 3. create=True, path doesn't exist:
-    gr = GitRepo(path2, create=True)
-    assert_is_instance(gr, GitRepo, "GitRepo was not created.")
-    assert_true(exists(opj(path2, '.git')))
-    ok_clean_git(path2, annex=False)
-
-    # 4. create=True, path exists, but no git repo:
-    gr = GitRepo(path, create=True)
-    assert_is_instance(gr, GitRepo, "GitRepo was not created.")
-    assert_true(exists(opj(path, '.git')))
-    ok_clean_git(path, annex=False)
-
-
-@with_tempfile
-def test_GitRepo_init_options(path):
-    # passing an option, not explicitly defined in GitRepo class:
-    gr = GitRepo(path, create=True, bare=True)
-
-    cfg = gr.repo.config_reader()
-    ok_(cfg.get_value(section="core", option="bare"))
-
-
-@with_tempfile
-@with_tempfile
-def test_GitRepo_equals(path1, path2):
-
-    repo1 = GitRepo(path1)
-    repo2 = GitRepo(path1)
-    ok_(repo1 == repo2)
-    eq_(repo1, repo2)
-    repo2 = GitRepo(path2)
-    assert_not_equal(repo1, repo2)
-    ok_(repo1 != repo2)
-
-
-@assert_cwd_unchanged
-@with_testrepos('.*git.*', flavors=local_testrepo_flavors)
-@with_tempfile
-def test_GitRepo_add(src, path):
-
-    gr = GitRepo.clone(src, path)
-    filename = get_most_obscure_supported_name()
-    with open(opj(path, filename), 'w') as f:
-        f.write("File to add to git")
-    added = gr.add(filename)
-
-    assert_equal(added, {'success': True, 'file': filename})
-    assert_in(filename, gr.get_indexed_files(),
-              "%s not successfully added to %s" % (filename, path))
-    # uncommitted:
-    ok_(gr.dirty)
-
-    filename = "another.txt"
-    with open(opj(path, filename), 'w') as f:
-        f.write("Another file to add to git")
-    assert_raises(AssertionError, gr.add, filename, git=False)
-    assert_raises(AssertionError, gr.add, filename, git=None)
-
-    # include committing:
-    added2 = gr.add(filename, commit=True, msg="Add two files.")
-    assert_equal(added2, {'success': True, 'file': filename})
-
-    assert_in(filename, gr.get_indexed_files(),
-              "%s not successfully added to %s" % (filename, path))
-    ok_clean_git(path)
-
-
-@assert_cwd_unchanged
-@with_tree(tree={
-    'd': {'f1': 'content1',
-          'f2': 'content2'},
-    'file': 'content3',
-    'd2': {'f1': 'content1',
-          'f2': 'content2'},
-    'file2': 'content3'
-
-    })
-def test_GitRepo_remove(path):
-
-    gr = GitRepo(path, create=True)
-    gr.add('*')
-    gr.commit("committing all the files")
-
-    eq_(gr.remove('file'), ['file'])
-    eq_(set(gr.remove('d', r=True, f=True)), {'d/f1', 'd/f2'})
-
-    eq_(set(gr.remove('*', r=True, f=True)), {'file2', 'd2/f1', 'd2/f2'})
-
-
-@assert_cwd_unchanged
-@with_tempfile
-def test_GitRepo_commit(path):
-
-    gr = GitRepo(path)
-    filename = get_most_obscure_supported_name()
-    with open(opj(path, filename), 'w') as f:
-        f.write("File to add to git")
-
-    gr.add(filename)
-    gr.commit("Testing GitRepo.commit().")
-    ok_clean_git(gr)
-    eq_("Testing GitRepo.commit().{}".format(linesep),
-        gr.repo.head.commit.message)
-
-    with open(opj(path, filename), 'w') as f:
-        f.write("changed content")
-
-    gr.add(filename)
-    gr.commit("commit with options", options=to_options(dry_run=True))
-    # wasn't actually committed:
-    ok_(gr.dirty)
-
-    # commit with empty message:
-    gr.commit()
-    ok_clean_git(gr)
-
-    # nothing to commit doesn't raise by default:
-    gr.commit()
-    # but does with careless=False:
-    assert_raises(CommandError, gr.commit, careless=False)
-
-    # committing untracked file raises:
-    with open(opj(path, "untracked"), "w") as f:
-        f.write("some")
-    assert_raises(FileNotInRepositoryError, gr.commit, files="untracked")
-    # not existing file as well:
-    assert_raises(FileNotInRepositoryError, gr.commit, files="not-existing")
-
-
-@with_testrepos(flavors=local_testrepo_flavors)
-@with_tempfile
-def test_GitRepo_get_indexed_files(src, path):
-
-    gr = GitRepo.clone(src, path)
-    idx_list = gr.get_indexed_files()
-
-    runner = Runner()
-    out = runner(['git', 'ls-files'], cwd=path)
-    out_list = list(filter(bool, out[0].split('\n')))
-
-    for item in idx_list:
-        assert_in(item, out_list, "%s not found in output of git ls-files in %s" % (item, path))
-    for item in out_list:
-        assert_in(item, idx_list, "%s not found in output of get_indexed_files in %s" % (item, path))
-
-
-@with_tree([
-    ('empty', ''),
-    ('d1', (
-        ('empty', ''),
-        ('d2',
-            (('empty', ''),
-             )),
-        )),
-    ])
-@assert_cwd_unchanged(ok_to_chdir=True)
-def test_normalize_path(git_path):
-
-    gr = GitRepo(git_path)
-
-    # cwd is currently outside the repo, so any relative path
-    # should be interpreted as relative to `annex_path`
-    assert_raises(FileNotInRepositoryError, _normalize_path, gr.path, getpwd())
-
-    result = _normalize_path(gr.path, "testfile")
-    eq_(result, "testfile", "_normalize_path() returned %s" % result)
-
-    # result = _normalize_path(gr.path, opj('.', 'testfile'))
-    # eq_(result, "testfile", "_normalize_path() returned %s" % result)
-    #
-    # result = _normalize_path(gr.path, opj('testdir', '..', 'testfile'))
-    # eq_(result, "testfile", "_normalize_path() returned %s" % result)
-    # Note: By now, normpath within normalize_paths() is disabled, therefore
-    # disable these tests.
-
-    result = _normalize_path(gr.path, opj('testdir', 'testfile'))
-    eq_(result, opj("testdir", "testfile"), "_normalize_path() returned %s" % result)
-
-    result = _normalize_path(gr.path, opj(git_path, "testfile"))
-    eq_(result, "testfile", "_normalize_path() returned %s" % result)
-
-    # now we are inside, so
-    # OLD PHILOSOPHY: relative paths are relative to cwd and have
-    # to be converted to be relative to annex_path
-    # NEW PHILOSOPHY: still relative to repo! unless starts with . (curdir) or .. (pardir)
-    with chpwd(opj(git_path, 'd1', 'd2')):
-
-        result = _normalize_path(gr.path, "testfile")
-        eq_(result, 'testfile', "_normalize_path() returned %s" % result)
-
-        # if not joined as directory name but just a prefix to the filename, should
-        # behave correctly
-        for d in (curdir, pardir):
-            result = _normalize_path(gr.path, d + "testfile")
-            eq_(result, d + 'testfile', "_normalize_path() returned %s" % result)
-
-        result = _normalize_path(gr.path, opj(curdir, "testfile"))
-        eq_(result, opj('d1', 'd2', 'testfile'), "_normalize_path() returned %s" % result)
-
-        result = _normalize_path(gr.path, opj(pardir, 'testfile'))
-        eq_(result, opj('d1', 'testfile'), "_normalize_path() returned %s" % result)
-
-        assert_raises(FileNotInRepositoryError, _normalize_path, gr.path, opj(git_path, '..', 'outside'))
-
-        result = _normalize_path(gr.path, opj(git_path, 'd1', 'testfile'))
-        eq_(result, opj('d1', 'testfile'), "_normalize_path() returned %s" % result)
-
-
-def test_GitRepo_files_decorator():
-
-    class testclass(object):
-        def __init__(self):
-            self.path = opj('some', 'where')
-
-        # TODO
-        # yoh:  logic is alien to me below why to have two since both look identical!
-        @normalize_paths
-        def decorated_many(self, files):
-            return files
-
-        @normalize_paths
-        def decorated_one(self, file_):
-            return file_
-
-    test_instance = testclass()
-
-    # When a single file passed -- single path returned
-    obscure_filename = get_most_obscure_supported_name()
-    file_to_test = opj(test_instance.path, 'deep', obscure_filename)
-    # file doesn't exist
-    eq_(test_instance.decorated_one(file_to_test),
-                 _normalize_path(test_instance.path, file_to_test))
-    eq_(test_instance.decorated_one(file_to_test),
-                 _normalize_path(test_instance.path, file_to_test))
-
-    file_to_test = obscure_filename
-    eq_(test_instance.decorated_many(file_to_test),
-                 _normalize_path(test_instance.path, file_to_test))
-    eq_(test_instance.decorated_one(file_to_test),
-                 _normalize_path(test_instance.path, file_to_test))
-
-
-    file_to_test = opj(obscure_filename, 'beyond', 'obscure')
-    eq_(test_instance.decorated_many(file_to_test),
-                 _normalize_path(test_instance.path, file_to_test))
-
-    file_to_test = opj(getpwd(), 'somewhere', 'else', obscure_filename)
-    assert_raises(FileNotInRepositoryError, test_instance.decorated_many,
-                  file_to_test)
-
-    # If a list passed -- list returned
-    files_to_test = ['now', opj('a list', 'of'), 'paths']
-    expect = []
-    for item in files_to_test:
-        expect.append(_normalize_path(test_instance.path, item))
-    eq_(test_instance.decorated_many(files_to_test), expect)
-
-    eq_(test_instance.decorated_many(''), [])
-
-    assert_raises(ValueError, test_instance.decorated_many, 1)
-    assert_raises(ValueError, test_instance.decorated_one, 1)
-
-
-@skip_if_no_network
-@with_testrepos(flavors=local_testrepo_flavors)
-@with_tempfile
-def test_GitRepo_remote_add(orig_path, path):
-
-    gr = GitRepo.clone(orig_path, path)
-    out = gr.get_remotes()
-    assert_in('origin', out)
-    eq_(len(out), 1)
-    gr.add_remote('github', 'git://github.com/datalad/testrepo--basic--r1')
-    out = gr.get_remotes()
-    assert_in('origin', out)
-    assert_in('github', out)
-    eq_(len(out), 2)
-    eq_('git://github.com/datalad/testrepo--basic--r1', gr.config['remote.github.url'])
-
-
-@with_testrepos(flavors=local_testrepo_flavors)
-@with_tempfile
-def test_GitRepo_remote_remove(orig_path, path):
-
-    gr = GitRepo.clone(orig_path, path)
-    gr.add_remote('github', 'git://github.com/datalad/testrepo--basic--r1')
-    gr.remove_remote('github')
-    out = gr.get_remotes()
-    eq_(len(out), 1)
-    assert_in('origin', out)
-
-
-@with_testrepos(flavors=local_testrepo_flavors)
-@with_tempfile
-def test_GitRepo_get_remote_url(orig_path, path):
-
-    gr = GitRepo.clone(orig_path, path)
-    gr.add_remote('github', 'git://github.com/datalad/testrepo--basic--r1')
-    eq_(gr.get_remote_url('origin'), orig_path)
-    eq_(gr.get_remote_url('github'),
-                 'git://github.com/datalad/testrepo--basic--r1')
-
-
-@with_testrepos(flavors=local_testrepo_flavors)
-@with_tempfile
-@with_tempfile
-def test_GitRepo_pull(test_path, orig_path, clone_path):
-
-    origin = GitRepo.clone(test_path, orig_path)
-    clone = GitRepo.clone(orig_path, clone_path)
-    filename = get_most_obscure_supported_name()
-
-    with open(opj(orig_path, filename), 'w') as f:
-        f.write("New file.")
-    origin.add(filename)
-    origin.commit("new file added.")
-    clone.pull()
-    assert_true(exists(opj(clone_path, filename)))
-
-    # While at it, let's test _get_remotes_having_commit a bit
-    clone.add_remote("very_origin", test_path)
-    clone.fetch("very_origin")
-    assert_equal(
-        clone._get_remotes_having_commit(clone.get_hexsha()),
-        ['origin']
-    )
-    prev_commit = clone.get_hexsha('HEAD^')
-    assert_equal(
-        set(clone._get_remotes_having_commit(prev_commit)),
-        {'origin', 'very_origin'}
-    )
-
-
-@with_testrepos(flavors=local_testrepo_flavors)
-@with_tempfile
-@with_tempfile
-def test_GitRepo_fetch(test_path, orig_path, clone_path):
-
-    origin = GitRepo.clone(test_path, orig_path)
-    clone = GitRepo.clone(orig_path, clone_path)
-    filename = get_most_obscure_supported_name()
-
-    origin.checkout("new_branch", ['-b'])
-    with open(opj(orig_path, filename), 'w') as f:
-        f.write("New file.")
-    origin.add(filename)
-    origin.commit("new file added.")
-
-    fetched = clone.fetch(remote='origin')
-    # test FetchInfo list returned by fetch
-    eq_([u'origin/' + clone.get_active_branch(), u'origin/new_branch'],
-        [commit.name for commit in fetched])
-
-    ok_clean_git(clone.path, annex=False)
-    assert_in("origin/new_branch", clone.get_remote_branches())
-    assert_in(filename, clone.get_files("origin/new_branch"))
-    assert_false(exists(opj(clone_path, filename)))  # not checked out
-
-    # create a remote without an URL:
-    origin.add_remote('not-available', 'git://example.com/not/existing')
-    origin.config.unset('remote.not-available.url', where='local')
-
-    # fetch without provided URL
-    fetched = origin.fetch('not-available')
-    # nothing was done, nothing returned:
-    eq_([], fetched)
-
-
-
-@skip_ssh
-@with_testrepos('.*basic.*', flavors=['local'])
-@with_tempfile
-def test_GitRepo_ssh_fetch(remote_path, repo_path):
-    from datalad import ssh_manager
-
-    remote_repo = GitRepo(remote_path, create=False)
-    url = "ssh://localhost" + abspath(remote_path)
-    socket_path = opj(ssh_manager.socket_dir, get_connection_hash('localhost'))
-    repo = GitRepo(repo_path, create=True)
-    repo.add_remote("ssh-remote", url)
-
-    # we don't know any branches of the remote:
-    eq_([], repo.get_remote_branches())
-
-    fetched = repo.fetch(remote="ssh-remote")
-    assert_in('ssh-remote/master', [commit.name for commit in fetched])
-    ok_clean_git(repo)
-
-    # the connection is known to the SSH manager, since fetch() requested it:
-    assert_in(socket_path, ssh_manager._connections)
-    # and socket was created:
-    ok_(exists(socket_path))
-
-    # we actually fetched it:
-    assert_in('ssh-remote/master', repo.get_remote_branches())
-
-
-@skip_ssh
-@with_tempfile
-@with_tempfile
-def test_GitRepo_ssh_pull(remote_path, repo_path):
-    from datalad import ssh_manager
-
-    remote_repo = GitRepo(remote_path, create=True)
-    url = "ssh://localhost" + abspath(remote_path)
-    socket_path = opj(ssh_manager.socket_dir, get_connection_hash('localhost'))
-    repo = GitRepo(repo_path, create=True)
-    repo.add_remote("ssh-remote", url)
-
-    # modify remote:
-    remote_repo.checkout("ssh-test", ['-b'])
-    with open(opj(remote_repo.path, "ssh_testfile.dat"), "w") as f:
-        f.write("whatever")
-    remote_repo.add("ssh_testfile.dat")
-    remote_repo.commit("ssh_testfile.dat added.")
-
-    # file is not locally known yet:
-    assert_not_in("ssh_testfile.dat", repo.get_indexed_files())
-
-    # pull changes:
-    repo.pull(remote="ssh-remote", refspec=remote_repo.get_active_branch())
-    ok_clean_git(repo.path, annex=False)
-
-    # the connection is known to the SSH manager, since fetch() requested it:
-    assert_in(socket_path, ssh_manager._connections)
-    # and socket was created:
-    ok_(exists(socket_path))
-
-    # we actually pulled the changes
-    assert_in("ssh_testfile.dat", repo.get_indexed_files())
-
-
-@skip_ssh
-@with_tempfile
-@with_tempfile
-def test_GitRepo_ssh_push(repo_path, remote_path):
-    from datalad import ssh_manager
-
-    remote_repo = GitRepo(remote_path, create=True)
-    url = "ssh://localhost" + abspath(remote_path)
-    socket_path = opj(ssh_manager.socket_dir, get_connection_hash('localhost'))
-    repo = GitRepo(repo_path, create=True)
-    repo.add_remote("ssh-remote", url)
-
-    # modify local repo:
-    repo.checkout("ssh-test", ['-b'])
-    with open(opj(repo.path, "ssh_testfile.dat"), "w") as f:
-        f.write("whatever")
-    repo.add("ssh_testfile.dat")
-    repo.commit("ssh_testfile.dat added.")
-
-    # file is not known to the remote yet:
-    assert_not_in("ssh_testfile.dat", remote_repo.get_indexed_files())
-
-    # push changes:
-    pushed = repo.push(remote="ssh-remote", refspec="ssh-test")
-    # test PushInfo object for
-    assert_in("ssh-remote/ssh-test", [commit.remote_ref.name for commit in pushed])
-
-    # the connection is known to the SSH manager, since fetch() requested it:
-    assert_in(socket_path, ssh_manager._connections)
-    # and socket was created:
-    ok_(exists(socket_path))
-
-    # remote now knows the changes:
-    assert_in("ssh-test", remote_repo.get_branches())
-    assert_in("ssh_testfile.dat", remote_repo.get_files("ssh-test"))
-
-    # amend to make it require "--force":
-    repo.commit("amended", options=['--amend'])
-    # push without --force should yield an error:
-    pushed = repo.push(remote="ssh-remote", refspec="ssh-test")
-    assert_in("[rejected] (non-fast-forward)", pushed[0].summary)
-    # now push using force:
-    repo.push(remote="ssh-remote", refspec="ssh-test", force=True)
-    # correct commit message in remote:
-    assert_in("amended",
-              list(remote_repo.get_branch_commits('ssh-test'))[-1].summary)
-
-
-@with_tempfile
-@with_tempfile
-def test_GitRepo_push_n_checkout(orig_path, clone_path):
-
-    origin = GitRepo(orig_path)
-    clone = GitRepo.clone(orig_path, clone_path)
-    filename = get_most_obscure_supported_name()
-
-    with open(opj(clone_path, filename), 'w') as f:
-        f.write("New file.")
-    clone.add(filename)
-    clone.commit("new file added.")
-    # TODO: need checkout first:
-    clone.push('origin', '+master:new-branch')
-    origin.checkout('new-branch')
-    assert_true(exists(opj(orig_path, filename)))
-
-
-@with_tempfile
-@with_tempfile
-@with_tempfile
-def test_GitRepo_remote_update(path1, path2, path3):
+import os.path as op
 
-    git1 = GitRepo(path1)
-    git2 = GitRepo(path2)
-    git3 = GitRepo(path3)
-
-    git1.add_remote('git2', path2)
-    git1.add_remote('git3', path3)
-
-    # Setting up remote 'git2'
-    with open(opj(path2, 'masterfile'), 'w') as f:
-        f.write("git2 in master")
-    git2.add('masterfile')
-    git2.commit("Add something to master.")
-    git2.checkout('branch2', ['-b'])
-    with open(opj(path2, 'branch2file'), 'w') as f:
-        f.write("git2 in branch2")
-    git2.add('branch2file')
-    git2.commit("Add something to branch2.")
-
-    # Setting up remote 'git3'
-    with open(opj(path3, 'masterfile'), 'w') as f:
-        f.write("git3 in master")
-    git3.add('masterfile')
-    git3.commit("Add something to master.")
-    git3.checkout('branch3', ['-b'])
-    with open(opj(path3, 'branch3file'), 'w') as f:
-        f.write("git3 in branch3")
-    git3.add('branch3file')
-    git3.commit("Add something to branch3.")
-
-    git1.update_remote()
-
-    # checkouts are 'tests' themselves, since they'll raise CommandError
-    # if something went wrong
-    git1.checkout('branch2')
-    git1.checkout('branch3')
+import pytest
 
-    branches1 = git1.get_branches()
-    eq_({'branch2', 'branch3'}, set(branches1))
+import datalad.utils as ut
+from datalad.api import (
+    create,
+    install,
+    save,
+)
+from datalad.distribution.dataset import Dataset
+from datalad.support.annexrepo import AnnexRepo
+from datalad.support.exceptions import CommandError
+from datalad.tests.utils_pytest import (
+    DEFAULT_BRANCH,
+    OBSCURE_FILENAME,
+    SkipTest,
+    assert_in,
+    assert_in_results,
+    assert_not_in,
+    assert_raises,
+    assert_repo_status,
+    assert_result_count,
+    assert_status,
+    chpwd,
+    create_tree,
+    eq_,
+    known_failure,
+    known_failure_windows,
+    maybe_adjust_repo,
+    neq_,
+    ok_,
+    patch,
+    skip_if_adjusted_branch,
+    skip_wo_symlink_capability,
+    swallow_logs,
+    swallow_outputs,
+    with_tempfile,
+    with_tree,
+)
+from datalad.utils import (
+    Path,
+    ensure_list,
+    rmtree,
+)
+
+tree_arg = dict(tree={'test.txt': 'some',
+                      'test_annex.txt': 'some annex',
+                      'test1.dat': 'test file 1',
+                      'test2.dat': 'test file 2',
+                      OBSCURE_FILENAME: 'blobert',
+                      'dir': {'testindir': 'someother',
+                              OBSCURE_FILENAME: 'none'},
+                      'dir2': {'testindir3': 'someother3'}})
 
 
-# TODO: Why was it "flavors=local_testrepo_flavors" ? What's the windows issue here?
-@with_testrepos('.*git.*', flavors=['clone'])
-@with_tempfile
-def test_GitRepo_get_files(url, path):
+@with_tempfile()
+def test_save(path=None):
 
-    gr = GitRepo.clone(url, path)
+    ds = Dataset(path).create(annex=False)
 
-    # get the expected files via os for comparison:
-    os_files = set()
-    for (dirpath, dirnames, filenames) in os.walk(path):
-        rel_dir = os.path.relpath(dirpath, start=path)
-        if rel_dir.startswith(".git"):
-            continue
-        for file_ in filenames:
-            file_path = os.path.normpath(opj(rel_dir, file_))
-            os_files.add(file_path)
-
-    # get the files via GitRepo:
-    local_files = set(gr.get_files())
-    remote_files = set(gr.get_files(branch="origin/master"))
-
-    eq_(local_files, set(gr.get_indexed_files()))
-    eq_(local_files, remote_files)
-    eq_(local_files, os_files)
-
-    # create a different branch:
-    gr.checkout('new_branch', ['-b'])
-    filename = 'another_file.dat'
-    with open(opj(path, filename), 'w') as f:
+    with open(op.join(path, "new_file.tst"), "w") as f:
         f.write("something")
-    gr.add(filename)
-    gr.commit("Added.")
 
-    # now get the files again:
-    local_files = set(gr.get_files())
-    eq_(local_files, os_files.union({filename}))
-    # retrieve remote branch again, which should not have changed:
-    remote_files = set(gr.get_files(branch="origin/master"))
-    eq_(remote_files, os_files)
-    eq_(set([filename]), local_files.difference(remote_files))
-
-    # switch back and query non-active branch:
-    gr.checkout('master')
-    local_files = set(gr.get_files())
-    branch_files = set(gr.get_files(branch="new_branch"))
-    eq_(set([filename]), branch_files.difference(local_files))
+    ds.repo.add("new_file.tst", git=True)
+    ok_(ds.repo.dirty)
 
+    ds.save(message="add a new file")
+    assert_repo_status(path, annex=isinstance(ds.repo, AnnexRepo))
 
-@with_tree(tree={
-    'd1': {'f1': 'content1',
-           'f2': 'content2'},
-    'file': 'content3',
-    'd2': {'f1': 'content1',
-           'f2': 'content2'},
-    'file2': 'content3'
+    with open(op.join(path, "new_file.tst"), "w") as f:
+        f.write("modify")
 
-    })
-def test_GitRepo__get_files_history(path):
+    ok_(ds.repo.dirty)
+    ds.save(message="modified new_file.tst")
+    assert_repo_status(path, annex=isinstance(ds.repo, AnnexRepo))
+
+    # save works without ds and files given in the PWD
+    with open(op.join(path, "new_file.tst"), "w") as f:
+        f.write("rapunzel")
+    with chpwd(path):
+        save(message="love rapunzel")
+    assert_repo_status(path, annex=isinstance(ds.repo, AnnexRepo))
 
-    gr = GitRepo(path, create=True)
-    gr.add('d1')
-    gr.commit("commit d1")
-    #import pdb; pdb.set_trace()
+    # and also without `-a` when things are staged
+    with open(op.join(path, "new_file.tst"), "w") as f:
+        f.write("exotic")
+    ds.repo.add("new_file.tst", git=True)
+    with chpwd(path):
+        save(message="love marsians")
+    assert_repo_status(path, annex=isinstance(ds.repo, AnnexRepo))
 
-    gr.add(['d2', 'file'])
-    gr.commit("commit d2")
+    files = ['one.txt', 'two.txt']
+    for fn in files:
+        with open(op.join(path, fn), "w") as f:
+            f.write(fn)
+
+    ds.save([op.join(path, f) for f in files])
+    # superfluous call to save (alll saved it already), should not fail
+    # but report that nothing was saved
+    assert_status('notneeded', ds.save(message="set of new files"))
+    assert_repo_status(path, annex=isinstance(ds.repo, AnnexRepo))
+
+    # create subdataset
+    subds = ds.create('subds')
+    assert_repo_status(path, annex=isinstance(ds.repo, AnnexRepo))
+    # modify subds
+    with open(op.join(subds.path, "some_file.tst"), "w") as f:
+        f.write("something")
+    subds.save()
+    assert_repo_status(subds.path, annex=isinstance(subds.repo, AnnexRepo))
+    # ensure modified subds is committed
+    ds.save()
+    assert_repo_status(path, annex=isinstance(ds.repo, AnnexRepo))
+
+    # now introduce a change downstairs
+    subds.create('someotherds')
+    assert_repo_status(subds.path, annex=isinstance(subds.repo, AnnexRepo))
+    ok_(ds.repo.dirty)
+    # and save via subdataset path
+    ds.save('subds', version_tag='new_sub')
+    assert_repo_status(path, annex=isinstance(ds.repo, AnnexRepo))
+    tags = ds.repo.get_tags()
+    ok_(len(tags) == 1)
+    eq_(tags[0], dict(hexsha=ds.repo.get_hexsha(), name='new_sub'))
+    # fails when retagged, like git does
+    res = ds.save(version_tag='new_sub', on_failure='ignore')
+    assert_status('error', res)
+    assert_result_count(
+        res, 1,
+        action='save', type='dataset', path=ds.path,
+        message=('cannot tag this version: %s',
+                 "fatal: tag 'new_sub' already exists"))
 
-    # commit containing files of d1
-    d1_commit = next(gr._get_files_history([opj(path, 'd1', 'f1'), opj(path, 'd1', 'f1')]))
-    assert_equal(str(d1_commit.message), 'commit d1\n')
 
-    # commit containing files of d2
-    d2_commit_gen = gr._get_files_history([opj(path, 'd2', 'f1'), opj(path, 'd2', 'f1')])
-    assert_equal(str(next(d2_commit_gen).message), 'commit d2\n')
-    assert_raises(StopIteration, next, d2_commit_gen)  # no more commits with files of d2
+@with_tempfile()
+def test_save_message_file(path=None):
+    ds = Dataset(path).create()
+    with assert_raises(ValueError):
+        ds.save("blah", message="me", message_file="and me")
+
+    create_tree(path, {"foo": "x",
+                       "msg": "add foo"})
+    ds.repo.add("foo")
+    ds.save(message_file=op.join(ds.path, "msg"))
+    # ATTN: Consider corresponding branch so that this check works when we're
+    # on an adjusted branch too (e.g., when this test is executed under
+    # Windows).
+    eq_(ds.repo.format_commit("%s", DEFAULT_BRANCH),
+        "add foo")
 
-    # union of commits containing passed objects
-    commits_union = gr._get_files_history([opj(path, 'd1', 'f1'), opj(path, 'd2', 'f1'), opj(path, 'file')])
-    assert_equal(str(next(commits_union).message), 'commit d2\n')
-    assert_equal(str(next(commits_union).message), 'commit d1\n')
-    assert_raises(StopIteration, next, commits_union)
 
-    # file2 not commited, so shouldn't exist in commit history
-    no_such_commits = gr._get_files_history([opj(path, 'file2')])
-    assert_raises(StopIteration, next, no_such_commits)
+@with_tempfile()
+def check_renamed_file(recursive, annex, path):
+    ds = Dataset(path).create(annex=annex)
+    create_tree(path, {'old': ''})
+    ds.repo.add('old')
+    ds.repo.call_git(["mv"], files=["old", "new"])
+    ds.save(recursive=recursive)
+    assert_repo_status(path)
+
+    # https://github.com/datalad/datalad/issues/6558
+    new = (ds.pathobj / "new")
+    new.unlink()
+    new.mkdir()
+    (new / "file").touch()
+    ds.repo.call_git(["add"], files=[str(new / "file")])
+    ds.save(recursive=recursive)
+    assert_repo_status(path)
+
+
+@pytest.mark.parametrize(
+    "recursive,annex",
+    itertools.product(
+        (False, ),  #, True TODO when implemented
+        (True, False),
+    )
+)
+def test_renamed_file(recursive, annex):
+    check_renamed_file(recursive, annex)
 
 
-@with_testrepos('.*git.*', flavors=local_testrepo_flavors)
 @with_tempfile(mkdir=True)
-@with_tempfile
-def test_GitRepo_get_toppath(repo, tempdir, repo2):
-    reporeal = realpath(repo)
-    eq_(GitRepo.get_toppath(repo, follow_up=False), reporeal)
-    eq_(GitRepo.get_toppath(repo), repo)
-    # Generate some nested directory
-    GitRepo(repo2, create=True)
-    repo2real = realpath(repo2)
-    nested = opj(repo2, "d1", "d2")
-    os.makedirs(nested)
-    eq_(GitRepo.get_toppath(nested, follow_up=False), repo2real)
-    eq_(GitRepo.get_toppath(nested), repo2)
-    # and if not under git, should return None
-    eq_(GitRepo.get_toppath(tempdir), None)
+def test_subdataset_save(path=None):
+    parent = Dataset(path).create()
+    sub = parent.create('sub')
+    assert_repo_status(parent.path)
+    create_tree(parent.path, {
+        "untracked": 'ignore',
+        'sub': {
+            "new": "wanted"}})
+    sub.save('new')
+    # defined state: one untracked, modified (but clean in itself) subdataset
+    assert_repo_status(sub.path)
+    assert_repo_status(parent.path, untracked=['untracked'], modified=['sub'])
+
+    # `save sub` does not save the parent!!
+    with chpwd(parent.path):
+        assert_status('notneeded', save(dataset=sub.path))
+    assert_repo_status(parent.path, untracked=['untracked'], modified=['sub'])
+    # `save -u .` saves the state change in the subdataset,
+    # but leaves any untracked content alone
+    with chpwd(parent.path):
+        assert_status('ok', parent.save(updated=True))
+    assert_repo_status(parent.path, untracked=['untracked'])
+
+    # get back to the original modified state and check that -S behaves in
+    # exactly the same way
+    create_tree(parent.path, {
+        'sub': {
+            "new2": "wanted2"}})
+    sub.save('new2')
+    assert_repo_status(parent.path, untracked=['untracked'], modified=['sub'])
+
+    # https://github.com/datalad/datalad/issues/6843
+    # saving subds within super must not add 2nd copy of the submodule within .gitmodules
+    with chpwd(sub.path):
+        # op.sep is critical to trigger saving within (although should not
+        # be relevant sice no changes within sub)
+        res = save(dataset="^", path=sub.path + op.sep)
+    assert_repo_status(parent.path, untracked=['untracked'])
+    git_modules = (parent.pathobj / ".gitmodules")
+    # there was nothing to do for .gitmodules
+    # TODO: enable assert_result_count(res, 0, path=str(git_modules))
+    # more thorough test that it also was not modified.
+    # ensure that .gitmodules does not have duplicate entries
+    submodules = [
+        l.strip()
+        for l in git_modules.read_text().splitlines()
+        if l.strip().split(' ', 1)[0] == '[submodule'
+    ]
+    assert len(submodules) == 1
 
 
 @with_tempfile(mkdir=True)
-def test_GitRepo_dirty(path):
+def test_subsuperdataset_save(path=None):
+    # Verify that when invoked without recursion save does not
+    # cause querying of subdatasets of the subdataset
+    # see https://github.com/datalad/datalad/issues/4523
+    parent = Dataset(path).create()
+    # Create 3 levels of subdatasets so later to check operation
+    # with or without --dataset being specified
+    sub1 = parent.create('sub1')
+    sub2 = parent.create(sub1.pathobj / 'sub2')
+    sub3 = parent.create(sub2.pathobj / 'sub3')
+    assert_repo_status(path)
+    # now we will lobotomize that sub3 so git would fail if any query is performed.
+    (sub3.pathobj / '.git' / 'config').chmod(0o000)
+    try:
+        sub3.repo.call_git(['ls-files'], read_only=True)
+        raise SkipTest
+    except CommandError:
+        # desired outcome
+        pass
+    # the call should proceed fine since neither should care about sub3
+    # default is no recursion
+    parent.save('sub1')
+    sub1.save('sub2')
+    assert_raises(CommandError, parent.save, 'sub1', recursive=True)
+    # and should not fail in the top level superdataset
+    with chpwd(parent.path):
+        save('sub1')
+    # or in a subdataset above the problematic one
+    with chpwd(sub1.path):
+        save('sub2')
 
-    repo = GitRepo(path, create=True)
-    ok_(not repo.dirty)
 
-    # untracked file
-    with open(opj(path, 'file1.txt'), 'w') as f:
-        f.write('whatever')
-    ok_(repo.dirty)
-    # staged file
-    repo.add('file1.txt')
-    ok_(repo.dirty)
-    # clean again
-    repo.commit("file1.txt added")
-    ok_(not repo.dirty)
-    # modify to be the same
-    with open(opj(path, 'file1.txt'), 'w') as f:
-        f.write('whatever')
-    ok_(not repo.dirty)
-    # modified file
-    with open(opj(path, 'file1.txt'), 'w') as f:
-        f.write('something else')
-    ok_(repo.dirty)
-    # clean again
-    repo.add('file1.txt')
-    repo.commit("file1.txt modified")
-    ok_(not repo.dirty)
+@skip_wo_symlink_capability
+@with_tempfile(mkdir=True)
+def test_symlinked_relpath(path=None):
+    # initially ran into on OSX https://github.com/datalad/datalad/issues/2406
+    os.makedirs(op.join(path, "origin"))
+    dspath = op.join(path, "linked")
+    os.symlink('origin', dspath)
+    ds = Dataset(dspath).create()
+    create_tree(dspath, {
+        "mike1": 'mike1',  # will be added from topdir
+        "later": "later",  # later from within subdir
+        "d": {
+            "mike2": 'mike2', # to be added within subdir
+        }
+    })
 
-    # TODO: submodules
+    # in the root of ds
+    with chpwd(dspath):
+        ds.repo.add("mike1", git=True)
+        ds.save(message="committing", path="./mike1")
+
+    # Let's also do in subdirectory as CWD, check that relative path
+    # given to a plain command (not dataset method) are treated as
+    # relative to CWD
+    with chpwd(op.join(dspath, 'd')):
+        save(dataset=ds.path,
+             message="committing",
+             path="mike2")
+
+        later = op.join(op.pardir, "later")
+        ds.repo.add(later, git=True)
+        save(dataset=ds.path, message="committing", path=later)
 
+    assert_repo_status(dspath)
 
 
+@skip_wo_symlink_capability
 @with_tempfile(mkdir=True)
-def test_GitRepo_get_merge_base(src):
-    repo = GitRepo(src, create=True)
-    with open(opj(src, 'file.txt'), 'w') as f:
-        f.write('load')
-    repo.add('*')
-    repo.commit('committing')
-
-    assert_raises(ValueError, repo.get_merge_base, [])
-    branch1 = repo.get_active_branch()
-    branch1_hexsha = repo.get_hexsha()
-    eq_(len(branch1_hexsha), 40)
-    eq_(repo.get_merge_base(branch1), branch1_hexsha)
-
-    # Let's create a detached branch
-    branch2 = "_detach_"
-    repo.checkout(branch2, options=["--orphan"])
-    # it will have all the files
-    # Must not do:  https://github.com/gitpython-developers/GitPython/issues/375
-    # repo.git_add('.')
-    repo.add('*')
-    # NOTE: fun part is that we should have at least a different commit message
-    # so it results in a different checksum ;)
-    repo.commit("committing again")
-    assert(repo.get_indexed_files())  # we did commit
-    assert(repo.get_merge_base(branch1) is None)
-    assert(repo.get_merge_base([branch2, branch1]) is None)
-
-    # Let's merge them up -- then merge base should match the master
-    repo.merge(branch1, allow_unrelated=True)
-    eq_(repo.get_merge_base(branch1), branch1_hexsha)
-
-    # if points to some empty/non-existing branch - should also be None
-    assert(repo.get_merge_base(['nonexistent', branch2]) is None)
-
+def test_bf1886(path=None):
+    parent = Dataset(path).create()
+    parent.create('sub')
+    assert_repo_status(parent.path)
+    # create a symlink pointing down to the subdataset, and add it
+    os.symlink('sub', op.join(parent.path, 'down'))
+    parent.save('down')
+    assert_repo_status(parent.path)
+    # now symlink pointing up
+    os.makedirs(op.join(parent.path, 'subdir', 'subsubdir'))
+    os.symlink(op.join(op.pardir, 'sub'), op.join(parent.path, 'subdir', 'up'))
+    parent.save(op.join('subdir', 'up'))
+    # 'all' to avoid the empty dir being listed
+    assert_repo_status(parent.path, untracked_mode='all')
+    # now symlink pointing 2xup, as in #1886
+    os.symlink(
+        op.join(op.pardir, op.pardir, 'sub'),
+        op.join(parent.path, 'subdir', 'subsubdir', 'upup'))
+    parent.save(op.join('subdir', 'subsubdir', 'upup'))
+    assert_repo_status(parent.path)
+    # simultaneously add a subds and a symlink pointing to it
+    # create subds, but don't register it
+    create(op.join(parent.path, 'sub2'))
+    os.symlink(
+        op.join(op.pardir, op.pardir, 'sub2'),
+        op.join(parent.path, 'subdir', 'subsubdir', 'upup2'))
+    parent.save(['sub2', op.join('subdir', 'subsubdir', 'upup2')])
+    assert_repo_status(parent.path)
+    # full replication of #1886: the above but be in subdir of symlink
+    # with no reference dataset
+    create(op.join(parent.path, 'sub3'))
+    os.symlink(
+        op.join(op.pardir, op.pardir, 'sub3'),
+        op.join(parent.path, 'subdir', 'subsubdir', 'upup3'))
+    # need to use absolute paths
+    with chpwd(op.join(parent.path, 'subdir', 'subsubdir')):
+        save([op.join(parent.path, 'sub3'),
+              op.join(parent.path, 'subdir', 'subsubdir', 'upup3')])
+    assert_repo_status(parent.path)
+
+
+@with_tree({
+    '1': '',
+    '2': '',
+    '3': ''})
+def test_gh2043p1(path=None):
+    # this tests documents the interim agreement on what should happen
+    # in the case documented in gh-2043
+    ds = Dataset(path).create(force=True)
+    ds.save('1')
+    assert_repo_status(ds.path, untracked=['2', '3'])
+    ds.unlock('1')
+    assert_repo_status(
+        ds.path,
+        # on windows we are in an unlocked branch by default, hence
+        # we would see no change
+        modified=[] if ds.repo.is_managed_branch() else ['1'],
+        untracked=['2', '3'])
+    # save(.) should recommit unlocked file, and not touch anything else
+    # this tests the second issue in #2043
+    with chpwd(path):
+        # only save modified bits
+        save(path='.', updated=True)
+    # state of the file (unlocked/locked) is committed as well, and the
+    # test doesn't lock the file again
+    assert_repo_status(ds.path, untracked=['2', '3'])
+    with chpwd(path):
+        # but when a path is given, anything that matches this path
+        # untracked or not is added/saved
+        save(path='.')
+    # state of the file (unlocked/locked) is committed as well, and the
+    # test doesn't lock the file again
+    assert_repo_status(ds.path)
+
+
+@with_tree({
+    'staged': 'staged',
+    'untracked': 'untracked'})
+def test_bf2043p2(path=None):
+    ds = Dataset(path).create(force=True)
+    ds.repo.add('staged')
+    assert_repo_status(ds.path, added=['staged'], untracked=['untracked'])
+    # save -u does not commit untracked content
+    # this tests the second issue in #2043
+    with chpwd(path):
+        save(updated=True)
+    assert_repo_status(ds.path, untracked=['untracked'])
 
-@with_tempfile(mkdir=True)
-def test_GitRepo_git_get_branch_commits(src):
 
-    repo = GitRepo(src, create=True)
-    with open(opj(src, 'file.txt'), 'w') as f:
-        f.write('load')
-    repo.add('*')
-    repo.commit('committing')
-
-    commits_default = list(repo.get_branch_commits())
-    commits = list(repo.get_branch_commits('master'))
-    eq_(commits, commits_default)
-
-    eq_(len(commits), 1)
-    commits_stop0 = list(repo.get_branch_commits(stop=commits[0].hexsha))
-    eq_(commits_stop0, [])
-    commits_hexsha = list(repo.get_branch_commits(value='hexsha'))
-    commits_hexsha_left = list(repo.get_branch_commits(value='hexsha', limit='left-only'))
-    eq_([commits[0].hexsha], commits_hexsha)
-    # our unittest is rudimentary ;-)
-    eq_(commits_hexsha_left, commits_hexsha)
-
-    raise SkipTest("TODO: Was more of a smoke test -- improve testing")
-
-
-def test_split_remote_branch():
-    r, b = split_remote_branch("MyRemote/SimpleBranch")
-    eq_(r, "MyRemote")
-    eq_(b, "SimpleBranch")
-    r, b = split_remote_branch("MyRemote/Branch/with/slashes")
-    eq_(r, "MyRemote")
-    eq_(b, "Branch/with/slashes")
-    assert_raises(AssertionError, split_remote_branch, "NoSlashesAtAll")
-    assert_raises(AssertionError, split_remote_branch, "TrailingSlash/")
-
-
-def test_get_added_files_commit_msg():
-    f = GitRepo._get_added_files_commit_msg
-    eq_(f([]), 'No files were added')
-    eq_(f(["f1"]), 'Added 1 file\n\nFiles:\nf1')
-    eq_(f(["f1", "f2"]), 'Added 2 files\n\nFiles:\nf1\nf2')
+@with_tree({
+    OBSCURE_FILENAME + u'_staged': 'staged',
+    OBSCURE_FILENAME + u'_untracked': 'untracked'})
+def test_encoding(path=None):
+    staged = OBSCURE_FILENAME + u'_staged'
+    untracked = OBSCURE_FILENAME + u'_untracked'
+    ds = Dataset(path).create(force=True)
+    ds.repo.add(staged)
+    assert_repo_status(ds.path, added=[staged], untracked=[untracked])
+    ds.save(updated=True)
+    assert_repo_status(ds.path, untracked=[untracked])
+
+
+@with_tree(**tree_arg)
+def test_add_files(path=None):
+    ds = Dataset(path).create(force=True)
+
+    test_list_1 = ['test_annex.txt']
+    test_list_2 = ['test.txt']
+    test_list_3 = ['test1.dat', 'test2.dat']
+    test_list_4 = [op.join('dir', 'testindir'),
+                   op.join('dir', OBSCURE_FILENAME)]
+
+    for arg in [(test_list_1[0], False),
+                (test_list_2[0], True),
+                (test_list_3, False),
+                (test_list_4, False)]:
+        # special case 4: give the dir:
+        if arg[0] == test_list_4:
+            result = ds.save('dir', to_git=arg[1])
+            status = ds.repo.get_content_annexinfo(['dir'])
+        else:
+            result = ds.save(arg[0], to_git=arg[1])
+            for a in ensure_list(arg[0]):
+                assert_result_count(result, 1, path=str(ds.pathobj / a))
+            status = ds.repo.get_content_annexinfo(
+                ut.Path(p) for p in ensure_list(arg[0]))
+        for f, p in status.items():
+            if arg[1]:
+                assert p.get('key', None) is None, f
+            else:
+                assert p.get('key', None) is not None, f
 
 
+@with_tree(**tree_arg)
 @with_tempfile(mkdir=True)
-@with_tempfile(mkdir=True)
-def test_git_custom_calls(path, path2):
-    # we need a GitRepo instance
-    repo = GitRepo(path, create=True)
-    with open(opj(path, "cc_test.dat"), 'w') as f:
-        f.write("test_git_custom_calls")
-
-    out, err = repo._gitpy_custom_call('add', 'cc_test.dat')
-
-    # actually executed:
-    assert_in("cc_test.dat", repo.get_indexed_files())
-    ok_(repo.dirty)
-
-    # call using cmd_options:
-    out, err = repo._gitpy_custom_call('commit',
-                                       cmd_options={'m': 'added file'})
-    ok_clean_git(path, annex=False)
-    # check output:
-    assert_in("1 file changed", out)
-    assert_in("cc_test.dat", out)
-    eq_('', err)
-
-    # impossible 'add' call should raise ...
-    assert_raises(GitCommandError, repo._gitpy_custom_call,
-                  'add', 'not_existing', expect_fail=False)
-    # .. except we expect it to fail:
-    repo._gitpy_custom_call('add', 'not_existing', expect_fail=True)
-
-    # log outputs:
-    with swallow_logs(new_level=logging.DEBUG) as cm:
-        out, err = repo._gitpy_custom_call('status',
-                                           log_stdout=True,
-                                           log_stderr=True)
-
-        assert_in("On branch master", out)
-        assert_in("nothing to commit", out)
-        eq_("", err)
-        for line in out.splitlines():
-            assert_in("stdout| " + line, cm.out)
-
-    # don't log outputs:
-    with swallow_logs(new_level=logging.DEBUG) as cm:
-        out, err = repo._gitpy_custom_call('status',
-                                           log_stdout=False,
-                                           log_stderr=False)
-
-        assert_in("On branch master", out)
-        assert_in("nothing to commit", out)
-        eq_("", err)
-        eq_("", cm.out)
-
-    # use git_options:
-    # Note: 'path2' doesn't contain a git repository
-    with assert_raises(GitCommandError) as cm:
-        repo._gitpy_custom_call('status', git_options={'C': path2})
-    assert_in("-C %s status" % path2, str(cm.exception))
-    assert_in("fatal: Not a git repository", str(cm.exception))
-
-    # TODO: How to test 'env'?
+def test_add_subdataset(path=None, other=None):
+    subds = create(op.join(path, 'dir'), force=True)
+    ds = create(path, force=True)
+    ok_(subds.repo.dirty)
+    ok_(ds.repo.dirty)
+    assert_not_in('dir', ds.subdatasets(result_xfm='relpaths'))
+    # "add everything in subds to subds"
+    save(dataset=subds.path)
+    assert_repo_status(subds.path)
+    assert_not_in('dir', ds.subdatasets(result_xfm='relpaths'))
+    # but with a base directory we add the dataset subds as a subdataset
+    # to ds
+    res = ds.save(subds.path)
+    assert_in_results(res, action="add", path=subds.path, refds=ds.path)
+    res = ds.subdatasets()
+    assert_result_count(res, 1)
+    assert_result_count(
+        res, 1,
+        # essentials
+        path=op.join(ds.path, 'dir'),
+        gitmodule_url='./dir',
+        gitmodule_name='dir',
+    )
+    #  create another one
+    other = create(other)
+    # install into superdataset, but don't add
+    other_clone = install(source=other.path, path=op.join(ds.path, 'other'))
+    # little dance to get the revolution-type dataset
+    other_clone = Dataset(other_clone.path)
+    ok_(other_clone.is_installed)
+    assert_not_in('other', ds.subdatasets(result_xfm='relpaths'))
+    # now add, it should pick up the source URL
+    ds.save('other')
+    # and that is why, we can reobtain it from origin
+    ds.drop('other', what='all', reckless='kill', recursive=True)
+    ok_(not other_clone.is_installed())
+    ds.get('other')
+    ok_(other_clone.is_installed())
+
+
+# CommandError: command '['git', '-c', 'receive.autogc=0', '-c', 'gc.auto=0', 'annex', 'add', '--json', '--', 'empty', 'file.txt']' failed with exitcode 1
+# Failed to run ['git', '-c', 'receive.autogc=0', '-c', 'gc.auto=0', 'annex', 'add', '--json', '--', 'empty', 'file.txt'] under 'C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\datalad_temp_tree_j2mk92y3'. Exit code=1.
+@known_failure_windows
+@with_tree(tree={
+    'file.txt': 'some text',
+    'empty': '',
+    'file2.txt': 'some text to go to annex',
+    '.gitattributes': '* annex.largefiles=(not(mimetype=text/*))'}
+)
+def test_add_mimetypes(path=None):
+    ds = Dataset(path).create(force=True)
+    ds.repo.add('.gitattributes')
+    ds.repo.commit('added attributes to git explicitly')
+    # now test that those files will go into git/annex correspondingly
+    # WINDOWS FAILURE NEXT
+    __not_tested__ = ds.save(['file.txt', 'empty'])
+    assert_repo_status(path, untracked=['file2.txt'])
+    # But we should be able to force adding file to annex when desired
+    ds.save('file2.txt', to_git=False)
+    # check annex file status
+    annexinfo = ds.repo.get_content_annexinfo()
+    for path, in_annex in (
+           # Empty one considered to be  application/octet-stream
+           # i.e. non-text
+           ('empty', True),
+           ('file.txt', False),
+           ('file2.txt', True)):
+        # low-level API report -> repo path reference, no ds path
+        p = ds.repo.pathobj / path
+        assert_in(p, annexinfo)
+        if in_annex:
+            assert_in('key', annexinfo[p], p)
+        else:
+            assert_not_in('key', annexinfo[p], p)
 
 
-@with_testrepos(flavors=['local'])
 @with_tempfile(mkdir=True)
-def test_get_tracking_branch(o_path, c_path):
-
-    clone = GitRepo.clone(o_path, c_path)
-    # Note, that the default branch might differ even if it is always 'master'.
-    # For direct mode annex repositories it would then be "annex/direct/master"
-    # for example. Therefore use whatever branch is checked out by default:
-    master_branch = clone.get_active_branch()
-    ok_(master_branch)
-
-    eq_(('origin', 'refs/heads/' + master_branch),
-        clone.get_tracking_branch())
-
-    clone.checkout('new_branch', ['-b'])
-
-    eq_((None, None), clone.get_tracking_branch())
+def test_gh1597(path=None):
+    ds = Dataset(path).create()
+    sub = ds.create('sub')
+    res = ds.subdatasets()
+    assert_result_count(res, 1, path=sub.path)
+    # now modify .gitmodules with another command
+    ds.subdatasets(contains=sub.path, set_property=[('this', 'that')])
+    # now modify low-level
+    with open(op.join(ds.path, '.gitmodules'), 'a') as f:
+        f.write('\n')
+    assert_repo_status(ds.path, modified=['.gitmodules'])
+    ds.save('.gitmodules')
+    # must not come under annex management
+    assert_not_in(
+        'key',
+        ds.repo.get_content_annexinfo(paths=['.gitmodules']).popitem()[1])
 
-    eq_(('origin', 'refs/heads/' + master_branch),
-        clone.get_tracking_branch(master_branch))
 
-
-@with_testrepos('submodule_annex', flavors=['clone'])
-def test_submodule_deinit(path):
-
-    top_repo = AnnexRepo(path, create=False)
-    eq_({'subm 1', '2'}, {s.name for s in top_repo.get_submodules()})
-    # note: here init=True is ok, since we are using it just for testing
-    with swallow_logs(new_level=logging.WARN) as cml:
-        top_repo.update_submodule('subm 1', init=True)
-        assert_in('Do not use update_submodule with init=True', cml.out)
-    top_repo.update_submodule('2', init=True)
-
-    # ok_(all([s.module_exists() for s in top_repo.get_submodules()]))
-    # TODO: old assertion above if non-bare? (can't use "direct mode" in test_gitrepo)
-    # Alternatively: New testrepo (plain git submodules) and have a dedicated
-    # test for annexes in addition
-    ok_(all([GitRepo.is_valid_repo(opj(top_repo.path, s.path))
-             for s in top_repo.get_submodules()]))
-
-    # modify submodule:
-    with open(opj(top_repo.path, 'subm 1', 'file_ut.dat'), "w") as f:
-        f.write("some content")
-
-    assert_raises(CommandError, top_repo.deinit_submodule, 'sub1')
-
-    # using force should work:
-    top_repo.deinit_submodule('subm 1', force=True)
-
-    ok_(not top_repo.repo.submodule('subm 1').module_exists())
-
-
-@with_testrepos(".*basic_git.*", flavors=['local'])
 @with_tempfile(mkdir=True)
-def test_GitRepo_add_submodule(source, path):
-
-    top_repo = GitRepo(path, create=True)
-
-    top_repo.add_submodule('sub', name='sub', url=source)
-    top_repo.commit('submodule added')
-    eq_([s.name for s in top_repo.get_submodules()], ['sub'])
-    ok_clean_git(path)
-    ok_clean_git(opj(path, 'sub'))
-
-
-def test_GitRepo_update_submodule():
-    raise SkipTest("TODO")
-
-
-def test_GitRepo_get_submodules():
-    raise SkipTest("TODO")
-
-
-def test_kwargs_to_options():
-
-    class Some(object):
-
-        @kwargs_to_options(split_single_char_options=True)
-        def f_decorated_split(self, options=None):
-            return options
-
-        @kwargs_to_options(split_single_char_options=False,
-                           target_kw='another')
-        def f_decorated_no_split(self, another=None):
-            return another
-
-    res = Some().f_decorated_split(C="/some/path", m=3, b=True, more_fancy=['one', 'two'])
-    ok_(isinstance(res, list))
-    eq_(res, ['-C', "/some/path", '-b', '-m', '3',
-              '--more-fancy=one', '--more-fancy=two'])
-
-    res = Some().f_decorated_no_split(f='some')
-    eq_(res, ['-fsome'])
-
-
-def test_to_options():
-
-    class Some(object):
-
-        def cmd_func(self, git_options=None, annex_options=None, options=None):
-
-            git_options = git_options[:] if git_options else []
-            annex_options = annex_options[:] if annex_options else []
-            options = options[:] if options else []
-
-            faked_cmd_call = ['git'] + git_options + ['annex'] + \
-                             annex_options + ['my_cmd'] + options
-
-            return faked_cmd_call
-
-    eq_(Some().cmd_func(options=to_options(m="bla", force=True)),
-        ['git', 'annex', 'my_cmd', '--force', '-m', 'bla'])
-
-    eq_(Some().cmd_func(git_options=to_options(C="/some/where"),
-                        annex_options=to_options(JSON=True),
-                        options=to_options(unused=True)),
-        ['git', '-C', '/some/where', 'annex', '--JSON', 'my_cmd', '--unused'])
-
-    eq_(Some().cmd_func(git_options=to_options(C="/some/where", split_single_char_options=False),
-                        annex_options=to_options(JSON=True),
-                        options=to_options(unused=True)),
-        ['git', '-C/some/where', 'annex', '--JSON', 'my_cmd', '--unused'])
-
-
-@with_tempfile
-def test_GitRepo_count_objects(repo_path):
-
-    repo = GitRepo(repo_path, create=True)
-    # test if dictionary returned
-    eq_(isinstance(repo.count_objects, dict), True)
-    # test if dictionary contains keys and values we expect
-    empty_count = {'count': 0, 'garbage': 0,  'in-pack': 0, 'packs': 0, 'prune-packable': 0,
-                   'size': 0, 'size-garbage': 0, 'size-pack': 0}
-    eq_(empty_count, repo.count_objects)
-
-
-@with_tempfile
-def test_get_missing(path):
-    repo = GitRepo(path, create=True)
-    os.makedirs(opj(path, 'deep'))
-    with open(opj(path, 'test1'), 'w') as f:
-        f.write('some')
-    with open(opj(path, 'deep', 'test2'), 'w') as f:
-        f.write('some more')
-    repo.add('.', commit=True)
-    ok_clean_git(path, annex=False)
-    os.unlink(opj(path, 'test1'))
-    eq_(repo.get_missing_files(), ['test1'])
-    rmtree(opj(path, 'deep'))
-    eq_(sorted(repo.get_missing_files()), [opj('deep', 'test2'), 'test1'])
-    # nothing is actually known to be deleted
-    eq_(repo.get_deleted_files(), [])
-    # do proper removal
-    repo.remove(opj(path, 'test1'))
-    # no longer missing
-    eq_(repo.get_missing_files(), [opj('deep', 'test2')])
-    # but deleted
-    eq_(repo.get_deleted_files(), ['test1'])
-
-
-@with_tempfile
-def test_optimized_cloning(path):
-    # make test repo with one file and one commit
-    originpath = opj(path, 'origin')
-    repo = GitRepo(originpath, create=True)
-    with open(opj(originpath, 'test'), 'w') as f:
-        f.write('some')
-    repo.add('test')
-    repo.commit('init')
-    ok_clean_git(originpath, annex=False)
-    from glob import glob
-
-    def _get_inodes(repo):
-        return dict(
-            [(os.path.join(*o.split(os.sep)[-2:]),
-              os.stat(o).st_ino)
-             for o in glob(os.path.join(repo.repo.git_dir,
-                                        'objects', '*', '*'))])
-
-    origin_inodes = _get_inodes(repo)
-    # now clone it in different ways and see what happens to the object storage
-    from datalad.support.network import get_local_file_url
-    clonepath = opj(path, 'clone')
-    for src in (originpath, get_local_file_url(originpath)):
-        # deprecated
-        assert_raises(DeprecatedError, GitRepo, url=src, path=clonepath)
-        clone = GitRepo.clone(url=src, path=clonepath, create=True)
-        clone_inodes = _get_inodes(clone)
-        eq_(origin_inodes, clone_inodes, msg='with src={}'.format(src))
-        rmtree(clonepath)
-#        del clone
-#        gc.collect()
-        # Note: del needed, since otherwise WeakSingletonRepo would just
-        # return the original object in second run
-
-
-@with_tempfile
-@with_tempfile
-def test_GitRepo_gitpy_injection(path, path2):
-
-    gr = GitRepo(path, create=True)
-    gr._GIT_COMMON_OPTIONS.extend(['test-option'])
-
-    with assert_raises(GitCommandError) as cme:
-        gr.repo.git.unknown_git_command()
-    assert_in('test-option', exc_str(cme.exception))
-
-    # once set, these option should be persistent across git calls:
-    with assert_raises(GitCommandError) as cme:
-        gr.repo.git.another_unknown_git_command()
-    assert_in('test-option', exc_str(cme.exception))
-
-    # but other repos should not be affected:
-    gr2 = GitRepo(path2, create=True)
-    with assert_raises(GitCommandError) as cme:
-        gr2.repo.git.unknown_git_command()
-    assert_not_in('test-option', exc_str(cme.exception))
+def test_gh1597_simpler(path=None):
+    ds = Dataset(path).create()
+    # same goes for .gitattributes
+    with open(op.join(ds.path, '.gitignore'), 'a') as f:
+        f.write('*.swp\n')
+    ds.save('.gitignore')
+    assert_repo_status(ds.path)
+    # put .gitattributes in some subdir and add all, should also go into Git
+    attrfile = op.join ('subdir', '.gitattributes')
+    ds.repo.set_gitattributes(
+        [('*', dict(mycustomthing='this'))],
+        attrfile)
+    assert_repo_status(ds.path, untracked=[attrfile], untracked_mode='all')
+    ds.save()
+    assert_repo_status(ds.path)
+    # no annex key, not in annex
+    assert_not_in(
+        'key',
+        ds.repo.get_content_annexinfo([ut.Path(attrfile)]).popitem()[1])
 
 
 @with_tempfile(mkdir=True)
-@with_tempfile(mkdir=True)
-def test_GitRepo_flyweight(path1, path2):
-
-    repo1 = GitRepo(path1, create=True)
-    assert_is_instance(repo1, GitRepo)
-    # instantiate again:
-    repo2 = GitRepo(path1, create=False)
-    assert_is_instance(repo2, GitRepo)
-    # the very same object:
-    ok_(repo1 is repo2)
-
-    # reference the same in a different way:
-    with chpwd(path1):
-        repo3 = GitRepo(relpath(path1, start=path2), create=False)
-    # it's the same object:
-    ok_(repo1 is repo3)
-
-    # and realpath attribute is the same, so they are still equal:
-    ok_(repo1 == repo3)
+def test_update_known_submodule(path=None):
+    def get_baseline(p):
+        ds = Dataset(p).create()
+        sub = create(str(ds.pathobj / 'sub'))
+        assert_repo_status(ds.path, untracked=['sub'])
+        return ds
+    # attempt one
+    ds = get_baseline(op.join(path, 'wo_ref'))
+    with chpwd(ds.path):
+        save(recursive=True)
+    assert_repo_status(ds.path)
+
+    # attempt two, same as above but call add via reference dataset
+    ds = get_baseline(op.join(path, 'w_ref'))
+    ds.save(recursive=True)
+    assert_repo_status(ds.path)
 
 
 @with_tempfile(mkdir=True)
-@with_tempfile()
-def test_GitRepo_flyweight_monitoring_inode(path, store):
-    # testing for issue #1512
-    check_repo_deals_with_inode_change(GitRepo, path, store)
+def test_add_recursive(path=None):
+    # make simple hierarchy
+    parent = Dataset(path).create()
+    assert_repo_status(parent.path)
+    sub1 = parent.create(op.join('down', 'sub1'))
+    assert_repo_status(parent.path)
+    sub2 = parent.create('sub2')
+    # next one make the parent dirty
+    subsub = sub2.create('subsub')
+    assert_repo_status(parent.path, modified=['sub2'])
+    res = parent.save()
+    assert_repo_status(parent.path)
+
+    # now add content deep in the hierarchy
+    create_tree(subsub.path, {'new': 'empty'})
+    assert_repo_status(parent.path, modified=['sub2'])
+
+    # recursive add should not even touch sub1, because
+    # it knows that it is clean
+    res = parent.save(recursive=True, jobs=5)
+    # the key action is done
+    assert_result_count(
+        res, 1, path=op.join(subsub.path, 'new'), action='add', status='ok')
+    # saved all the way up
+    assert_result_count(res, 3, action='save', status='ok')
+    assert_repo_status(parent.path)
+
+
+@with_tree(**tree_arg)
+def test_relpath_add(path=None):
+    ds = Dataset(path).create(force=True)
+    with chpwd(op.join(path, 'dir')):
+        eq_(save('testindir')[0]['path'],
+            op.join(ds.path, 'dir', 'testindir'))
+        # and now add all
+        save('..')
+    # auto-save enabled
+    assert_repo_status(ds.path)
 
 
-@with_tree(tree={'ignore-sub.me': {'a_file.txt': 'some content'},
-                 'ignore.me': 'ignored content',
-                 'dontigno.re': 'other content'})
-def test_GitRepo_gitignore(path):
-
-    gr = GitRepo(path, create=True)
-    sub = GitRepo(opj(path, 'ignore-sub.me'))
-
-    from ..exceptions import GitIgnoreError
-
-    with open(opj(path, '.gitignore'), "w") as f:
-        f.write("*.me")
-
-    with assert_raises(GitIgnoreError) as cme:
-        gr.add('ignore.me')
-    eq_(cme.exception.paths, ['ignore.me'])
-
-    with assert_raises(GitIgnoreError) as cme:
-        gr.add_submodule(path='ignore-sub.me')
-    eq_(cme.exception.paths, ['ignore-sub.me'])
-
-    with assert_raises(GitIgnoreError) as cme:
-        gr.add(['ignore.me', 'dontigno.re', opj('ignore-sub.me', 'a_file.txt')])
-    eq_(set(cme.exception.paths), {'ignore.me', 'ignore-sub.me'})
+@skip_wo_symlink_capability
+@with_tempfile()
+def test_bf2541(path=None):
+    ds = create(path)
+    subds = ds.create('sub')
+    assert_repo_status(ds.path)
+    os.symlink('sub', op.join(ds.path, 'symlink'))
+    with chpwd(ds.path):
+        res = save(recursive=True)
+    assert_repo_status(ds.path)
 
-    eq_(gr.get_git_attributes(), {})  # nothing is recorded within .gitattributes
 
+@with_tempfile()
+def test_remove_subds(path=None):
+    ds = create(path)
+    ds.create('sub')
+    ds.create(op.join('sub', 'subsub'))
+    assert_repo_status(ds.path)
+    assert_result_count(
+        ds.subdatasets(), 1,
+        path=op.join(ds.path, 'sub'))
+    # all good at this point, subdataset known, dataset clean
+    # now have some external force wipe out the subdatasets
+    rmtree(op.join(ds.path, 'sub'))
+    assert_result_count(
+        ds.status(), 1,
+        path=op.join(ds.path, 'sub'),
+        state='deleted')
+    # a single call to save() must fix up the mess
+    assert_status('ok', ds.save())
+    assert_repo_status(ds.path)
 
-@with_tempfile(mkdir=True)
-def test_GitRepo_set_remote_url(path):
 
-    gr = GitRepo(path, create=True)
-    gr.add_remote('some', 'http://example.com/.git')
-    assert_equal(gr.config['remote.some.url'],
-                 'http://example.com/.git')
-    # change url:
-    gr.set_remote_url('some', 'http://believe.it')
-    assert_equal(gr.config['remote.some.url'],
-                 'http://believe.it')
-
-    # set push url:
-    gr.set_remote_url('some', 'ssh://whatever.ru', push=True)
-    assert_equal(gr.config['remote.some.pushurl'],
-                 'ssh://whatever.ru')
-
-    # add remote without url
-    url2 = 'http://repo2.example.com/.git'
-    gr.add_remote('some-without-url', url2)
-    assert_equal(gr.config['remote.some-without-url.url'], url2)
-    # "remove" it
-    gr.config.unset('remote.some-without-url.url', where='local')
-    with assert_raises(KeyError):
-        gr.config['remote.some-without-url.url']
-    eq_(set(gr.get_remotes()), {'some', 'some-without-url'})
-    eq_(set(gr.get_remotes(with_urls_only=True)), {'some'})
+@with_tempfile()
+def test_partial_unlocked(path=None):
+    # https://github.com/datalad/datalad/issues/1651
+    ds = create(path)
+    (ds.pathobj / 'normal.txt').write_text(u'123')
+    ds.save()
+    assert_repo_status(ds.path)
+    ds.unlock('normal.txt')
+    ds.save()
+    # mixed git and git-annex'ed files
+    (ds.pathobj / 'ingit.txt').write_text(u'234')
+    ds.save(to_git=True)
+    (ds.pathobj / 'culprit.txt').write_text(u'345')
+    (ds.pathobj / 'ingit.txt').write_text(u'modified')
+    ds.save()
+    assert_repo_status(ds.path)
+    # but now a change in the attributes
+    if '10.20220127' <= ds.repo.git_annex_version < '10.20220322':
+        raise SkipTest("annex bug https://git-annex.branchable.com/bugs/Change_to_annex.largefiles_leaves_repo_modified/")
+    ds.unlock('culprit.txt')
+    ds.repo.set_gitattributes([
+        ('*', {'annex.largefiles': 'nothing'})])
+    ds.save()
+    assert_repo_status(ds.path)
+
+
+@with_tree({'.gitattributes': "* annex.largefiles=(largerthan=4b)",
+            "foo": "in annex"})
+def test_save_partial_commit_shrinking_annex(path=None):
+    # This is a variation on the test above. The main difference is that there
+    # are other staged changes in addition to the unlocked filed.
+    ds = create(path, force=True)
+    ds.save()
+    assert_repo_status(ds.path)
+    ds.unlock(path="foo")
+    create_tree(ds.path, tree={"foo": "a", "staged": ""},
+                remove_existing=True)
+    # Even without this staged change, a plain 'git commit -- foo' would fail
+    # with git-annex's partial index error, but save (or more specifically
+    # GitRepo.save_) drops the pathspec if there are no staged changes.
+    ds.repo.add("staged", git=True)
+    ds.save(path="foo")
+    assert_repo_status(ds.path, added=["staged"])
 
 
-@with_tempfile(mkdir=True)
-def test_get_git_attributes(path):
+@with_tempfile()
+def test_path_arg_call(path=None):
+    ds = create(path)
+    for testfile in (
+            ds.pathobj / 'abs.txt',
+            ds.pathobj / 'rel.txt'):
+        testfile.write_text(u'123')
+        # we used to resolve relative paths against a dataset just given by
+        # a path, but we no longer do that
+        #save(dataset=ds.path, path=[testfile.name], to_git=True)
+        save(dataset=ds, path=[testfile.name], to_git=True)
+
+
+# one can't create these file names on FAT/NTFS systems
+@skip_if_adjusted_branch
+@with_tempfile
+def test_windows_incompatible_names(path=None):
+    ds = Dataset(path).create()
+    create_tree(path, {
+        'imgood': 'Look what a nice name I have',
+        'illegal:character.txt': 'strange choice of name',
+        'spaceending ': 'who does these things?',
+        'lookmumadot.': 'why would you do this?',
+        'COM1.txt': 'I am a serial port',
+        'dirs with spaces': {
+            'seriously?': 'you are stupid',
+            'why somuch?wrongstuff.': "I gave up"
+        },
+    })
+    ds.repo.config.set('datalad.save.windows-compat-warning', 'error')
+    ds.save('.datalad/config')
+    res = ds.save(on_failure='ignore')
+    # check that none of the 6 problematic files was saved, but the good one was
+    assert_result_count(res, 6, status='impossible', action='save')
+    assert_result_count(res, 1, status='ok', action='save')
+
+    # check that the warning is emitted
+    ds.repo.config.set('datalad.save.windows-compat-warning', 'warning')
+    ds.save('.datalad/config')
+    with swallow_logs(new_level=logging.WARN) as cml:
+        ds.save()
+        cml.assert_logged(
+            "Some elements of your dataset are not compatible with Windows "
+            "systems. Disable this check by changing "
+            "datalad.save.windows-compat-warning or consider renaming the "
+            "following elements:")
+        assert_in("Elements using a reserved filename:", cml.out)
+        assert_in("Elements with illegal characters:", cml.out)
+        assert_in("Elements ending with a dot:", cml.out)
+        assert_in("Elements ending with a space:", cml.out)
+
+    # check that a setting of 'none' really does nothing
+    ds.repo.config.set('datalad.save.windows-compat-warning', 'none')
+    ds.save('.datalad/config')
+    create_tree(path, {
+        'more illegal:characters?.py': 'My arch nemesis uses Windows and I will'
+                                       'destroy them! Muahahaha'
+    })
+    with swallow_logs(new_level=logging.WARN) as cml:
+        res = ds.save()
+        # we shouldn't see warnings
+        assert_not_in(
+            "Some elements of your dataset are not compatible with Windows "
+            "systems. Disable this check by changing "
+            "datalad.save.windows-compat-warning or consider renaming the "
+            "following elements:", cml.out)
+        # make sure the file is saved successfully
+        assert_result_count(res, 1, status='ok', action='save')
 
-    gr = GitRepo(path, create=True)
-    eq_(gr.get_git_attributes(), {})  # nothing is recorded within .gitattributes
 
-    create_tree(gr.path, {'.gitattributes': "* tag\n* sec.key=val"})
-    # ATM we do not do any translation of values, so if it is just a tag, it
-    # would be what git returns -- "set"
-    eq_(gr.get_git_attributes(), {'tag': 'set', 'sec.key': 'val'})
+@with_tree(tree={
+    'file.txt': 'some text',
+    'd1': {
+        'subrepo': {
+            'subfile': 'more repo text',
+        },
+    },
+    'd2': {
+        'subds': {
+            'subfile': 'more ds text',
+        },
+    },
+})
+def test_surprise_subds(path=None):
+    # https://github.com/datalad/datalad/issues/3139
+    ds = create(path, force=True)
+    # a lonely repo without any commit
+    somerepo = AnnexRepo(path=op.join(path, 'd1', 'subrepo'), create=True)
+    # a proper subdataset
+    subds = create(op.join(path, 'd2', 'subds'), force=True)
+
+    # If subrepo is an adjusted branch, it would have a commit, making most of
+    # this test irrelevant because it is about the unborn branch edge case.
+    adjusted = somerepo.is_managed_branch()
+    # save non-recursive
+    res = ds.save(recursive=False, on_failure='ignore')
+    if not adjusted:
+        # We get an appropriate error about no commit being checked out.
+        assert_in_results(res, action='add_submodule', status='error')
+
+    # the content of both subds and subrepo are not added to their
+    # respective parent as no --recursive was given
+    assert_repo_status(subds.path, untracked=['subfile'])
+    assert_repo_status(somerepo.path, untracked=['subfile'])
+
+    if adjusted:
+        # adjusted branch: #datalad/3178 (that would have a commit)
+        modified = [subds.repo.pathobj, somerepo.pathobj]
+        untracked = []
+    else:
+        # Newer Git versions refuse to add a sub-repository with no commits
+        # checked out.
+        modified = [subds.repo.pathobj]
+        untracked = ['d1']
+        assert_repo_status(ds.path, modified=modified, untracked=untracked)
+        assert_not_in(ds.repo.pathobj / 'd1' / 'subrepo' / 'subfile',
+                      ds.repo.get_content_info())
+
+
+@with_tree({"foo": ""})
+def test_bf3285(path=None):
+    ds = Dataset(path).create(force=True)
+    # Note: Using repo.pathobj matters in the "TMPDIR=/var/tmp/sym\ link" case
+    # because assert_repo_status is based off of {Annex,Git}Repo.path, which is
+    # the realpath'd path (from the processing in _flyweight_id_from_args).
+    subds = create(ds.repo.pathobj.joinpath("subds"))
+    # Explicitly saving a path does not save an untracked, unspecified
+    # subdataset.
+    ds.save("foo")
+    assert_repo_status(ds.path, untracked=[subds.path])
+
+
+@with_tree({"outside": "",
+            "ds": {"within": ""}})
+def test_on_failure_continue(path=None):
+    ds = Dataset(op.join(path, "ds")).create(force=True)
+    # save() calls status() in a way that respects on_failure.
+    assert_in_results(
+        ds.save(path=[op.join(path, "outside"),
+                      op.join(path, "ds", "within")],
+                on_failure="ignore"),
+        action="status",
+        status="error")
+    # save() continued despite the failure and saved ds/within.
+    assert_repo_status(ds.path)
+
+
+@with_tree(tree={OBSCURE_FILENAME: "abc"})
+def test_save_obscure_name(path=None):
+    ds = Dataset(path).create(force=True)
+    fname = OBSCURE_FILENAME
+    # Just check that we don't fail with a unicode error.
+    with swallow_outputs():
+        ds.save(path=fname, result_renderer="default")
 
 
-@with_tempfile(mkdir=True)
-def test_get_tags(path):
-    gr = GitRepo(path, create=True)
-    eq_(gr.get_tags(), [])
-
-    # Explicitly override the committer date because tests may set it to a
-    # fixed value, but we want to check that the returned tags are sorted by
-    # the committer date.
-    with patch.dict("os.environ", {"GIT_COMMITTER_DATE":
-                                   "Thu, 07 Apr 2005 22:13:13 +0200"}):
-        create_tree(gr.path, {'file': ""})
-        gr.add('file')
-        gr.commit(msg="msg")
-        eq_(gr.get_tags(), [])
-
-        gr.tag("nonannotated")
-        tags1 = [{'name': 'nonannotated', 'hexsha': gr.get_hexsha()}]
-        eq_(gr.get_tags(), tags1)
-
-    with patch.dict("os.environ", {"GIT_COMMITTER_DATE":
-                                   "Fri, 08 Apr 2005 22:13:13 +0200"}):
-
-        create_tree(gr.path, {'file': "123"})
-        gr.add('file')
-        gr.commit(msg="changed")
-
-    gr.tag("annotated", message="annotation")
-    tags2 = tags1 + [{'name': 'annotated', 'hexsha': gr.get_hexsha()}]
-    eq_(gr.get_tags(), tags2)
-
-
-@with_tree(tree={'1': ""})
-def test_get_committed_date(path):
-    gr = GitRepo(path, create=True)
-    assert_equal(gr.get_commit_date(), None)
-
-    # Let's make a commit with a custom date
-    DATE = "Wed Mar 14 03:47:30 2018 -0000"
-    DATE_EPOCH = 1520999250
-    gr.add('1')
-    gr.commit("committed", date=DATE)
-    gr = GitRepo(path, create=True)
-    date = gr.get_commit_date()
-    neq_(date, None)
-    eq_(date, DATE_EPOCH)
-
-    eq_(date, gr.get_commit_date('master'))
-    # and even if we get into a detached head
-    gr.checkout(gr.get_hexsha())
-    eq_(gr.get_active_branch(), None)
-    eq_(date, gr.get_commit_date('master'))
+@with_tree(tree={
+    ".dot": "ab", "nodot": "cd",
+    "nodot-subdir": {".dot": "ef", "nodot": "gh"},
+    ".dot-subdir": {".dot": "ij", "nodot": "kl"}})
+def check_save_dotfiles(to_git, save_path, path):
+    # Note: Take relpath to work with Travis "TMPDIR=/var/tmp/sym\ link" run.
+    paths = [Path(op.relpath(op.join(root, fname), path))
+             for root, _, fnames in os.walk(op.join(path, save_path or ""))
+             for fname in fnames]
+    ok_(paths)
+    ds = Dataset(path).create(force=True)
+    ds.save(save_path, to_git=to_git)
+    if save_path is None:
+        assert_repo_status(ds.path)
+    repo = ds.repo
+    annexinfo = repo.get_content_annexinfo()
+
+    def _check(fn, p):
+        fn("key", annexinfo[repo.pathobj / p], p)
+
+    if to_git:
+        def check(p):
+            _check(assert_not_in, p)
+    else:
+        def check(p):
+            _check(assert_in, p)
+
+    for path in paths:
+        check(path)
+
+
+@pytest.mark.parametrize(
+    "git,save_path",
+    itertools.product(
+        [True, False, None],
+        [None, "nodot-subdir"],
+    )
+)
+def test_save_dotfiles(git, save_path):
+    check_save_dotfiles(git, save_path)
+
+
+@with_tempfile
+def test_save_nested_subs_explicit_paths(path=None):
+    ds = Dataset(path).create()
+    spaths = [Path("s1"), Path("s1", "s2"), Path("s1", "s2", "s3")]
+    for spath in spaths:
+        Dataset(ds.pathobj / spath).create()
+    ds.save(path=spaths)
+    eq_(set(ds.subdatasets(recursive=True, result_xfm="relpaths")),
+        set(map(str, spaths)))
+
+
+@with_tempfile
+def test_save_gitrepo_annex_subds_adjusted(path=None):
+    ds = Dataset(path).create(annex=False)
+    subds = ds.create("sub")
+    maybe_adjust_repo(subds.repo)
+    (subds.pathobj / "foo").write_text("foo")
+    subds.save()
+    ds.save()
+    assert_repo_status(ds.path)
+
+
+@known_failure
+@with_tempfile
+def test_save_adjusted_partial(path=None):
+    ds = Dataset(path).create()
+    subds = ds.create("sub")
+    maybe_adjust_repo(subds.repo)
+    (subds.pathobj / "foo").write_text("foo")
+    subds.save()
+    (ds.pathobj / "other").write_text("staged, not for committing")
+    ds.repo.call_git(["add", "other"])
+    ds.save(path=["sub"])
+    assert_repo_status(ds.path, added=["other"])
+
+
+@with_tempfile
+def test_save_diff_ignore_submodules_config(path=None):
+    ds = Dataset(path).create()
+    subds = ds.create("sub")
+    (subds.pathobj / "foo").write_text("foo")
+    subds.save()
+    ds.repo.config.set("diff.ignoreSubmodules", "all",
+                       scope="local", reload=True)
+    # Saving a subdataset doesn't fail when diff.ignoreSubmodules=all.
+    ds.save()
+    assert_repo_status(ds.path)
+
+
+@with_tree({"subdir": {"foo": "foocontent"}})
+def test_save_git_mv_fixup(path=None):
+    ds = Dataset(path).create(force=True)
+    ds.save()
+    assert_repo_status(ds.path)
+    ds.repo.call_git(["mv", op.join("subdir", "foo"), "foo"])
+    ds.save()
+    # Was link adjusted properly?  (gh-3686)
+    assert (ds.pathobj / 'foo').read_text() == "foocontent"
+    # all clean
+    assert_repo_status(ds.path)
+
+
+@with_tree(tree={'somefile': 'file content',
+                 'subds': {'file_in_sub': 'other'}})
+def test_save_amend(dspath=None):
+
+    dspath = Path(dspath)
+    file_in_super = dspath / 'somefile'
+    file_in_sub = dspath / 'subds' / 'file_in_sub'
+
+    # test on a hierarchy including a plain git repo:
+    ds = Dataset(dspath).create(force=True, annex=False)
+    subds = ds.create('subds', force=True)
+    ds.save(recursive=True)
+    assert_repo_status(ds.repo)
+
+    # recursive and amend are mutually exclusive:
+    for d in (ds, subds):
+        assert_raises(ValueError, d.save, recursive=True, amend=True)
+
+    # in an annex repo the branch we are interested in might not be the active
+    # branch (adjusted):
+    sub_branch = subds.repo.get_corresponding_branch()
+
+    # amend in subdataset w/ new message; otherwise empty amendment:
+    last_sha = subds.repo.get_hexsha(sub_branch)
+    subds.save(message="new message in sub", amend=True)
+    # we did in fact commit something:
+    neq_(last_sha, subds.repo.get_hexsha(sub_branch))
+    # repo is clean:
+    assert_repo_status(subds.repo)
+    # message is correct:
+    eq_(subds.repo.format_commit("%B", sub_branch).strip(),
+        "new message in sub")
+    # actually replaced the previous commit:
+    assert_not_in(last_sha, subds.repo.get_branch_commits_(sub_branch))
+
+    # amend modifications in subdataset w/o new message
+    if not subds.repo.is_managed_branch():
+        subds.unlock('file_in_sub')
+    file_in_sub.write_text("modified again")
+    last_sha = subds.repo.get_hexsha(sub_branch)
+    subds.save(amend=True)
+    neq_(last_sha, subds.repo.get_hexsha(sub_branch))
+    assert_repo_status(subds.repo)
+    # message unchanged:
+    eq_(subds.repo.format_commit("%B", sub_branch).strip(),
+        "new message in sub")
+    # actually replaced the previous commit:
+    assert_not_in(last_sha, subds.repo.get_branch_commits_(sub_branch))
+
+    # save --amend with nothing to amend with:
+    res = subds.save(amend=True)
+    assert_result_count(res, 1)
+    assert_result_count(res, 1, status='notneeded', action='save')
+
+    # amend in superdataset w/ new message; otherwise empty amendment:
+    last_sha = ds.repo.get_hexsha()
+    ds.save(message="new message in super", amend=True)
+    neq_(last_sha, ds.repo.get_hexsha())
+    assert_repo_status(subds.repo)
+    eq_(ds.repo.format_commit("%B").strip(), "new message in super")
+    assert_not_in(last_sha, ds.repo.get_branch_commits_())
+
+    # amend modifications in superdataset w/o new message
+    file_in_super.write_text("changed content")
+    if not subds.repo.is_managed_branch():
+        subds.unlock('file_in_sub')
+    file_in_sub.write_text("modified once again")
+    last_sha = ds.repo.get_hexsha()
+    last_sha_sub = subds.repo.get_hexsha(sub_branch)
+    ds.save(amend=True)
+    neq_(last_sha, ds.repo.get_hexsha())
+    eq_(ds.repo.format_commit("%B").strip(), "new message in super")
+    assert_not_in(last_sha, ds.repo.get_branch_commits_())
+    # we didn't mess with the subds:
+    assert_repo_status(ds.repo, modified=["subds"])
+    eq_(last_sha_sub, subds.repo.get_hexsha(sub_branch))
+    eq_(subds.repo.format_commit("%B", sub_branch).strip(),
+        "new message in sub")
+
+    # save --amend with nothing to amend with:
+    last_sha = ds.repo.get_hexsha()
+    res = ds.save(amend=True)
+    assert_result_count(res, 1)
+    assert_result_count(res, 1, status='notneeded', action='save')
+    eq_(last_sha, ds.repo.get_hexsha())
+    # we didn't mess with the subds:
+    assert_repo_status(ds.repo, modified=["subds"])
+    eq_(last_sha_sub, subds.repo.get_hexsha(sub_branch))
+    eq_(subds.repo.format_commit("%B", sub_branch).strip(),
+        "new message in sub")
+
+    # amend with different identity:
+    orig_author = ds.repo.format_commit("%an")
+    orig_email = ds.repo.format_commit("%ae")
+    orig_date = ds.repo.format_commit("%ad")
+    orig_committer = ds.repo.format_commit("%cn")
+    orig_committer_mail = ds.repo.format_commit("%ce")
+    eq_(orig_author, orig_committer)
+    eq_(orig_email, orig_committer_mail)
+    with patch.dict('os.environ',
+                    {'GIT_COMMITTER_NAME': 'Hopefully Different',
+                     'GIT_COMMITTER_EMAIL': 'hope.diff@example.com'}):
+
+        ds.config.reload(force=True)
+        ds.save(amend=True, message="amend with hope")
+    # author was kept:
+    eq_(orig_author, ds.repo.format_commit("%an"))
+    eq_(orig_email, ds.repo.format_commit("%ae"))
+    eq_(orig_date, ds.repo.format_commit("%ad"))
+    # committer changed:
+    eq_(ds.repo.format_commit("%cn"), "Hopefully Different")
+    eq_(ds.repo.format_commit("%ce"), "hope.diff@example.com")
+
+    # corner case: amend empty commit with no parent:
+    rmtree(str(dspath))
+    # When adjusted branch is enforced by git-annex detecting a crippled FS,
+    # git-annex produces an empty commit before switching to adjusted branch:
+    # "commit before entering adjusted branch"
+    # The commit by `create` would be the second one already.
+    # Therefore go with plain annex repo and create an (empty) commit only when
+    # not on adjusted branch:
+    repo = AnnexRepo(dspath, create=True)
+    if not repo.is_managed_branch():
+        repo.commit(msg="initial", options=['--allow-empty'])
+    ds = Dataset(dspath)
+    branch = ds.repo.get_corresponding_branch() or ds.repo.get_active_branch()
+    # test pointless if we start with more than one commit
+    eq_(len(list(ds.repo.get_branch_commits_(branch))),
+        1,
+        msg="More than on commit '{}': {}".format(
+            branch, ds.repo.call_git(['log', branch]))
+        )
+    last_sha = ds.repo.get_hexsha(branch)
+
+    ds.save(message="new initial commit", amend=True)
+    assert_repo_status(ds.repo)
+    eq_(len(list(ds.repo.get_branch_commits_(branch))),
+        1,
+        msg="More than on commit '{}': {}".format(
+            branch, ds.repo.call_git(['log', branch]))
+        )
+    assert_not_in(last_sha, ds.repo.get_branch_commits_(branch))
+    eq_(ds.repo.format_commit("%B", branch).strip(), "new initial commit")
+
+
+@with_tempfile
+def test_save_sub_trailing_sep_bf6547(path=None):
+    ds = Dataset(path).create()
+    # create not-yet-subdataset inside
+    subds = Dataset(ds.pathobj / 'sub').create()
+    ds.save(path='sub' + os.path.sep)
+    assert_in_results(
+        ds.subdatasets(result_renderer='disabled'),
+        path=subds.path,
+    )
+    # make sure it has the .gitmodules record
+    assert 'sub' in (ds.pathobj / '.gitmodules').read_text()
```

### Comparing `datalad-0.9.3/datalad/support/tests/test_digests.py` & `datalad-1.0.0/datalad/support/tests/test_digests.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,26 +1,30 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 
 from os.path import join as opj
+
+from datalad.tests.utils_pytest import (
+    assert_equal,
+    with_tree,
+)
+
 from ..digests import Digester
-from ...tests.utils import with_tree
-from ...tests.utils import assert_equal
 
 
 @with_tree(tree={'sample.txt': '123',
                  '0': chr(0),
                  'long.txt': '123abz\n'*1000000})
-def test_digester(path):
+def test_digester(path=None):
     digester = Digester()
     assert_equal(
         digester(opj(path, 'sample.txt')),
         {
             'md5': '202cb962ac59075b964b07152d234b70',
             'sha1': '40bd001563085fc35165329ea1ff5c5ecbdbbeef',
             'sha256': 'a665a45920422f9d417e4867efdc4fb8a04a1f3fff1fa07e998e86f7f7a27ae3',
```

### Comparing `datalad-0.9.3/datalad/support/tests/test_stats.py` & `datalad-1.0.0/datalad/support/tests/test_stats.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,22 +1,27 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 
-from ..stats import ActivityStats, _COUNTS
+from ...tests.utils_pytest import (
+    assert_equal,
+    assert_in,
+    assert_not_equal,
+    assert_raises,
+)
+from ..stats import (
+    _COUNTS,
+    ActivityStats,
+)
 
-from ...tests.utils import assert_equal
-from ...tests.utils import assert_not_equal
-from ...tests.utils import assert_raises
-from ...tests.utils import assert_in
 
 def test_ActivityStats_basic():
     stats = ActivityStats()
     assert_raises(AttributeError, setattr, stats, "unknown_attribute", 1)
 
     for c in _COUNTS:
         assert_equal(getattr(stats, c), 0)
```

### Comparing `datalad-0.9.3/datalad/support/tests/test_status.py` & `datalad-1.0.0/datalad/support/tests/test_status.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,19 +1,23 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 
+from datalad.tests.utils_pytest import (
+    assert_equal,
+    assert_not_equal,
+)
+
 from ..status import FileStatus
-from ...tests.utils import assert_equal, assert_false
-from nose.tools import assert_not_equal
+
 
 def test_FileStatus_basic():
     assert_equal(FileStatus(size=0), FileStatus(size=0))
     assert_not_equal(FileStatus(size=0), FileStatus(size=1))
     # mtimes allow trimming if one is int
     assert_equal(FileStatus(mtime=0), FileStatus(mtime=0.9999))
     assert_equal(FileStatus(mtime=0), FileStatus(mtime=0.0001))
```

### Comparing `datalad-0.9.3/datalad/support/tests/test_external_versions.py` & `datalad-1.0.0/datalad/support/tests/test_external_versions.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,54 +1,68 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 
 import logging
-
+import pytest
 from os import linesep
 
-from ...dochelpers import exc_str
-from ...version import __version__
-from ..external_versions import ExternalVersions, LooseVersion
-from ..exceptions import CommandError
-from ..exceptions import OutdatedExternalDependency
-from ...support.annexrepo import AnnexRepo
-from ...tests.utils import (
-    with_tempfile,
+from datalad import __version__
+from datalad.cmd import StdOutErrCapture, WitlessRunner
+from datalad.support.annexrepo import AnnexRepo
+from datalad.support.exceptions import (
+    CommandError,
+    MissingExternalDependency,
+    OutdatedExternalDependency,
+)
+from datalad.tests.utils_pytest import (
+    SkipTest,
+    assert_equal,
+    assert_false,
+    assert_greater,
+    assert_greater_equal,
+    assert_in,
+    assert_not_in,
+    assert_raises,
+    assert_true,
     create_tree,
+    patch,
+    set_annex_version,
     swallow_logs,
+    with_tempfile,
 )
 
-from mock import patch
-from nose.tools import (
-    assert_true, assert_false,
-    assert_equal, assert_greater_equal, assert_greater,
-    assert_raises, assert_in
+from ..external_versions import (
+    ExternalVersions,
+    LooseVersion,
 )
-from nose import SkipTest
-from six import PY3
 
-if PY3:
-    # just to ease testing
-    def cmp(a, b):
-        return (a > b) - (a < b)
+
+# just to ease testing
+def cmp(a, b):
+    return (a > b) - (a < b)
 
 
 def test_external_versions_basic():
     ev = ExternalVersions()
     our_module = 'datalad'
     assert_equal(ev.versions, {})
     assert_equal(ev[our_module], __version__)
     # and it could be compared
     assert_greater_equal(ev[our_module], __version__)
+    # We got some odd failure in this test not long are after switching to versionner
+    # https://github.com/datalad/datalad/issues/5785.  Verify that we do get expected
+    # data types
+    our_version = ev[our_module].version
+    assert isinstance(our_version, (str, list)), f"Got {our_version!r} of type {type(our_version)}"
     assert_greater(ev[our_module], '0.1')
     assert_equal(list(ev.keys()), [our_module])
     assert_true(our_module in ev)
     assert_false('unknown' in ev)
 
     # all are LooseVersions now
     assert_true(isinstance(ev[our_module], LooseVersion))
@@ -67,57 +81,63 @@
     # And that thing is "True", i.e. present
     assert(ev['os'])
     # but not comparable with anything besides itself (was above)
     assert_raises(TypeError, cmp, ev['os'], '0')
     assert_raises(TypeError, assert_greater, ev['os'], '0')
 
     return
-    # Code below is from original duecredit, and we don't care about
-    # testing this one
-    # And we can get versions based on modules themselves
-    from datalad.tests import mod
-    assert_equal(ev[mod], mod.__version__)
-
-    # Check that we can get a copy of the versions
-    versions_dict = ev.versions
-    versions_dict[our_module] = "0.0.1"
-    assert_equal(versions_dict[our_module], "0.0.1")
-    assert_equal(ev[our_module], __version__)
+    ## Code below is from original duecredit, and we don't care about
+    ## testing this one
+    ## And we can get versions based on modules themselves
+    #from datalad.tests import mod
+    #assert_equal(ev[mod], mod.__version__)
+
+    ## Check that we can get a copy of the versions
+    #versions_dict = ev.versions
+    #versions_dict[our_module] = "0.0.1"
+    #assert_equal(versions_dict[our_module], "0.0.1")
+    #assert_equal(ev[our_module], __version__)
+
+
+def test_external_version_contains():
+    ev = ExternalVersions()
+    assert_true("datalad" in ev)
+    assert_false("does not exist" in ev)
 
 
 def test_external_versions_unknown():
     assert_equal(str(ExternalVersions.UNKNOWN), 'UNKNOWN')
 
 
 def _test_external(ev, modname):
     try:
         exec("import %s" % modname, globals(), locals())
     except ImportError:
         raise SkipTest("External %s not present" % modname)
     except Exception as e:
-        raise SkipTest("External %s fails to import: %s" % (modname, exc_str(e)))
+        raise SkipTest("External %s fails to import" % modname) from e
     assert (ev[modname] is not ev.UNKNOWN)
     assert_greater(ev[modname], '0.0.1')
     assert_greater('1000000.0', ev[modname])  # unlikely in our lifetimes
 
 
 def test_external_versions_popular_packages():
     ev = ExternalVersions()
 
     for modname in ('scipy', 'numpy', 'mvpa2', 'sklearn', 'statsmodels', 'pandas',
-                    'matplotlib', 'psychopy'):
-        yield _test_external, ev, modname
+                    'matplotlib', 'psychopy', 'github'):
+        _test_external(ev, modname)
 
     # more of a smoke test
     assert_false(linesep in ev.dumps())
     assert_true(ev.dumps(indent=True).endswith(linesep))
 
 
 @with_tempfile(mkdir=True)
-def test_external_versions_rogue_module(topd):
+def test_external_versions_rogue_module(topd=None):
     ev = ExternalVersions()
     # if module throws some other non-ImportError exception upon import
     # we must not crash, but issue a warning
     modname = 'verycustomrogue__'
     create_tree(topd, {modname + '.py': 'raise Exception("pickaboo")'})
     with patch('sys.path', [topd]), \
         swallow_logs(new_level=logging.WARNING) as cml:
@@ -145,52 +165,55 @@
     assert_equal(ev['bogus'], None)
     assert_equal(set(ev.versions), {'cmd:annex', 'cmd:git', 'cmd:system-git'})
 
 
 def test_ancient_annex():
 
     class _runner(object):
-        def run(self, cmd):
+        def run(self, cmd, *args, **kwargs):
             if '--raw' in cmd:
                 raise CommandError
-            return "git-annex version: 0.1", ""
+            return dict(stdout="git-annex version: 0.1", stderr="")
 
     ev = ExternalVersions()
     with patch('datalad.support.external_versions._runner', _runner()):
         assert_equal(ev['cmd:annex'], '0.1')
 
 
 def _test_annex_version_comparison(v, cmp_):
     class _runner(object):
-        def run(self, cmd):
-            return v, ""
+        def run(self, cmd, *args, **kwargs):
+            return dict(stdout=v, stderr="")
 
     ev = ExternalVersions()
-    with patch('datalad.support.external_versions._runner', _runner()), \
+    with set_annex_version(None), \
+         patch('datalad.support.external_versions._runner', _runner()), \
          patch('datalad.support.annexrepo.external_versions',
                ExternalVersions()):
         ev['cmd:annex'] < AnnexRepo.GIT_ANNEX_MIN_VERSION
         if cmp_ in (1, 0):
             AnnexRepo._check_git_annex_version()
             if cmp_ == 0:
                 assert_equal(AnnexRepo.git_annex_version, v)
         elif cmp == -1:
             with assert_raises(OutdatedExternalDependency):
+                ev.check('cmd:annex', min_version=AnnexRepo.GIT_ANNEX_MIN_VERSION)
+            with assert_raises(OutdatedExternalDependency):
                 AnnexRepo._check_git_annex_version()
 
 
 def test_annex_version_comparison():
     # see https://github.com/datalad/datalad/issues/1128
     for cmp_, base in [(-1, '6.2011'), (1, "2100.0")]:
         # there could be differing versions of a version
         #   release, snapshot, neurodebian build of a snapshot
         for v in base, base + '-g0a34f08', base + '+gitg9f179ae-1~ndall+1':
             # they all must be comparable to our specification of min version
-            yield _test_annex_version_comparison, v, cmp_
-    yield _test_annex_version_comparison, str(AnnexRepo.GIT_ANNEX_MIN_VERSION), 0
+            _test_annex_version_comparison(v, cmp_)
+    _test_annex_version_comparison(str(AnnexRepo.GIT_ANNEX_MIN_VERSION), 0)
 
 
 def _test_list_tuple(thing):
     version = ExternalVersions._deduce_version(thing)
     assert_greater(version, '0.0.1')
     assert_greater('0.2', version)
     assert_equal('0.1', version)
@@ -202,29 +225,75 @@
     class thing_with_tuple_version:
         __version__ = (0, 1)
 
     class thing_with_list_version:
         __version__ = [0, 1]
 
     for v in thing_with_list_version, thing_with_tuple_version, '0.1', (0, 1), [0, 1]:
-        yield _test_list_tuple, v
+        _test_list_tuple(v)
 
 
 def test_system_ssh_version():
+    try:
+        WitlessRunner().run(['ssh', '-V'], protocol=StdOutErrCapture)
+    except FileNotFoundError as exc:
+        pytest.skip(f"no ssh binary available: {exc}")
     ev = ExternalVersions()
     assert ev['cmd:system-ssh']  # usually we have some available at boxes we test
 
+
+def test_ssh_versions():
     for s, v in [
         ('OpenSSH_7.4p1 Debian-6, OpenSSL 1.0.2k  26 Jan 2017', '7.4p1'),
+        ('OpenSSH_8.1p1, LibreSSL 2.7.3', '8.1p1'),
+        ('OpenSSH_for_Windows_8.1p1, LibreSSL 3.0.2', '8.1p1'),
     ]:
         ev = ExternalVersions()
         # TODO: figure out leaner way
         class _runner(object):
-            def run(self, cmd, expect_fail, expect_stderr):
-                return "", s
+            def run(self, cmd, *args, **kwargs):
+                return dict(stdout="", stderr=s)
         with patch('datalad.support.external_versions._runner', _runner()):
             assert_equal(ev['cmd:system-ssh'], v)
 
 
 def test_humanize():
     # doesn't provide __version__
-    assert ExternalVersions()['humanize']
+    assert ExternalVersions()['humanize']
+
+
+def test_check():
+    ev = ExternalVersions()
+    # should be all good
+    ev.check('datalad')
+    ev.check('datalad', min_version=__version__)
+
+    with assert_raises(MissingExternalDependency):
+        ev.check('dataladkukaracha')
+    with assert_raises(MissingExternalDependency) as cme:
+        ev.check('dataladkukaracha', min_version="buga", msg="duga")
+
+    assert_in("duga", str(cme.value))
+
+    with assert_raises(OutdatedExternalDependency):
+        ev.check('datalad', min_version="10000000")  # we will never get there!
+
+
+def test_add():
+    ev = ExternalVersions()
+    ev.add('custom1', lambda: "0.1.0")
+    assert_in("custom1=0.1.0", ev.dumps(query=True))
+    assert_not_in("numpy", ev.INTERESTING)  # we do not have it by default yet
+    assert_not_in("numpy=", ev.dumps(query=True))
+    ev.add('numpy')
+    try:
+        import numpy
+    except ImportError:
+        # no numpy, we do not have some bogus entry
+        assert_not_in("numpy=", ev.dumps(query=True))
+    else:
+        assert_in("numpy=%s" % numpy.__version__, ev.dumps(query=True))
+    assert_in("custom1=0.1.0", ev.dumps(query=True))  # we still have that one
+
+    # override with a new function will work
+    ev.add('custom1', lambda: "0.2.0")
+    assert_in("custom1=0.2.0", ev.dumps(query=True))
```

### Comparing `datalad-0.9.3/datalad/support/keyring_.py` & `datalad-1.0.0/datalad/support/keyring_.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,66 +1,99 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 """Adapters and decorators for keyrings
 """
 
 import os
+import logging
+lgr = logging.getLogger('datalad.support.keyring')
 
 
 class Keyring(object):
     """Adapter to keyring module
 
     It also delays import of keyring which takes 300ms I guess due to all plugins etc
     """
-    def __init__(self):
-        self.__keyring = None
+    def __init__(self, keyring_backend=None):
+        """
+
+        Parameters
+        ----------
+        keyring_backend: keyring.backend.KeyringBackend, optional
+          Specific keyring to use.  If not provided, the one returned by
+          `keyring.get_keyring()` is used
+        """
+        self.__keyring = keyring_backend
+        self.__keyring_mod = None
 
     @property
     def _keyring(self):
-        if self.__keyring is None:
-            # Setup logging for keyring if we are debugging, althought keyring's logging
+        if self.__keyring_mod is None:
+            # Setup logging for keyring if we are debugging, although keyring's logging
             # is quite scarce ATM
             from datalad.log import lgr
             import logging
             lgr_level = lgr.getEffectiveLevel()
             if lgr_level < logging.DEBUG:
                 keyring_lgr = logging.getLogger('keyring')
                 keyring_lgr.setLevel(lgr_level)
                 keyring_lgr.handlers = lgr.handlers
             lgr.debug("Importing keyring")
             import keyring
-            self.__keyring = keyring
+            self.__keyring_mod = keyring
 
+        if self.__keyring is None:
+            from datalad.log import lgr
+            # we use module bound interfaces whenever we were not provided a dedicated
+            # backend
+            self.__keyring = self.__keyring_mod
+            the_keyring = self.__keyring_mod.get_keyring()
+            if the_keyring.name.lower().startswith('null '):
+                lgr.warning(
+                    "Keyring module returned '%s', no credentials will be provided",
+                    the_keyring.name
+                )
         return self.__keyring
 
     @classmethod
     def _get_service_name(cls, name):
         return "datalad-%s" % str(name)
 
     # proxy few methods of interest explicitly, to be rebound to the module's
     def get(self, name, field):
-        # consult environment, might be provided there
-        val = self._keyring.get_password(self._get_service_name(name), field)
-        if val is None:
-            val = os.environ.get(('DATALAD_%s_%s' % (name, field)).replace('-', '_'), None)
-        return val
+        # consult environment, might be provided there and should take precedence
+        # NOTE: This env var specification is outdated and not advertised
+        # anymmore, but needs to be supported. For example, it is used with and
+        # was advertised for
+        # https://github.com/datalad-datasets/human-connectome-project-openaccess
+        env_var = ('DATALAD_%s_%s' % (name, field)).replace('-', '_')
+        lgr.log(5, 'Credentials lookup attempt via env var %s', env_var)
+        if env_var in os.environ:
+            return os.environ[env_var]
+        return self._keyring.get_password(self._get_service_name(name), field)
 
     def set(self, name, field, value):
         return self._keyring.set_password(self._get_service_name(name), field, value)
 
     def delete(self, name, field=None):
         if field is None:
             raise NotImplementedError("Deletion of all fields associated with a name")
-        return self._keyring.delete_password(self._get_service_name(name), field)
+        try:
+            return self._keyring.delete_password(self._get_service_name(name), field)
+        except self.__keyring_mod.errors.PasswordDeleteError as exc:
+            exc_str = str(exc).lower()
+            if 'not found' in exc_str or 'no such password' in exc_str:
+                return
+            raise
 
 
 class MemoryKeyring(object):
     """A simple keyring which just stores provided info in memory
 
     Primarily for testing
     """
```

### Comparing `datalad-0.9.3/datalad/support/param.py` & `datalad-1.0.0/datalad/support/param.py`

 * *Files 5% similar despite different names*

```diff
@@ -9,29 +9,34 @@
 """Parameter representation"""
 
 __docformat__ = 'restructuredtext'
 
 import re
 import textwrap
 import argparse
-from inspect import getargspec
+from datalad.utils import getargspec
 
 from .constraints import expand_constraint_spec
 
-_whitespace_re = re.compile('\n\s+|^\s+')
+_whitespace_re = re.compile(r'\n\s+|^\s+')
 
 
 class Parameter(object):
     """This class shall serve as a representation of a parameter.
     """
 
     # Known keyword arguments which we want to allow to pass over into
     # argparser.add_argument . Mentioned explicitly, since otherwise
     # are not verified while working in Python-only API
-    _KNOWN_ARGS = getargspec(argparse.Action.__init__)[0] + ['action']
+    # include_kwonlyargs=True is future-proofing since ATM in 3.9 there is no
+    # *, in Action.__init__ but could be added later, and semantically it
+    # makes sense to include those among _KNOWN_ARGS
+    _KNOWN_ARGS = getargspec(
+        argparse.Action.__init__, include_kwonlyargs=True
+    ).args + ['action']
 
     def __init__(self, constraints=None, doc=None, args=None, **kwargs):
         """Add constraints (validator) specifications and a docstring for
         a parameter.
 
         Parameters
         ----------
@@ -111,15 +116,15 @@
             if has_default:
                 paramsdoc += ", optional"
         paramsdoc = [paramsdoc]
 
         doc = self._doc
         if doc is None:
             doc = ''
-        doc.strip()
+        doc = doc.strip()
         if len(doc) and not doc.endswith('.'):
             doc += '.'
         if has_default:
             doc += " [Default: %r]" % (default,)
         # Explicitly deal with multiple spaces, for some reason
         # replace_whitespace is non-effective
         doc = _whitespace_re.sub(' ', doc)
```

### Comparing `datalad-0.9.3/datalad/support/annexrepo.py` & `datalad-1.0.0/datalad/support/annexrepo.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,396 +1,391 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 """Interface to git-annex by Joey Hess.
 
 For further information on git-annex see https://git-annex.branchable.com/.
 
 """
 
 import json
 import logging
-import math
 import os
 import re
-import shlex
-import tempfile
-import time
-
+import warnings
 from itertools import chain
+from multiprocessing import cpu_count
 from os import linesep
-from os import unlink
+from os.path import (
+    curdir,
+    exists,
+    isdir,
+)
 from os.path import join as opj
-from os.path import exists
-from os.path import islink
-from os.path import realpath
-from os.path import lexists
-from os.path import isdir
-from os.path import isabs
-from os.path import relpath
-from os.path import normpath
-from subprocess import Popen, PIPE
-from multiprocessing import cpu_count
-from weakref import WeakValueDictionary
-
-from six import string_types
-from six import iteritems
-from six.moves import filter
-from git import InvalidGitRepositoryError
-
-from datalad import ssh_manager
-from datalad.dochelpers import exc_str
-from datalad.dochelpers import borrowdoc
-from datalad.dochelpers import borrowkwargs
-from datalad.utils import linux_distribution_name
-from datalad.utils import nothing_cm
-from datalad.utils import auto_repr
-from datalad.utils import on_windows
-from datalad.utils import swallow_logs
-from datalad.utils import assure_list
-from datalad.utils import _path_
-from datalad.utils import generate_chunks
-from datalad.utils import CMD_MAX_ARG
-from datalad.support.json_py import loads as json_loads
-from datalad.cmd import GitRunner
-
+from os.path import (
+    lexists,
+    normpath,
+)
+from typing import Dict
+from weakref import (
+    WeakValueDictionary,
+    finalize,
+)
+
+import datalad.utils as ut
+from datalad.cmd import (  # KillOutput,
+    BatchedCommand,
+    GitWitlessRunner,
+    SafeDelCloseMixin,
+    StdOutCapture,
+    StdOutErrCapture,
+    WitlessProtocol,
+)
+from datalad.consts import WEB_SPECIAL_REMOTE_UUID
 # imports from same module:
-from .repo import RepoInterface
-from .gitrepo import GitRepo
-from .gitrepo import NoSuchPathError
-from .gitrepo import normalize_path
-from .gitrepo import normalize_paths
-from .gitrepo import GitCommandError
-from .gitrepo import to_options
-from . import ansi_colors
+from datalad.dataset.repo import RepoInterface
+from datalad.dochelpers import (
+    borrowdoc,
+    borrowkwargs,
+)
+from datalad.log import log_progress
+from datalad.runner.protocol import GeneratorMixIn
+from datalad.runner.utils import (
+    AssemblingDecoderMixIn,
+    LineSplitter,
+)
+
+from datalad.support.annex_utils import (
+    _fake_json_for_non_existing,
+    _get_non_existing_from_annex_output,
+    _sanitize_key,
+)
+from datalad.support.exceptions import CapturedException
+from datalad.ui import ui
+from datalad.utils import (
+    Path,
+    PurePosixPath,
+    auto_repr,
+    ensure_list,
+    on_windows,
+    split_cmdline,
+    unlink,
+)
+
+from .exceptions import (
+    AccessDeniedError,
+    AccessFailedError,
+    AnnexBatchCommandError,
+    CommandError,
+    CommandNotAvailableError,
+    DirectModeNoLongerSupportedError,
+    FileInGitError,
+    FileNotInAnnexError,
+    IncompleteResultsError,
+    InsufficientArgumentsError,
+    InvalidAnnexRepositoryError,
+    InvalidGitRepositoryError,
+    MissingExternalDependency,
+    NoSuchPathError,
+    OutdatedExternalDependency,
+    OutOfSpaceError,
+    RemoteNotAvailableError,
+)
 from .external_versions import external_versions
-from .exceptions import CommandNotAvailableError
-from .exceptions import CommandError
-from .exceptions import FileNotInAnnexError
-from .exceptions import FileInGitError
-from .exceptions import FileNotInRepositoryError
-from .exceptions import AnnexBatchCommandError
-from .exceptions import InsufficientArgumentsError
-from .exceptions import OutOfSpaceError
-from .exceptions import RemoteNotAvailableError
-from .exceptions import OutdatedExternalDependency
-from .exceptions import MissingExternalDependency
-from .exceptions import IncompleteResultsError
-from .exceptions import AccessDeniedError
-from .exceptions import AccessFailedError
+from .gitrepo import (
+    GitRepo,
+    normalize_path,
+    normalize_paths,
+    to_options,
+)
 
 lgr = logging.getLogger('datalad.annex')
 
-# Limit to # of CPUs and up to 8, but at least 3 to start with
-N_AUTO_JOBS = min(8, max(3, cpu_count()))
-
 
 class AnnexRepo(GitRepo, RepoInterface):
     """Representation of an git-annex repository.
 
     Paths given to any of the class methods will be interpreted as relative
     to PWD, in case this is currently beneath AnnexRepo's base dir
     (`self.path`). If PWD is outside of the repository, relative paths
     will be interpreted as relative to `self.path`. Absolute paths will be
     accepted either way.
     """
 
     # Begin Flyweight:
     _unique_instances = WeakValueDictionary()
+
+    def _flyweight_invalid(self):
+        return not self.is_valid_annex(allow_noninitialized=True)
+
     # End Flyweight:
 
-    # Web remote has a hard-coded UUID we might (ab)use
-    WEB_UUID = "00000000-0000-0000-0000-000000000001"
+    # Web remote UUID, kept here for backward compatibility
+    WEB_UUID = WEB_SPECIAL_REMOTE_UUID
 
     # To be assigned and checked to be good enough upon first call to AnnexRepo
     # 6.20160923 -- --json-progress for get
     # 6.20161210 -- annex add  to add also changes (not only new files) to git
     # 6.20170220 -- annex status provides --ignore-submodules
-    GIT_ANNEX_MIN_VERSION = '6.20170220'
+    # 6.20180416 -- annex handles unicode filenames more uniformly
+    # 6.20180913 -- annex fixes all known to us issues for v6
+    # 7          -- annex makes v7 mode default on crippled systems. We demand it for consistent operation
+    # 7.20190503 -- annex introduced mimeencoding support needed for our text2git
+    #
+    # When bumping this, check whether datalad.repo.version needs to be
+    # adjusted.
+    GIT_ANNEX_MIN_VERSION = '8.20200309'
     git_annex_version = None
-
-    def __init__(self, path, url=None, runner=None,
-                 direct=None, backend=None, always_commit=True, create=True,
+    supports_direct_mode = None
+    repository_versions = None
+    _version_kludges = {}
+
+    def __init__(self, path, runner=None,
+                 backend=None, always_commit=True,
+                 create=True, create_sanity_checks=True,
                  init=False, batch_size=None, version=None, description=None,
                  git_opts=None, annex_opts=None, annex_init_opts=None,
-                 repo=None):
+                 repo=None, fake_dates=False):
         """Creates representation of git-annex repository at `path`.
 
         AnnexRepo is initialized by giving a path to the annex.
         If no annex exists at that location, a new one is created.
         Optionally give url to clone from.
 
         Parameters
         ----------
         path: str
           Path to git-annex repository. In case it's not an absolute path, it's
           relative to PWD
-        url: str, optional
-          url to the to-be-cloned repository. Requires valid git url
-          according to
-          http://www.kernel.org/pub/software/scm/git/docs/git-clone.html#URLS .
         runner: Runner, optional
           Provide a Runner in case AnnexRepo shall not create it's own.
           This is especially needed in case of desired dry runs.
-        direct: bool, optional
-          If True, force git-annex to use direct mode
         backend: str, optional
           Set default backend used by this annex. This does NOT affect files,
           that are already annexed nor will it automatically migrate files,
           hat are 'getted' afterwards.
         create: bool, optional
           Create and initialize an annex repository at path, in case
           there is none. If set to False, and this repository is not an annex
           repository (initialized or not), an exception is raised.
+        create_sanity_checks: bool, optional
+          Passed to GitRepo.
         init: bool, optional
           Initialize git-annex repository (run "git annex init") if path is an
           annex repository which just was not yet initialized by annex (e.g. a
           fresh git clone). Note that if `create=True`, then initialization
           would happen
         batch_size: int, optional
           If specified and >0, instructs annex to batch this many commands before
           annex adds acts on git repository (e.g. adds them them to index for addurl).
         version: int, optional
           If given, pass as --version to `git annex init`
         description: str, optional
           Short description that humans can use to identify the
           repository/location, e.g. "Precious data on my laptop"
         """
-        if self.git_annex_version is None:
-            self._check_git_annex_version()
+
+        # BEGIN Repo validity test
+        # We want to fail early for tests, that would be performed a lot. In particular this is about
+        # AnnexRepo.is_valid_repo. We would use the latter to decide whether or not to call AnnexRepo() only for
+        # __init__ to then test the same things again. If we fail early we can save the additional test from outer
+        # scope.
+        do_init = False
+        super(AnnexRepo, self).__init__(
+            path, runner=runner,
+            create=create, create_sanity_checks=create_sanity_checks,
+            repo=repo, git_opts=git_opts, fake_dates=fake_dates)
+
+        # Check whether an annex already exists at destination
+        # XXX this doesn't work for a submodule!
+
+        # NOTE: We are in __init__ here and already know that GitRepo.is_valid_git is True, since super.__init__  was
+        #       called. Therefore: check_git=False
+        if not self.is_valid_annex(check_git=False):
+            # so either it is not annex at all or just was not yet initialized
+            # TODO: There's still potential to get a bit more performant. is_with_annex() is checking again, what
+            #       is_valid_annex did. However, this marginal here, considering the call to git-annex-init.
+            if self.is_with_annex():
+                # it is an annex repository which was not initialized yet
+                if create or init:
+                    lgr.debug('Annex repository was not yet initialized at %s.'
+                              ' Initializing ...' % self.path)
+                    do_init = True
+            elif create:
+                lgr.debug('Initializing annex repository at %s...', self.path)
+                do_init = True
+            else:
+                raise InvalidAnnexRepositoryError("No annex found at %s." % self.path)
+
+        # END Repo validity test
 
         # initialize
         self._uuid = None
-        self._annex_common_options = []
-        # Workaround for per-call config issue with git 2.11.0
-        self.GIT_DIRECT_MODE_WRAPPER_ACTIVE = False
-        self.GIT_DIRECT_MODE_PROXY = False
+        self._annex_common_options = ["-c", "annex.dotfiles=true"]
 
         if annex_opts or annex_init_opts:
             lgr.warning("TODO: options passed to git-annex and/or "
                         "git-annex-init are currently ignored.\n"
                         "options received:\n"
                         "git-annex: %s\ngit-annex-init: %s" %
                         (annex_opts, annex_init_opts))
 
-        fix_it = False
-        try:
-            super(AnnexRepo, self).__init__(path, url, runner=runner,
-                                            create=create, repo=repo,
-                                            git_opts=git_opts)
-        except GitCommandError as e:
-            if create and "Clone succeeded, but checkout failed." in str(e):
-                lgr.warning("Experienced issues while cloning. "
-                            "Trying to fix it, using git-annex-fsck.")
-                fix_it = True
-            else:
-                raise e
-
-        # check for possible SSH URLs of the remotes in order to set up
-        # shared connections:
-        for r in self.get_remotes():
-            for url in [self.get_remote_url(r),
-                        self.get_remote_url(r, push=True)]:
-                if url is not None:
-                    self._set_shared_connection(r, url)
-
+        # Below was initially introduced for setting for direct mode workaround,
+        # where we changed _GIT_COMMON_OPTIONS and had to avoid passing
+        # --worktree=. -c core.bare=False to git annex commands, so for their
+        # invocation we kept and used pristine version of the
+        # common options.  yoh thought it would be good to keep this as a copy
+        # just in case we do need to pass annex specific options, even if
+        # there is no need ATM
+        self._ANNEX_GIT_COMMON_OPTIONS = self._GIT_COMMON_OPTIONS[:]
         self.always_commit = always_commit
 
+        config = self.config
         if version is None:
-            version = self.config.get("datalad.repo.version", None)
+            version = config.get("datalad.repo.version", None)
             # we might get an empty string here
             # TODO: if we use obtain() instead, we get an error complaining
             # '' cannot be converted to int (via Constraint as defined for
             # "datalad.repo.version" in common_cfg
             # => Allow conversion to result in None?
-            if not version:
+            if version:
+                try:
+                    version = int(version)
+                except ValueError:
+                    # Just give a warning if things look off and let
+                    # git-annex-init complain if it can't actually handle it.
+                    lgr.warning(
+                        "Expected an int for datalad.repo.version, got %s",
+                        version)
+            else:
+                # The above comment refers to an empty string case. The commit
+                # (f12eb03f40) seems to deal with direct mode, so perhaps this
+                # isn't reachable anymore.
                 version = None
 
-        if fix_it:
+        if do_init:
             self._init(version=version, description=description)
-            self.fsck()
 
-        # Check whether an annex already exists at destination
-        # XXX this doesn't work for a submodule!
-        if not AnnexRepo.is_valid_repo(self.path):
-            # so either it is not annex at all or just was not yet initialized
-            if self.is_with_annex():
-                # it is an annex repository which was not initialized yet
-                if create or init:
-                    lgr.debug('Annex repository was not yet initialized at %s.'
-                              ' Initializing ...' % self.path)
-                    self._init(version=version, description=description)
-            elif create:
-                lgr.debug('Initializing annex repository at %s...' % self.path)
-                self._init(version=version, description=description)
-            else:
-                raise RuntimeError("No annex found at %s." % self.path)
-
-        self._direct_mode = None  # we don't know yet
+        # TODO: RM DIRECT  eventually, but should remain while we have is_direct_mode
+        self._direct_mode = None
 
-        # If we are in direct mode already, we need to make
-        # this instance aware of that. This especially means, that we need to
-        # adapt self._GIT_COMMON_OPTIONS by calling set_direct_mode().
+        # Handle cases of detecting repositories with no longer supported
+        # direct mode.
         # Could happen in case we didn't specify anything, but annex forced
         # direct mode due to FS or an already existing repo was in direct mode,
         if self._is_direct_mode_from_config():
-            self.set_direct_mode()
+            raise DirectModeNoLongerSupportedError(
+                self,
+                "Git configuration reports repository being in direct mode"
+            )
 
-        # - only force direct mode; don't force indirect mode
-        # - parameter `direct` has priority over config
-        if direct is None:
-            direct = (create or init) and \
-                     self.config.getbool("datalad", "repo.direct", default=False)
-        self._direct_mode = None  # we don't know yet
-        if direct and not self.is_direct_mode():
-            # direct mode is available below version 6 repos only.
-            # Note: If 'annex.version' is missing in .git/config for some
-            # reason, we need to try to set direct mode:
-            repo_version = self.config.getint("annex", "version")
-            if (repo_version is None) or (repo_version < 6):
-                lgr.debug("Switching to direct mode (%s)." % self)
-                self.set_direct_mode()
-            else:
-                # TODO: This may change to either not being a warning and/or
-                # to use 'git annex unlock' instead.
-                lgr.warning("direct mode not available for %s. Ignored." % self)
+        if config.getbool("datalad", "repo.direct", default=False):
+            raise DirectModeNoLongerSupportedError(
+                self,
+                "datalad.repo.direct configuration instructs to use direct mode"
+            )
 
-        self._batched = BatchedAnnexes(batch_size=batch_size)
+        self._batched = BatchedAnnexes(
+            batch_size=batch_size, git_options=self._ANNEX_GIT_COMMON_OPTIONS)
 
         # set default backend for future annex commands:
         # TODO: Should the backend option of __init__() also migrate
         # the annex, in case there are annexed files already?
         if backend:
             self.set_default_backend(backend, persistent=True)
 
+        # will be evaluated lazily
+        self._n_auto_jobs = None
+
+        # Finally, register a finalizer (instead of having a __del__ method).
+        # This will be called by garbage collection as well as "atexit". By
+        # keeping the reference here, we can also call it explicitly.
+        # Note, that we can pass required attributes to the finalizer, but not
+        # `self` itself. This would create an additional reference to the object
+        # and thereby preventing it from being collected at all.
+        self._finalizer = finalize(self, AnnexRepo._cleanup, self.path,
+                                   self._batched)
 
     def set_default_backend(self, backend, persistent=True, commit=True):
         """Set default backend
 
         Parameters
         ----------
         backend : str
         persistent : bool, optional
           If persistent, would add/commit to .gitattributes. If not -- would
           set within .git/config
         """
-        # TODO: 'annex.backends' actually is a space separated list.
-        # Figure out, whether we want to allow for a list here or what to
-        # do, if there is sth in that setting already
         if persistent:
             # could be set in .gitattributes or $GIT_DIR/info/attributes
-            if 'annex.backend' in self.get_git_attributes():
+            if 'annex.backend' in self.get_gitattributes('.')['.']:
                 lgr.debug(
                     "Not (re)setting backend since seems already set in git attributes"
                 )
             else:
                 lgr.debug("Setting annex backend to %s (persistently)", backend)
-                self.config.set('annex.backends', backend, where='local')
-                git_attributes_file = _path_(self.path, '.gitattributes')
-                git_attributes = ''
-                if exists(git_attributes_file):
-                    with open(git_attributes_file) as f:
-                        git_attributes = f.read()
-                with open(git_attributes_file, 'a') as f:
-                    if git_attributes and not git_attributes.endswith(os.linesep):
-                        f.write(os.linesep)
-                    f.write('* annex.backend=%s%s' % (backend, os.linesep))
+                git_attributes_file = '.gitattributes'
+                self.set_gitattributes(
+                    [('*', {'annex.backend': backend})],
+                    git_attributes_file)
                 self.add(git_attributes_file, git=True)
                 if commit:
                     self.commit(
                         "Set default backend for all files to be %s" % backend,
                         _datalad_msg=True,
                         files=[git_attributes_file]
                     )
         else:
             lgr.debug("Setting annex backend to %s (in .git/config)", backend)
-            self.config.set('annex.backends', backend, where='local')
+            self.config.set('annex.backend', backend, scope='local')
+
+    @classmethod
+    def _cleanup(cls, path, batched):
+
+        lgr.log(1, "Finalizer called on: AnnexRepo(%s)", path)
+
+        # Ben: With switching to finalize rather than del, I think the
+        #      safe_del_debug isn't needed anymore. However, time will tell and
+        #      it doesn't hurt.
+
+        def safe__del__debug(e):
+            """We might be too late in the game and either .debug or exc_str
+            are no longer bound"""
+            try:
+                return lgr.debug(str(e))
+            except (AttributeError, NameError):
+                return
 
-    def __del__(self):
         try:
-            if hasattr(self, '_batched') and self._batched is not None:
-                self._batched.close()
+            if batched is not None:
+                batched.close()
         except TypeError as e:
             # Workaround:
             # most likely something wasn't accessible anymore; doesn't really
             # matter since we wanted to delete it anyway.
             #
             # Nevertheless, in some cases might be an issue and it is a strange
             # thing to happen, since we check for things being None herein as
             # well as in super class __del__;
             # At least log it:
-            lgr.debug(exc_str(e))
-        try:
-            super(AnnexRepo, self).__del__()
-        except TypeError as e:
-            # see above
-            lgr.debug(exc_str(e))
-
-    def _set_shared_connection(self, remote_name, url):
-        """Make sure a remote with SSH URL uses shared connections.
-
-        Set ssh options for annex on a per call basis, using
-        '-c remote.<name>.annex-ssh-options'.
-
-        Note
-        ----
-        There's currently no solution for using these connections, if the SSH
-        URL is just connected to a file instead of a remote
-        (`annex addurl` for example).
-
-        Parameters
-        ----------
-        remote_name: str
-        url: str
-        """
-        from datalad.support.network import is_ssh
-        # Note:
-        #
-        # before any possible invocation of git-annex
-        # Temporary approach to ssh connection sharing:
-        # Register every ssh remote with the corresponding control master.
-        # Issues:
-        # - currently overwrites existing ssh config of the remote
-        # - request SSHConnection instance and write config even if no
-        #   connection needed (but: connection is not actually created/opened)
-        # - no solution for a ssh url of a file (annex addurl)
-
-        if is_ssh(url):
-            c = ssh_manager.get_connection(url)
-            ssh_cfg_var = "remote.{0}.annex-ssh-options".format(remote_name)
-            # options to add:
-            # Note: must use -S to overload -S provided by annex itself
-            # if we provide -o ControlPath=... it is not in effect
-            # Note: ctrl_path must not contain spaces, since it seems to be
-            # impossible to anyhow guard them here
-            # http://git-annex.branchable.com/bugs/cannot___40__or_how__63____41___to_pass_socket_path_with_a_space_in_its_path_via_annex-ssh-options/
-            cfg_string = "-o ControlMaster=auto -S %s" % c.ctrl_path
-            # read user-defined options from .git/config:
-            cfg_string_old = self.config.get(ssh_cfg_var, None)
-            self._annex_common_options += \
-                ['-c', 'remote.{0}.annex-ssh-options={1}{2}'
-                       ''.format(remote_name,
-                                 (cfg_string_old + " ") if cfg_string_old else "",
-                                 cfg_string
-                                 )]
+            safe__del__debug(e)
 
     def is_managed_branch(self, branch=None):
         """Whether `branch` is managed by git-annex.
 
-        ATM this returns true in direct mode (branch 'annex/direct/my_branch')
-        and if on an adjusted branch (annex v6 repository:
+        ATM this returns True if on an adjusted branch of annex v6+ repository:
         either 'adjusted/my_branch(unlocked)' or 'adjusted/my_branch(fixed)'
 
         Note: The term 'managed branch' is used to make clear it's meant to be
-        more general than the v6 'adjusted branch'.
+        more general than the v6+ 'adjusted branch'.
 
         Parameters
         ----------
         branch: str
           name of the branch; default: active branch
 
         Returns
@@ -399,462 +394,281 @@
           True if on a managed branch, False otherwise
         """
 
         if branch is None:
             branch = self.get_active_branch()
         # Note: `branch` might still be None, due to detached HEAD
         # (or no checkout at all)
-        if branch and \
-            (branch.startswith('annex/direct/') or
-             branch.startswith('adjusted/')):
-            return True
-        return False
+        return (branch and branch.startswith('adjusted/'))
 
     def get_corresponding_branch(self, branch=None):
-        """In case of a managed branch, get the corresponding one.
-
-        If `branch` is not a managed branch, return that branch without any
-        changes.
-
-        Note: Since default for `branch` is the active branch,
-        `get_corresponding_branch()` is equivalent to `get_active_branch()` if
-        the active branch is not a managed branch.
+        """Get the name of a potential corresponding branch.
 
         Parameters
         ----------
-        branch: str
-          name of the branch; defaults to active branch
+        branch: str, optional
+          Name of the branch to report a corresponding branch for;
+          defaults to active branch
 
         Returns
         -------
-        str
-          name of the corresponding branch if there is any, name of the queried
-          branch otherwise.
+        str or None
+          Name of the corresponding branch, or `None` if there is no
+          corresponding branch.
         """
 
         if branch is None:
             branch = self.get_active_branch()
 
         if self.is_managed_branch(branch):
-            if branch.startswith('annex/direct/'):
-                cor_branch = branch[13:]
-            elif branch.startswith('adjusted/'):
+            if branch.startswith('adjusted/'):
                 if branch.endswith('(unlocked)'):
                     cor_branch = branch[9:-10]
                 elif branch.endswith('(fixed)'):
                     cor_branch = branch[9:-7]
                 else:
                     cor_branch = branch[9:]
-                    lgr.warning("Unexpected naming of adjusted branch '{}'.{}"
-                                "Assuming '{}' to be the corresponding branch."
-                                "".format(branch, linesep, cor_branch))
+                    lgr.warning("Unexpected naming of adjusted branch '%s'.%s"
+                                "Assuming '%s' to be the corresponding branch.",
+                                branch, linesep, cor_branch)
             else:
                 raise NotImplementedError(
                     "Detection of annex-managed branch '{}' follows a pattern "
                     "not implemented herein.".format(branch))
             return cor_branch
 
         else:
-            return branch
+            return None
 
-    def get_tracking_branch(self, branch=None, corresponding=True):
+    def get_tracking_branch(self, branch=None, remote_only=False,
+                            corresponding=True):
         """Get the tracking branch for `branch` if there is any.
 
         By default returns the tracking branch of the corresponding branch if
         `branch` is a managed branch.
 
         Parameters
         ----------
         branch: str
           local branch to look up. If none is given, active branch is used.
+        remote_only : bool
+            Don't return a value if the upstream remote is set to "." (meaning
+            this repository).
         corresponding: bool
           If True actually look up the corresponding branch of `branch` (also if
           `branch` isn't explicitly given)
 
         Returns
         -------
         tuple
             (remote or None, refspec or None) of the tracking branch
         """
 
         if branch is None:
             branch = self.get_active_branch()
 
         return super(AnnexRepo, self).get_tracking_branch(
-                        branch=self.get_corresponding_branch(branch)
-                        if corresponding else branch)
+            remote_only=remote_only,
+            branch=(self.get_corresponding_branch(branch) or branch)
+            if corresponding else branch)
 
-    def _submodules_dirty_direct_mode(self,
-            untracked=True, deleted=True, modified=True, added=True,
-            type_changed=True, path=None):
-        """Get modified submodules
-
-        Workaround for http://git-annex.branchable.com/bugs/git_annex_status_fails_with_submodule_in_direct_mode/
-
-        This is using git-annex-status with --ignore-submodules to not let
-        git-status try to recurse into annex submodules without a working tree.
-        Therefore we need to do the recursion on our own.
-
-        Note, that added submodules will just be reported dirty. It's at very
-        least difficult to distinguish whether a submodule in direct mode was
-        just added or modified. ATM not worth the effort, I think.
-        This is leads to a bit inconsistent reportings by AnnexRepo.status()
-        whenever it needs to call this subroutine and there are added submodules.
-
-        Intended to be used by AnnexRepo.status() internally.
-        """
-
-        # Note: We do a lazy recursion. The only thing we need to know is
-        # whether or not a submodule is to be reported dirty. Once we already
-        # know it is, there's no need to go any deeper in the hierarchy.
-        # Apart from better performance, this also allows us to inspect each
-        # submodule separately, and therefore be able to deal with mixed
-        # hierarchies of git and annex submodules!
-
-        modified_subs = []
-        for sm in self.get_submodules():
-            sm_dirty = False
-
-            # First check for changes committed in the submodule, using
-            # git submodule summary -- path,
-            # since this can't be detected from within the submodule.
-            if self.is_submodule_modified(sm.name):
-                sm_dirty = True
-
-            # check state of annex submodules, that might be in direct mode
-            elif AnnexRepo.is_valid_repo(opj(self.path, sm.path),
-                                         allow_noninitialized=False):
-
-                sm_repo = AnnexRepo(opj(self.path, sm.path),
-                                    create=False, init=False)
-
-                sm_status = sm_repo.get_status(untracked=untracked, deleted=deleted,
-                                               modified=modified, added=added,
-                                               type_changed=type_changed,
-                                               submodules=False, path=path)
-                if any([bool(sm_status[i]) for i in sm_status]):
-                    sm_dirty = True
-
-            # check state of submodule, that is a plain git or not an
-            # initialized annex, which we can safely treat as a plain git, too.
-            elif GitRepo.is_valid_repo(opj(self.path, sm.path)):
-                sm_repo = GitRepo(opj(self.path, sm.path))
-
-                # TODO: Clarify issue: GitRepo.is_dirty() doesn't fit our parameters
-                if sm_repo.is_dirty(index=deleted or modified or added or type_changed,
-                                    working_tree=deleted or modified or added or type_changed,
-                                    untracked_files=untracked,
-                                    submodules=False, path=path):
-                    sm_dirty = True
-            else:
-                # uninitialized submodule
-                # it can't be dirty and we can't recurse any deeper:
-                continue
+    @classmethod
+    def _check_git_annex_version(cls):
+        ver = external_versions['cmd:annex']
+        # in case it is missing
+        msg = "Visit http://handbook.datalad.org/r.html?install " \
+              "for instructions on how to install DataLad and git-annex."
 
-            if sm_dirty:
-                # the submodule itself is dirty
-                modified_subs.append(sm.path)
-            else:
-                # the submodule itself is clean, recurse:
-                # TODO: This fails ATM with AttributeError, if sm is a GitRepo.
-                # we need get_status and this recursion method to be available
-                # to both classes. Issue: We need to be able to come back to
-                # AnnexRepo from GitRepo if there's again an annex beneath. But
-                # we can't import AnnexRepo in gitrepo.py.
-                modified_subs.extend(
-                    sm_repo._submodules_dirty_direct_mode(
-                        untracked=untracked, deleted=deleted,
-                        modified=modified, added=added,
-                        type_changed=type_changed, path=path
-                    ))
-
-        return modified_subs
-
-    def get_status(self, untracked=True, deleted=True, modified=True, added=True,
-                   type_changed=True, submodules=True, path=None):
-        """Return various aspects of the status of the annex repository
-
-        Note: Under certain circumstances newly added submodules might be
-        reported as 'modified' rather tha 'added'.
-        See `AnnexRepo._submodules_dirty_direct_mode` for details.
+        exc_kwargs = dict(
+            name="git-annex",
+            msg=msg,
+            ver=cls.GIT_ANNEX_MIN_VERSION
+        )
+        if not ver:
+            raise MissingExternalDependency(**exc_kwargs)
+        elif ver < cls.GIT_ANNEX_MIN_VERSION:
+            raise OutdatedExternalDependency(ver_present=ver, **exc_kwargs)
+        cls.git_annex_version = ver
 
-        Parameters
-        ----------
-        untracked
-        deleted
-        modified
-        added
-        type_changed
-        submodules
-        path
+    @classmethod
+    def check_direct_mode_support(cls):
+        """Does git-annex version support direct mode?
+
+        The result is cached at `cls.supports_direct_mode`.
 
         Returns
         -------
-
+        bool
         """
+        if cls.supports_direct_mode is None:
+            warnings.warn(
+                "DataLad's minimum git-annex version is above 7.20190912, "
+                "the last version to support direct mode. "
+                "The check_direct_mode_support method "
+                "and supports_direct_mode attribute will be removed "
+                "in an upcoming release.",
+                DeprecationWarning)
+            cls.supports_direct_mode = False
+        return cls.supports_direct_mode
 
-        self.precommit()
-
-        options = assure_list(path) if path else []
-        if not submodules:
-            options.extend(to_options(ignore_submodules='all'))
-
-        # BEGIN workaround bug (see self._submodules_dirty_direct_mode)
-        # internal call to 'git status' by 'git annex status' will fail
-        # in submodules without a working tree (direct mode)
-        # How to catch this case depends on annex version, since annex
-        # exits zero until version 6.20170307
-
-        def _fake_exception_wrapper(self, options_):
-            """generate a faked `CommandError` from logged stderr output"""
-
-            # this is for use with older annex, which didn't exit non-zero
-            # in case of the failure we are interested in
-
-            # TODO: If we are to keep this workaround (we probably rely on a
-            # newer annex anyway), we should not use swallow_logs, since we
-            # actually don't want to swallow it, but inspect it. Use a proper
-            # handler/filter for the logger instead to not create temp files via
-            # swallow_logs
-
-            old_log_state = self.cmd_call_wrapper.log_outputs
-            self.cmd_call_wrapper._log_opts['outputs'] = True
-
-            with swallow_logs(new_level=logging.ERROR) as cml:
-                # Note, that _run_annex_command_json returns a generator
-                json_list = \
-                    list(self._run_annex_command_json(
-                        'status', opts=options_, expect_stderr=False))
-            self.cmd_call_wrapper._log_opts['outputs'] = old_log_state
-            if "fatal:" in cml.out:
-                raise CommandError(cmd="git annex status",
-                                   msg=cml.out, stderr=cml.out)
-            return json_list
-
-        try:
-            if self.git_annex_version < '6.20170307':
-                json_list = _fake_exception_wrapper(self, options_=options)
-            else:
-                json_list = \
-                    list(self._run_annex_command_json(
-                        'status', opts=options, expect_stderr=False))
-        except CommandError as e:
-            if submodules and \
-               "fatal: " \
-               "This operation must be run in a work tree" in e.stderr and \
-               "failed in submodule" in e.stderr:
-                lgr.debug("git-annex-status failed probably due to submodule in"
-                          " direct mode. Trying to workaround.")
-                # try again, ignoring submodules:
-                options = [path] if path else []
-                options.extend(to_options(ignore_submodules='all'))
-                json_list = list(
-                    self._run_annex_command_json('status', opts=options)
-                )
-                # separately get modified submodules:
-                m_subs = \
-                    self._submodules_dirty_direct_mode(untracked=untracked,
-                                                       deleted=deleted,
-                                                       modified=modified,
-                                                       added=added,
-                                                       type_changed=type_changed,
-                                                       path=path)
-                json_list.extend({'file': p, 'status': 'M'} for p in m_subs)
-            else:
-                # not the known bug we want to catch
-                raise e
+    @classmethod
+    def check_repository_versions(cls):
+        """Get information on supported and upgradable repository versions.
 
-        # END workaround
+        The result is cached at `cls.repository_versions`.
 
-        key_mapping = [(untracked, 'untracked', '?'),
-                       (deleted, 'deleted', 'D'),
-                       (modified, 'modified', 'M'),
-                       (added, 'added', 'A'),
-                       (type_changed, 'type_changed', 'T')]
-        from datalad.utils import with_pathsep
-        return {key: [with_pathsep(i['file'])
-                      if isdir(opj(self.path, i['file'])) else i['file']
-                      # for consistency with 'git status' return directories
-                      # with trailing path separator
-                      for i in json_list if i['status'] == st]
-                for cond, key, st in key_mapping if cond}
-
-    @borrowdoc(GitRepo)
-    def is_dirty(self, index=True, working_tree=False, untracked_files=True,
-                 submodules=True, path=None):
-        # TODO: Add doc on how this differs from GitRepo.is_dirty()
-        # Parameter working_tree exists to meet the signature of GitRepo.is_dirty()
-
-        if working_tree:
-            # Note: annex repos don't always have a git working tree and the
-            # behaviour in direct mode or V6 repos is fundamentally different
-            # from that concept. There are no unstaged changes in direct mode
-            # for example. Therefore the need to call this method with
-            # 'working_tree=True' indicates invalid assumptions in the
-            # calling code.
+        Returns
+        -------
+        dict
+          supported -> list of supported versions (int)
+          upgradable -> list of upgradable versions (int)
+        """
+        if cls.repository_versions is None:
+            key_remap = {
+                "supported repository versions": "supported",
+                "upgrade supported from repository versions": "upgradable"}
+            out = GitWitlessRunner().run(
+                ["git", "annex", "version"],
+                protocol=StdOutErrCapture)
+            kvs = (ln.split(":", 1) for ln in out['stdout'].splitlines())
+            cls.repository_versions = {
+                key_remap[k]: list(map(int, v.strip().split()))
+                for k, v in kvs if k in key_remap}
+        return cls.repository_versions
 
-            # TODO: Better exception. InvalidArgumentError or sth ...
-            raise CommandNotAvailableError(
-                "Querying a git-annex repository for a clean/dirty "
-                "working tree is an invalid concept.")
-        # Again note, that 'annex status' isn't distinguishing staged and
-        # unstaged changes, since this makes little sense for an annex repo
-        # in general. Therefore we use only 'index' and 'untracked_files' to
-        # specify what kind of dirtyness we are interested in:
-        status = self.get_status(untracked=untracked_files, deleted=index,
-                                 modified=index, added=index,
-                                 type_changed=index, submodules=submodules,
-                                 path=path)
-        return any([bool(status[i]) for i in status])
+    @classmethod
+    def _check_version_kludges(cls, key):
+        """Cache some annex-version-specific kludges in one go.
 
-    @property
-    def untracked_files(self):
-        """Get a list of untracked files
+        Return the kludge under `key`.
         """
-        return self.get_status(untracked=True, deleted=False, modified=False,
-                               added=False, type_changed=False, submodules=False,
-                               path=None)['untracked']
+        kludges = cls._version_kludges
+        if kludges:
+            return kludges[key]
+
+        if cls.git_annex_version is None:
+            cls._check_git_annex_version()
+
+        ver = cls.git_annex_version
+        kludges["fromkey-supports-unlocked"] = ver > "8.20210428"
+        # applies to get, drop, move, copy, whereis
+        kludges["grp1-supports-batch-keys"] = ver >= "8.20210903"
+        # applies to find, findref to list all known.
+        # was added in 10.20221212-17-g0b2dd374d on 20221220.
+        kludges["find-supports-anything"] = ver >= "10.20221213"
+        # applies to log, unannex and may be other commands,
+        # was added 10.20230407 release, respecting core.quotepath
+        kludges["quotepath-respected"] = \
+            "yes" if ver >= '10.20230408' else \
+            "maybe" if ver > '10.20230407' else \
+            "no"
+        cls._version_kludges = kludges
+        return kludges[key]
 
     @classmethod
-    def _check_git_annex_version(cls):
-        ver = external_versions['cmd:annex']
-        # in case it is missing
-        if linux_distribution_name in {'debian', 'ubuntu'}:
-            msg = "Install  git-annex-standalone  from NeuroDebian " \
-                  "(http://neuro.debian.net)"
+    def _unquote_annex_path(cls, s):
+        """Remove surrounding "" around the filename, and unquote \"
+
+        This is minimal necessary transformation of the quoted filename in care of
+        core.quotepath=false, i.e. whenever all unicode characters remain as is.
+
+        All interfaces should aim to operate on --json machine readable output,
+        so we are not striving to have it super efficient here since should not be used
+        often.
+        """
+        respected = cls._check_version_kludges('quotepath-respected')
+        if respected == 'no':
+            return s
+        quoted = s.startswith('"') and s.endswith('"')
+        if respected in ('maybe', 'yes'):
+            # not necessarily correct if e.g. filename has "" around it originally
+            # but this is a check only for a range of development versions, so mostly
+            # for local/CI runs ATM
+            if not quoted:
+                return s
         else:
-            msg = "Visit http://git-annex.branchable.com/install/"
-        exc_kwargs = dict(
-            name="git-annex",
-            msg=msg,
-            ver=cls.GIT_ANNEX_MIN_VERSION
-        )
-        if not ver:
-            raise MissingExternalDependency(**exc_kwargs)
-        elif ver < cls.GIT_ANNEX_MIN_VERSION:
-            raise OutdatedExternalDependency(ver_present=ver, **exc_kwargs)
-        cls.git_annex_version = ver
+            raise RuntimeError(f"Got unknown {respected}")
+        return s[1:-1].replace(r'\"', '"')
 
     @staticmethod
     def get_size_from_key(key):
-        """A little helper to obtain size encoded in a key"""
-        try:
-            size_str = key.split('-', 2)[1].lstrip('s')
-        except IndexError:
-            # has no 2nd field in the key
+        """A little helper to obtain size encoded in a key
+
+        Returns
+        -------
+        int or None
+          size of the file or None if either no size is encoded in the key or
+          key was None itself
+
+        Raises
+        ------
+        ValueError
+          if key is considered invalid (at least its size-related part)
+        """
+        if not key:
             return None
-        return int(size_str) if size_str.isdigit() else None
+
+        # see: https://git-annex.branchable.com/internals/key_format/
+        key_parts = key.split('--')
+        key_fields = key_parts[0].split('-')
+        parsed = {field[0]: int(field[1:]) if field[1:].isdigit() else None
+                  for field in key_fields[1:]
+                  if field[0] in "sSC"}
+
+        # don't lookup the dict for the same things several times;
+        # Is there a faster (and more compact) way of doing this? Note, that
+        # locals() can't be updated.
+        s = parsed.get('s')
+        S = parsed.get('S')
+        C = parsed.get('C')
+
+        if S is None and C is None:
+            return s  # also okay if s is None as well -> no size to report
+        elif s is None:
+            # s is None, while S and/or C are not.
+            raise ValueError("invalid key: {}".format(key))
+        elif S and C:
+            if C <= int(s / S):
+                return S
+            else:
+                return s % S
+        else:
+            # S or C are given with the respective other one missing
+            raise ValueError("invalid key: {}".format(key))
 
     @normalize_path
     def get_file_size(self, path):
         fpath = opj(self.path, path)
         return 0 if not exists(fpath) else os.stat(fpath).st_size
 
-    # TODO: Once the PR containing super class 'Repo' was merged, move there and
-    # melt with GitRepo.get_toppath including tests for both
-    @classmethod
-    def get_toppath(cls, path, follow_up=True, git_options=None):
-        """Return top-level of a repository given the path.
+    def is_initialized(self):
+        """quick check whether this appears to be an annex-init'ed repo
+        """
+        # intended to avoid calling self._init, when it's not needed, since this check is clearly
+        # cheaper than git-annex-init (which would be safe to just call)
 
-        Parameters
-        -----------
-        follow_up : bool
-          If path has symlinks -- they get resolved by git.  If follow_up is
-          True, we will follow original path up until we hit the same resolved
-          path.  If no such path found, resolved one would be returned.
-        git_options: list of str
-          options to be passed to the git rev-parse call
-
-        Return None if no parent directory contains a git repository.
-        """
-
-        # first try plain git result:
-        toppath = GitRepo.get_toppath(path=path, follow_up=follow_up,
-                                      git_options=git_options)
-        if toppath == '':
-            # didn't fail, so git itself didn't come to the conclusion
-            # there is no repo, but we have no actual result;
-            # might be an annex in direct mode
-            if git_options is None:
-                git_options = []
-            # TODO: Apparently doesn't work with git 2.11.0
-            # Note: Since we are in a classmethod, GitRepo.get_toppath uses
-            # Runner directly instead of _git_custom_command, which is why the
-            # common mechanics for direct mode are not applied.
-            # This is why there is no solution for git 2.11 yet
-
-            # Note 2: Actually, the above issue is irrelevant. The git
-            # executable has no repository it is bound to, since it's the
-            # purpose of the call to find this repository. Therefore
-            # core.bare=False has no effect at all.
-
-            # Disabled. See notes.
-            # git_options.extend(['-c', 'core.bare=False'])
-            # toppath = GitRepo.get_toppath(path=path, follow_up=follow_up,
-            #                               git_options=git_options)
-
-            # basically a copy of code in GitRepo.get_toppath
-            # except it uses 'git rev-parse --git-dir' as a workaround for
-            # direct mode:
-
-            from os.path import dirname
-            from os import pardir
-
-            cmd = ['git']
-            if git_options:
-                cmd.extend(git_options)
-
-            cmd.append("rev-parse")
-            if external_versions['cmd:git'] >= '2.13.0':
-                cmd.append("--absolute-git-dir")
-            else:
-                cmd.append("--git-dir")
+        return (self.dot_git / 'annex').exists()
 
-            try:
-                toppath, err = GitRunner().run(
-                    cmd,
-                    cwd=path,
-                    log_stdout=True, log_stderr=True,
-                    expect_fail=True, expect_stderr=True)
-                toppath = toppath.rstrip('\n\r')
-            except CommandError:
-                return None
-            except OSError:
-                toppath = AnnexRepo.get_toppath(dirname(path),
-                                                follow_up=follow_up,
-                                                git_options=git_options)
-
-            if external_versions['cmd:git'] < '2.13.0':
-                # we got a path relative to `path` instead of an absolute one
-                toppath = opj(path, toppath)
-
-            # we got the git-dir. Assuming the root dir we are looking for is
-            # one level up:
-            toppath = realpath(normpath(opj(toppath, pardir)))
-
-            if follow_up:
-                path_ = path
-                path_prev = ""
-                while path_ and path_ != path_prev:  # on top /.. = /
-                    if realpath(path_) == toppath:
-                        toppath = path_
-                        break
-                    path_prev = path_
-                    path_ = dirname(path_)
+    @borrowdoc(GitRepo, 'is_valid_git')
+    def is_valid_annex(self, allow_noninitialized=False, check_git=True):
 
-        return toppath
+        initialized_annex = (self.is_valid_git() if check_git else True) and (self.dot_git / 'annex').exists()
+
+        if allow_noninitialized:
+            try:
+                return initialized_annex or ((self.is_valid_git() if check_git else True) and self.is_with_annex())
+            except (NoSuchPathError, InvalidGitRepositoryError):
+                return False
+        else:
+            return initialized_annex
 
     @classmethod
     def is_valid_repo(cls, path, allow_noninitialized=False):
         """Return True if given path points to an annex repository
         """
-        # Note: default value for allow_noninitialized=False is important
-        # for invalidating an instance via self._flyweight_invalid. If this is
-        # changed, we also need to override _flyweight_invalid and explicitly
-        # pass allow_noninitialized=False!
 
         def git_file_has_annex(p):
             """Return True if `p` contains a .git file, that points to a git
             dir with a subdir 'annex'"""
             _git = opj(p, '.git')
             if not os.path.isfile(_git):
                 return False
@@ -863,65 +677,86 @@
                 if line.startswith("gitdir: "):
                     return exists(opj(p, line[8:], 'annex'))
                 else:
                     lgr.debug("Invalid .git file: %s", _git)
                     return False
 
         initialized_annex = GitRepo.is_valid_repo(path) and \
-            (exists(opj(path, '.git', 'annex')) or
-             git_file_has_annex(path))
+                            (exists(opj(path, '.git', 'annex')) or
+                             git_file_has_annex(path))
 
         if allow_noninitialized:
             try:
-                return initialized_annex \
-                    or GitRepo(path, create=False, init=False).is_with_annex()
+                return initialized_annex or GitRepo(path, create=False, init=False).is_with_annex()
             except (NoSuchPathError, InvalidGitRepositoryError):
                 return False
         else:
             return initialized_annex
 
-    def add_remote(self, name, url, options=None):
-        """Overrides method from GitRepo in order to set
-        remote.<name>.annex-ssh-options in case of a SSH remote."""
-        super(AnnexRepo, self).add_remote(name, url, options if options else [])
-        self._set_shared_connection(name, url)
-
     def set_remote_url(self, name, url, push=False):
-        """Overrides method from GitRepo in order to set
-        remote.<name>.annex-ssh-options in case of a SSH remote."""
+        """Set the URL a remote is pointing to
 
-        super(AnnexRepo, self).set_remote_url(name, url, push=push)
-        self._set_shared_connection(name, url)
+        Sets the URL of the remote `name`. Requires the remote to already exist.
+
+        Parameters
+        ----------
+        name: str
+          name of the remote
+        url: str
+        push: bool
+          if True, set the push URL, otherwise the fetch URL;
+          if True, additionally set annexurl to `url`, to make sure annex uses
+          it to talk to the remote, since access via fetch URL might be
+          restricted.
+        """
+
+        if push:
+            # if we are to set a push url, also set 'annexUrl' for this remote,
+            # in order to make git-annex use it, when talking to the remote.
+            # (see http://git-annex.branchable.com/bugs/annex_ignores_pushurl_and_uses_only_url_upon___34__copy_--to__34__/)
+            var = 'remote.{0}.{1}'.format(name, 'annexurl')
+            self.config.set(var, url, scope='local', reload=True)
+        super(AnnexRepo, self).set_remote_url(name, url, push)
 
     def set_remote_dead(self, name):
         """Announce to annex that remote is "dead"
         """
-        return self._annex_custom_command([], ["git", "annex", "dead", name])
+        return self.call_annex(["dead", name])
 
     def is_remote_annex_ignored(self, remote):
         """Return True if remote is explicitly ignored"""
         return self.config.getbool(
             'remote.{}'.format(remote), 'annex-ignore',
             default=False
         )
 
     def is_special_annex_remote(self, remote, check_if_known=True):
-        """Return either remote is a special annex remote
+        """Return whether remote is a special annex remote
 
-        Decides based on the presence of diagnostic annex- options
-        for the remote
+        Decides based on the presence of an annex- option and lack of a
+        configured URL for the remote.
         """
         if check_if_known:
             if remote not in self.get_remotes():
                 raise RemoteNotAvailableError(remote)
-        sec = 'remote.{}'.format(remote)
-        for opt in ('annex-externaltype', 'annex-webdav'):
-            if self.config.has_option(sec, opt):
-                return True
-        return False
+        opts = self.config.options('remote.{}'.format(remote))
+        if "url" in opts:
+            is_special = False
+        elif any(o.startswith("annex-") for o in opts
+                 if o not in ["annex-uuid", "annex-ignore"]):
+            # It's possible that there isn't a special-remote related option
+            # (we only filter out a few common ones), but given that there is
+            # no URL it should be a good bet that this is a special remote.
+            is_special = True
+        else:
+            is_special = False
+            lgr.warning("Remote '%s' has no URL or annex- option. "
+                        "Is it mis-configured?",
+                        remote)
+        return is_special
 
     @borrowkwargs(GitRepo)
     def get_remotes(self,
                     with_urls_only=False,
                     exclude_special_remotes=False):
         """Get known (special-) remotes of the repository
 
@@ -941,141 +776,665 @@
             return [
                 remote for remote in remotes
                 if not self.is_special_annex_remote(remote, check_if_known=False)
             ]
         else:
             return remotes
 
-    def get_special_remotes(self):
+    def get_special_remotes(self, include_dead:bool = False) -> Dict[str, dict]:
         """Get info about all known (not just enabled) special remotes.
 
+        The present implementation is not able to report on special remotes
+        that have only been configured in a private annex repo
+        (annex.private=true).
+
+        Parameters
+        ----------
+        include_dead: bool, optional
+          Whether to include remotes announced dead.
+
         Returns
         -------
         dict
-          Keys are special remore UUIDs, values are dicts with arguments
-          for `git-annex enableremote`. This includes at least the 'type'
-          and 'name' of a special remote. Each type of special remote
-          may require addition arguments that will be available in the
-          respective dictionary.
+          Keys are special remote UUIDs. Each value is a dictionary with
+          configuration information git-annex has for the remote. This should
+          include the 'type' and 'name' as well as any `initremote` parameters
+          that git-annex stores.
+
+          Note: This is a faithful translation of git-annex:remote.log with one
+          exception. For a special remote initialized with the --sameas flag,
+          git-annex stores the special remote name under the "sameas-name" key,
+          we copy this value under the "name" key so that callers don't have to
+          check two places for the name. If you need to detect whether you're
+          working with a sameas remote, the presence of either "sameas-name" or
+          "sameas-uuid" is a reliable indicator.
         """
+        argspec = re.compile(r'^([^=]*)=(.*)$')
+        srs = {}
+
+        # We provide custom implementation to access this metadata since ATM
+        # no git-annex command exposes it on CLI.
+        #
+        # Information will potentially be obtained from remote.log within
+        # git-annex branch, and git-annex's journal, which might exist e.g.
+        # due to alwayscommit=false operations
+        sources = []
         try:
-            stdout, stderr = self._git_custom_command(
-                None, ['git', 'cat-file', 'blob', 'git-annex:remote.log'],
-                expect_fail=True)
+            sources.append(
+                list(
+                    self.call_git_items_(
+                        ['cat-file', 'blob', 'git-annex:remote.log'],
+                        read_only=True)
+                )
+            )
         except CommandError as e:
-            if 'Not a valid object name git-annex:remote.log' in e.stderr:
-                # no special remotes configures
-                return {}
+            if (
+                ('Not a valid object name git-annex:remote.log' in e.stderr) or  # e.g. git 2.30.2
+                ("fatal: path 'remote.log' does not exist in 'git-annex'" in e.stderr) or # e.g. 2.35.1+next.20220211-1
+                ("fatal: invalid object name 'git-annex'" in e.stderr) # e.g., 2.43.0
+            ):
+                # no special remotes configured - might still be in the journal
+                pass
             else:
-                # some unforseen error
+                # some unforeseen error
                 raise e
-        argspec = re.compile(r'^([^=]*)=(.*)$')
-        srs = {}
-        for line in stdout.splitlines():
+
+        journal_path = self.dot_git / "annex" / "journal" / "remote.log"
+        if journal_path.exists():
+            sources.append(journal_path.read_text().splitlines())
+
+        for line in chain(*sources):
             # be precise and split by spaces
             fields = line.split(' ')
             # special remote UUID
             sr_id = fields[0]
             # the rest are config args for enableremote
             sr_info = dict(argspec.match(arg).groups()[:2] for arg in fields[1:])
+            if "name" not in sr_info:
+                name = sr_info.get("sameas-name")
+                if name is None:
+                    lgr.warning(
+                        "Encountered git-annex remote without a name or "
+                        "sameas-name value: %s",
+                        sr_info)
+                else:
+                    sr_info["name"] = name
             srs[sr_id] = sr_info
+
+        # remove dead ones
+        if not include_dead:
+            # code largely copied from drop.py:_detect_nondead_annex_at_remotes
+            # but not using -p and rather blob as above
+            try:
+                for line in self.call_git_items_(
+                        ['cat-file', 'blob', 'git-annex:trust.log']):
+                    columns = line.split()
+                    if columns[1] == 'X':
+                        # .pop if present
+                        srs.pop(columns[0], None)
+            except CommandError as e:
+                # this is not a problem per-se, probably file is not there, just log
+                CapturedException(e)
         return srs
 
-    def __repr__(self):
-        return "<AnnexRepo path=%s (%s)>" % (self.path, type(self))
+    def _call_annex(self, args, files=None, jobs=None, protocol=StdOutErrCapture,
+                    git_options=None, stdin=None, merge_annex_branches=True,
+                    **kwargs):
+        """Internal helper to run git-annex commands
 
-    def _run_annex_command(self, annex_cmd,
-                           git_options=None, annex_options=None,
-                           backend=None, jobs=None,
-                           files=None,
-                           **kwargs):
-        """Helper to run actual git-annex calls
-
-        Unifies annex command calls.
+        Standard command options are applied in addition to the given arguments,
+        and certain error conditions are detected (if possible) and dedicated
+        exceptions are raised.
 
         Parameters
         ----------
-        annex_cmd: str
-            the actual git-annex command, like 'init' or 'add'
-        git_options: list of str
-            options to be passed to git
-        annex_options: list of str
-            options to be passed to the git-annex command
-        backend: str
-            backend to be used by this command; Currently this can also be
-            achieved by having an item '--backend=XXX' in annex_options.
-            This may change.
-        jobs : int
+        args: list
+          List of git-annex command arguments.
         files: list, optional
-            If command passes list of files
-        **kwargs
-            these are passed as additional kwargs to datalad.cmd.Runner.run()
+          If command passes list of files. If list is too long
+          (by number of files or overall size) it will be split, and multiple
+          command invocations will follow
+        jobs : int or 'auto', optional
+          If 'auto', the number of jobs will be determined automatically,
+          informed by the configuration setting
+          'datalad.runtime.max-annex-jobs'.
+        protocol : WitlessProtocol, optional
+          Protocol class to pass to GitWitlessRunner.run(). By default this is
+          StdOutErrCapture, which will provide default logging behavior and
+          guarantee that stdout/stderr are included in potential CommandError
+          exception.
+        git_options: list, optional
+          Additional arguments for Git to include in the git-annex call
+          (in a position prior to the 'annex' subcommand.
+        stdin: File-like, optional
+          stdin to connect to the git-annex process. Only used when `files`
+          is None.
+        merge_annex_branches: bool, optional
+          If False, annex.merge-annex-branches=false config will be set for
+          git-annex call.  Useful for operations which are not intended to
+          benefit from updating information about remote git-annexes
+        **kwargs:
+          Additional arguments are passed on to the WitlessProtocol constructor
+
+        Returns
+        -------
+        dict
+          Return value of WitlessRunner.run(). The content of the dict is
+          determined by the given `protocol`. By default, it provides git-annex's
+          stdout and stderr (under these key names)
 
         Raises
         ------
-        CommandNotAvailableError
-            if an annex command call returns "unknown command"
+        CommandError
+          If the call exits with a non-zero status.
+
+        OutOfSpaceError
+          If a corresponding statement was detected in git-annex's output on
+          stderr. Only supported if the given protocol captured stderr.
+
+        RemoteNotAvailableError
+          If a corresponding statement was detected in git-annex's output on
+          stderr. Only supported if the given protocol captured stderr.
         """
-        debug = ['--debug'] if lgr.getEffectiveLevel() <= 8 else []
-        backend = ['--backend=%s' % backend] if backend else []
+        if self.git_annex_version is None:
+            self._check_git_annex_version()
 
-        git_options = (git_options[:] if git_options else []) + self._GIT_COMMON_OPTIONS
-        annex_options = annex_options[:] if annex_options else []
-        if self._annex_common_options:
-            annex_options = self._annex_common_options + annex_options
+        # git portion of the command
+        cmd = ['git'] + self._ANNEX_GIT_COMMON_OPTIONS
+
+        if git_options:
+            cmd += git_options
 
         if not self.always_commit:
-            git_options += ['-c', 'annex.alwayscommit=false']
+            cmd += ['-c', 'annex.alwayscommit=false']
 
-        if git_options:
-            cmd_list = ['git'] + git_options + ['annex']
-        else:
-            cmd_list = ['git-annex']
-        if jobs:
-            annex_options += ['-J%d' % jobs]
+        if not merge_annex_branches:
+            cmd += ['-c', 'annex.merge-annex-branches=false']
+
+        # annex portion of the command
+        cmd.append('annex')
+        cmd += args
+
+        if lgr.getEffectiveLevel() <= 8:
+            cmd.append('--debug')
 
-        cmd_list += [annex_cmd] + backend + debug + annex_options
+        if self._annex_common_options:
+            cmd += self._annex_common_options
+
+        if jobs == 'auto':
+            # Limit to # of CPUs (but at least 3 to start with)
+            # and also an additional config constraint (by default 1
+            # due to https://github.com/datalad/datalad/issues/4404)
+            jobs = self._n_auto_jobs or min(
+                self.config.obtain('datalad.runtime.max-annex-jobs'),
+                max(3, cpu_count()))
+            # cache result to avoid repeated calls to cpu_count()
+            self._n_auto_jobs = jobs
+        if jobs and jobs != 1:
+            cmd.append('-J%d' % jobs)
+
+        runner = self._git_runner
+        env = None
+        if self.fake_dates_enabled:
+            env = self.add_fake_dates(runner.env)
 
-        if files:
-            cmd_list += ['--'] + files
         try:
-            return self.cmd_call_wrapper.run(cmd_list, **kwargs)
-        except CommandError as e:
-            if e.stderr and "git-annex: Unknown command '%s'" % annex_cmd in e.stderr:
-                raise CommandNotAvailableError(str(cmd_list),
-                                               "Unknown command:"
-                                               " 'git-annex %s'" % annex_cmd,
-                                               e.code, e.stdout, e.stderr)
+            if files:
+                if issubclass(protocol, GeneratorMixIn):
+                    return runner.run_on_filelist_chunks_items_(
+                        cmd,
+                        files,
+                        protocol=protocol,
+                        env=env,
+                        **kwargs)
+                else:
+                    return runner.run_on_filelist_chunks(
+                        cmd,
+                        files,
+                        protocol=protocol,
+                        env=env,
+                        **kwargs)
             else:
+                return runner.run(
+                    cmd,
+                    stdin=stdin,
+                    protocol=protocol,
+                    env=env,
+                    **kwargs)
+        except CommandError as e:
+            # Note: A call might result in several 'failures', that can be or
+            # cannot be handled here. Detection of something, we can deal with,
+            # doesn't mean there's nothing else to deal with.
+
+            # OutOfSpaceError:
+            # Note:
+            # doesn't depend on anything in stdout. Therefore check this before
+            # dealing with stdout
+            out_of_space_re = re.search(
+                "not enough free space, need (.*) more", e.stderr
+            )
+            if out_of_space_re:
+                raise OutOfSpaceError(cmd=['annex'] + args,
+                                      sizemore_msg=out_of_space_re.groups()[0])
+
+            # RemoteNotAvailableError:
+            remote_na_re = re.search(
+                "there is no available git remote named \"(.*)\"", e.stderr
+            )
+            if remote_na_re:
+                raise RemoteNotAvailableError(cmd=['annex'] + args,
+                                              remote=remote_na_re.groups()[0])
+
+            # TEMP: Workaround for git-annex bug, where it reports success=True
+            # for annex add, while simultaneously complaining, that it is in
+            # a submodule:
+            # TODO: For now just reraise. But independently on this bug, it
+            # makes sense to have an exception for that case
+            in_subm_re = re.search(
+                "fatal: Pathspec '(.*)' is in submodule '(.*)'", e.stderr
+            )
+            if in_subm_re:
+                raise e
+
+            # we don't know how to handle this, just pass it on
+            raise
+
+    def _call_annex_records(self, args, files=None, jobs=None,
+                            git_options=None,
+                            stdin=None,
+                            merge_annex_branches=True,
+                            progress=False,
+                            **kwargs):
+        """Internal helper to run git-annex commands with JSON result processing
+
+        `_call_annex()` is used for git-annex command execution, using
+        AnnexJsonProtocol.
+
+        Parameters
+        ----------
+        args: list
+          See `_call_annex()` for details.
+        files: list, optional
+          See `_call_annex()` for details.
+        jobs : int or 'auto', optional
+          See `_call_annex()` for details.
+        git_options: list, optional
+          See `_call_annex()` for details.
+        stdin: File-like, optional
+          See `_call_annex()` for details.
+        merge_annex_branches: bool, optional
+          See `_call_annex()` for details.
+        **kwargs:
+          Additional arguments are passed on to the AnnexJsonProtocol constructor
+
+        Returns
+        -------
+        list(dict)
+          List of parsed result records.
+
+        Raises
+        ------
+        CommandError
+          See `_call_annex()` for details.
+        OutOfSpaceError
+          See `_call_annex()` for details.
+        RemoteNotAvailableError
+          See `_call_annex()` for details.
+        RuntimeError
+          Output from the git-annex process was captured, but no structured
+          records could be parsed.
+        """
+        protocol = AnnexJsonProtocol
+
+        args = args[:] + ['--json', '--json-error-messages']
+        if progress:
+            args += ['--json-progress']
+
+        out = None
+        try:
+            out = self._call_annex(
+                args,
+                files=files,
+                jobs=jobs,
+                protocol=protocol,
+                git_options=git_options,
+                stdin=stdin,
+                merge_annex_branches=merge_annex_branches,
+                **kwargs,
+            )
+        except CommandError as e:
+            not_existing = None
+            if e.kwargs.get('stdout_json'):
+                # See if may be it was within stdout_json, as e.g. was added around
+                # 10.20230407-99-gbe36e208c2 to 'add' together with
+                # 'message-id': 'FileNotFound'
+                out = {'stdout_json': e.kwargs.get('stdout_json', [])}
+                not_existing = []
+                for j in out['stdout_json']:
+                    if j.get('message-id') == 'FileNotFound':
+                        not_existing.append(j['file'])
+                        # for consistency with our "_fake_json_for_non_existing" records
+                        # but not overloading one if there is one
+                        j.setdefault('note', 'not found')
+
+            if not not_existing:
+                # Workaround for not existing files as long as older annex doesn't
+                # report it within JSON.
+                # see http://git-annex.branchable.com/bugs/copy_does_not_reflect_some_failed_copies_in_--json_output/
+                not_existing = _get_non_existing_from_annex_output(e.stderr)
+                if not_existing:
+                    if not out:
+                        out = {'stdout_json': []}
+                    out['stdout_json'].extend(_fake_json_for_non_existing(not_existing, args[0]))
+
+            # Note: insert additional code here to analyse failure and possibly
+            # raise a custom exception
+
+            # If it was not about non-existing but running failed -- re-raise
+            if not not_existing:
+                raise e
+
+            #if e.stderr:
+            #    # else just warn about present errors
+            #    shorten = lambda x: x[:1000] + '...' if len(x) > 1000 else x
+
+            #    _log = lgr.debug if kwargs.get('expect_fail', False) else lgr.warning
+            #    _log(
+            #        "Running %s resulted in stderr output: %s",
+            #        args, shorten(e.stderr)
+            #    )
+
+        # git-annex fails to non-zero exit when reporting an error on
+        # non-existing paths in some versions and/or commands.
+        # Hence, check for it on non-failure, too. This became apparent with
+        # annex 10.20220222, but was a somewhat "hidden" issue for longer.
+        #
+        # Note, that this may become unnecessary after annex'
+        # ce91f10132805d11448896304821b0aa9c6d9845 (Feb 28, 2022)
+        # "fix annex.skipunknown false error propagation"
+        if 'stderr' in out:
+            not_existing = _get_non_existing_from_annex_output(out['stderr'])
+            if not_existing:
+                if out is None:
+                    out = {'stdout_json': []}
+                out['stdout_json'].extend(
+                    _fake_json_for_non_existing(not_existing, args[0])
+                )
+
+        json_objects = out.pop('stdout_json')
+
+        if out.get('stdout'):
+            if json_objects:
+                # We at least received some valid json output, so warn about
+                # non-json output and continue.
+                lgr.warning("Received non-json lines for --json command: %s",
+                            out)
+            else:
+                raise RuntimeError(
+                    "Received no json output for --json command, only:\n{}"
+                    .format(out))
+
+        # A special remote might send a message via "info". This is supposed
+        # to be printed by annex but in case of
+        # `--json` is returned by annex as "{'info': '<message>'}". See
+        # https://git-annex.branchable.com/design/external_special_remote_protocol/#index5h2
+        #
+        # So, Ben thinks we should just spit it out here, since everything
+        # calling _call_annex_records is concerned with the actual results
+        # being returned. Moreover, this kind of response is special to
+        # particular special remotes rather than particular annex commands.
+        # So, likely there's nothing callers could do about it other than
+        # spitting it out.
+        return_objects = []
+        for obj in json_objects:
+            if len(obj.keys()) == 1 and obj['info']:
+                lgr.info(obj['info'])
+            else:
+                return_objects.append(obj)
+
+        return return_objects
+
+    def _call_annex_records_items_(self,
+                                   args,
+                                   files=None,
+                                   jobs=None,
+                                   git_options=None,
+                                   stdin=None,
+                                   merge_annex_branches=True,
+                                   progress=False,
+                                   **kwargs):
+        """Yielding git-annex command execution with JSON result processing
+
+        `_call_annex()` is used for git-annex command execution, using
+        GeneratorAnnexJsonProtocol. This means _call_annex() will yield
+        results as soon as they are available.
+
+        For a description of the parameters and raised exceptions, please
+        refer to _call_annex_records().
+
+        Returns
+        -------
+        Generator(something)
+        list(dict)
+          List of parsed result records.
+        """
+        protocol_class = GeneratorAnnexJsonProtocol
+
+        args = args[:] + ['--json', '--json-error-messages']
+        if progress:
+            args += ['--json-progress']
+
+        json_objects_received = False
+        try:
+            for json_object in self._call_annex(
+                                      args,
+                                      files=files,
+                                      jobs=jobs,
+                                      protocol=protocol_class,
+                                      git_options=git_options,
+                                      stdin=stdin,
+                                      merge_annex_branches=merge_annex_branches,
+                                      **kwargs):
+                if len(json_object) == 1 and json_object.get('info', None):
+                    lgr.info(json_object['info'])
+                else:
+                    json_objects_received = True
+                    yield json_object
+
+        except CommandError as e:
+            # Note: Workaround for not existing files as long as annex doesn't
+            # report it within JSON response:
+            # see http://git-annex.branchable.com/bugs/copy_does_not_reflect_some_failed_copies_in_--json_output/
+            not_existing = _get_non_existing_from_annex_output(e.stderr)
+            yield from _fake_json_for_non_existing(not_existing, args[0])
+
+            # Note: insert additional code here to analyse failure and possibly
+            # raise a custom exception
+
+            # if we didn't raise before, just depend on whether or not we seem
+            # to have some json to return. It should contain information on
+            # failure in keys 'success' and 'note'
+            # TODO: This is not entirely true. 'annex status' may return empty,
+            # while there was a 'fatal:...' in stderr, which should be a
+            # failure/exception
+            # Or if we had empty stdout but there was stderr
+            if json_objects_received is False and e.stderr:
                 raise e
 
-    def _run_simple_annex_command(self, *args, **kwargs):
-        """Run an annex command and return its output, of which expect 1 line
+        # In contrast to _call_annex_records, this method does not warn about
+        # additional non-JSON data on stdout, nor does is raise a RuntimeError
+        # if only non-JSON data was received on stdout.
+        return
+
+    def call_annex_records(self, args, files=None):
+        """Call annex with `--json*` to request structured result records
 
-        Just a little helper to interact with basic annex commands and process
-        their output while ignoring some messages
+        This method behaves like `call_annex()`, but returns parsed result
+        records.
 
         Parameters
         ----------
-        **kwargs: all passed into _run
+        args : list of str
+          Arguments to pass to `annex`.
+        files : list of str, optional
+          File arguments to pass to `annex`. The advantage of passing these here
+          rather than as part of `args` is that the call will be split into
+          multiple calls to avoid exceeding the maximum command line length.
+
+        Returns
+        -------
+        list(dict)
+          List of parsed result records.
+
+        Raises
+        ------
+        CommandError if the call exits with a non-zero status. All result
+        records captured until the non-zero exit are available in the
+        exception's `kwargs`-dict attribute under key 'stdout_json'.
+
+        See `_call_annex()` for more information on Exceptions.
         """
-        out, err = self._run_annex_command(
-            *args, **kwargs
-        )
-        lines = out.rstrip('\n').splitlines()
-        # ignore some lines which might appear on a fresh clone
+        return self._call_annex_records(args, files=files)
+
+    def call_annex(self, args, files=None):
+        """Call annex and return standard output.
+
+        Parameters
+        ----------
+        args : list of str
+          Arguments to pass to `annex`.
+        files : list of str, optional
+          File arguments to pass to `annex`. The advantage of passing these here
+          rather than as part of `args` is that the call will be split into
+          multiple calls to avoid exceeding the maximum command line length.
+
+        Returns
+        -------
+        standard output (str)
+
+        Raises
+        ------
+        See `_call_annex()` for information on Exceptions.
+        """
+        return self._call_annex(
+            args,
+            files=files,
+            protocol=StdOutErrCapture)['stdout']
+
+    def call_annex_success(self, args, files=None):
+        """Call git-annex and return true if the call exit code of 0.
+
+        All parameters match those described for `call_annex`.
+
+        Returns
+        -------
+        bool
+        """
+        try:
+            self.call_annex(args, files)
+        except CommandError:
+            return False
+        return True
+
+    def call_annex_items_(self, args, files=None, sep=None):
+        """Call git-annex, splitting output on `sep`.
+
+        Parameters
+        ----------
+        args : list of str
+          Arguments to pass to `git-annex`.
+        files : list of str, optional
+          File arguments to pass to `annex`. The advantage of passing these here
+          rather than as part of `args` is that the call will be split into
+          multiple calls to avoid exceeding the maximum command line length.
+        sep : str, optional
+          Split the output by `str.split(sep)` rather than `str.splitlines`.
+
+        Returns
+        -------
+        Generator that yields output items.
+
+        Raises
+        ------
+        See `_call_annex()` for information on Exceptions.
+        """
+        class GeneratorStdOutErrCapture(GeneratorMixIn,
+                                        AssemblingDecoderMixIn,
+                                        StdOutErrCapture):
+            def __init__(self):
+                GeneratorMixIn.__init__(self)
+                AssemblingDecoderMixIn.__init__(self)
+                StdOutErrCapture.__init__(self)
+
+            def pipe_data_received(self, fd, data):
+                if fd == 1:
+                    self.send_result(
+                        ("stdout", self.decode(fd, data, self.encoding)))
+                    return
+                super().pipe_data_received(fd, data)
+
+        line_splitter = LineSplitter(separator=sep)
+        for source, content in self._call_annex(
+                                args,
+                                files=files,
+                                protocol=GeneratorStdOutErrCapture):
+
+            if source == "stdout":
+                yield from line_splitter.process(content)
+
+        remaining_content = line_splitter.finish_processing()
+        if remaining_content is not None:
+            yield remaining_content
+
+    def call_annex_oneline(self, args, files=None):
+        """Call annex for a single line of output.
+
+        This method filters prior output line selection to exclude git-annex
+        status output that is triggered by command execution, but is not
+        related to the particular command. This includes lines like:
+
+          (merging ... into git-annex)
+          (recording state ...)
+
+        Parameters
+        ----------
+        args : list of str
+          Arguments to pass to `annex`.
+        files : list of str, optional
+          File arguments to pass to `annex`. The advantage of passing these here
+          rather than as part of `args` is that the call will be split into
+          multiple calls to avoid exceeding the maximum command line length.
+
+        Returns
+        -------
+        str
+          Either a single output line, or an empty string if there was no
+          output.
+        Raises
+        ------
+        AssertionError if there is more than one line of output.
+
+        See `_call_annex()` for information on Exceptions.
+        """
+        # ignore some lines
         # see https://git-annex.branchable.com/todo/output_of_wanted___40__and_possibly_group_etc__41___should_not_be_polluted_with___34__informational__34___messages/
-        lines_ = [
-            l for l in lines
-            if not re.search(
-                '\((merging .* into git-annex|recording state ).*\.\.\.\)', l
+        # that links claims it is fixed, but '(recording state in git...)'
+        # still appear as of 8.20201103-1
+        lines = [
+            l for l in self.call_annex_items_(args, files=files)
+            if l and not re.search(
+                r'\((merging .* into git-annex|recording state ).*\.\.\.\)', l
             )
         ]
-        assert(len(lines_) <= 1)
-        return lines_[0] if lines_ else None
+
+        if len(lines) > 1:
+            raise AssertionError(
+                "Expected {} to return single line, but it returned {}"
+                .format(["git", 'annex'] + args, lines))
+        return lines[0] if lines else ''
 
     def _is_direct_mode_from_config(self):
         """Figure out if in direct mode from the git config.
 
         Since relies on reading config, expensive to be used often
 
         Returns
@@ -1090,19 +1449,14 @@
     def is_direct_mode(self):
         """Return True if annex is in direct mode
 
         Returns
         -------
         True if in direct mode, False otherwise.
         """
-        # TEMP: Disable lazy loading and make sure to read from file every time
-        # instead, since we might have several instances pointing to the very
-        # same repo atm. TODO: We can remove that, right?
-        self.repo.config_reader()._is_initialized = False
-        self.repo.config_reader().read()
         self._direct_mode = None
 
         if self._direct_mode is None:
             # we need to figure it out
             self._direct_mode = self._is_direct_mode_from_config()
         return self._direct_mode
 
@@ -1113,97 +1467,25 @@
         -------
         True if on crippled filesystem, False otherwise
         """
 
         self.config.reload()
         return self.config.getbool("annex", "crippledfilesystem", False)
 
-    def set_direct_mode(self, enable_direct_mode=True):
-        """Switch to direct or indirect mode
-
-        Parameters
-        ----------
-        enable_direct_mode: bool
-            True means switch to direct mode,
-            False switches to indirect mode
-
-        Raises
-        ------
-        CommandNotAvailableError
-            in case you try to switch to indirect mode on a crippled filesystem
+    @property
+    def supports_unlocked_pointers(self):
+        """Return True if repository version supports unlocked pointers.
         """
-        if self.is_crippled_fs() and not enable_direct_mode:
-            raise CommandNotAvailableError(
-                cmd="git-annex indirect",
-                msg="Can't switch to indirect mode on that filesystem.")
-
-        self._run_annex_command('direct' if enable_direct_mode else 'indirect',
-                                expect_stderr=True)
-        self.config.reload()
-
-        # For paranoid we will just re-request
-        self._direct_mode = None
-        assert(self.is_direct_mode() == enable_direct_mode)
-
-        if self.is_direct_mode():
-            # adjust git options for plain git calls on this repo:
-            # Note: Not sure yet, whether this solves the issue entirely or we
-            # still need 'annex proxy' in some cases ...
-
-            lgr.debug("detected git version: %s" % external_versions['cmd:git'])
-
-            if external_versions['cmd:git'] >= '2.9.0':
-                # workaround for git 2.9.0, which for some reason ignores the
-                # per-call config "-c core.bare=False", but respects the value
-                # if it is set in .git/config
-                self.GIT_DIRECT_MODE_WRAPPER_ACTIVE = True
-
-            # TEMP: nevertheless use this option to inject it into gitpython
-            # TODO: Solve it and change to "elif"
-            if 'core.bare=False' not in self._GIT_COMMON_OPTIONS:
-                # standard direct mode procedure part I:
-                self._GIT_COMMON_OPTIONS.extend(['-c', 'core.bare=False'])
-            if '--work-tree=' not in self._GIT_COMMON_OPTIONS:
-                # standard direct mode procedure part II:
-                self._GIT_COMMON_OPTIONS.append('--work-tree=.')
-
-    def _git_custom_command(self, *args, **kwargs):
-
-        if self.GIT_DIRECT_MODE_PROXY:
-            proxy_str = "git annex proxy -- "
-            proxy_list = ['git', 'annex', 'proxy', '--']
-            cmd = kwargs.pop("cmd_str", None)
-            if not cmd:
-                cmd = args[1]
-            assert(cmd is not None)
-
-            if isinstance(cmd, string_types):
-                cmd = proxy_str + cmd
-            else:
-                cmd = proxy_list + cmd
-
-            args = (args[0], cmd) + args[2:]
-            return super(AnnexRepo, self)._git_custom_command(*args, **kwargs)
-
-        elif self.GIT_DIRECT_MODE_WRAPPER_ACTIVE:
-            old = self.config.get('core.bare')
-            lgr.debug("old config: %s(%s)" % (old, type(old)))
-            if old is not False:
-                self.config.set('core.bare', 'False', where='local')
-
-            out, err = super(AnnexRepo, self)._git_custom_command(*args, **kwargs)
-
-            if old is None:
-                self.config.unset('core.bare', where='local')
-            elif old:
-                self.config.set('core.bare', old, where='local')
-            return out, err
-
-        else:
-            return super(AnnexRepo, self)._git_custom_command(*args, **kwargs)
+        try:
+            return self.config.getint("annex", "version") >= 6
+        except KeyError:
+            # If annex.version isn't set (e.g., an uninitialized repo), assume
+            # that unlocked pointers are supported given that they are with the
+            # minimum git-annex version.
+            return True
 
     def _init(self, version=None, description=None):
         """Initializes an annex repository.
 
         Note: This is intended for private use in this class by now.
         If you have an object of this class already,
         there shouldn't be a need to 'init' again.
@@ -1212,48 +1494,54 @@
         # MIH: this function is required for re-initing repos. The logic
         # in the constructor is rather convoluted and doesn't acknowledge
         # the case of a perfectly healthy annex that just needs a new
         # description
         # will keep leading underscore in the name for know, but this is
         # not private
         # TODO: provide git and git-annex options.
-        # TODO: Document (or implement respectively) behaviour in special cases
-        # like direct mode (if it's different), not existing paths, etc.
         opts = []
         if description is not None:
             opts += [description]
         if version is not None:
+            version = str(version)
+            supported_versions = AnnexRepo.check_repository_versions()['supported']
+            if version not in supported_versions:
+                first_supported_version = int(supported_versions[0])
+                if int(version) < first_supported_version:
+                    lgr.info("Annex repository version %s will be upgraded to %s or later version",
+                             version, first_supported_version)
+                    # and if it is higher than any supported -- we will just let git-annex to do
+                    # what it wants to do
             opts += ['--version', '{0}'.format(version)]
-        if not len(opts):
-            opts = None
 
+        # TODO: RM DIRECT?  or RF at least ?
         # Note: git-annex-init kills a possible tracking branch for
         # 'annex/direct/my_branch', if we just cloned from a repo in direct
         # mode. We want to preserve the information about the tracking branch,
         # as if the source repo wasn't in direct mode.
         # Note 2: Actually we do it for all 'managed branches'. This might turn
         # out to not be necessary
         sections_to_preserve = ["branch.{}".format(branch)
                                 for branch in self.get_branches()
                                 if self.is_managed_branch(branch)
                                 and "branch.{}".format(branch) in
                                 self.config.sections()]
         for sct in sections_to_preserve:
             orig_branch = sct[7:]
-            new_branch = self.get_corresponding_branch(orig_branch)
+            new_branch = \
+                self.get_corresponding_branch(orig_branch) or orig_branch
             new_section = "branch.{}".format(new_branch)
             for opt in self.config.options(sct):
                 orig_value = self.config.get_value(sct, opt)
                 new_value = orig_value.replace(orig_branch, new_branch)
                 self.config.add(var=new_section + "." + opt,
                                 value=new_value,
-                                where='local',
+                                scope='local',
                                 reload=False)
-
-        self._run_annex_command('init', annex_options=opts)
+        self._call_annex(['init'] + opts, protocol=AnnexInitOutput)
         # TODO: When to expect stderr?
         # on crippled filesystem for example (think so)?
         self.config.reload()
 
     @normalize_paths
     def get(self, files, remote=None, options=None, jobs=None, key=False):
         """Get the actual content of files
@@ -1274,67 +1562,77 @@
 
         Returns
         -------
         files : list of dict
         """
         options = options[:] if options else []
 
+        if self.config.get("annex.retry") is None:
+            options.extend(
+                ["-c",
+                 "annex.retry={}".format(
+                     self.config.obtain("datalad.annex.retry"))])
+
         if remote:
             if remote not in self.get_remotes():
                 raise RemoteNotAvailableError(
                     remote=remote,
-                    cmd="get",
+                    cmd="annex get",
                     msg="Remote is not known. Known are: %s"
                     % (self.get_remotes(),)
                 )
+            self._maybe_open_ssh_connection(remote)
             options += ['--from', remote]
 
         # analyze provided files to decide which actually are needed to be
         # fetched
 
         if not key:
             expected_downloads, fetch_files = self._get_expected_files(
-                files, ['--not', '--in', 'here'])
+                files, ['--not', '--in', '.'],
+                merge_annex_branches=False  # interested only in local info
+            )
         else:
             fetch_files = files
             assert len(files) == 1, "When key=True only a single file be provided"
             expected_downloads = {files[0]: AnnexRepo.get_size_from_key(files[0])}
 
         if not fetch_files:
             lgr.debug("No files found needing fetching.")
             return []
 
         if len(fetch_files) != len(files):
-            lgr.info("Actually getting %d files", len(fetch_files))
-
-        # options  might be the '--key' which should go last
-        options = ['--json-progress'] + options
+            lgr.debug("Actually getting %d files", len(fetch_files))
 
         # TODO: provide more meaningful message (possibly aggregating 'note'
         #  from annex failed ones
         # TODO: reproduce DK's bug on OSX, and either switch to
         #  --batch mode (I don't think we have --progress support in long
         #  alive batch processes ATM),
         if key:
-            kwargs = {'opts': options + ['--key'] + files}
+            cmd = ['get'] + options + ['--key'] + files
+            files_arg = None
         else:
-            kwargs = {'opts': options, 'files': files}
-        results = self._run_annex_command_json(
-            'get',
+            cmd = ['get'] + options
+            files_arg = files
+        results = self._call_annex_records(
+            cmd,
             # TODO: eventually make use of --batch mode
+            files=files_arg,
             jobs=jobs,
-            expected_entries=expected_downloads,
-            **kwargs
+            progress=True,
+            # filter(bool,   to avoid trying to add up None's when size is not known
+            total_nbytes=sum(filter(bool, expected_downloads.values())),
         )
         results_list = list(results)
         # TODO:  should we here compare fetch_files against result_list
         # and vomit an exception of incomplete download????
         return results_list
 
-    def _get_expected_files(self, files, expr):
+    def _get_expected_files(self, files, expr, merge_annex_branches=True):
         """Given a list of files, figure out what to be downloaded
 
         Parameters
         ----------
         files
         expr: list
           Expression to be passed into annex's find
@@ -1349,16 +1647,17 @@
         lgr.debug("Determine what files match the query to work with")
         # Let's figure out first which files/keys and of what size to download
         expected_files = {}
         fetch_files = []
         keys_seen = set()
         unknown_sizes = []  # unused atm
         # for now just record total size, and
-        for j in self._run_annex_command_json(
-                'find', opts=expr, files=files
+        for j in self._call_annex_records(
+                ['find'] + expr, files=files,
+                merge_annex_branches=merge_annex_branches
         ):
             # TODO: some files might not even be here.  So in current fancy
             # output reporting scheme we should then theoretically handle
             # those cases here and say 'impossible' or something like that
             if not j.get('success', True):
                 # TODO: I guess do something with yielding and filtering for
                 # what need to be done and what not
@@ -1376,32 +1675,24 @@
                 expected_files[key] = int(size)
             else:
                 expected_files[key] = None
                 unknown_sizes.append(j['file'])
         return expected_files, fetch_files
 
     @normalize_paths
-    def add(self, files, git=None, backend=None, options=None, commit=False,
-            msg=None,
-            jobs=None,
-            git_options=None, annex_options=None, _datalad_msg=False,
-            update=False):
+    def add(self, files, git=None, backend=None, options=None, jobs=None,
+            git_options=None, annex_options=None, update=False):
         """Add file(s) to the repository.
 
         Parameters
         ----------
         files: list of str
           list of paths to add to the annex
         git: bool
           if True, add to git instead of annex.
-        commit: bool
-          whether or not to directly commit
-        msg: str
-          commit message in case `commit=True`. A default message, containing
-          the list of files that were added, is created by default.
         backend:
         options:
         update: bool
           --update option for git-add. From git's manpage:
            Update the index just where it already has an entry matching
            <pathspec>. This removes as well as modifies index entries to match
            the working tree, but adds no new files.
@@ -1412,338 +1703,217 @@
            subdirectories).
 
            Note: Used only, if a call to git-add instead of git-annex-add is
            performed
 
         Returns
         -------
-        list of dict
+        list of dict or dict
         """
 
+        return list(self.add_(
+            files, git=git, backend=backend, options=options, jobs=jobs,
+            git_options=git_options, annex_options=annex_options, update=update
+        ))
+
+    def add_(self, files, git=None, backend=None, options=None, jobs=None,
+            git_options=None, annex_options=None, update=False):
+        """Like `add`, but returns a generator"""
         if update and not git:
             raise InsufficientArgumentsError("option 'update' requires 'git', too")
 
         if git_options:
             # TODO: note that below we would use 'add with --dry-run
             # so passed here options might need to be passed into it??
             lgr.warning("add: git_options not yet implemented. Ignored.")
 
         if annex_options:
             lgr.warning("annex_options not yet implemented. Ignored.")
 
         options = options[:] if options else []
+
+        # TODO: RM DIRECT? not clear if this code didn't become "generic" and
+        #       not only "direct mode" specific, so kept for now.
         # Note: As long as we support direct mode, one should not call
         # super().add() directly. Once direct mode is gone, we might remove
         # `git` parameter and call GitRepo's add() instead.
 
         def _get_to_be_added_recs(paths):
             """Try to collect what actually is going to be added
 
             This is used for progress information
             """
 
-            if self.is_direct_mode():
-                # we already know we can't use --dry-run
-                return self._process_git_get_output(
-                    linesep.join(["'{}'".format(p) for p in paths]))
-            else:
-                # Note: if a path involves a submodule in direct mode, while we
-                # are not in direct mode at current level, we might still fail.
-                # Hence the except clause is still needed. However, this is
-                # unlikely, since direct mode usually should be used only, if it
-                # was enforced by FS and/or OS and therefore concerns the entire
-                # hierarchy.
-                _git_options = ['--dry-run', '-N', '--ignore-missing']
-                try:
-                    return super(AnnexRepo, self).add(
-                        files, git_options=_git_options, update=update)
-                except CommandError as e:
-                    if re.match(
-                            r'.*This operation must be run in a work tree.*git status.*failed in submodule',
-                            e.stderr,
-                            re.MULTILINE | re.DOTALL):
-
-                        lgr.warning(
-                            "Known bug in direct mode."
-                            "We can't use --dry-run when there are submodules in "
-                            "direct mode, because the internal call to git status "
-                            "fails. To be resolved by using (Dataset's) status "
-                            "instead of a git-add --dry-run altogether.")
-                        # fake the return for now
-                        return self._process_git_get_output(
-                            linesep.join(["'{}'".format(f) for f in files]))
-                    else:
-                        # unexpected failure
-                        raise e
+            # TODO: RM DIRECT? might remain useful to detect submods left in direct mode
+            # Note: if a path involves a submodule in direct mode, while we
+            # are not in direct mode at current level, we might still fail.
+            # Hence the except clause is still needed. However, this is
+            # unlikely, since direct mode usually should be used only, if it
+            # was enforced by FS and/or OS and therefore concerns the entire
+            # hierarchy.
+            _git_options = ['--dry-run', '-N', '--ignore-missing']
+            try:
+                for r in super(AnnexRepo, self).add_(
+                        files, git_options=_git_options, update=update):
+                    yield r
+                    return
+            except CommandError as e:
+                ce = CapturedException(e)
+                # TODO: RM DIRECT?  left for detection of direct mode submodules
+                if AnnexRepo._is_annex_work_tree_message(e.stderr):
+                    raise DirectModeNoLongerSupportedError(
+                        self) from e
+                raise
 
         # Theoretically we could have done for git as well, if it could have
         # been batched
-        # Call git annex add for any to have full control of either to go
+        # Call git annex add for any to have full control of whether to go
         # to git or to annex
         # 1. Figure out what actually will be added
         to_be_added_recs = _get_to_be_added_recs(files)
         # collect their sizes for the progressbar
         expected_additions = {
             rec['file']: self.get_file_size(rec['file'])
             for rec in to_be_added_recs
         }
 
         # if None -- leave it to annex to decide
-        if git is not None:
-            if 'annex.version' in self.config and \
-                    self.config.getint("annex", "version") == 6:
-                # Note: For now ugly workaround to prevent unexpected
-                # outcome when adding to git. See:
-                # <http://git-annex.branchable.com/bugs/mysterious_dependency_of_git_annex_status_output_of_the_added_file/>
-                lgr.warning("Workaround: Wait for {} to add to git ({})."
-                            "".format(files, self))
-                time.sleep(1)
-
-            options += [
-                '-c',
-                'annex.largefiles=%s' % (('anything', 'nothing')[int(git)])
-            ]
-            if git:
-                # to maintain behaviour similar to git
-                options += ['--include-dotfiles']
+        if git is False:
+            options.append("--force-large")
 
-        if git and update:
+        if git:
             # explicitly use git-add with --update instead of git-annex-add
             # TODO: This might still need some work, when --update AND files
             # are specified!
-            if self.is_direct_mode():
-                self.GIT_DIRECT_MODE_PROXY = True
-            try:
-                return_list = super(AnnexRepo, self).add(
-                                           files,
-                                           # Note: committing is dealed with
-                                           # later on
-                                           commit=False,
-                                           msg=msg,
-                                           git=True,
-                                           git_options=git_options,
-                                           _datalad_msg=_datalad_msg,
-                                           update=update)
-            finally:
-                if self.is_direct_mode():
-                    # don't accidentally cause other git calls to be done
-                    # via annex-proxy
-                    self.GIT_DIRECT_MODE_PROXY = False
-
-        else:
-            return_list = list(self._run_annex_command_json(
-                'add',
-                opts=options,
-                files=files,
-                backend=backend,
-                expect_fail=True,
-                jobs=jobs,
-                expected_entries=expected_additions,
-                expect_stderr=True
-            ))
-
-        if commit:
-            if msg is None:
-                # TODO: centralize JSON handling
-                if isinstance(return_list, list):
-                    file_list = [d['file'] for d in return_list if d['success']]
-                elif isinstance(return_list, dict):
-                    file_list = [return_list['file']] \
-                        if return_list['success'] else []
-                else:
-                    raise ValueError("Unexpected return type: %s" %
-                                     type(return_list))
-                msg = self._get_added_files_commit_msg(file_list)
-            self.commit(msg, _datalad_msg=_datalad_msg)  # TODO: For consisteny: Also json return value (success)?
-        return return_list
-
-    def proxy(self, git_cmd, **kwargs):
-        """Use git-annex as a proxy to git
-
-        This is needed in case we are in direct mode, since there's no git
-        working tree, that git can handle.
-
-        Parameters
-        ----------
-        git_cmd: list of str
-            the actual git command
-        `**kwargs`: dict, optional
-            passed to _run_annex_command
-
-        Returns
-        -------
-        (stdout, stderr)
-            output of the command call
-        """
-        # TODO: We probably don't need it anymore
+            for r in super(AnnexRepo, self).add(
+                    files,
+                    git=True,
+                    git_options=git_options,
+                    update=update):
+                yield r
 
-        if not self.is_direct_mode():
-            lgr.warning("proxy() called in indirect mode: %s" % git_cmd)
-            raise CommandNotAvailableError(cmd="git annex proxy",
-                                           msg="Proxy doesn't make sense"
-                                               " if not in direct mode.")
-        return self._run_annex_command('proxy',
-                                       annex_options=['--'] + git_cmd,
-                                       **kwargs)
+        else:
+            if backend:
+                options.extend(('--backend', backend))
+            for r in self._call_annex_records(
+                    ['add'] + options,
+                    files=files,
+                    jobs=jobs,
+                    total_nbytes=sum(expected_additions.values())):
+                yield r
 
     @normalize_paths
-    def get_file_key(self, files):
-        """Get key of an annexed file.
+    def get_file_key(self, files, batch=None):
+        """DEPRECATED. Use get_content_annexinfo()
 
-        Parameters
-        ----------
-        files: str or list
-            file(s) to look up
+        See the method body for how to use get_content_annexinfo() to
+        replace get_file_key().
 
-        Returns
-        -------
-        str or list
-            keys used by git-annex for each of the files;
-            in case of a list an empty string is returned if there was no key
-            for that file
+        For single-file queries it is recommended to consider
+        get_file_annexinfo()
         """
+        import warnings
+        warnings.warn(
+            "AnnexRepo.get_file_key() is deprecated, "
+            "use get_content_annexinfo() instead.",
+            DeprecationWarning)
+
+        # this is only needed, because a previous implementation wanted to
+        # disect reasons for not being able to report a key: file not there,
+        # file in git, but not annexed. If not for that, this could be
+        #init = None
+        init = dict(
+            zip(
+                [self.pathobj / f for f in files],
+                [{} for i in range(len(files))]
+            )
+        )
+        info = self.get_content_annexinfo(
+            files,
+            init=init,
+        )
+        keys = [r.get('key', '') for r in info.values()]
 
-        if len(files) > 1:
-            return self._batched.get('lookupkey',
-                                     git_options=self._GIT_COMMON_OPTIONS,
-                                     path=self.path)(files)
-        else:
-            files = files[0]
-            # single file
-            # keep current implementation
-            # TODO: This should change, but involves more RF'ing and an
-            # alternative regarding FileNotInAnnexError
-            cmd_str = 'git annex lookupkey %s' % files  # have a string for messages
+        # everything below is only needed to achieve compatibility with the
+        # complex behavior of a previous implementation if not for that, we
+        # could achieve uniform behavior regardless of input specifics with a
+        # simple
+        #return keys
+
+        if batch is not True and len(files) == 1 and '' in keys:
+            not_found = [
+                p
+                for p, r in info.items()
+                if r.get('success') is False and r.get('note') == 'not found'
+            ]
+            if not_found:
+                raise FileNotInAnnexError(
+                    cmd='find',
+                    msg=f"File not in annex: {not_found}",
+                    filename=not_found)
+
+            no_annex = [p for p, r in info.items() if not r]
+            if no_annex:
+                raise FileInGitError(
+                    cmd='find',
+                    msg=f"File not in annex, but git: {no_annex}",
+                    filename=no_annex)
 
-            try:
-                out, err = self._run_annex_command(
-                    'lookupkey',
-                    files=[files],
-                    expect_fail=True
-                )
-            except CommandError as e:
-                if e.code == 1:
-                    if not exists(opj(self.path, files)):
-                        raise IOError(e.code, "File not found.", files)
-                    # XXX you don't like me because I can be real slow!
-                    elif files in self.get_indexed_files():
-                        # if we got here, the file is present and in git,
-                        # but not in the annex
-                        raise FileInGitError(cmd=cmd_str,
-                                             msg="File not in annex, but git: %s"
-                                                 % files,
-                                             filename=files)
-                    else:
-                        raise FileNotInAnnexError(cmd=cmd_str,
-                                                  msg="File not in annex: %s"
-                                                      % files,
-                                                  filename=files)
-                else:
-                    # Not sure, whether or not this can actually happen
-                    raise e
+        if batch is True and len(files) == 1 and len(keys) == 1:
+            keys = keys[0]
 
-            entries = out.rstrip(linesep).splitlines()
-            # filter out the ones which start with (: http://git-annex.branchable.com/bugs/lookupkey_started_to_spit_out___34__debug__34___messages_to_stdout/?updated
-            entries = list(filter(lambda x: not x.startswith('('), entries))
-            if len(entries) > 1:
-                lgr.warning("Got multiple entries in reply asking for a key of a file: %s"
-                            % (str(entries)))
-            elif not entries:
-                raise FileNotInAnnexError("Could not get a key for a file(s) %s -- empty output" % files)
-            return entries[0]
+        return keys
 
     @normalize_paths
-    def lock(self, files, options=None):
-        """undo unlock
-
-        Use  this to undo an unlock command if you don't want to modify the
-        files any longer, or have made modifications you want to discard.
-
-        Parameters
-        ----------
-        files: list of str
-        options: list of str
-        """
-
-        options = options[:] if options else []
-        self._run_annex_command('lock', annex_options=options, files=files)
-        # note: there seems to be no output by annex if success.
-
-    @normalize_paths
-    def unlock(self, files, options=None):
+    def unlock(self, files):
         """unlock files for modification
 
+        Note: This method is silent about errors in unlocking a file (e.g, the
+        file has not content). Use the higher-level interface.unlock to get
+        more informative reporting.
+
         Parameters
         ----------
         files: list of str
-        options: list of str
 
         Returns
         -------
         list of str
           successfully unlocked files
         """
-
-        options = options[:] if options else []
-
-        if self.is_direct_mode():
-
-            # TODO:
-            # If anything there should be a CommandNotAvailableError now:
-            lgr.debug("'%s' is in direct mode, "
-                      "'annex unlock' not available", self)
-            lgr.warning("In direct mode there is no 'unlock'. However if "
-                        "the file's content is present, it is kind of "
-                        "unlocked. Therefore just checking whether this is "
-                        "the case.")
-            # TODO/FIXME:
-            # Note: the following isn't exactly nice, if `files` is a dir.
-            # For a "correct" result we would need to report all files within
-            # potential dir(s) in `files`, that are annexed and have content.
-            # Also note, that even now files in git might be reported "unlocked",
-            # since they have content. This might be a confusing result.
-            # On the other hand, this is solved on the level of Dataset.unlock
-            # by annotating those paths 'notneeded' beforehand.
-            return [f for f in files if self.file_has_content(f)]
-
-        else:
-
-            # TODO: catch and parse output if failed (missing content ...)
-            std_out, std_err = \
-                self._run_annex_command(
-                    'unlock', annex_options=options, files=files
-                )
-
-            return [line.split()[1]
-                    for line in std_out.splitlines()
-                    if line.split()[0] == 'unlock' and line.split()[-1] == 'ok']
+        if not files:
+            return
+        return [j["file"] for j in
+                self.call_annex_records(["unlock"], files=files)
+                if j["success"]]
 
     def adjust(self, options=None):
         """enter an adjusted branch
 
-        This command is only available in a v6 git-annex repository.
+        This command is only available in a v6+ git-annex repository.
 
         Parameters
         ----------
         options: list of str
           currently requires '--unlock' or '--fix';
           default: --unlock
         """
         # TODO: Do we want to catch the case that
         # "adjusted/<current_branch_name>(unlocked)" already exists and
         # just check it out? Or fail like annex itself does?
 
         # version check:
-        if not self.config.get("annex.version") == '6':
-            raise CommandNotAvailableError(cmd='git annex adjust',
-                                           msg='git-annex-adjust requires a '
-                                               'version 6 repository')
+        if not self.supports_unlocked_pointers:
+            raise CommandNotAvailableError(
+                cmd='git annex adjust',
+                msg=('git-annex-adjust requires a '
+                     'version that supports unlocked pointers'))
 
         options = options[:] if options else to_options(unlock=True)
-        self._run_annex_command('adjust', annex_options=options)
+        self.call_annex(['adjust'] + options)
 
     @normalize_paths
     def unannex(self, files, options=None):
         """undo accidental add command
 
         Use this to undo an accidental git annex add command. Note that for
         safety, the content of the file remains in the annex, until you use git
@@ -1757,188 +1927,189 @@
         Returns
         -------
         list of str
           successfully unannexed files
         """
 
         options = options[:] if options else []
-
-        std_out, std_err = self._run_annex_command(
-            'unannex', annex_options=options, files=files
-        )
-        return [line.split()[1] for line in std_out.splitlines()
-                if line.split()[0] == 'unannex' and line.split()[-1] == 'ok']
+        prefix = 'unannex'
+        suffix = 'ok'
+        return [
+            # we cannot .split here since filename could have spaces
+            self._unquote_annex_path(line[len(prefix) + 1 : -(len(suffix) + 1)])
+            for line in self.call_annex_items_(['unannex'] + options, files=files)
+            if line.split()[0] == prefix and line.split()[-1] == suffix
+        ]
 
     @normalize_paths(map_filenames_back=True)
     def find(self, files, batch=False):
-        """Provide annex info for file(s).
+        """Run `git annex find` on file(s).
 
         Parameters
         ----------
         files: list of str
             files to find under annex
         batch: bool, optional
             initiate or continue with a batched run of annex find, instead of just
-            calling a single git annex find command
+            calling a single git annex find command. If any items in `files`
+            are directories, this value is treated as False.
 
         Returns
         -------
-        list
-          list with filename if file found else empty string
-        """
-        objects = []
-        if batch:
-            objects = self._batched.get('find', git_options=self._GIT_COMMON_OPTIONS, path=self.path)(files)
+        A dictionary the maps each item in `files` to its `git annex find`
+        result. Items without a successful result will be an empty string, and
+        multi-item results (which can occur for if `files` includes a
+        directory) will be returned as a list.
+        """
+        objects = {}
+        # Ignore batch=True if any path is a directory because `git annex find
+        # --batch` always returns an empty string for directories.
+        if batch and not any(isdir(opj(self.path, f)) for f in files):
+            find = self._batched.get(
+                'find', json=True, path=self.path,
+                # Since we are just interested in local information
+                git_options=['-c', 'annex.merge-annex-branches=false']
+            )
+            objects = {f: json_out.get("file", "")
+                       for f, json_out in zip(files, find(files))}
         else:
             for f in files:
                 try:
-                    obj, er = self._run_annex_command(
-                        'find', files=[f], expect_fail=True
+                    res = self._call_annex(
+                        ['find', "--print0"],
+                        files=[f],
+                        merge_annex_branches=False,
                     )
-                    objects.append(obj)
+                    items = res['stdout'].rstrip("\0").split("\0")
+                    objects[f] = items[0] if len(items) == 1 else items
                 except CommandError:
-                    objects.append('')
+                    objects[f] = ''
 
         return objects
 
+    def _check_files(self, fn, files, batch):
+        # Helper that isolates the common logic in `file_has_content` and
+        # `is_under_annex`. `fn` is the annex command used to do the check, and
+        # `quick_fn` is the non-annex variant.
+        pointers = self.supports_unlocked_pointers
+        # We're only concerned about modified files in V6+ mode. In V5
+        # `find` returns an empty string for unlocked files.
+        #
+        # ATTN: test_AnnexRepo_file_has_content has a failure before Git
+        # v2.13 (tested back to v2.9) because this diff call unexpectedly
+        # reports a type change as modified.
+        modified = [
+            f for f in self.call_git_items_(
+                ['diff', '--name-only', '-z'], sep='\0')
+            if f] if pointers else []
+        annex_res = fn(files, normalize_paths=False, batch=batch)
+        return [bool(annex_res.get(f) and
+                     not (pointers and normpath(f) in modified))
+                for f in files]
+
     @normalize_paths
-    def file_has_content(self, files, allow_quick=True, batch=False):
+    def file_has_content(self, files, allow_quick=False, batch=False):
         """Check whether files have their content present under annex.
 
         Parameters
         ----------
         files: list of str
             file(s) to check for being actually present.
         allow_quick: bool, optional
-            allow quick check, based on having a symlink into .git/annex/objects.
-            Works only in non-direct mode (TODO: thin mode)
+            This is no longer supported.
 
         Returns
         -------
         list of bool
-            For each input file states either file has content locally
+            For each input file states whether file has content locally
         """
         # TODO: Also provide option to look for key instead of path
-
-        if self.is_direct_mode() or batch or not allow_quick:  # TODO: thin mode
-            # TODO: Also provide option to look for key instead of path
-            find = self.find(files, normalize_paths=False, batch=batch)
-            return [bool(filename) for filename in find]
-        else:  # ad-hoc check which should be faster than call into annex
-            out = []
-            for f in files:
-                filepath = opj(self.path, f)
-                if islink(filepath):                    # if symlink
-                    # find abspath of node pointed to by symlink
-                    target_path = realpath(filepath)  # realpath OK
-                    # TODO: checks for being not outside of this repository
-                    # Note: ben removed '.git/' from '.git/annex/objects',
-                    # since it is not true for submodules, whose '.git' is a
-                    # symlink and being resolved to some
-                    # '.git/modules/.../annex/objects'
-                    out.append(exists(target_path) and 'annex/objects' in target_path)
-                else:
-                    out.append(False)
-            return out
+        return self._check_files(self.find, files, batch)
 
     @normalize_paths
-    def is_under_annex(self, files, allow_quick=True, batch=False):
+    def is_under_annex(self, files, allow_quick=False, batch=False):
         """Check whether files are under annex control
 
         Parameters
         ----------
         files: list of str
             file(s) to check for being under annex
         allow_quick: bool, optional
-            allow quick check, based on having a symlink into .git/annex/objects.
-            Works only in non-direct mode (TODO: thin mode)
+            This is no longer supported.
 
         Returns
         -------
         list of bool
-            For each input file states either file is under annex
+            For each input file states whether file is under annex
         """
         # theoretically in direct mode files without content would also be
         # broken symlinks on the FSs which support it, but that would complicate
         # the matters
-        if self.is_direct_mode() or batch or not allow_quick:  # TODO: thin mode
-            # no other way but to call whereis and if anything returned for it
-            info = self.info(files, normalize_paths=False, batch=batch)
-            # info is a dict... khe khe -- "thanks" Yarik! ;)
-            return [bool(info[f]) for f in files]
-        else:  # ad-hoc check which should be faster than call into annex
-            out = []
-            for f in files:
-                filepath = opj(self.path, f)
-                # todo checks for being not outside of this repository
-                # Note: ben removed '.git/' from '.git/annex/objects',
-                # since it is not true for submodules, whose '.git' is a
-                # symlink and being resolved to some
-                # '.git/modules/.../annex/objects'
-                out.append(
-                    islink(filepath)
-                    and 'annex/objects' in realpath(filepath)  # realpath OK
-                )
-            return out
+
+        # This is an ugly hack to prevent files from being treated as
+        # remotes by `git annex info`. See annex's `nameToUUID'`.
+        files = [opj(curdir, f) for f in files]
+
+        def check(files, **kwargs):
+            # Filter out directories because it doesn't make sense to ask if
+            # they are under annex control and `info` can only handle
+            # non-directories.
+            return self.info([f for f in files if not isdir(f)],
+                             fast=True, **kwargs)
+
+        return self._check_files(check, files, batch)
 
     def init_remote(self, name, options):
         """Creates a new special remote
 
         Parameters
         ----------
         name: str
             name of the special remote
         """
         # TODO: figure out consistent way for passing options + document
-
-        self._run_annex_command('initremote', annex_options=[name] + options)
+        self.call_annex(['initremote'] + [name] + options)
         self.config.reload()
 
-    def enable_remote(self, name, env=None):
+    def enable_remote(self, name, options=None, env=None):
         """Enables use of an existing special remote
 
         Parameters
         ----------
         name: str
             name, the special remote was created with
+        options: list, optional
         """
 
+        # MIH thinks there should be no `env` argument at all
+        # https://github.com/datalad/datalad/issues/5162
+        env = env or self._git_runner.env
         try:
-            self._run_annex_command(
-                'enableremote',
-                annex_options=[name],
-                expect_fail=True,
-                log_stderr=True,
-                env=env)
+            from unittest.mock import patch
+            with patch.object(self._git_runner, 'env', env):
+                # TODO: outputs are nohow used/displayed. Eventually convert to
+                # to a generator style yielding our "dict records"
+                self.call_annex(['enableremote', name] + ensure_list(options))
         except CommandError as e:
             if re.match(r'.*StatusCodeException.*statusCode = 401', e.stderr):
                 raise AccessDeniedError(e.stderr)
             elif 'FailedConnectionException' in e.stderr:
                 raise AccessFailedError(e.stderr)
             else:
                 raise e
         self.config.reload()
 
-    def merge_annex(self, remote=None):
-        """Merge git-annex branch
-
-        Merely calls `sync` with the appropriate arguments.
-
-        Parameters
-        ----------
-        remote: str, optional
-          Name of a remote to be "merged".
-        """
-        self.sync(
-            remotes=remote, push=False, pull=False, commit=False, content=False,
-            all=False)
+    def merge_annex(self, remote=None):  # do not use anymore, use localsync()
+        self.localsync(remote)
 
     def sync(self, remotes=None, push=True, pull=True, commit=True,
              content=False, all=False, fast=False):
-        """Synchronize local repository with remotes
+        """This method is deprecated, use call_annex(['sync', ...]) instead.
+
+        Synchronize local repository with remotes
 
         Use  this  command  when you want to synchronize the local repository
         with one or more of its remotes. You can specify the remotes (or
         remote groups) to sync with by name; the default if none are specified
         is to sync with all remotes.
 
         Parameters
@@ -1960,55 +2131,42 @@
           This option, when combined with `content`, makes all available
           versions of all files be synced, when preferred content settings
           allow
         fast : bool
           Only sync with the remotes with the lowest annex-cost value
           configured
         """
-        # In direct mode annex-sync fails, if commit=True
-        # apparently sync is calling git status internally, which then fails
-        # in the submodule. (As we already know)
-        # stdout:
-        # commit  add second ok
-        # (recording state in git...)
-        #
-        # failed
-        # (recording state in git...)
-        #
-        # stderr:
-        # fatal: This operation must be run in a work tree
-        # fatal: 'git status --porcelain' failed in submodule submod
-        # git-annex: user error (xargs ["-0","git","--git-dir=.git","--work-tree=.","--literal-pathspecs","add","-f"] exited 123)
-        # fatal: This operation must be run in a work tree
-        # fatal: 'git status --porcelain' failed in submodule submod
-        # git-annex: user error (xargs ["-0","git","--git-dir=.git","--work-tree=.","--literal-pathspecs","add","-f"] exited 123)
-
-        # TODO: Workaround
-
+        import warnings
+        warnings.warn(
+            "AnnexRepo.sync() is deprecated, use call_annex(['sync', ...]) "
+            "instead.",
+            DeprecationWarning)
         args = []
         args.extend(to_options(push=push, no_push=not push,
                                # means: '--push' if push else '--no-push'
                                pull=pull, no_pull=not pull,
                                commit=commit, no_commit=not commit,
                                content=content, no_content=not content,
                                all=all,
                                fast=fast))
-        args.extend(assure_list(remotes))
-        self._run_annex_command('sync', annex_options=args)
+        args.extend(ensure_list(remotes))
+        self.call_annex(['sync'] + args)
 
     @normalize_path
     def add_url_to_file(self, file_, url, options=None, backend=None,
                         batch=False, git_options=None, annex_options=None,
                         unlink_existing=False):
         """Add file from url to the annex.
 
         Downloads `file` from `url` and add it to the annex.
         If annex knows `file` already,
         records that it can be downloaded from `url`.
 
+        Note: Consider using the higher-level `download_url` instead.
+
         Parameters
         ----------
         file_: str
 
         url: str
 
         options: list
@@ -2032,123 +2190,147 @@
         if git_options:
             lgr.warning("add_url_to_file: git_options not yet implemented. Ignored.")
 
         if annex_options:
             lgr.warning("annex_options not yet implemented. Ignored.")
 
         options = options[:] if options else []
+        if backend:
+            options.extend(('--backend', backend))
         git_options = []
-        kwargs = dict(backend=backend)
         if lexists(opj(self.path, file_)) and \
                 unlink_existing and \
                 not self.is_under_annex(file_):
             # already under git, we can't addurl for under annex
             lgr.warning(
                 "File %s:%s is already under git, removing so it could possibly"
                 " be added under annex", self, file_
             )
-            os.unlink(opj(self.path, file_))
-        if not batch:
-            self._run_annex_command(
-                'addurl',
-                annex_options=options + ['--file=%s' % file_] + [url],
-                log_online=True, log_stderr=False,
-                **kwargs
+            unlink(opj(self.path, file_))
+        if not batch or self.fake_dates_enabled:
+            if batch:
+                lgr.debug("Not batching addurl call "
+                          "because fake dates are enabled")
+            files_opt = '--file=%s' % file_
+            out_json = self._call_annex_records(
+                ['addurl'] + options + [files_opt] + [url],
+                progress=True,
             )
-            # Don't capture stderr, since download progress provided by wget uses
-            # stderr.
+            if len(out_json) != 1:
+                raise AssertionError(
+                    "should always be a single-item list, Got: %s"
+                    % str(out_json))
+            # Make the output's structure match bcmd's.
+            out_json = out_json[0]
+            # Don't capture stderr, since download progress provided by wget
+            # uses stderr.
         else:
             options += ['--with-files']
             if backend:
                 options += ['--backend=%s' % backend]
             # Initializes (if necessary) and obtains the batch process
             bcmd = self._batched.get(
                 # Since backend will be critical for non-existing files
                 'addurl_to_file_backend:%s' % backend,
                 annex_cmd='addurl',
-                git_options=self._GIT_COMMON_OPTIONS + git_options,
+                git_options=git_options,
                 annex_options=options,  # --raw ?
                 path=self.path,
                 json=True
             )
             try:
                 out_json = bcmd((url, file_))
             except Exception as exc:
                 # if isinstance(exc, IOError):
                 #     raise
                 raise AnnexBatchCommandError(
                     cmd="addurl",
-                    msg="Adding url %s to file %s failed due to %s" % (url, file_, exc_str(exc)))
-            assert(out_json['command'] == 'addurl')
-            if not out_json.get('success', False):
-                raise AnnexBatchCommandError(
+                    msg="Adding url %s to file %s failed" % (url, file_)) from exc
+            assert \
+                (out_json.get('command') == 'addurl'), \
+                "no exception was raised and no 'command' in result out_json=%s" % str(out_json)
+        if not out_json.get('success', False):
+            raise (AnnexBatchCommandError if batch else CommandError)(
                     cmd="addurl",
-                    msg="Error, annex reported failure for addurl: %s"
-                    % str(out_json))
-            return out_json
+                    msg="Error, annex reported failure for addurl (url='%s'): %s"
+                    % (url, str(out_json)))
+        return out_json
 
     def add_urls(self, urls, options=None, backend=None, cwd=None,
                  jobs=None,
                  git_options=None, annex_options=None):
         """Downloads each url to its own file, which is added to the annex.
 
+        .. deprecated:: 0.17
+            Use add_url_to_file() or call_annex() instead.
+
         Parameters
         ----------
         urls: list of str
 
         options: list, optional
             options to the annex command
 
         cwd: string, optional
             working directory from within which to invoke git-annex
         """
+        warnings.warn(
+            "AnnexRepo.add_urls() is deprecated and will be removed in a "
+            "future release. Use AnnexRepo.add_url_to_file() or "
+            "AnnexRepo.call_annex() instead.",
+            DeprecationWarning)
 
         if git_options:
             lgr.warning("add_urls: git_options not yet implemented. Ignored.")
 
+        git_options = []
+        if cwd:
+            git_options.extend(('-C', cwd))
+
         if annex_options:
             lgr.warning("annex_options not yet implemented. Ignored.")
 
         options = options[:] if options else []
 
-        self._run_annex_command('addurl', annex_options=options + urls,
-                                backend=backend, log_online=True,
-                                log_stderr=False, cwd=cwd)
-        # Don't capture stderr, since download progress provided by wget uses
-        # stderr.
-
-        # currently simulating similar return value, assuming success
-        # for all files:
-        # TODO: Make return values consistent across both *Repo classes!
-        return [{u'file': f, u'success': True} for f in urls]
+        if backend:
+            options.extend(('--backend', backend))
+
+        return self._call_annex_records(
+            ['addurl'] + options + urls,
+            git_options=git_options,
+            progress=True)
 
     @normalize_path
     def rm_url(self, file_, url):
         """Record that the file is no longer available at the url.
 
         Parameters
         ----------
         file_: str
 
         url: str
         """
-
-        self._run_annex_command('rmurl', files=[file_, url])
+        self.call_annex(['rmurl'], files=[file_, url])
 
     @normalize_path
     def get_urls(self, file_, key=False, batch=False):
         """Get URLs for a file/key
 
         Parameters
         ----------
         file_: str
         key: bool, optional
-            Either provided files are actually annex keys
+            Whether provided files are actually annex keys
+
+        Returns
+        -------
+        A list of URLs
         """
-        return self.whereis(file_, output='full', batch=batch)[AnnexRepo.WEB_UUID]['urls']
+        locations = self.whereis(file_, output='full', key=key, batch=batch)
+        return locations.get(WEB_SPECIAL_REMOTE_UUID, {}).get('urls', [])
 
     @normalize_paths
     def drop(self, files, options=None, key=False, jobs=None):
         """Drops the content of annexed files from this repository.
 
         Drops only if possible with respect to required minimal number of
         available copies.
@@ -2176,37 +2358,35 @@
         if not files and \
                 (not options or
                  not any([o in options for o in
                           ["--all", "--unused", "--key", "--incomplete"]])):
             raise InsufficientArgumentsError("drop() requires at least to "
                                              "specify 'files' or 'options'")
 
-        options = assure_list(options)
+        options = ensure_list(options)
 
         if key:
             # we can't drop multiple in 1 line, and there is no --batch yet, so
             # one at a time
-            files = assure_list(files)
+            files = ensure_list(files)
             options = options + ['--key']
             res = [
-                self._run_annex_command_json(
-                    'drop',
-                    opts=options + [k],
+                self._call_annex_records(
+                    ['drop'] + options + [k],
                     jobs=jobs)
                 for k in files
             ]
             # `normalize_paths` ... magic, useful?
             if len(files) == 1:
                 return res[0]
             else:
                 return res
         else:
-            return self._run_annex_command_json(
-                'drop',
-                opts=options,
+            return self._call_annex_records(
+                ['drop'] + options,
                 files=files,
                 jobs=jobs)
 
     def drop_key(self, keys, options=None, batch=False):
         """Drops the content of annexed files from this repository referenced by keys
 
         Dangerous: it drops without checking for required minimal number of
@@ -2216,211 +2396,61 @@
         ----------
         keys: list of str, str
 
         batch: bool, optional
             initiate or continue with a batched run of annex dropkey, instead of just
             calling a single git annex dropkey command
         """
-        keys = [keys] if isinstance(keys, string_types) else keys
+        keys = [keys] if isinstance(keys, str) else keys
 
         options = options[:] if options else []
         options += ['--force']
-        if not batch:
-            json_objects = self._run_annex_command_json(
-                'dropkey', opts=options, files=keys, expect_stderr=True
+        if not batch or self.fake_dates_enabled:
+            if batch:
+                lgr.debug("Not batching drop_key call "
+                          "because fake dates are enabled")
+            json_objects = self.call_annex_records(
+                ['dropkey'] + options, files=keys
             )
         else:
             json_objects = self._batched.get(
                 'dropkey',
-                git_options=self._GIT_COMMON_OPTIONS,
                 annex_options=options, json=True, path=self.path
             )(keys)
+        # TODO: RF to be consistent with the rest (IncompleteResultError or alike)
+        # and/or completely refactor since drop above also has key option
         for j in json_objects:
             assert j.get('success', True)
 
     # TODO: a dedicated unit-test
     def _whereis_json_to_dict(self, j):
         """Convert json record returned by annex whereis --json to our dict representation for it
         """
-        assert (j.get('success', True) is True)
         # process 'whereis' containing list of remotes
         remotes = {remote['uuid']: {x: remote.get(x, None)
                                     for x in ('description', 'here', 'urls')
                                     }
-                   for remote in j.get('whereis')}
-        if self.WEB_UUID in remotes:
-            assert(remotes[self.WEB_UUID]['description'] == 'web')
+                   for remote in j['whereis']}
         return remotes
 
-    def _run_annex_command_json(self, command,
-                                opts=None,
-                                jobs=None,
-                                files=None,
-                                expected_entries=None,
-                                **kwargs):
-        """Run an annex command with --json and load output results into a tuple of dicts
-
-        Parameters
-        ----------
-        expected_entries : dict, optional
-          If provided `filename/key: size` dictionary, will be used to create
-          ProcessAnnexProgressIndicators to display progress
-        """
-        progress_indicators = None
-        try:
-            if expected_entries:
-                progress_indicators = ProcessAnnexProgressIndicators(
-                    expected=expected_entries
-                )
-                kwargs = kwargs.copy()
-                kwargs.update(dict(
-                    log_stdout=progress_indicators,
-                    log_stderr='offline',  # False, # to avoid lock down
-                    log_online=True
-                ))
-            # TODO: refactor to account for possible --batch ones
-            annex_options = ['--json']
-            if jobs == 'auto':
-                jobs = N_AUTO_JOBS
-            if jobs and jobs != 1:
-                annex_options += ['-J%d' % jobs]
-            if opts:
-                annex_options += opts
-
-            # TODO: RF to use --batch where possible instead of splitting
-            # into multiple invocations
-            if not files:
-                file_chunks = [[]]
-            else:
-                files = assure_list(files)
-                maxl = max(map(len, files))
-                chunk_size = CMD_MAX_ARG // maxl
-                file_chunks = generate_chunks(files, chunk_size)
-            out, err = "", ""
-            for file_chunk in file_chunks:
-                out_, err_ = self._run_annex_command(
-                    command,
-                    annex_options=annex_options + ['--'] + file_chunk,
-                    **kwargs)
-                out += out_
-                err += err_
-        except CommandError as e:
-            # Note: A call might result in several 'failures', that can be or
-            # cannot be handled here. Detection of something, we can deal with,
-            # doesn't mean there's nothing else to deal with.
-
-            # OutOfSpaceError:
-            # Note:
-            # doesn't depend on anything in stdout. Therefore check this before
-            # dealing with stdout
-            out_of_space_re = re.search(
-                "not enough free space, need (.*) more", e.stderr
-            )
-            if out_of_space_re:
-                raise OutOfSpaceError(cmd="annex %s" % command,
-                                      sizemore_msg=out_of_space_re.groups()[0])
-
-            # RemoteNotAvailableError:
-            remote_na_re = re.search(
-                "there is no available git remote named \"(.*)\"", e.stderr
-            )
-            if remote_na_re:
-                raise RemoteNotAvailableError(cmd="annex %s" % command,
-                                              remote=remote_na_re.groups()[0])
-
-            # TEMP: Workaround for git-annex bug, where it reports success=True
-            # for annex add, while simultaneously complaining, that it is in
-            # a submodule:
-            # TODO: For now just reraise. But independently on this bug, it
-            # makes sense to have an exception for that case
-            in_subm_re = re.search(
-                "fatal: Pathspec '(.*)' is in submodule '(.*)'", e.stderr
-            )
-            if in_subm_re:
-                raise e
-
-            # Note: try to approach the covering of potential annex failures
-            # in a more general way:
-            # first check stdout:
-            if all([line.startswith('{') and line.endswith('}')
-                    for line in e.stdout.splitlines()]):
-                # we have the usual json output on stdout. Therefore we can
-                # probably return and don't need to raise; so get stdout
-                # for json loading:
-                out = e.stdout
-            else:
-                out = None
-
-            # Note: Workaround for not existing files as long as annex doesn't
-            # report it within JSON response:
-            # see http://git-annex.branchable.com/bugs/copy_does_not_reflect_some_failed_copies_in_--json_output/
-            not_existing = [
-                line.split()[1] for line in e.stderr.splitlines()
-                if line.startswith('git-annex:') and
-                   line.endswith('not found')
-            ]
-            if not_existing:
-                if out is None:
-                    # we create the error reporting herein. If all files were
-                    # not found, there is nothing on stdout and we don't need
-                    # anything
-                    out = ""
-                if not out.endswith(linesep):
-                    out += linesep
-                out += linesep.join(
-                    ['{{"command": "{cmd}", "file": "{path}", '
-                     '"note": "{note}",'
-                     '"success":false}}'.format(
-                         cmd=command, path=f, note="not found")
-                     for f in not_existing])
-
-            # Note: insert additional code here to analyse failure and possibly
-            # raise a custom exception
-
-            # if we didn't raise before, just depend on whether or not we seem
-            # to have some json to return. It should contain information on
-            # failure in keys 'success' and 'note'
-            # TODO: This is not entirely true. 'annex status' may return empty,
-            # while there was a 'fatal:...' in stderr, which should be a
-            # failure/exception
-            # Or if we had empty stdout but there was stderr
-            if out is None or (not out and e.stderr):
-                raise e
-            if e.stderr:
-                # else just warn about present errors
-                shorten = lambda x: x[:1000] + '...' if len(x) > 1000 else x
-                lgr.warning(
-                    "Running %s resulted in stderr output: %s",
-                    command, shorten(e.stderr)
-                )
-        finally:
-            if progress_indicators:
-                progress_indicators.finish()
-
-        json_objects = (json_loads(line)
-                        for line in out.splitlines() if line.startswith('{'))
-        # protect against progress leakage
-        json_objects = [j for j in json_objects if 'byte-progress' not in j]
-        return json_objects
-
     # TODO: reconsider having any magic at all and maybe just return a list/dict always
     @normalize_paths
     def whereis(self, files, output='uuids', key=False, options=None, batch=False):
         """Lists repositories that have actual content of file(s).
 
         Parameters
         ----------
         files: list of str
             files to look for
         output: {'descriptions', 'uuids', 'full'}, optional
             If 'descriptions', a list of remotes descriptions returned is per
             each file. If 'full', for each file a dictionary of all fields
             is returned as returned by annex
         key: bool, optional
-            Either provided files are actually annex keys
+            Whether provided files are actually annex keys
         options: list, optional
             Options to pass into git-annex call
 
         Returns
         -------
         list of list of unicode  or dict
             if output == 'descriptions', contains a list of descriptions of remotes
@@ -2437,49 +2467,86 @@
 
                 {'00000000-0000-0000-0000-000000000001': {
                   'description': 'web',
                   'here': False,
                   'urls': ['http://127.0.0.1:43442/about.txt', 'http://example.com/someurl']
                 }}
         """
-        if batch:
-            lgr.warning("TODO: --batch mode for whereis.  Operating serially")
-
         OUTPUTS = {'descriptions', 'uuids', 'full'}
         if output not in OUTPUTS:
             raise ValueError(
                 "Unknown value output=%r. Known are %s"
                 % (output, ', '.join(map(repr, OUTPUTS)))
             )
 
-        options = assure_list(options, copy=True)
-        if key:
-            kwargs = {'opts': options + ["--key"] + files}
+        options = ensure_list(options, copy=True)
+        if batch:
+            # TODO: --batch-keys was added to 8.20210903
+            if key:
+                if not self._check_version_kludges("grp1-supports-batch-keys"):
+                    raise ValueError("batch=True for `key=True` requires git-annex >= 8.20210903")
+                bkw = {'batch_opt': '--batch-keys'}
+            else:
+                bkw = {}
+            bcmd = self._batched.get('whereis', annex_options=options,
+                                     json=True, path=self.path, **bkw)
+            json_objects = bcmd(files)
         else:
-            kwargs = {'files': files}
+            cmd = ['whereis'] + options
+
+            def _call_cmd(cmd, files=None):
+                """Helper to reuse consistently in case of --key and not invocations"""
+                try:
+                    return self.call_annex_records(cmd, files=files)
+                except CommandError as e:
+                    if e.stderr.startswith('Invalid'):
+                        # would happen when git-annex is called with incompatible options
+                        raise
+                    # whereis may exit non-zero when there are too few known copies
+                    # callers of whereis are interested in exactly that information,
+                    # which we deliver via result, not via exception
+                    return e.kwargs.get('stdout_json', [])
+
+            if key:
+                # whereis --key takes only a single key at a time so we need to loop
+                json_objects = []
+                for k in files:
+                    json_objects.extend(_call_cmd(cmd + ["--key", k]))
+            else:
+                json_objects = _call_cmd(cmd, files)
+
+        # json_objects can contain entries w/o a "whereis" field. Unknown to
+        # git paths in particular are returned in such records. Code below is
+        # only concerned with actual whereis results.
+        whereis_json_objects = [o for o in json_objects if "whereis" in
+                                o.keys()]
 
-        json_objects = self._run_annex_command_json('whereis', **kwargs)
         if output in {'descriptions', 'uuids'}:
             return [
                 [remote.get(output[:-1]) for remote in j.get('whereis')]
                 if j.get('success') else []
-                for j in json_objects
+                for j in whereis_json_objects
             ]
         elif output == 'full':
             # TODO: we might want to optimize storage since many remotes entries will be the
             # same so we could just reuse them instead of brewing copies
             return {
-                j['key' if (key or '--all' in options) else 'file']
+                j['key']
+                if (key or '--all' in options)
+                # report is always POSIX, but normalize_paths wants to match against
+                # the native representation
+                else str(Path(PurePosixPath(j['file'])))
+                if on_windows else j['file']
                 : self._whereis_json_to_dict(j)
-                for j in json_objects
-                if not j.get('key').endswith('.this-is-a-test-key')
+                for j in whereis_json_objects
+                if not j.get('key', '').endswith('.this-is-a-test-key')
             }
 
     # TODO:
-    # I think we should make interface cleaner and less ambigious for those annex
+    # I think we should make interface cleaner and less ambiguous for those annex
     # commands which could operate on globs, files, and entire repositories, separating
     # those out, e.g. annex_info_repo, annex_info_files at least.
     # If we make our calling wrappers work without relying on invoking from repo topdir,
     # then returned filenames would not need to be mapped, so we could easily work on dirs
     # and globs.
     # OR if explicit filenames list - return list of matching entries, if globs/dirs -- return dict?
     @normalize_paths(map_filenames_back=True)
@@ -2496,73 +2563,123 @@
         dict
           Info for each file
         """
 
         options = ['--bytes', '--fast'] if fast else ['--bytes']
 
         if not batch:
-            json_objects = self._run_annex_command_json(
-                'info', opts=options, files=files)
+            json_objects = self._call_annex_records(
+                ['info'] + options, files=files, merge_annex_branches=False,
+                exception_on_error=False,
+            )
         else:
+            # according to passing of the test_AnnexRepo_is_under_annex
+            # test with batch=True, there is no need for explicit
+            # exception_on_error=False, batched process does not raise
+            # CommandError.
             json_objects = self._batched.get(
                 'info',
-                git_options=self._GIT_COMMON_OPTIONS,
-                annex_options=options, json=True, path=self.path
+                annex_options=options, json=True, path=self.path,
+                git_options=['-c', 'annex.merge-annex-branches=false']
             )(files)
 
         # Some aggressive checks. ATM info can be requested only per file
         # json_objects is a generator, let's keep it that way
         # assert(len(json_objects) == len(files))
         # and that they all have 'file' equal to the passed one
         out = {}
         for j, f in zip(json_objects, files):
-            assert(j.pop('file') == f)
+            # Starting with version of annex 8.20200330-100-g957a87b43
+            # annex started to normalize relative paths.
+            # ref: https://github.com/datalad/datalad/issues/4431
+            # Use normpath around each side to ensure it is the same file
+            assert normpath(j.pop('file')) == normpath(f)
             if not j['success']:
                 j = None
             else:
                 assert(j.pop('success') is True)
                 # convert size to int
                 j['size'] = int(j['size']) if 'unknown' not in j['size'] else None
                 # and pop the "command" field
                 j.pop("command")
             out[f] = j
         return out
 
-    def repo_info(self, fast=False):
+    def repo_info(self, fast=False, merge_annex_branches=True):
         """Provide annex info for the entire repository.
 
+        Parameters
+        ----------
+        fast : bool, optional
+          Pass `--fast` to `git annex info`.
+        merge_annex_branches : bool, optional
+          Whether to allow git-annex if needed to merge annex branches, e.g. to
+          make sure up to date descriptions for git annex remotes
+
         Returns
         -------
         dict
           Info for the repository, with keys matching the ones returned by annex
         """
 
         options = ['--bytes', '--fast'] if fast else ['--bytes']
 
-        json_records = list(self._run_annex_command_json('info', opts=options))
+        json_records = list(self._call_annex_records(
+            ['info'] + options, merge_annex_branches=merge_annex_branches)
+        )
         assert(len(json_records) == 1)
 
         # TODO: we need to abstract/centralize conversion from annex fields
         # For now just tune up few for immediate usability
         info = json_records[0]
         for k in info:
             if k.endswith(' size') or k.endswith(' disk space') or k.startswith('size of '):
-                info[k] = int(info[k].split()[0])
+                size = info[k].split()[0]
+                if size.isdigit():
+                    info[k] = int(size)
+                else:
+                    lgr.debug("Size %r reported to be %s, setting to None", k, size)
+                    info[k] = None
         assert(info.pop('success'))
         assert(info.pop('command') == 'info')
         return info  # just as is for now
 
-    def get_annexed_files(self, with_content_only=False):
+    def get_annexed_files(self, with_content_only=False, patterns=None):
         """Get a list of files in annex
+
+        Parameters
+        ----------
+        with_content_only : bool, optional
+            Only list files whose content is present.
+        patterns : list, optional
+            Globs to pass to annex's `--include=`. Files that match any of
+            these will be returned (i.e., they'll be separated by `--or`).
+
+        Returns
+        -------
+        A list of POSIX file names
         """
-        # TODO: Review!!
-        args = [] if with_content_only else ['--include', "*"]
-        out, err = self._run_annex_command('find', annex_options=args)
+        if not patterns:
+            args = [] if with_content_only else ['--include', "*"]
+        else:
+            if len(patterns) == 1:
+                args = ['--include', patterns[0]]
+            else:
+                args = ['-(']
+                for pat in patterns[:-1]:
+                    args.extend(['--include', pat, "--or"])
+                args.extend(['--include', patterns[-1]])
+                args.append('-)')
+
+            if with_content_only:
+                args.extend(['--in', '.'])
         # TODO: JSON
-        return out.splitlines()
+        return list(
+            self.call_annex_items_(
+                ['find', '-c', 'annex.merge-annex-branches=false'] + args))
 
     def get_preferred_content(self, property, remote=None):
         """Get preferred content configuration of a repository or remote
 
         Parameters
         ----------
         property : {'wanted', 'required', 'group'}
@@ -2570,31 +2687,28 @@
         remote : str, optional
           If not specified (None), returns the property for the local
           repository.
 
         Returns
         -------
         str
-          Either the setting is returned, or an empty string if there
-          is none.
+          Whether the setting is returned, or `None` if there is none.
 
         Raises
         ------
         ValueError
           If an unknown property label is given.
 
         CommandError
           If the annex call errors.
         """
         if property not in ('wanted', 'required', 'group'):
             raise ValueError(
                 'unknown preferred content property: {}'.format(property))
-        return self._run_simple_annex_command(
-            property,
-            annex_options=[remote or '.'])
+        return self.call_annex_oneline([property, remote or '.']) or None
 
     def set_preferred_content(self, property, expr, remote=None):
         """Set preferred content configuration of a repository or remote
 
         Parameters
         ----------
         property : {'wanted', 'required', 'group'}
@@ -2618,154 +2732,38 @@
 
         CommandError
           If the annex call errors.
         """
         if property not in ('wanted', 'required', 'group'):
             raise ValueError(
                 'unknown preferred content property: {}'.format(property))
-        return self._run_simple_annex_command(
-            property,
-            annex_options=[remote or '.', expr])
+        return self.call_annex_oneline([property, remote or '.', expr])
 
     def get_groupwanted(self, name):
         """Get `groupwanted` expression for a group `name`
 
         Parameters
         ----------
         name : str
            Name of the groupwanted group
         """
-        return self._run_simple_annex_command(
-            'groupwanted', annex_options=[name]
-        )
+        return self.call_annex_oneline(['groupwanted', name])
 
     def set_groupwanted(self, name, expr):
         """Set `expr` for the `name` groupwanted"""
-        return self._run_simple_annex_command(
-            'groupwanted', annex_options=[name, expr]
-        )
+        return self.call_annex_oneline(['groupwanted', name, expr])
 
     def precommit(self):
         """Perform pre-commit maintenance tasks, such as closing all batched annexes
         since they might still need to flush their changes into index
         """
         if self._batched is not None:
             self._batched.close()
         super(AnnexRepo, self).precommit()
 
-    @borrowdoc(GitRepo)
-    def commit(self, msg=None, options=None, _datalad_msg=False,
-               careless=True, files=None, proxy=False):
-        self.precommit()
-
-        # Note: `proxy` is for explicitly enforcing the use of git-annex-proxy
-        #       in direct mode. This is needed in very special cases, which
-        #       might go away once we figured out a better way. In any case, it
-        #       should turn into something that is automatically considered and
-        #       not done by the caller of this method.
-
-        if proxy:
-            if not self.is_direct_mode():
-                raise CommandNotAvailableError(
-                    cmd="git-annex-proxy",
-                    msg="git-annex-proxy is available in direct mode only")
-            else:
-                if _datalad_msg:
-                    msg = self._get_prefixed_commit_msg(msg)
-                if not msg:
-                    if options:
-                        if "--allow-empty-message" not in options:
-                            options.append("--allow-empty-message")
-                    else:
-                        options = ["--allow-empty-message"]
-
-                # committing explicitly given paths in direct mode via proxy
-                # used to fail, because absolute paths are used. Using annex
-                # proxy this leads to an error (path outside repository)
-                if files:
-                    files = assure_list(files)
-                    if options is None:
-                        options = []
-                    for i in range(len(files)):
-                        if isabs(files[i]):
-                            options.append(normpath(relpath(files[i],
-                                                            start=self.path)))
-                        else:
-                            options.append(files[i])
-                try:
-                    self.proxy(['git', 'commit'] + (['-m', msg] if msg else []) +
-                               (options if options else []),
-                               expect_stderr=True, expect_fail=True)
-                except CommandError as e:
-                    if 'nothing to commit' in e.stdout:
-                        if careless:
-                            lgr.debug("nothing to commit in {}. "
-                                      "Ignored.".format(self))
-                        else:
-                            raise
-                    elif 'no changes added to commit' in e.stdout or \
-                            'nothing added to commit' in e.stdout:
-                        if careless:
-                            lgr.debug("no changes added to commit in {}. "
-                                      "Ignored.".format(self))
-                        else:
-                            raise
-                    elif "did not match any file(s) known to git." in e.stderr:
-                        # TODO:
-                        # Improve FileNotInXXXXError classes to better deal with
-                        # multiple files; Also consider PathOutsideRepositoryError
-                        raise FileNotInRepositoryError(cmd=e.cmd,
-                                                       msg="File(s) unknown to git",
-                                                       code=e.code,
-                                                       filename=linesep.join(
-                                                    [l for l in e.stderr.splitlines()
-                                                     if l.startswith("pathspec")]))
-                    else:
-                        raise
-        else:
-
-            # Note: See the note on `proxy` parameter at the top of this method.
-            #       Trying to automatically use git-annex-proxy, whenever we
-            #       fail to commit the usual way via options to git in direct
-            #       mode. In particular this can happen if sth was staged via
-            #       git-annex-proxy, which is needed for --update option for
-            #       example.
-
-            try:
-                super(AnnexRepo, self).commit(msg, options,
-                                              _datalad_msg=_datalad_msg,
-                                              careless=careless, files=files)
-            except CommandError as e:
-                if self.is_direct_mode() and \
-                   "fatal: This operation must be run in a work tree" in \
-                   e.stderr:
-                    lgr.debug("Commit failed. "
-                              "Trying to commit via git-annex-proxy.")
-                    self.commit(msg, options, _datalad_msg=_datalad_msg,
-                                careless=careless, files=files, proxy=True)
-                else:
-                    raise 
-
-    @normalize_paths(match_return_type=False)
-    def remove(self, files, force=False, **kwargs):
-        """Remove files from git/annex (works in direct mode as well)
-
-        Parameters
-        ----------
-        files
-        force: bool, optional
-        """
-
-        # TODO: parameter 'force' unnecessary => kwargs / to_options
-        self.precommit()  # since might interfere
-
-        return super(AnnexRepo, self).remove(files, force=force,
-                                             normalize_paths=False,
-                                             **kwargs)
-
     def get_contentlocation(self, key, batch=False):
         """Get location of the key content
 
         Normally under .git/annex objects in indirect mode and within file
         tree in direct mode.
 
         Unfortunately there is no (easy) way to discriminate situations
@@ -2785,22 +2783,19 @@
         str
             path relative to the top directory of the repository. If no content
             is present, empty string is returned
         """
 
         if not batch:
             try:
-                out, err = self._run_annex_command('contentlocation',
-                                                   annex_options=[key],
-                                                   expect_fail=True)
-                return out.rstrip(linesep).splitlines()[0]
+                return next(self.call_annex_items_(['contentlocation', key]))
             except CommandError:
                 return ''
         else:
-            return self._batched.get('contentlocation', git_options=self._GIT_COMMON_OPTIONS, path=self.path)(key)
+            return self._batched.get('contentlocation', path=self.path)(key)
 
     @normalize_paths(serialize=True)
     def is_available(self, file_, remote=None, key=False, batch=False):
         """Check if file or key is available (from a remote)
 
         In case if key or remote is misspecified, it wouldn't fail but just keep
         returning False, although possibly also complaining out loud ;)
@@ -2809,83 +2804,65 @@
         ----------
         file_: str
             Filename or a key
         remote: str, optional
             Remote which to check.  If None, possibly multiple remotes are checked
             before positive result is reported
         key: bool, optional
-            Either provided files are actually annex keys
+            Whether provided files are actually annex keys
         batch: bool, optional
             Initiate or continue with a batched run of annex checkpresentkey
 
         Returns
         -------
         bool
             with True indicating that file/key is available from (the) remote
         """
 
         if key:
             key_ = file_
         else:
-            key_ = self.get_file_key(file_)  # ?, batch=batch
+            # TODO with eval_availability=True, the following call
+            # would already provide the answer to is_available? for
+            # the local annex
+            key_ = self.get_file_annexinfo(file_)['key']  # ?, batch=batch
 
-        annex_input = (key_,) if not remote else (key_, remote)
+        annex_input = [key_,] if not remote else [key_, remote]
 
         if not batch:
+            return self.call_annex_success(['checkpresentkey'] + annex_input)
+        else:
+            annex_cmd = ["checkpresentkey"] + ([remote] if remote else [])
             try:
-                out, err = self._run_annex_command('checkpresentkey',
-                                                   annex_options=list(annex_input),
-                                                   expect_fail=True)
-                assert(not out)
-                return True
+                out = self._batched.get(
+                    ':'.join(annex_cmd), annex_cmd,
+                    path=self.path)(key_)
             except CommandError:
+                # git-annex runs in batch mode, but will still signal some
+                # errors, e.g. an unknown remote, by exiting with a non-zero
+                # return code.
                 return False
-        else:
-            annex_cmd = ["checkpresentkey"] + ([remote] if remote else [])
-            out = self._batched.get(':'.join(annex_cmd), annex_cmd, git_options=self._GIT_COMMON_OPTIONS, path=self.path)(key_)
             try:
                 return {
+                    # happens on travis in direct/heavy-debug mode, that process
+                    # exits and closes stdout (upon unknown key) before we could
+                    # read it, so we get None as the stdout.
+                    # see https://github.com/datalad/datalad/issues/2330
+                    # but it is associated with an unknown key, and for consistency
+                    # we report False there too, as to ''
+                    None: False,
                     '': False,  # when remote is misspecified ... stderr carries the msg
                     '0': False,
                     '1': True,
                 }[out]
             except KeyError:
                 raise ValueError(
                     "Received output %r from annex, whenever expect 0 or 1" % out
                 )
 
-    @normalize_paths(match_return_type=False)
-    def _annex_custom_command(
-            self, files, cmd_str, log_stdout=True, log_stderr=True,
-            log_online=False, expect_stderr=False, cwd=None, env=None,
-            shell=None, expect_fail=False):
-        """Allows for calling arbitrary commands.
-
-        Helper for developing purposes, i.e. to quickly implement git-annex
-        commands for proof of concept.
-
-        Parameters
-        ----------
-        files: list of files
-        cmd_str: str
-            arbitrary command str. `files` is appended to that string.
-
-        Returns
-        -------
-        stdout, stderr
-        """
-        cmd = shlex.split(cmd_str + " " + " ".join(files), posix=not on_windows) \
-            if isinstance(cmd_str, string_types) \
-            else cmd_str + files
-        return self.cmd_call_wrapper.run(
-            cmd,
-            log_stderr=log_stderr, log_stdout=log_stdout, log_online=log_online,
-            expect_stderr=expect_stderr,
-            cwd=cwd, env=env, shell=shell, expect_fail=expect_fail)
-
     @normalize_paths
     def migrate_backend(self, files, backend=None):
         """Changes the backend used for `file`.
 
         The backend used for the key-value of `files`. Only files currently
         present are migrated.
         Note: There will be no notification if migrating fails due to the
@@ -2900,17 +2877,23 @@
             default backend of this instance will be used.
         """
 
         if self.is_direct_mode():
             raise CommandNotAvailableError(
                 'git-annex migrate',
                 "Command 'migrate' is not available in direct mode.")
-        self._run_annex_command('migrate',
-                                annex_options=files,
-                                backend=backend)
+        self._call_annex(
+            ['migrate'] + (['--backend', backend] if backend else []),
+            files=files,
+        )
+
+    @classmethod
+    def get_key_backend(cls, key):
+        """Get the backend from a given key"""
+        return key.split('-', 1)[0]
 
     @normalize_paths
     def get_file_backend(self, files):
         """Get the backend currently used for file(s).
 
         Parameters
         ----------
@@ -2919,30 +2902,97 @@
         Returns
         -------
         list of str
             For each file in input list indicates the used backend by a str
             like "SHA256E" or "MD5".
         """
 
-        return [self.get_file_key(f).split('-')[0] for f in files]
+        return [
+            p.get('backend', '')
+            for p in self.get_content_annexinfo(files, init=None).values()
+        ]
 
     @property
     def default_backends(self):
         self.config.reload()
+        # TODO: Deprecate and remove this property? It's used in the tests and
+        # datalad-crawler.
+        #
+        # git-annex used to try the list of backends in annex.backends in
+        # order. Now it takes annex.backend if set, falling back to the first
+        # value of annex.backends. See 4c1e3210f (annex.backend is the new name
+        # for what was annex.backends, 2017-05-09).
+        backend = self.get_gitattributes('.')['.'].get(
+            'annex.backend',
+            self.config.get("annex.backend", default=None))
+        if backend:
+            return [backend]
+
         backends = self.config.get("annex.backends", default=None)
         if backends:
             return backends.split()
         else:
             return None
 
-    def fsck(self):
-        self._run_annex_command('fsck')
+    # comment out presently unnecessary functionality, bring back once needed
+    #def fsck(self, paths=None, remote=None, fast=False, incremental=False,
+    #         limit=None, annex_options=None, git_options=None):
+    def fsck(self, paths=None, remote=None, fast=False,
+             annex_options=None, git_options=None):
+        """Front-end for git-annex fsck
 
-    # TODO: we probably need to override get_file_content, since it returns the
-    # symlink's target instead of the actual content.
+        Parameters
+        ----------
+        paths : list
+          Limit operation to specific paths.
+        remote : str
+          If given, the identified remote will be fsck'ed instead of the
+          local repository.
+        fast : bool
+          If True, typically means that no actual content is being verified,
+          but tests are limited to the presence of files.
+        """
+        #incremental : bool or {'continue'} or SCHEDULE
+        #  If given, `fsck` is called with `--incremental`. If 'continue',
+        #  `fsck` is additionally called with `--more`, and any other argument
+        #  is given to `--incremental-schedule`.
+        #limit : str or all
+        #  If the function `all` is given, `fsck` is called with `--all`. Any
+        #  other value is passed on to `--branch`.
+        args = [] if annex_options is None else list(annex_options)
+        if fast:
+            args.append('--fast')
+        if remote:
+            args.append('--from={}'.format(remote))
+        #if limit:
+        #    # looks funky, but really is a test if the `all` function was passed
+        #    # alternatives would have been 1) a dedicated argument (would need
+        #    # a check for mutual exclusivity with --branch), or 2) a str-type
+        #    # special values that has no meaning in Git and is less confusing
+        #    if limit is all:
+        #        args.append('--all')
+        #    else:
+        #        args.append('--branch={}'.format(limit))
+        #if incremental == 'continue':
+        #    args.append('--more')
+        #elif incremental:
+        #    args.append('--incremental')
+        #    if not (incremental is True):
+        #        args.append('--incremental-schedule={}'.format(incremental))
+        try:
+            return self._call_annex_records(
+                ['fsck'] + args,
+                files=paths,
+                git_options=git_options,
+            )
+        except CommandError as e:
+            # fsck may exit non-zero when there are too few known copies
+            # callers of whereis are interested in exactly that information,
+            # which we deliver via result, not via exception
+            return e.kwargs.get('stdout_json', [])
 
     # We need --auto and --fast having exposed  TODO
     @normalize_paths(match_return_type=False)  # get a list even in case of a single item
     def copy_to(self, files, remote, options=None, jobs=None):
         """Copy the actual content of `files` to `remote`
 
         Parameters
@@ -2953,14 +3003,18 @@
             name of remote to copy `files` to
 
         Returns
         -------
         list of str
            files successfully copied
         """
+        warnings.warn(
+            "AnnexRepo.copy_to() is deprecated and will be removed in a "
+            "future release. Use the Dataset method push() instead.",
+            DeprecationWarning)
 
         # find --in here --not --in remote
         # TODO: full support of annex copy options would lead to `files` being
         # optional. This means to check for whether files or certain options are
         # given and fail or just pass everything as is and try to figure out,
         # what was going on when catching CommandError
 
@@ -2972,48 +3026,53 @@
         # Note:
         # In case of single path, 'annex copy' will fail, if it cannot copy it.
         # With multiple files, annex will just skip the ones, it cannot deal
         # with. We'll do the same and report back what was successful
         # (see return value).
         # Therefore raise telling exceptions before even calling annex:
         if len(files) == 1:
-            if not isdir(files[0]):
-                self.get_file_key(files[0])
+            # Note, that for isdir we actually need an absolute path (which we don't get via normalize_paths)
+            if not isdir(opj(self.path, files[0])):
+                # for non-existing paths, get_file_annexinfo() will raise already
+                if self.get_file_annexinfo(files[0]).get('key') is None:
+                    raise FileInGitError(f'No known annex key for a file {files[0]}. Cannot copy')
 
         # TODO: RF -- logic is duplicated with get() -- the only difference
         # is the verb (copy, copy) or (get, put) and remote ('here', remote)?
         if '--key' not in options:
             expected_copys, copy_files = self._get_expected_files(
-                files, ['--in', 'here', '--not', '--in', remote])
+                files, ['--in', '.', '--not', '--in', remote])
         else:
             copy_files = files
             assert(len(files) == 1)
             expected_copys = {files[0]: AnnexRepo.get_size_from_key(files[0])}
 
         if not copy_files:
             lgr.debug("No files found needing copying.")
             return []
 
         if len(copy_files) != len(files):
-            lgr.info("Actually copying %d files", len(copy_files))
+            lgr.debug("Actually copying %d files", len(copy_files))
 
-        annex_options = ['--to=%s' % remote, '--json-progress']
+        self._maybe_open_ssh_connection(remote)
+        annex_options = ['--to=%s' % remote]
         if options:
-            annex_options.extend(shlex.split(options))
+            annex_options.extend(split_cmdline(options))
+
+        # filter out keys with missing size info
+        total_nbytes = sum(i for i in expected_copys.values() if i) or None
 
         # TODO: provide more meaningful message (possibly aggregating 'note'
         #  from annex failed ones
-        results = self._run_annex_command_json(
-            'copy',
-            opts=annex_options,
+        results = self._call_annex_records(
+            ['copy'] + annex_options,
             files=files,  # copy_files,
             jobs=jobs,
-            expected_entries=expected_copys
-            #log_stdout=True, log_stderr=not log_online,
-            #log_online=log_online, expect_stderr=True
+            progress=True,
+            total_nbytes=total_nbytes,
         )
         results_list = list(results)
         # XXX this is the only logic different ATM from get
         # check if any transfer failed since then we should just raise an Exception
         # for now to guarantee consistent behavior with non--json output
         # see https://github.com/datalad/datalad/pull/1349#discussion_r103639456
         from operator import itemgetter
@@ -3036,16 +3095,14 @@
 
         Returns
         -------
         str
           Returns a the annex UUID, if there is any, or `None` otherwise.
         """
         if not self._uuid:
-            if not self.repo:
-                return None
             self._uuid = self.config.get('annex.uuid', default=None)
         return self._uuid
 
     def get_description(self, uuid=None):
         """Get annex repository description
 
         Parameters
@@ -3063,15 +3120,15 @@
             (lambda x: x['here']) \
             if uuid is None \
             else (lambda x: x['uuid'] == uuid)
 
         matches = list(set(chain.from_iterable(
             [
                 [r['description'] for r in remotes if match(r)]
-                for k, remotes in iteritems(info)
+                for k, remotes in info.items()
                 if k.endswith(' repositories')
             ]
         )))
 
         if len(matches) == 1:
             # single hit as it should
             return matches[0]
@@ -3080,72 +3137,70 @@
                 "Found multiple hits while searching. Returning first among: %s",
                 str(matches)
             )
             return matches[0]
         else:
             return None
 
-    def set_remote_url(self, name, url, push=False):
-        """Set the URL a remote is pointing to
-
-        Sets the URL of the remote `name`. Requires the remote to already exist.
-
-        Parameters
-        ----------
-        name: str
-          name of the remote
-        url: str
-        push: bool
-          if True, set the push URL, otherwise the fetch URL;
-          if True, additionally set annexurl to `url`, to make sure annex uses
-          it to talk to the remote, since access via fetch URL might be
-          restricted.
-        """
-
-        if push:
-            # if we are to set a push url, also set 'annexUrl' for this remote,
-            # in order to make git-annex use it, when talking to the remote.
-            # (see http://git-annex.branchable.com/bugs/annex_ignores_pushurl_and_uses_only_url_upon___34__copy_--to__34__/)
-            var = 'remote.{0}.{1}'.format(name, 'annexurl')
-            self.config.set(var, url, where='local', reload=True)
-        super(AnnexRepo, self).set_remote_url(name, url, push)
-
-    def get_metadata(self, files, timestamps=False):
+    def get_metadata(self, files, timestamps=False, batch=False):
         """Query git-annex file metadata
 
         Parameters
         ----------
-        files : str or list(str)
-          One or more paths for which metadata is to be queried.
+        files : str or iterable(str)
+          One or more paths for which metadata is to be queried. If one
+          or more paths could be directories, `batch=False` must be given
+          to prevent git-annex given an error. Due to technical limitations,
+          such error will lead to a hanging process.
         timestamps: bool, optional
           If True, the output contains a '<metadatakey>-lastchanged'
           key for every metadata item, reflecting the modification
           time, as well as a 'lastchanged' key with the most recent
           modification time of any metadata item.
+        batch: bool, optional
+          If True, a `metadata --batch` process will be used, and only
+          confirmed annex'ed files can be queried (else query will hang
+          indefinitely). If False, invokes without --batch, and gives all files
+          as arguments (this can be problematic with a large number of files).
 
         Returns
         -------
         generator
           One tuple per file (could be more items than input arguments
           when directories are given). First tuple item is the filename,
           second item is a dictionary with metadata key/value pairs. Note that annex
           metadata tags are stored under the key 'tag', which is a
           regular metadata item that can be manipulated like any other.
         """
-        if not files:
-            return
-        files = assure_list(files)
-        opts = ['--json']
-        for res in self._run_annex_command_json(
-                'metadata', opts=opts, files=files):
-            yield (
-                res['file'],
+        def _format_response(res):
+            return (
+                str(Path(PurePosixPath(res['file']))),
                 res['fields'] if timestamps else \
                 {k: v for k, v in res['fields'].items()
-                 if not k.endswith('lastchanged')})
+                 if not k.endswith('lastchanged')}
+            )
+
+        if not files:
+            return
+        if batch is False:
+            # we can be lazy
+            files = ensure_list(files)
+        else:
+            if isinstance(files, str):
+                files = [files]
+            # anything else is assumed to be an iterable (e.g. a generator)
+        if batch is False:
+            for res in self.call_annex_records(['metadata'], files=files):
+                yield _format_response(res)
+        else:
+            # batch mode is different: we need to compose a JSON request object
+            batched = self._batched.get('metadata', json=True, path=self.path)
+            for f in files:
+                res = batched.proc1(json.dumps({'file': f}))
+                yield _format_response(res)
 
     def set_metadata(
             self, files, reset=None, add=None, init=None,
             remove=None, purge=None, recursive=False):
         """Manipulate git-annex file-metadata
 
         Parameters
@@ -3175,20 +3230,28 @@
           list is removed from the metadata.
         recursive : bool, optional
           If False, fail (with CommandError) when directory paths
           are given as `files`.
 
         Returns
         -------
-        generator
+        list
           JSON obj per modified file
         """
+        return list(self.set_metadata_(
+            files, reset=reset, add=add, init=init,
+            remove=remove, purge=purge, recursive=recursive))
+
+    def set_metadata_(
+            self, files, reset=None, add=None, init=None,
+            remove=None, purge=None, recursive=False):
+        """Like set_metadata() but returns a generator"""
 
         def _genspec(expr, d):
-            return [expr.format(k, v) for k, vs in d.items() for v in assure_list(vs)]
+            return [expr.format(k, v) for k, vs in d.items() for v in ensure_list(vs)]
 
         args = []
         spec = []
         for expr, d in (('{}={}', reset),
                         ('{}+={}', add),
                         ('{}?={}', init),
                         ('{}-={}', remove)):
@@ -3202,36 +3265,764 @@
                         for j in i)
         if not args:
             return
 
         if recursive:
             args.append('--force')
 
-        for jsn in self._run_annex_command_json(
-                'metadata',
-                args,
+        # Make sure that batch add/addurl operations are closed so that we can
+        # operate on files that were just added.
+        self.precommit()
+
+        for jsn in self.call_annex_records(
+                ['metadata'] + args,
                 files=files):
             yield jsn
 
+    # TODO: RM DIRECT?  might remain useful to detect submods left in direct mode
+    @staticmethod
+    def _is_annex_work_tree_message(out):
+        return re.match(
+            r'.*This operation must be run in a work tree.*'
+            r'git status.*failed in submodule',
+            out,
+            re.MULTILINE | re.DOTALL | re.IGNORECASE)
+
+
+    def _mark_content_availability(self, info):
+        objectstore = self.pathobj.joinpath(
+            self.path, GitRepo.get_git_dir(self), 'annex', 'objects')
+        for f, r in info.items():
+            if 'key' not in r or 'has_content' in r:
+                # not annexed or already processed
+                continue
+            # test hashdirmixed first, as it is used in non-bare repos
+            # which be a more frequent target
+            # TODO optimize order based on some check that reveals
+            # what scheme is used in a given annex
+            r['has_content'] = False
+            # some keys like URL-s700145--https://arxiv.org/pdf/0904.3664v1.pdf
+            # require sanitization to be able to mark content availability
+            # correctly. Can't limit to URL backend only; custom key backends
+            # may need it, too
+            key = _sanitize_key(r['key'])
+            for testpath in (
+                    # ATM git-annex reports hashdir in native path
+                    # conventions and the actual file path `f` in
+                    # POSIX, weird...
+                    # we need to test for the actual key file, not
+                    # just the containing dir, as on windows the latter
+                    # may not always get cleaned up on `drop`
+                    objectstore.joinpath(
+                        ut.Path(r['hashdirmixed']), key, key),
+                    objectstore.joinpath(
+                        ut.Path(r['hashdirlower']), key, key)):
+                if testpath.exists():
+                    r.pop('hashdirlower', None)
+                    r.pop('hashdirmixed', None)
+                    r['objloc'] = str(testpath)
+                    r['has_content'] = True
+                    break
+
+    def get_file_annexinfo(self, path, ref=None, eval_availability=False,
+                           key_prefix=''):
+        """Query annex properties for a single file
+
+        This is the companion to get_content_annexinfo() and offers
+        simplified usage for single-file queries (the result lookup
+        based on a path is not necessary.
+
+        All keyword arguments have identical names and semantics as
+        their get_content_annexinfo() counterparts. See their
+        documentation for more information.
+
+        Parameters
+        ----------
+        path : Path or str
+          A single path to a file in the repository.
+
+        Returns
+        -------
+        dict
+          Keys and values match the values returned by get_content_annexinfo().
+          If a file has no annex properties (i.e., a file that is directly
+          checked into Git and is not annexed), the returned dictionary is
+          empty.
+
+        Raises
+        ------
+        ValueError
+          When a given path is not matching a single file, but resolves to
+          multiple files (e.g. a directory path)
+        NoSuchPathError
+          When the given path does not match any file in a repository
+        """
+        info = {k: v
+                for k, v in self.get_content_annexinfo(
+                    [path],
+                    init=None,
+                    ref=ref,
+                    eval_availability=eval_availability).items()}
+        if len(info) > 1:
+            raise ValueError(
+                "AnnexRepo.get_file_annexinfo() can handle handle a single "
+                f"file path, but {path} resolved to {len(info)} paths")
+        elif not info:
+            # no error, there is a file, but we know nothing about it
+            return {}
+        path, props = info.popitem()
+        # turn a file not found situation into an exception
+        if props.get('success') is False and props.get('note') == 'not found':
+            raise NoSuchPathError(path)
+        # fold path into the report to give easy access to a normalized,
+        # resolved Path instance
+        props['path'] = path
+        return props
+
+    def get_content_annexinfo(
+            self, paths=None, init='git', ref=None, eval_availability=False,
+            key_prefix='', **kwargs):
+        """
+        Parameters
+        ----------
+        paths : list or None
+          Specific paths to query info for. In `None`, info is reported for all
+          content.
+        init : 'git' or dict-like or None
+          If set to 'git' annex content info will amend the output of
+          GitRepo.get_content_info(), otherwise the dict-like object
+          supplied will receive this information and the present keys will
+          limit the report of annex properties. Alternatively, if `None`
+          is given, no initialization is done, and no limit is in effect.
+        ref : gitref or None
+          If not None, annex content info for this Git reference will be
+          produced, otherwise for the content of the present worktree.
+        eval_availability : bool
+          If this flag is given, evaluate whether the content of any annex'ed
+          file is present in the local annex.
+        **kwargs :
+          Additional arguments for GitRepo.get_content_info(), if `init` is
+          set to 'git'.
+
+        Returns
+        -------
+        dict
+          The keys/values match those reported by GitRepo.get_content_info().
+          In addition, the following properties are added to each value
+          dictionary:
+
+          `type`
+            Can be 'file', 'symlink', 'dataset', 'directory', where 'file'
+            is also used for annex'ed files (corrects a 'symlink' report
+            made by `get_content_info()`.
+          `key`
+            Annex key of a file (if an annex'ed file)
+          `bytesize`
+            Size of an annexed file in bytes.
+          `has_content`
+            Bool whether a content object for this key exists in the local
+            annex (with `eval_availability`)
+          `objloc`
+            pathlib.Path of the content object in the local annex, if one
+            is available (with `eval_availability`)
+        """
+        if init is None:
+            info = dict()
+        elif init == 'git':
+            info = super(AnnexRepo, self).get_content_info(
+                paths=paths, ref=ref, **kwargs)
+        else:
+            info = init
+
+        if not paths and paths is not None:
+            return info
+
+        # use this funny-looking option with both find and findref
+        # it takes care of git-annex reporting on any known key, regardless
+        # of whether or not it actually (did) exist in the local annex.
+        if self._check_version_kludges("find-supports-anything"):
+            cmd = ['--anything']
+        else:
+            # --include=* was recommended by Joey in
+            # https://git-annex.branchable.com/todo/add_--all___40__or_alike__41___to_find_and_findref/
+            cmd = ['--include=*']
+        files = None
+        if ref:
+            cmd = ['findref'] + cmd
+            cmd.append(ref)
+        else:
+            cmd = ['find'] + cmd
+            # stringify any pathobjs
+            if paths:  # we have early exit above in case of [] and not None
+                files = [str(p) for p in paths]
+            else:
+                cmd += ['--include', '*']
+
+        for j in self.call_annex_records(cmd, files=files):
+            path = self.pathobj.joinpath(ut.PurePosixPath(j['file']))
+            rec = info.get(path, None)
+            if rec is None:
+                # git didn't report on this path
+                if j.get('success', None) is False:
+                    # Annex reports error on that file. Create an error entry,
+                    # as we can't currently yield a prepared error result from
+                    # within here.
+                    rec = {'status': 'error', 'state': 'unknown'}
+                elif init is not None:
+                    # init constraint knows nothing about this path -> skip
+                    continue
+                else:
+                    rec = {}
+            rec.update({'{}{}'.format(key_prefix, k): j[k]
+                       for k in j if k != 'file' and k != 'error-messages'})
+            # change annex' `error-messages` into singular to match result
+            # records:
+            if j.get('error-messages', None):
+                rec['error_message'] = '\n'.join(m.strip() for m in j['error-messages'])
+            if 'bytesize' in rec:
+                # it makes sense to make this an int that one can calculate with
+                # with
+                try:
+                    rec['bytesize'] = int(rec['bytesize'])
+                except ValueError:
+                    # this would only ever happen, if the recorded key itself
+                    # has no size info. Even for a URL key, this would mean
+                    # that the server would have to not report size info at all
+                    # but it does actually happen, e.g.
+                    # URL--http&c%%ciml.info%dl%v0_9%ciml-v0_9-all.pdf
+                    # from github.com/datalad-datasets/machinelearning-books
+                    lgr.debug('Failed to convert "%s" to integer bytesize',
+                              rec['bytesize'])
+                    # remove the field completely to avoid ambiguous semantics
+                    # of None/NaN etc.
+                    del rec['bytesize']
+            if rec.get('type') == 'symlink' and rec.get('key') is not None:
+                # we have a tracked symlink with an associated annex key
+                # this is only a symlink for technical reasons, but actually
+                # a file from the user perspective.
+                # homogenization of this kind makes the report more robust
+                # across different representations of a repo
+                # (think adjusted branches ...)
+                rec['type'] = 'file'
+            info[path] = rec
+        # TODO make annex availability checks optional and move in here
+        if eval_availability:
+            self._mark_content_availability(info)
+        return info
+
+    def annexstatus(self, paths=None, untracked='all'):
+        """
+        .. deprecated:: 0.16
+            Use get_content_annexinfo() or the test helper
+            :py:func:`datalad.tests.utils_pytest.get_annexstatus` instead.
+        """
+        info = self.get_content_annexinfo(
+            paths=paths,
+            eval_availability=False,
+            init=self.get_content_annexinfo(
+                paths=paths,
+                ref='HEAD',
+                eval_availability=False,
+                init=self.status(
+                    paths=paths,
+                    eval_submodule_state='full')
+            )
+        )
+        self._mark_content_availability(info)
+        return info
+
+    def _save_add(self, files, git=None, git_opts=None):
+        """Simple helper to add files in save()"""
+        from datalad.interface.results import get_status_dict
+
+        # alter default behavior of git-annex by considering dotfiles
+        # too
+        # however, this helper is controlled by save() which itself
+        # operates on status() which itself honors .gitignore, so
+        # there is a standard mechanism that is uniform between Git
+        # Annex repos to decide on the behavior on a case-by-case
+        # basis
+        options = []
+        # if None -- leave it to annex to decide
+        if git is False:
+            options.append("--force-large")
+        if on_windows:
+            # git-annex ignores symlinks on windows
+            # https://github.com/datalad/datalad/issues/2955
+            # check if there are any and pass them to git-add
+            symlinks_toadd = {
+                p: props for p, props in files.items()
+                if props.get('type', None) == 'symlink'}
+            if symlinks_toadd:
+                for r in GitRepo._save_add(
+                        self,
+                        symlinks_toadd,
+                        git_opts=git_opts):
+                    yield r
+            # trim `files` of symlinks
+            files = {
+                p: props for p, props in files.items()
+                if props.get('type', None) != 'symlink'}
+
+        expected_additions = None
+        if ui.is_interactive:
+            # without an interactive UI there is little benefit from
+            # progressbar info, hence save the stat calls
+            expected_additions = {p: self.get_file_size(p) for p in files}
+
+        if git is True:
+            yield from GitRepo._save_add(self, files, git_opts=git_opts)
+        else:
+            for r in self._call_annex_records(
+                    ['add'] + options,
+                    files=list(files.keys()),
+                    # TODO
+                    jobs=None,
+                    total_nbytes=sum(expected_additions.values())
+                    if expected_additions else None):
+                yield get_status_dict(
+                    action=r.get('command', 'add'),
+                    refds=self.pathobj,
+                    type='file',
+                    path=(self.pathobj / ut.PurePosixPath(r['file']))
+                    if 'file' in r else None,
+                    status='ok' if r.get('success', None) else 'error',
+                    key=r.get('key', None),
+                    message='\n'.join(r['error-messages'])
+                    if 'error-messages' in r else None,
+                    logger=lgr)
+
+    def _save_post(self, message, files, partial_commit,
+                   amend=False, allow_empty=False):
+
+        if amend and self.is_managed_branch() and \
+                self.format_commit("%B").strip() == "git-annex adjusted branch":
+            # We must not directly amend on an adjusted branch, but fix it
+            # up after the fact. That is if HEAD is a git-annex commit.
+            # Otherwise we still can amend-commit normally.
+            # Note, that this may involve creating an empty commit first.
+            amend = False
+            adjust_amend = True
+        else:
+            adjust_amend = False
+
+        # first do standard GitRepo business
+        super(AnnexRepo, self)._save_post(
+            message, files, partial_commit, amend,
+            allow_empty=allow_empty or adjust_amend)
+        # then sync potential managed branches
+        self.localsync(managed_only=True)
+        if adjust_amend:
+            # We committed in an adjusted branch, but the goal is to amend in
+            # corresponding branch.
+
+            adjusted_branch = self.get_active_branch()
+            corresponding_branch = self.get_corresponding_branch()
+            old_sha = self.get_hexsha(corresponding_branch)
+
+            org_commit_pointer = corresponding_branch + "~1"
+            author_name, author_email, author_date, \
+            old_parent, old_message = self.format_commit(
+                "%an%x00%ae%x00%ad%x00%P%x00%B", org_commit_pointer).split('\0')
+            new_env = (self._git_runner.env
+                       if self._git_runner.env else os.environ).copy()
+            # `message` might be empty - we need to take it from the to be
+            # amended commit in that case:
+            msg = message or old_message
+            new_env.update({
+                'GIT_AUTHOR_NAME': author_name,
+                'GIT_AUTHOR_EMAIL': author_email,
+                'GIT_AUTHOR_DATE': author_date
+            })
+            commit_cmd = ["commit-tree",
+                          corresponding_branch + "^{tree}",
+                          "-m", msg]
+            if old_parent:
+                commit_cmd.extend(["-p", old_parent])
+            out, _ = self._call_git(commit_cmd, env=new_env, read_only=False)
+            new_sha = out.strip()
+
+            self.update_ref("refs/heads/" + corresponding_branch,
+                            new_sha, old_sha)
+            self.update_ref("refs/basis/" + adjusted_branch,
+                            new_sha, old_sha)
+            self.localsync(managed_only=True)
+
+    def localsync(self, remote=None, managed_only=False):
+        """Consolidate the local git-annex branch and/or managed branches.
+
+        This method calls `git annex sync` to perform purely local operations
+        that:
+
+        1. Update the corresponding branch of any managed branch.
+
+        2. Synchronize the local 'git-annex' branch with respect to particular
+           or all remotes (as currently reflected in the local state of their
+           remote 'git-annex' branches).
+
+        If a repository has git-annex's 'synced/...' branches these will be
+        updated.  Otherwise, such branches that are created by `git annex sync`
+        are removed again after the sync is complete.
+
+        Parameters
+        ----------
+        remote : str or list, optional
+          If given, specifies the name of one or more remotes to sync against.
+          If not given, all remotes are considered.
+        managed_only : bool, optional
+          Only perform a sync if a managed branch with a corresponding branch
+          is detected. By default, a sync is always performed.
+        """
+        branch = self.get_active_branch()
+        corresponding_branch = self.get_corresponding_branch(branch)
+        branch = corresponding_branch or branch
+
+        if managed_only and not corresponding_branch:
+            lgr.debug('No sync necessary, no corresponding branch detected')
+            return
+
+        lgr.debug(
+            "Sync local 'git-annex' branch%s.",
+            ", and corresponding '{}' branch".format(corresponding_branch)
+            if corresponding_branch else '')
+
+        synced_branch = 'synced/{}'.format(branch)
+        had_synced_branch = synced_branch in self.get_branches()
+        cmd = ['sync']
+        if remote:
+            cmd.extend(ensure_list(remote))
+        cmd.extend([
+            # disable any external interaction and other magic
+            '--no-push', '--no-pull', '--no-commit', '--no-resolvemerge',
+            '--no-content'])
+        self.call_annex(cmd)
+        # a sync can establish new config (e.g. annex-uuid for a remote)
+        self.config.reload()
+        # cleanup sync'ed branch if we caused it
+        if not had_synced_branch and synced_branch in self.get_branches():
+            lgr.debug('Remove previously non-existent %s branch after sync',
+                      synced_branch)
+            self.call_git(
+                ['branch', '-d', synced_branch],
+            )
+
+
+class AnnexJsonProtocol(WitlessProtocol):
+    """Subprocess communication protocol for `annex ... --json` commands
+
+    Importantly, parsed JSON content is returned as a result, not string output.
+
+    This protocol also handles git-annex's JSON-style progress reporting.
+    """
+    # capture both streams and handle messaging completely
+    proc_out = True
+    proc_err = True
+
+    def __init__(self, done_future=None, total_nbytes=None):
+        if done_future is not None:
+            warnings.warn("`done_future` argument is ignored "
+                          "and will be removed in a future release",
+                          DeprecationWarning)
+        super().__init__()
+        # to collect parsed JSON command output
+        self.json_out = []
+        self._global_pbar_id = 'annexprogress-{}'.format(id(self))
+        self.total_nbytes = total_nbytes
+        self._unprocessed = None
+
+    def add_to_output(self, json_object):
+        self.json_out.append(json_object)
+
+    def connection_made(self, transport):
+        super().connection_made(transport)
+        self._pbars = set()
+        # overall counter of processed bytes (computed from key reports)
+        self._byte_count = 0
+        if self.total_nbytes:
+            # init global pbar, do here to be on top of first file
+            log_progress(
+                lgr.info,
+                self._global_pbar_id,
+                'Start annex operation',
+                # do not crash if no command is reported
+                unit=' Bytes',
+                label='Total',
+                total=self.total_nbytes,
+                noninteractive_level=5,
+            )
+            self._pbars.add(self._global_pbar_id)
+
+    def pipe_data_received(self, fd, data):
+        if fd != 1:
+            # let the base class decide what to do with it
+            super().pipe_data_received(fd, data)
+            return
+        if self._unprocessed:
+            data = self._unprocessed + data
+            self._unprocessed = None
+        # this is where the JSON records come in
+        lines = data.splitlines()
+        data_ends_with_eol = data.endswith(os.linesep.encode())
+        del data
+        for iline, line in enumerate(lines):
+            try:
+                j = json.loads(line)
+            except Exception as exc:
+                if line.strip():
+                    # do not complain on empty lines
+                    if iline == len(lines) - 1 and not data_ends_with_eol:
+                        lgr.debug("Caught %s while trying to parse JSON line %s which might "
+                                  "be not yet a full line", exc, line)
+                        # it is the last line and fails to parse -- it can/likely
+                        # to happen that it was not a complete line and that buffer
+                        # got filled up/provided before the end of line.
+                        # Store it so that it can be prepended to data in the next call.
+                        self._unprocessed = line
+                        break
+                    # TODO turn this into an error result, or put the exception
+                    # onto the result future -- needs more thought
+                    lgr.error('Received undecodable JSON output: %s', line)
+                continue
+            self._proc_json_record(j)
+
+    def _get_pbar_id(self, record):
+        # NOTE: Look at the "action" field for byte-progress records and the
+        # top-level `record` for the final record. The action record as a whole
+        # should be stable link across byte-progress records, but a subset of
+        # the keys is hard coded below so that the action record can be linked
+        # to the final one.
+        info = record.get("action") or record
+        return 'annexprogress-{}-{}'.format(
+            id(self),
+            hash(frozenset((k, info.get(k))
+                           for k in ["command", "key", "file"])))
+
+    def _get_pbar_label(self, action):
+        # do not crash if no command is reported
+        label = action.get('command', '').capitalize()
+        target = action.get('file') or action.get('key')
+        if target:
+            label += " " + target
+
+        if label:
+            from datalad.ui import utils as ui_utils
+
+            # Reserving 55 characters for the progress bar is based
+            # approximately off what used to be done in the now-removed
+            # (948ccf3e18) ProcessAnnexProgressIndicators.
+            max_label_width = ui_utils.get_console_width() - 55
+            if max_label_width < 0:
+                # We're squeezed. Just show bar.
+                label = ""
+            elif len(label) > max_label_width:
+                mid = max_label_width // 2
+                label = label[:mid] + " .. " + label[-mid:]
+        return label
+
+    def _proc_json_record(self, j):
+        # check for progress reports and act on them immediately
+        # but only if there is something to build a progress report from
+        pbar_id = self._get_pbar_id(j)
+        known_pbar = pbar_id in self._pbars
+        action = j.get('action')
+
+        is_progress = action and 'byte-progress' in j
+        # ignore errors repeatedly reported in progress messages. Final message
+        # will contain them
+        if action and not is_progress:
+            for err_msg in action.pop('error-messages', []):
+                lgr.error(err_msg)
+
+        if known_pbar and (not is_progress or
+                           j.get('byte-progress') == j.get('total-size')):
+            # take a known pbar down, completion or broken report
+            log_progress(
+                lgr.info,
+                pbar_id,
+                'Finished annex action: {}'.format(action),
+                noninteractive_level=5,
+            )
+            self._pbars.discard(pbar_id)
+            if is_progress:
+                # The final record is yet to come.
+                return
+
+        if is_progress:
+            if not known_pbar:
+                # init the pbar, the is some progress left to be made
+                # worth it
+                log_progress(
+                    lgr.info,
+                    pbar_id,
+                    'Start annex action: {}'.format(action),
+                    label=self._get_pbar_label(action),
+                    unit=' Bytes',
+                    total=float(j.get('total-size', 0)),
+                    noninteractive_level=5,
+                )
+                self._pbars.add(pbar_id)
+            log_progress(
+                lgr.info,
+                pbar_id,
+                j.get('percent-progress', 0),
+                update=float(j.get('byte-progress', 0)),
+                noninteractive_level=5,
+            )
+            # do not let progress reports leak into the return value
+            return
+        # update overall progress, do not crash when there is no key property
+        # in the report (although there should be one)
+        key_bytes = AnnexRepo.get_size_from_key(j.get('key', None))
+        if key_bytes:
+            self._byte_count += key_bytes
+        # don't do anything to the results for now in terms of normalization
+        # TODO the protocol could be made aware of the runner's CWD and
+        # also any dataset the annex command is operating on. This would
+        # enable 'file' property conversion to absolute paths
+        self.add_to_output(j)
+
+        if self.total_nbytes:
+            if self.total_nbytes <= self._byte_count:
+                # discard global pbar
+                log_progress(
+                    lgr.info,
+                    self._global_pbar_id,
+                    'Finished annex {}'.format(j.get('command', '')),
+                    noninteractive_level=5,
+                )
+                self._pbars.discard(self._global_pbar_id)
+            else:
+                # log actual progress
+                log_progress(
+                    lgr.info,
+                    self._global_pbar_id,
+                    j.get('file', ''),
+                    update=self._byte_count,
+                    noninteractive_level=5,
+                )
+
+    def _prepare_result(self):
+        # first let the base class do its thing
+        results = super()._prepare_result()
+        # now amend the results, make clear in the key-name that these records
+        # came from stdout -- may not be important here or now, but it is easy
+        # to imagine structured output on stderr at some point
+        results['stdout_json'] = self.json_out
+        return results
+
+    def process_exited(self):
+        # take down any progress bars that were not closed orderly
+        for pbar_id in self._pbars:
+            log_progress(
+                lgr.info,
+                pbar_id,
+                'Finished',
+                noninteractive_level=5,
+            )
+        if self._unprocessed:
+            lgr.error(
+                "%d bytes of received undecodable JSON output remain: %s",
+                len(self._unprocessed), self._unprocessed
+            )
+        super().process_exited()
+
+
+class GeneratorAnnexJsonProtocol(GeneratorMixIn, AnnexJsonProtocol):
+    def __init__(self,
+                 done_future=None,
+                 total_nbytes=None):
+        GeneratorMixIn.__init__(self)
+        AnnexJsonProtocol.__init__(self, done_future, total_nbytes)
+
+    def add_to_output(self, json_object):
+        self.send_result(json_object)
+
+
+class GeneratorAnnexJsonNoStderrProtocol(GeneratorAnnexJsonProtocol):
+    def __init__(self,
+                 done_future=None,
+                 total_nbytes=None):
+        GeneratorMixIn.__init__(self)
+        AnnexJsonProtocol.__init__(self, done_future, total_nbytes)
+        self.stderr_output = bytearray()
+
+    def pipe_data_received(self, fd, data):
+        if fd == 2:
+            self.stderr_output += data
+            # let the base class decide what to do with it
+        super().pipe_data_received(fd, data)
+
+    def process_exited(self):
+        super().process_exited()
+        if self.stderr_output:
+            raise CommandError(
+                msg="Unexpected stderr output",
+                stderr=self.stderr_output.decode())
+
+
+class AnnexInitOutput(WitlessProtocol, AssemblingDecoderMixIn):
+    proc_out = True
+    proc_err = True
+
+    def __init__(self, done_future=None, encoding=None):
+        WitlessProtocol.__init__(self, done_future, encoding)
+        AssemblingDecoderMixIn.__init__(self)
+
+    def pipe_data_received(self, fd, byts):
+        line = self.decode(fd, byts, self.encoding)
+        if fd == 1:
+            res = re.search("(scanning for .* files)", line, flags=re.IGNORECASE)
+            if res:
+                lgr.info("%s (this may take some time)", res.groups()[0])
+        elif fd == 2:
+            lgr.info(line.strip())
+
+
+@auto_repr(short=False)
+class BatchedAnnex(BatchedCommand):
+    """Container for an annex process which would allow for persistent communication
+    """
+
+    def __init__(self, annex_cmd, git_options=None, annex_options=None, path=None,
+                 json=False, output_proc=None, batch_opt='--batch'):
+        if not isinstance(annex_cmd, list):
+            annex_cmd = [annex_cmd]
+        cmd = \
+            ['git'] + \
+            (git_options if git_options else []) + \
+            ['annex'] + \
+            annex_cmd + \
+            (annex_options if annex_options else []) + \
+            (['--json', '--json-error-messages'] if json else []) + \
+            [batch_opt] + \
+            (['--debug'] if lgr.getEffectiveLevel() <= 8 else [])
+        output_proc = \
+            output_proc if output_proc else readline_json if json else None
+        super(BatchedAnnex, self).__init__(
+            cmd,
+            path=path,
+            output_proc=output_proc)
+
 
 # TODO: Why was this commented out?
 # @auto_repr
-class BatchedAnnexes(dict):
+class BatchedAnnexes(SafeDelCloseMixin, dict):
     """Class to contain the registry of active batch'ed instances of annex for
     a repository
     """
-    def __init__(self, batch_size=0):
+    def __init__(self, batch_size=0, git_options=None):
         self.batch_size = batch_size
+        self.git_options = git_options or []
         super(BatchedAnnexes, self).__init__()
 
-    def get(self, codename, annex_cmd=None, **kwargs):
+    def get(self, codename, annex_cmd=None, **kwargs) -> BatchedAnnex:
         if annex_cmd is None:
             annex_cmd = codename
 
-        git_options = kwargs.pop('git_options', [])
+        git_options = self.git_options + kwargs.pop('git_options', [])
         if self.batch_size:
             git_options += ['-c', 'annex.queuesize=%d' % self.batch_size]
 
         # START RF/BF: extend codename to respect different options the process
         # is running with
         # TODO: Eventually there should be more RF'ing, since the actually used
         # codenames are partially reflecting this already. Any options used
@@ -3260,354 +4051,33 @@
         """Close communication to all the batched annexes
 
         It does not remove them from the dictionary though
         """
         for p in self.values():
             p.close()
 
-    def __del__(self):
-        self.close()
-
-
-def readline_rstripped(stdout):
-    #return iter(stdout.readline, b'').next().rstrip()
-    return stdout.readline().rstrip()
-
 
 def readlines_until_ok_or_failed(stdout, maxlines=100):
     """Read stdout until line ends with ok or failed"""
     out = ''
     i = 0
-    lgr.log(3, "Trying to receive from %s" % stdout)
+    lgr.log(3, "Trying to receive from %s", stdout)
     while not stdout.closed:
         i += 1
         if maxlines > 0 and i > maxlines:
             raise IOError("Expected no more than %d lines. So far received: %r" % (maxlines, out))
         lgr.log(2, "Expecting a line")
         line = stdout.readline()
-        lgr.log(2, "Received line %r" % line)
+        lgr.log(2, "Received line %r", line)
         out += line
         if re.match(r'^.*\b(failed|ok)$', line.rstrip()):
             break
     return out.rstrip()
 
 
 def readline_json(stdout):
-    return json_loads(stdout.readline().strip())
-
-
-@auto_repr
-class BatchedAnnex(object):
-    """Container for an annex process which would allow for persistent communication
-    """
-
-    def __init__(self, annex_cmd, git_options=None, annex_options=None, path=None,
-                 json=False,
-                 output_proc=None):
-        if not isinstance(annex_cmd, list):
-            annex_cmd = [annex_cmd]
-        self.annex_cmd = annex_cmd
-        self.git_options = git_options if git_options else []
-        annex_options = annex_options if annex_options else []
-        self.annex_options = annex_options + (['--json'] if json else [])
-        self.path = path
-        if output_proc is None:
-            output_proc = readline_json if json else readline_rstripped
-        self.output_proc = output_proc
-        self._process = None
-        self._stderr_out = None
-        self._stderr_out_fname = None
-
-    def _initialize(self):
-        # TODO -- should get all those options about --debug and --backend which are used/composed
-        # in AnnexRepo class
-        lgr.debug("Initiating a new process for %s" % repr(self))
-        cmd = ['git'] + self.git_options + \
-              ['annex'] + self.annex_cmd + self.annex_options + ['--batch']  # , '--debug']
-        lgr.log(5, "Command: %s" % cmd)
-        # TODO: look into _run_annex_command  to support default options such as --debug
-        #
-        # according to the internet wisdom there is no easy way with subprocess
-        # while avoid deadlocks etc.  We would need to start a thread/subprocess
-        # to timeout etc
-        # kwargs = dict(bufsize=1, universal_newlines=True) if PY3 else {}
-        self._stderr_out, self._stderr_out_fname = tempfile.mkstemp()
-        self._process = Popen(
-            cmd, stdin=PIPE, stdout=PIPE,  stderr=self._stderr_out,
-            env=GitRunner.get_git_environ_adjusted(),
-            cwd=self.path,
-            bufsize=1,
-            universal_newlines=True  # **kwargs
-        )
-
-    def _check_process(self, restart=False):
-        """Check if the process was terminated and restart if restart
-
-        """
-        process = self._process
-        if process and process.poll():
-            lgr.warning("Process %s was terminated with returncode %s" % (process, process.returncode))
-            self.close()
-        if self._process is None and restart:
-            lgr.warning("Restarting the process due to previous failure")
-            self._initialize()
-
-    def __call__(self, cmds):
-        """
-
-        Parameters
-        ----------
-        cmds : str or tuple or list of (str or tuple)
-
-        Returns
-        -------
-        str or list
-          Output received from annex.  list in case if cmds was a list
-        """
-        # TODO: add checks -- may be process died off and needs to be reinitiated
-        if not self._process:
-            self._initialize()
-
-        input_multiple = isinstance(cmds, list)
-        if not input_multiple:
-            cmds = [cmds]
-
-        output = []
-
-        for entry in cmds:
-            if not isinstance(entry, string_types):
-                entry = ' '.join(entry)
-            entry = entry + '\n'
-            lgr.log(5, "Sending %r to batched annex %s" % (entry, self))
-            # apparently communicate is just a one time show
-            # stdout, stderr = self._process.communicate(entry)
-            # according to the internet wisdom there is no easy way with subprocess
-            self._check_process(restart=True)
-            process = self._process  # _check_process might have restarted it
-            process.stdin.write(entry)  # .encode())
-            process.stdin.flush()
-            lgr.log(5, "Done sending.")
-            # TODO: somehow do catch stderr which might be there or not
-            #stderr = str(process.stderr) if process.stderr.closed else None
-            self._check_process(restart=False)
-            # We are expecting a single line output
-            # TODO: timeouts etc
-            #import pdb; pdb.set_trace()
-            stdout = self.output_proc(process.stdout) if not process.stdout.closed else None
-            #if stderr:
-            #    lgr.warning("Received output in stderr: %r" % stderr)
-            lgr.log(5, "Received output: %r" % stdout)
-            output.append(stdout)
-
-        return output if input_multiple else output[0]
-
-    def __del__(self):
-        self.close()
-
-    def close(self):
-        """Close communication and wait for process to terminate"""
-        if self._stderr_out:
-            # close possibly still open fd
-            os.fdopen(self._stderr_out).close()
-            self._stderr_out = None
-        if self._stderr_out_fname and os.path.exists(self._stderr_out_fname):
-            # remove the file where we kept dumping stderr
-            os.unlink(self._stderr_out_fname)
-            self._stderr_out_fname = None
-        if self._process:
-            process = self._process
-            lgr.debug("Closing stdin of %s and waiting process to finish", process)
-            process.stdin.close()
-            process.stdout.close()
-            process.wait()
-            self._process = None
-            lgr.debug("Process %s has finished", process)
-
-
-class ProcessAnnexProgressIndicators(object):
-    """'Filter' for annex --json output to react to progress indicators
-
-    Instance of this beast should be passed into log_stdout option
-    for git-annex commands runner
-    """
-
-    def __init__(self, expected=None):
-        """
-
-        Parameters
-        ----------
-        expected: dict, optional
-           key -> size, expected entries (e.g. downloads)
-        """
-        # looking forward for multiple downloads at the same time
-        self.pbars = {}
-        self.total_pbar = None
-        self.expected = expected
-        self._failed = 0
-        self._succeeded = 0
-        self.start()
-
-    def start(self):
-        if self.expected:
-            from datalad.ui import ui
-            total = sum(filter(bool, self.expected.values()))
-            self.total_pbar = ui.get_progressbar(
-                label="Total", total=total)
-            self.total_pbar.start()
-
-    def _update_pbar(self, pbar, new_value):
-        """Updates pbar while also updating possibly total pbar"""
-        old_value = getattr(pbar, '_old_value', 0)
-        # due to http://git-annex.branchable.com/bugs/__34__byte-progress__34___could_jump_down_upon_initiating_re-download_--_report_actual_one_first__63__/?updated
-        # we will just skip the first update to avoid possible incorrect
-        # reporting
-        if not getattr(pbar, '_first_skipped', False):
-            setattr(pbar, '_first_skipped', True)
-            lgr.log(1, "Skipped first update of pbar %s", pbar)
-            return
-        setattr(pbar, '_old_value', new_value)
-        diff = new_value - old_value
-        if diff < 0:
-            # so above didn't help!
-            # use warnings not lgr.warn since we apparently swallow stuff
-            # upstairs!  Also it would take care about issuing it only once
-            import warnings
-            warnings.warn(
-                "Got negative diff for progressbar. old_value=%r, new_value=%r"
-                " no more warnings should come for this one and we will not update"
-                " until values start to make sense" % (old_value, new_value))
-            return
-        if self.total_pbar:
-            self.total_pbar.update(diff, increment=True)
-        pbar.update(new_value)
-
-    def _log_info(self, msg):
-        """Helper to log a message, so we need to clear up the pbars first"""
-        if self.total_pbar:
-            self.total_pbar.clear()
-        for pbar in self.pbars.values():
-            pbar.clear()
-        lgr.info(msg)
-
-    def __call__(self, line):
-        try:
-            j = json.loads(line)
-        except:
-            # if we fail to parse, just return this precious thing for
-            # possibly further processing
-            return line
-
-        # Process some messages which remotes etc might push to us
-        if list(j) == ['info']:
-            # Just INFO was received without anything else -- we log it at INFO
-            info = j['info']
-            if info.startswith('PROGRESS-JSON: '):
-                j_ = json_loads(info[len('PROGRESS-JSON: '):])
-                if ('command' in j_ and 'key' in j_) or 'byte-progress' in j_:
-                    j = j_
-                else:
-                    self._log_info(info)
-            else:
-                self._log_info(info)
-                return
-
-        target_size = None
-        if 'command' in j and 'key' in j:
-            # might be the finish line message
-            j_download_id = (j['command'], j['key'])
-            pbar = self.pbars.pop(j_download_id, None)
-            if j.get('success') in {True, 'true'}:
-                self._succeeded += 1
-                if pbar:
-                    self._update_pbar(pbar, pbar.total)
-                elif self.total_pbar:
-                    # we didn't have a pbar for this download, so total should
-                    # get it all at once
-                    try:
-                        size_j = self.expected[j['key']]
-                    except:
-                        size_j = None
-                    target_size = size_j or AnnexRepo.get_size_from_key(j['key'])
-                    self.total_pbar.update(target_size, increment=True)
-            else:
-                lgr.log(5, "Message with failed status: %s" % str(j))
-                self._failed += 1
-
-            if self.total_pbar:
-                failed_str = (
-                    ", " + ansi_colors.color_word("%d failed" % self._failed,
-                                                  ansi_colors.RED)) \
-                    if self._failed else ''
-
-                self.total_pbar.set_desc(
-                    "Total (%d ok%s out of %d)" % (
-                        self._succeeded,
-                        failed_str,
-                        len(self.expected)
-                        if self.expected
-                        else self._succeeded + self._failed))
-                # seems to be of no effect to force it repaint
-                self.total_pbar.refresh()
-
-            if pbar:
-                pbar.finish()
-
-        if 'byte-progress' not in j:
-            # some other thing than progress
-            return line
-
-        def get_size_from_perc_complete(count, perc):
-            return int(math.ceil(int(count) / (float(perc) / 100.))) \
-                if perc else 0
-
-        # so we have a progress indicator, let's deal with it
-        action = j['action']
-        download_item = action.get('file') or action.get('key')
-        download_id = (action['command'], action['key'])
-        if download_id not in self.pbars:
-            # New download!
-            from datalad.ui import ui
-            from datalad.ui import utils as ui_utils
-            # TODO: whenever target size gets reported -- used it!
-            # http://git-annex.branchable.com/todo/interface_to_the___34__progress__34___of_annex_operations/#comment-6bbc26aae9867603863050ddcb09a9a0
-            # for now deduce from key or approx from '%'
-            # TODO: unittest etc to check when we have a relaxed
-            # URL without any size known in advance
-            if not target_size:
-                target_size = \
-                    AnnexRepo.get_size_from_key(action.get('key')) or \
-                    get_size_from_perc_complete(
-                        j['byte-progress'],
-                        j.get('percent-progress', '').rstrip('%')
-                    ) or \
-                    0
-            w, h = ui_utils.get_terminal_size()
-            w = w or 80  # default to 80
-            title = str(download_item)
-            pbar_right = 50
-            title_len = w - pbar_right - 4  # (4 for reserve)
-            if len(title) > title_len:
-                half = title_len//2 - 2
-                title = '%s .. %s' % (title[:half], title[-half:])
-            pbar = self.pbars[download_id] = ui.get_progressbar(
-                label=title, total=target_size)
-            pbar.start()
-
-        lgr.log(1, "Updating pbar for download_id=%s. annex: %s.\n",
-                download_id, j)
-        self._update_pbar(
-            self.pbars[download_id],
-            int(j.get('byte-progress'))
-        )
-
-    def finish(self):
-        if self.total_pbar:
-            self.total_pbar.finish()
-            self.total_pbar = None
-        if self.pbars:
-            lgr.warning("Still have %d active progress bars when stopping",
-                        len(self.pbars))
-        for pbar in self.pbars.values():
-            pbar.finish()
-        self.pbars = {}
-        self._failed = 0
-        self._succeeded = 0
+    toload = stdout.readline().strip()
+    try:
+        return json.loads(toload) if toload else {}
+    except json.JSONDecodeError:
+        lgr.error('Received undecodable JSON output: %s', toload)
+        return {}
```

### Comparing `datalad-0.9.3/datalad/support/_lru_cache2.py` & `datalad-1.0.0/datalad/core/local/resulthooks.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,183 +1,211 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""LRU cache backport from python3 to python2
+"""Utility functions for result hooks
 
-Origin: http://code.activestate.com/recipes/578078-py26-and-py30-backport-of-python-33s-lru-cache/
-
-Since it is a backport, treated as distributed under license of the Python
-itself
 """
 
-from collections import namedtuple
-from functools import update_wrapper
-from threading import RLock
-
-_CacheInfo = namedtuple("CacheInfo", ["hits", "misses", "maxsize", "currsize"])
-
-
-class _HashedSeq(list):
-    __slots__ = 'hashvalue'
-
-    def __init__(self, tup, hash=hash):
-        self[:] = tup
-        self.hashvalue = hash(tup)
-
-    def __hash__(self):
-        return self.hashvalue
-
-
-def _make_key(args, kwds, typed,
-             kwd_mark = (object(),),
-             fasttypes = {int, str, frozenset, type(None)},
-             sorted=sorted, tuple=tuple, type=type, len=len):
-    'Make a cache key from optionally typed positional and keyword arguments'
-    key = args
-    if kwds:
-        sorted_items = sorted(kwds.items())
-        key += kwd_mark
-        for item in sorted_items:
-            key += item
-    if typed:
-        key += tuple(type(v) for v in args)
-        if kwds:
-            key += tuple(type(v) for k, v in sorted_items)
-    elif len(key) == 1 and type(key[0]) in fasttypes:
-        return key[0]
-    return _HashedSeq(key)
+__docformat__ = 'restructuredtext'
 
+import logging
+import json
 
-def lru_cache(maxsize=100, typed=False):
-    """Least-recently-used cache decorator.
+from datalad.support.exceptions import CapturedException
 
-    If *maxsize* is set to None, the LRU features are disabled and the cache
-    can grow without bound.
+lgr = logging.getLogger('datalad.core.local.resulthooks')
 
-    If *typed* is True, arguments of different types will be cached separately.
-    For example, f(3.0) and f(3) will be treated as distinct calls with
-    distinct results.
 
-    Arguments to the cached function must be hashable.
-
-    View the cache statistics named tuple (hits, misses, maxsize, currsize) with
-    f.cache_info().  Clear the cache and statistics with f.cache_clear().
-    Access the underlying function with f.__wrapped__.
-
-    See:  http://en.wikipedia.org/wiki/Cache_algorithms#Least_Recently_Used
+def get_jsonhooks_from_config(cfg):
+    """Parse out hook definitions given a ConfigManager instance
 
+    Returns
+    -------
+    dict
+      where keys are hook names/labels, and each value is a dict with
+      three keys: 'cmd' contains the name of the to-be-executed DataLad
+      command; 'args' has a JSON-encoded string with a dict of keyword
+      arguments for the command (format()-language based placeholders
+      can be present); 'match' holds a JSON-encoded string representing
+      a dict with key/value pairs that need to match a result in order
+      for a hook to be triggered.
     """
-
-    # Users should only access the lru_cache through its public API:
-    #       cache_info, cache_clear, and f.__wrapped__
-    # The internals of the lru_cache are encapsulated for thread safety and
-    # to allow the implementation to change (including a possible C version).
-
-    def decorating_function(user_function):
-
-        cache = dict()
-        stats = [0, 0]                  # make statistics updateable non-locally
-        HITS, MISSES = 0, 1             # names for the stats fields
-        make_key = _make_key
-        cache_get = cache.get           # bound method to lookup key or return None
-        _len = len                      # localize the global len() function
-        lock = RLock()                  # because linkedlist updates aren't threadsafe
-        root = []                       # root of the circular doubly linked list
-        root[:] = [root, root, None, None]      # initialize by pointing to self
-        nonlocal_root = [root]                  # make updateable non-locally
-        PREV, NEXT, KEY, RESULT = 0, 1, 2, 3    # names for the link fields
-
-        if maxsize == 0:
-
-            def wrapper(*args, **kwds):
-                # no caching, just do a statistics update after a successful call
-                result = user_function(*args, **kwds)
-                stats[MISSES] += 1
-                return result
-
-        elif maxsize is None:
-
-            def wrapper(*args, **kwds):
-                # simple caching without ordering or size limit
-                key = make_key(args, kwds, typed)
-                result = cache_get(key, root)   # root used here as a unique not-found sentinel
-                if result is not root:
-                    stats[HITS] += 1
-                    return result
-                result = user_function(*args, **kwds)
-                cache[key] = result
-                stats[MISSES] += 1
-                return result
-
+    hooks = {}
+    for h in cfg.keys():
+        if not (h.startswith('datalad.result-hook.') and h.endswith('.match-json')):
+            continue
+        hook_basevar = h[:-11]
+        hook_name = hook_basevar[20:]
+        # do not use a normal `get()` here, because it reads the committed dataset
+        # config too. That means a datalad update can silently bring in new
+        # procedure definitions from the outside, and in some sense enable
+        # remote code execution by a 3rd-party
+        call = cfg.get_from_source(
+            'local',
+            '{}.call-json'.format(hook_basevar),
+            None
+        )
+        if not call:
+            lgr.warning(
+                'Incomplete result hook configuration %s in %s',
+                hook_basevar, cfg)
+            continue
+        # split command from any args
+        call = call.split(maxsplit=1)
+        # get the match specification in JSON format
+        try:
+            match = json.loads(cfg.get(h))
+        except Exception as e:
+            ce = CapturedException(e)
+            lgr.warning(
+                'Invalid match specification in %s: %s [%s], '
+                'hook will be skipped',
+                h, cfg.get(h), ce)
+            continue
+
+        hooks[hook_name] = dict(
+            cmd=call[0],
+            # support no-arg calls too
+            args=call[1] if len(call) > 1 else '{{}}',
+            match=match,
+        )
+    return hooks
+
+
+def match_jsonhook2result(hook, res, match):
+    """Evaluate a hook's result match definition against a concrete result
+
+    A match definition is a dict that can contain any number of keys. For each
+    key it is tested, if the value matches the one in a given result.
+    If all present key/value pairs match, the hook is executed. In addition to
+    ``==`` tests, ``in``, ``not in``, and ``!=`` tests are supported. The
+    test operation can be given by wrapping the test value into a list, the
+    first item is the operation label 'eq', 'neq', 'in', 'nin'; the second value
+    is the test value (set). Example::
+
+        {
+          "type": ["in", ["file", "directory"]],
+          "action": "get",
+          "status": "notneeded"
+        }
+
+    If a to be tested value is a list, an 'eq' operation needs to be specified
+    explicitly in order to disambiguate the definition.
+
+    Parameters
+    ----------
+    hook : str
+      Name of the hook
+    res : dict
+      Result dictionary
+    match : dict
+      Match definition (see above for details).
+
+    Returns
+    -------
+    bool
+      True if the given result matches the hook's match definition, or
+      False otherwise.
+    """
+    for k, v in match.items():
+        # do not test 'k not in res', because we could have a match that
+        # wants to make sure that a particular value is not present, and
+        # not having the key would be OK in that case
+
+        # in case the target value is an actual list, an explicit action 'eq'
+        # must be given
+        action, val = (v[0], v[1]) if isinstance(v, list) else ('eq', v)
+        if action == 'eq':
+            if k in res and res[k] == val:
+                continue
+        elif action == 'neq':
+            if k not in res or res[k] != val:
+                continue
+        elif action == 'in':
+            if k in res and res[k] in val:
+                continue
+        elif action == 'nin':
+            if k not in res or res[k] not in val:
+                continue
         else:
-
-            def wrapper(*args, **kwds):
-                # size limited caching that tracks accesses by recency
-                key = make_key(args, kwds, typed) if kwds or typed else args
-                with lock:
-                    link = cache_get(key)
-                    if link is not None:
-                        # record recent use of the key by moving it to the front of the list
-                        root, = nonlocal_root
-                        link_prev, link_next, key, result = link
-                        link_prev[NEXT] = link_next
-                        link_next[PREV] = link_prev
-                        last = root[PREV]
-                        last[NEXT] = root[PREV] = link
-                        link[PREV] = last
-                        link[NEXT] = root
-                        stats[HITS] += 1
-                        return result
-                result = user_function(*args, **kwds)
-                with lock:
-                    root, = nonlocal_root
-                    if key in cache:
-                        # getting here means that this same key was added to the
-                        # cache while the lock was released.  since the link
-                        # update is already done, we need only return the
-                        # computed result and update the count of misses.
-                        pass
-                    elif _len(cache) >= maxsize:
-                        # use the old root to store the new key and result
-                        oldroot = root
-                        oldroot[KEY] = key
-                        oldroot[RESULT] = result
-                        # empty the oldest link and make it the new root
-                        root = nonlocal_root[0] = oldroot[NEXT]
-                        oldkey = root[KEY]
-                        oldvalue = root[RESULT]
-                        root[KEY] = root[RESULT] = None
-                        # now update the cache dictionary for the new links
-                        del cache[oldkey]
-                        cache[key] = oldroot
-                    else:
-                        # put result in a new link at the front of the list
-                        last = root[PREV]
-                        link = [last, root, key, result]
-                        last[NEXT] = root[PREV] = cache[key] = link
-                    stats[MISSES] += 1
-                return result
-
-        def cache_info():
-            """Report cache statistics"""
-            with lock:
-                return _CacheInfo(stats[HITS], stats[MISSES], maxsize, len(cache))
-
-        def cache_clear():
-            """Clear the cache and cache statistics"""
-            with lock:
-                cache.clear()
-                root = nonlocal_root[0]
-                root[:] = [root, root, None, None]
-                stats[:] = [0, 0]
-
-        wrapper.__wrapped__ = user_function
-        wrapper.cache_info = cache_info
-        wrapper.cache_clear = cache_clear
-        return update_wrapper(wrapper, user_function)
-
-    return decorating_function
+            lgr.warning(
+                'Unknown result comparison operation %s for hook %s, skipped',
+                action, hook)
+        # indentation level is intended!
+        return False
+    return True
+
+
+def run_jsonhook(hook, spec, res, dsarg=None):
+    """Execute a hook on a given result
+
+    A hook definition's 'call' specification may contain placeholders that
+    will be expanded using matching values in the given result record. In
+    addition to keys in the result a '{dsarg}' placeholder is supported.
+    The characters '{' and '}' in the 'call' specification that are not part
+    of format() placeholders have to be escaped as '{{' and '}}'. Example
+    'call' specification to execute the DataLad ``unlock`` command::
+
+        unlock {{"dataset": "{dsarg}", "path": "{path}"}}
+
+    Parameters
+    ----------
+    hook : str
+      Name of the hook
+    spec : dict
+      Hook definition as returned by `get_hooks_from_config()`
+    res : dict
+      Result records that were found to match the hook definition.
+    dsarg : Dataset or str or None, optional
+      Value to substitute a {dsarg} placeholder in a hook 'call' specification
+      with. Non-string values are automatically converted.
+
+    Yields
+    ------
+    dict
+      Any result yielded by the command executed as hook.
+    """
+    import datalad.api as dl
+    cmd_name = spec['cmd']
+    if not hasattr(dl, cmd_name):
+        # TODO maybe a proper error result?
+        lgr.warning(
+            'Hook %s requires unknown command %s, skipped',
+            hook, cmd_name)
+        return
+    cmd = getattr(dl, cmd_name)
+    # apply potential substitutions on the string form of the args
+    # for this particular result
+    # take care of proper JSON encoding for each value
+    enc = json.JSONEncoder().encode
+    # we have to ensure JSON encoding of all values (some might be Path instances),
+    # we are taking off the outer quoting, to enable flexible combination
+    # of individual items in supplied command and argument templates
+    args = spec['args'].format(
+        # we cannot use a dataset instance directly but must take the
+        # detour over the path location in order to have string substitution
+        # be possible
+        dsarg='' if dsarg is None else enc(dsarg.path).strip('"')
+        if isinstance(dsarg, dl.Dataset) else enc(dsarg).strip('"'),
+        # skip any present logger that we only carry for internal purposes
+        **{k: enc(str(v)).strip('"') for k, v in res.items() if k != 'logger'})
+    # now load
+    try:
+        args = json.loads(args)
+    except Exception as e:
+        ce = CapturedException(e)
+        lgr.warning(
+            'Invalid argument specification for hook %s '
+            '(after parameter substitutions): %s [%s], '
+            'hook will be skipped',
+            hook, args, ce)
+        return
+    # only debug level, the hook can issue its own results and communicate
+    # through them
+    lgr.debug('Running hook %s: %s%s', hook, cmd_name, args)
+    for r in cmd(**args):
+        yield r
```

### Comparing `datalad-0.9.3/datalad/support/constraints.py` & `datalad-1.0.0/datalad/support/constraints.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,41 +1,53 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 """Helper for parameter validation, documentation and conversion"""
 
 __docformat__ = 'restructuredtext'
 
 import re
-from six.moves import map as map
 
 
 def _strip_typerepr(s):
     """Strip away <class '...'> and <type '...'> decorations for docstrings
     """
-    return re.sub("<(class|type) '(\S+)'>", r'\2', s)
+    return re.sub(r"<(class|type) '(\S+)'>", r'\2', s)
 
 
 def _type_str(t):
     """Get string human-readable representation of a data type
 
     If type (t) is given as a tuple, assume ability to choose any of the
     listed types, so those types listing get joined with |
     """
     if isinstance(t, tuple):
         s = ' or '.join(map(_type_str, t))
         return ("(%s)" % s) if len(t) > 1 else s
     return _strip_typerepr(str(t))
 
 
+class _NoneDeprecated:
+    """A helper construct to ease migrations for option=None.
+
+    Should be instantiated, so it gains `__repr__` and thus would render nicely
+    in docs etc.
+    """
+    def __repr__(self):
+        return 'None(DEPRECATED)'
+
+
+NoneDeprecated = _NoneDeprecated()
+
+
 class Constraint(object):
     """Base class for input value conversion/validation.
 
     These classes are also meant to be able to generate appropriate
     documentation on an appropriate parameter value.
     """
 
@@ -77,17 +89,16 @@
         Parameters
         ----------
         dtype : functor
         """
         self._dtype = dtype
 
     def __call__(self, value):
-        from six import binary_type, text_type
         if hasattr(value, '__iter__') and \
-                not (isinstance(value, (binary_type, text_type))):
+                not (isinstance(value, (bytes, str))):
             return list(map(self._dtype, value))
         else:
             return self._dtype(value)
 
     def short_description(self):
         return _type_str(self._dtype)
 
@@ -120,15 +131,15 @@
         ----------
         dtype : functor
         """
         self._dtype = dtype
         super(EnsureListOf, self).__init__()
 
     def __call__(self, value):
-        return list(map(self._dtype, value))
+        return list(map(self._dtype, ([value] if isinstance(value, str) else value)))
 
     def short_description(self):
         return 'list(%s)' % _type_str(self._dtype)
 
     def long_description(self):
         return "value must be convertible to %s" % self.short_description()
 
@@ -142,15 +153,15 @@
         ----------
         dtype : functor
         """
         self._dtype = dtype
         super(EnsureTupleOf, self).__init__()
 
     def __call__(self, value):
-        return tuple(map(self._dtype, value))
+        return tuple(map(self._dtype, ([value] if isinstance(value, str) else value)))
 
     def short_description(self):
         return 'tuple(%s)' % _type_str(self._dtype)
 
     def long_description(self):
         return "value must be convertible to %s" % self.short_description()
 
@@ -159,18 +170,17 @@
     """Ensure that an input is a bool.
 
     A couple of literal labels are supported, such as:
     False: '0', 'no', 'off', 'disable', 'false'
     True: '1', 'yes', 'on', 'enable', 'true'
     """
     def __call__(self, value):
-        from six import binary_type, text_type
         if isinstance(value, bool):
             return value
-        elif isinstance(value, (binary_type, text_type)):
+        elif isinstance(value, (bytes, str)):
             value = value.lower()
             if value in ('0', 'no', 'off', 'disable', 'false'):
                 return False
             elif value in ('1', 'yes', 'on', 'enable', 'true'):
                 return True
         raise ValueError(
             "value '{}' must be convertible to boolean".format(
@@ -196,16 +206,15 @@
            Minimal length for a string.
         """
         assert(min_len >= 0)
         self._min_len = min_len
         super(EnsureStr, self).__init__()
 
     def __call__(self, value):
-        from six import binary_type, text_type
-        if not isinstance(value, (binary_type, text_type)):
+        if not isinstance(value, (bytes, str)):
             # do not perform a blind conversion ala str(), as almost
             # anything can be converted and the result is most likely
             # unintended
             raise ValueError("%s is not a string" % repr(value))
         if len(value) < self._min_len:
             raise ValueError("%r is shorter than of minimal length %d"
                              % (value, self._min_len))
@@ -214,31 +223,58 @@
     def long_description(self):
         return 'value must be a string'
 
     def short_description(self):
         return 'str'
 
 
+class EnsureStrPrefix(EnsureStr):
+    """Ensure an input is a string that starts with a given prefix.
+    """
+    def __init__(self, prefix):
+        """
+        Parameters
+        ----------
+        prefix : str
+           Mandatory prefix.
+        """
+        self._prefix = prefix
+        super().__init__()
+
+    def __call__(self, value):
+        super().__call__(value)
+        if not value.startswith(self._prefix):
+            raise ValueError("%r does not start with '%s'"
+                             % (value, self._prefix))
+        return value
+
+    def long_description(self):
+        return "value must start with '{}'".format(self._prefix)
+
+    def short_description(self):
+        return '{}...'.format(self._prefix)
+
+
 class EnsureNone(Constraint):
     """Ensure an input is of value `None`"""
     def __call__(self, value):
-        if value is None:
+        if value is None or isinstance(value, _NoneDeprecated):
             return None
         else:
             raise ValueError("value must be `None`")
 
     def short_description(self):
         return 'None'
 
     def long_description(self):
         return 'value must be `None`'
 
 
 class EnsureCallable(Constraint):
-    """Ensure an input is of value `None`"""
+    """Ensure an input is a callable object"""
     def __call__(self, value):
         if hasattr(value, '__call__'):
             return value
         else:
             raise ValueError("value must be a callable")
 
     def short_description(self):
@@ -259,19 +295,22 @@
            Possible accepted values.
         """
         self._allowed = values
         super(EnsureChoice, self).__init__()
 
     def __call__(self, value):
         if value not in self._allowed:
-            raise ValueError("value is not one of %s" % (self._allowed,))
+            raise ValueError(f"value {value} is not one of {self._allowed}")
         return value
 
     def long_description(self):
-        return 'value must be one of %s' % (str(self._allowed),)
+        return 'value must be one of [CMD: %s CMD][PY: %s PY]' % (
+            str(tuple(i for i in self._allowed if i is not None)),
+            str(self._allowed)
+        )
 
     def short_description(self):
         return '{%s}' % ', '.join([repr(c) for c in self._allowed])
 
 
 class EnsureKeyChoice(EnsureChoice):
     """Ensure value under a key in an input is in a set of possible values"""
@@ -334,15 +373,39 @@
         max_str = 'inf' if self._max is None else str(self._max)
         return 'value must be in range [%s, %s]' % (min_str, max_str)
 
     def short_description(self):
         return None
 
 
-class AltConstraints(Constraint):
+class _MultiConstraint(Constraint):
+    """Helper class to override the description methods to reported
+    multiple constraints
+    """
+    def _get_description(self, attr):
+        cs = [
+            getattr(c, attr)()
+            for c in self.constraints
+            if hasattr(c, attr)
+        ]
+        cs = [c for c in cs if c is not None]
+        doc = ' or '.join(cs)
+        if len(cs) > 1:
+            return f'({doc})'
+        else:
+            return doc
+
+    def long_description(self):
+        return self._get_description('long_description')
+
+    def short_description(self):
+        return self._get_description('short_description')
+
+
+class AltConstraints(_MultiConstraint):
     """Logical OR for constraints.
 
     An arbitrary number of constraints can be given. They are evaluated in the
     order in which they were specified. The value returned by the first
     constraint that does not raise an exception is the global return value.
 
     Documentation is aggregated for all alternative constraints.
@@ -367,36 +430,19 @@
     def __call__(self, value):
         e_list = []
         for c in self.constraints:
             try:
                 return c(value)
             except Exception as e:
                 e_list.append(e)
-        raise ValueError("all alternative constraints (%s) violated while testing value %r"
-                         % (self.constraints, value))
-
-    def long_description(self):
-        cs = [c.long_description() for c in self.constraints if hasattr(c, 'long_description')]
-        doc = ', or '.join(cs)
-        if len(cs) > 1:
-            return '(%s)' % doc
-        else:
-            return doc
-
-    def short_description(self):
-        cs = [c.short_description() for c in self.constraints
-              if hasattr(c, 'short_description') and not c.short_description() is None]
-        doc = ' or '.join(cs)
-        if len(cs) > 1:
-            return '(%s)' % doc
-        else:
-            return doc
+        raise ValueError(f"{value} does not match any alternative: "
+                         f"{self.constraints} {e_list}")
 
 
-class Constraints(Constraint):
+class Constraints(_MultiConstraint):
     """Logical AND for constraints.
 
     An arbitrary number of constraints can be given. They are evaluated in the
     order in which they were specified. The return value of each constraint is
     passed an input into the next. The return value of the last constraint
     is the global return value. No intermediate exceptions are caught.
 
@@ -420,30 +466,14 @@
         return self
 
     def __call__(self, value):
         for c in (self.constraints):
             value = c(value)
         return value
 
-    def long_description(self):
-        cs = [c.long_description() for c in self.constraints if hasattr(c, 'long_description')]
-        doc = ', and '.join(cs)
-        if len(cs) > 1:
-            return '(%s)' % doc
-        else:
-            return doc
-
-    def short_description(self):
-        cs = [c.short_description() for c in self.constraints
-              if hasattr(c, 'short_description') and not c.short_description() is None]
-        doc = ' and '.join(cs)
-        if len(cs) > 1:
-            return '(%s)' % doc
-        else:
-            return doc
 
 constraint_spec_map = {
     'float': EnsureFloat(),
     'int': EnsureInt(),
     'bool': EnsureBool(),
     'str': EnsureStr(),
 }
@@ -456,8 +486,8 @@
     """
     if spec is None or hasattr(spec, '__call__'):
         return spec
     else:
         try:
             return constraint_spec_map[spec]
         except KeyError:
-            raise ValueError("unsupport constraint specification '%r'" % (spec,))
+            raise ValueError("unsupported constraint specification '%r'" % (spec,))
```

### Comparing `datalad-0.9.3/datalad/support/gitrepo.py` & `datalad-1.0.0/datalad/utils.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,2255 +1,2845 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""Interface to Git via GitPython
 
-For further information on GitPython see http://gitpython.readthedocs.org/
-
-"""
+from __future__ import annotations
 
+import builtins
+import collections
+import gc
+import glob
+import gzip
+import inspect
 import logging
-import re
-import shlex
 import os
-from os import linesep
-from os.path import join as opj
-from os.path import exists
-from os.path import normpath
-from os.path import isabs
-from os.path import commonprefix
-from os.path import relpath
-from os.path import realpath
-from os.path import dirname
-from os.path import basename
-from os.path import curdir
-from os.path import pardir
-from os.path import sep
+import os.path as op
+import platform
 import posixpath
-from weakref import WeakValueDictionary
+import re
+import shutil
+import stat
+import string
+import sys
+import tempfile
+import threading
+import time
+import warnings
+from collections.abc import (
+    Callable,
+    Iterable,
+    Iterator,
+    Sequence,
+)
+from contextlib import contextmanager
+from copy import copy as shallow_copy
+from functools import (
+    lru_cache,
+    wraps,
+)
+from itertools import tee
+# this import is required because other modules import opj from here.
+from os.path import (
+    abspath,
+    basename,
+    commonprefix,
+    curdir,
+    dirname,
+    exists,
+    expanduser,
+    expandvars,
+    isabs,
+    isdir,
+    islink,
+)
+from os.path import join as opj
+from os.path import (
+    lexists,
+    normpath,
+    pardir,
+    relpath,
+    sep,
+    split,
+    splitdrive,
+)
+from pathlib import (
+    Path,
+    PurePath,
+    PurePosixPath,
+)
+from shlex import quote as shlex_quote
+from shlex import split as shlex_split
+from tempfile import NamedTemporaryFile
+from time import sleep
+from types import (
+    ModuleType,
+    TracebackType,
+)
+from typing import (
+    IO,
+    Any,
+    Dict,
+    List,
+    NamedTuple,
+    Optional,
+    TextIO,
+    Tuple,
+    TypeVar,
+    Union,
+    cast,
+    overload,
+)
+
+# from datalad.dochelpers import get_docstring_split
+from datalad.consts import TIMESTAMP_FMT
+from datalad.support.exceptions import CapturedException
+from datalad.typing import (
+    K,
+    Literal,
+    P,
+    T,
+    V,
+)
+
+# handle this dance once, and import pathlib from here
+# in all other places
+
+lgr = logging.getLogger("datalad.utils")
+
+lgr.log(5, "Importing datalad.utils")
+#
+# Some useful variables
+#
+platform_system = platform.system().lower()
+on_windows = platform_system == 'windows'
+on_osx = platform_system == 'darwin'
+on_linux = platform_system == 'linux'
+
+# COPY_BUFSIZE sort of belongs into datalad.consts, but that would lead to
+# circular import due to `on_windows`
+try:
+    from shutil import COPY_BUFSIZE  # type: ignore[attr-defined]
+except ImportError:  # pragma: no cover
+    # too old
+    from datalad.utils import on_windows
+
+    # from PY3.10
+    COPY_BUFSIZE = 1024 * 1024 if on_windows else 64 * 1024
+
+
+# Takes ~200msec, so should not be called at import time
+@lru_cache()  # output should not change through life time of datalad process
+def get_linux_distribution() -> tuple[str, str, str]:
+    """Compatibility wrapper for {platform,distro}.linux_distribution().
+    """
+    if hasattr(platform, "linux_distribution"):
+        # Use deprecated (but faster) method if it's available.
+        with warnings.catch_warnings():
+            warnings.filterwarnings("ignore", category=DeprecationWarning)
+            result = platform.linux_distribution()
+    else:
+        import distro  # We require this for Python 3.8 and above.
+        return (
+            distro.id(),
+            distro.version(),
+            distro.codename(),
+        )
+    return result
 
 
-from six import string_types
-from six import add_metaclass
-from functools import wraps
-import git as gitpy
-from git.exc import GitCommandError
-from git.exc import NoSuchPathError
-from git.exc import InvalidGitRepositoryError
-from git.objects.blob import Blob
-
-from datalad import ssh_manager
-from datalad.cmd import GitRunner
-from datalad.consts import GIT_SSH_COMMAND
-from datalad.dochelpers import exc_str
-from datalad.config import ConfigManager
-from datalad.utils import assure_list
-from datalad.utils import optional_args
-from datalad.utils import on_windows
-from datalad.utils import getpwd
-from datalad.utils import updated
-from datalad.utils import posix_relpath
-
-
-# imports from same module:
-from .external_versions import external_versions
-from .exceptions import CommandError
-from .exceptions import DeprecatedError
-from .exceptions import FileNotInRepositoryError
-from .exceptions import MissingBranchError
-from .network import RI, PathRI
-from .network import is_ssh
-from .repo import Flyweight
-from .repo import RepoInterface
-
-# shortcuts
-_curdirsep = curdir + sep
-_pardirsep = pardir + sep
-
-
-lgr = logging.getLogger('datalad.gitrepo')
-
-# Override default GitPython's DB backend to talk directly to git so it doesn't
-# interfere with possible operations performed by gc/repack
-default_git_odbt = gitpy.GitCmdObjectDB
-
-# TODO: Figure out how GIT_PYTHON_TRACE ('full') is supposed to be used.
-# Didn't work as expected on a first try. Probably there is a neatier way to
-# log Exceptions from git commands.
+# Those weren't used for any critical decision making, thus we just set them to None
+# Use get_linux_distribution() directly where needed
+linux_distribution_name = linux_distribution_release = None
+
+# Maximal length of cmdline string
+# Query the system and use hardcoded "knowledge" if None
+# probably   getconf ARG_MAX   might not be available
+# The last one would be the most conservative/Windows
+CMD_MAX_ARG_HARDCODED = 2097152 if on_linux else 262144 if on_osx else 32767
+try:
+    CMD_MAX_ARG = os.sysconf('SC_ARG_MAX')
+    assert CMD_MAX_ARG > 0
+    if CMD_MAX_ARG > CMD_MAX_ARG_HARDCODED * 1e6:
+        # workaround for some kind of a bug which comes up with python 3.4
+        # see https://github.com/datalad/datalad/issues/3150
+        # or on older CentOS with conda and python as new as 3.9
+        # see https://github.com/datalad/datalad/issues/5943
+        # TODO: let Yarik know that the world is a paradise now whenever 1e6
+        # is not large enough
+        CMD_MAX_ARG = min(CMD_MAX_ARG, CMD_MAX_ARG_HARDCODED)
+except Exception as exc:
+    # ATM (20181005) SC_ARG_MAX available only on POSIX systems
+    # so exception would be thrown e.g. on Windows, or
+    # somehow during Debian build for nd14.04 it is coming up with -1:
+    # https://github.com/datalad/datalad/issues/3015
+    CMD_MAX_ARG = CMD_MAX_ARG_HARDCODED
+    lgr.debug(
+        "Failed to query or got useless SC_ARG_MAX sysconf, "
+        "will use hardcoded value: %s", exc)
+# Even with all careful computations we do, due to necessity to account for
+# environment and what not, we still could not figure out "exact" way to
+# estimate it, but it was shown that 300k safety margin on linux was sufficient.
+# https://github.com/datalad/datalad/pull/2977#issuecomment-436264710
+# 300k is ~15%, so to be safe, and for paranoid us we will just use up to 50%
+# of the length for "safety margin".  We might probably still blow due to
+# env vars, unicode, etc...  so any hard limit imho is not a proper solution
+CMD_MAX_ARG = int(0.5 * CMD_MAX_ARG)
+lgr.debug(
+    "Maximal length of cmdline string (adjusted for safety margin): %d",
+    CMD_MAX_ARG)
 
+#
+# Little helpers
+#
 
-# TODO: ignore leading and/or trailing underscore to allow for
-# python-reserved words
-@optional_args
-def kwargs_to_options(func, split_single_char_options=True,
-                      target_kw='options'):
-    """Decorator to provide convenient way to pass options to command calls.
+# `getargspec` has been deprecated in Python 3.
+class ArgSpecFake(NamedTuple):
+    args: list[str]
+    varargs: Optional[str]
+    keywords: Optional[str]
+    defaults: Optional[tuple[Any, ...]]
+
+
+# adding cache here somehow does break it -- even 'datalad wtf' does not run
+# @lru_cache()  # signatures stay the same, why to "redo"? brings it into ns from mks
+def getargspec(func: Callable[..., Any], *, include_kwonlyargs: bool=False) -> ArgSpecFake:
+    """Compat shim for getargspec deprecated in python 3.
+
+    The main difference from inspect.getargspec (and inspect.getfullargspec
+    for that matter) is that by using inspect.signature we are providing
+    correct args/defaults for functools.wraps'ed functions.
+
+    `include_kwonlyargs` option was added to centralize getting all args,
+    even the ones which are kwonly (follow the ``*,``).
+
+    For internal use and not advised for use in 3rd party code.
+    Please use inspect.signature directly.
+    """
+    # We use signature, and not getfullargspec, because only signature properly
+    # "passes" args from a functools.wraps decorated function.
+    # Note: getfullargspec works Ok on wrapt-decorated functions
+    f_sign = inspect.signature(func)
+    # Loop through parameters and compose argspec
+    args: list[str] = []
+    varargs: Optional[str] = None
+    keywords: Optional[str] = None
+    defaults: dict[str, Any] = {}
+    # Collect all kwonlyargs into a dedicated dict - name: default
+    kwonlyargs: dict[str, Any] = {}
+    P = inspect.Parameter
+
+    for p_name, p in f_sign.parameters.items():
+        if p.kind in (P.POSITIONAL_ONLY, P.POSITIONAL_OR_KEYWORD):
+            assert not kwonlyargs  # yoh: must not come after kwonlyarg
+            args.append(p_name)
+            if p.default is not P.empty:
+                defaults[p_name] = p.default
+        elif p.kind == P.VAR_POSITIONAL:
+            varargs = p_name
+        elif p.kind == P.VAR_KEYWORD:
+            keywords = p_name
+        elif p.kind == P.KEYWORD_ONLY:
+            assert p.default is not P.empty
+            kwonlyargs[p_name] = p.default
+
+    if kwonlyargs:
+        if not include_kwonlyargs:
+            raise ValueError(
+                'Function has keyword-only parameters or annotations, either use '
+                'inspect.signature() API which can support them, or provide include_kwonlyargs=True '
+                'to this function'
+            )
+        else:
+            args.extend(list(kwonlyargs))
+            defaults.update(kwonlyargs)
+
+    # harmonize defaults to how original getargspec returned them -- just a tuple
+    d_defaults = None if not defaults else tuple(defaults.values())
+    return ArgSpecFake(args, varargs, keywords, d_defaults)
+
+
+# Definitions to be (re)used in the next function
+_SIG_P = inspect.Parameter
+_SIG_KIND_SELECTORS: dict[str, set[int]] = {
+    'pos_only': {_SIG_P.POSITIONAL_ONLY,},
+    'pos_any': {_SIG_P.POSITIONAL_ONLY, _SIG_P.POSITIONAL_OR_KEYWORD},
+    'kw_any': {_SIG_P.POSITIONAL_OR_KEYWORD, _SIG_P.KEYWORD_ONLY},
+    'kw_only': {_SIG_P.KEYWORD_ONLY,},
+}
+_SIG_KIND_SELECTORS['any'] = set().union(*_SIG_KIND_SELECTORS.values())
+
+
+@lru_cache()  # signatures stay the same, why to "redo"? brings it into ns from mks
+def get_sig_param_names(f: Callable[..., Any], kinds: tuple[str, ...]) -> tuple[list[str], ...]:
+    """A helper to selectively return parameters from inspect.signature.
+
+    inspect.signature is the ultimate way for introspecting callables.  But
+    its interface is not so convenient for a quick selection of parameters
+    (AKA arguments) of desired type or combinations of such.  This helper
+    should make it easier to retrieve desired collections of parameters.
+
+    Since often it is desired to get information about multiple specific types
+    of parameters, `kinds` is a list, so in a single invocation of `signature`
+    and looping through the results we can obtain all information.
 
     Parameters
     ----------
-    func: Callable
-        function to decorate
-    split_single_char_options: bool
-        whether or not to split key and value of single char keyword arguments
-        into two subsequent entries of the list
-    target_kw: str
-        keyword argument to pass the generated list of cmdline arguments to
+    f: callable
+    kinds: tuple with values from {'pos_any', 'pos_only', 'kw_any', 'kw_only', 'any'}
+      Is a list of what kinds of args to return in result (tuple). Each element
+      should be one of: 'any_pos' - positional or keyword which could be used
+      positionally. 'kw_only' - keyword only (cannot be used positionally) arguments,
+      'any_kw` - any keyword (could be a positional which could be used as a keyword),
+      `any` -- any type from the above.
 
     Returns
     -------
-    Callable
+    tuple:
+      Each element is a list of parameters (names only) of that "kind".
+    """
+    selectors: list[set[int]] = []
+    for kind in kinds:
+        if kind not in _SIG_KIND_SELECTORS:
+            raise ValueError(f"Unknown 'kind' {kind}. Known are: {', '.join(_SIG_KIND_SELECTORS)}")
+        selectors.append(_SIG_KIND_SELECTORS[kind])
+
+    out: list[list[str]] = [[] for _ in kinds]
+    for p_name, p in inspect.signature(f).parameters.items():
+        for i, selector in enumerate(selectors):
+            if p.kind in selector:
+                out[i].append(p_name)
+
+    return tuple(out)
+
+
+def any_re_search(regexes: str | list[str], value: str) -> bool:
+    """Return if any of regexes (list or str) searches successfully for value"""
+    for regex in ensure_tuple_or_list(regexes):
+        if re.search(regex, value):
+            return True
+    return False
+
+
+def not_supported_on_windows(msg: Optional[str]=None) -> None:
+    """A little helper to be invoked to consistently fail whenever functionality is
+    not supported (yet) on Windows
     """
+    if on_windows:
+        raise NotImplementedError("This functionality is not yet implemented for Windows OS"
+                                  + (": %s" % msg if msg else ""))
 
-    # TODO: don't overwrite options, but join
 
-    @wraps(func)
-    def newfunc(self, *args, **kwargs):
-        t_kwargs = dict()
-        t_kwargs[target_kw] = \
-            gitpy.Git().transform_kwargs(
-                split_single_char_options=split_single_char_options,
-                **kwargs)
-        return func(self, *args, **t_kwargs)
-    return newfunc
+def get_home_envvars(new_home: str | Path) -> dict[str, str]:
+    """Return dict with env variables to be adjusted for a new HOME
+
+    Only variables found in current os.environ are adjusted.
+
+    Parameters
+    ----------
+    new_home: str or Path
+      New home path, in native to OS "schema"
+    """
+    new_home = str(new_home)
+    out = {'HOME': new_home}
+    if on_windows:
+        # requires special handling, since it has a number of relevant variables
+        # and also Python changed its behavior and started to respect USERPROFILE only
+        # since python 3.8: https://bugs.python.org/issue36264
+        out['USERPROFILE'] = new_home
+        out['HOMEDRIVE'], out['HOMEPATH'] = splitdrive(new_home)
+
+    return {v: val for v, val in out.items() if v in os.environ}
+
+
+def _is_stream_tty(stream: Optional[IO]) -> bool:
+    try:
+        # TODO: check on windows if hasattr check would work correctly and
+        # add value:
+        return stream is not None and stream.isatty()
+    except ValueError as exc:
+        # Who knows why it is a ValueError, but let's try to be specific
+        # If there is a problem with I/O - non-interactive, otherwise reraise
+        if "I/O" in str(exc):
+            return False
+        raise
+
+
+def is_interactive() -> bool:
+    """Return True if all in/outs are open and tty.
+
+    Note that in a somewhat abnormal case where e.g. stdin is explicitly
+    closed, and any operation on it would raise a
+    `ValueError("I/O operation on closed file")` exception, this function
+    would just return False, since the session cannot be used interactively.
+    """
+    return all(_is_stream_tty(s) for s in (sys.stdin, sys.stdout, sys.stderr))
+
+
+def get_ipython_shell() -> Optional[Any]:
+    """Detect if running within IPython and returns its `ip` (shell) object
+
+    Returns None if not under ipython (no `get_ipython` function)
+    """
+    try:
+        return get_ipython()  # type: ignore[name-defined]
+    except NameError:
+        return None
+
 
+def md5sum(filename: str | Path) -> str:
+    """Compute an MD5 sum for the given file
+    """
+    from datalad.support.digests import Digester
+    return Digester(digests=['md5'])(filename)['md5']
+
+
+_encoded_dirsep = r'\\'  if on_windows else r'/'
+_VCS_REGEX = r'%s\.(?:git|gitattributes|svn|bzr|hg)(?:%s|$)' % (
+    _encoded_dirsep, _encoded_dirsep)
+_DATALAD_REGEX = r'%s\.(?:datalad)(?:%s|$)' % (
+    _encoded_dirsep, _encoded_dirsep)
 
-def to_options(**kwargs):
-    """Transform keyword arguments into a list of cmdline options
+
+def find_files(regex: str, topdir: str | Path = curdir, exclude: Optional[str]=None, exclude_vcs: bool =True, exclude_datalad: bool =False, dirs: bool =False) -> Iterator[str]:
+    """Generator to find files matching regex
 
     Parameters
     ----------
-    split_single_char_options: bool
+    regex: string
+    exclude: string, optional
+      Matches to exclude
+    exclude_vcs:
+      If True, excludes commonly known VCS subdirectories.  If string, used
+      as regex to exclude those files (regex: `%r`)
+    exclude_datalad:
+      If True, excludes files known to be datalad meta-data files (e.g. under
+      .datalad/ subdirectory) (regex: `%r`)
+    topdir: string, optional
+      Directory where to search
+    dirs: bool, optional
+      Whether to match directories as well as files
+    """
+    for dirpath, dirnames, filenames in os.walk(topdir):
+        names = (dirnames + filenames) if dirs else filenames
+        # TODO: might want to uniformize on windows to use '/'
+        paths = (op.join(dirpath, name) for name in names)
+        for path in filter(re.compile(regex).search, paths):
+            path = path.rstrip(sep)
+            if exclude and re.search(exclude, path):
+                continue
+            if exclude_vcs and re.search(_VCS_REGEX, path):
+                continue
+            if exclude_datalad and re.search(_DATALAD_REGEX, path):
+                continue
+            yield path
+find_files.__doc__ %= (_VCS_REGEX, _DATALAD_REGEX)  # type: ignore[operator]
 
-    kwargs:
 
-    Returns
-    -------
-    list
+def expandpath(path: str | Path, force_absolute: bool =True) -> str:
+    """Expand all variables and user handles in a path.
+
+    By default return an absolute path
     """
-    # TODO: borrow_docs!
+    path = expandvars(expanduser(path))
+    if force_absolute:
+        path = abspath(path)
+    return path
+
+
+def posix_relpath(path: str | Path, start: Optional[str | Path]=None) -> str:
+    """Behave like os.path.relpath, but always return POSIX paths...
+
+    on any platform."""
+    # join POSIX style
+    return posixpath.join(
+        # split and relpath native style
+        # python2.7 ntpath implementation of relpath cannot handle start=None
+        *split(
+            relpath(path, start=start if start is not None else '')))
+
+
+def is_explicit_path(path: str | Path) -> bool:
+    """Return whether a path explicitly points to a location
+
+    Any absolute path, or relative path starting with either '../' or
+    './' is assumed to indicate a location on the filesystem. Any other
+    path format is not considered explicit."""
+    path = expandpath(path, force_absolute=False)
+    return isabs(path) \
+        or path.startswith(os.curdir + os.sep) \
+        or path.startswith(os.pardir + os.sep)
+
 
-    return gitpy.Git().transform_kwargs(**kwargs)
+def rotree(path: str | Path, ro: bool =True, chmod_files: bool =True) -> None:
+    """To make tree read-only or writable
 
+    Parameters
+    ----------
+    path : string
+      Path to the tree/directory to chmod
+    ro : bool, optional
+      Whether to make it R/O (default) or RW
+    chmod_files : bool, optional
+      Whether to operate also on files (not just directories)
+    """
+    if ro:
+        chmod = lambda f: os.chmod(f, os.stat(f).st_mode & ~stat.S_IWRITE)
+    else:
+        chmod = lambda f: os.chmod(f, os.stat(f).st_mode | stat.S_IWRITE | stat.S_IREAD)
+
+    for root, dirs, files in os.walk(path, followlinks=False):
+        if chmod_files:
+            for f in files:
+                fullf = op.join(root, f)
+                # might be the "broken" symlink which would fail to stat etc
+                if exists(fullf):
+                    chmod(fullf)
+        chmod(root)
 
-def _normalize_path(base_dir, path):
-    """Helper to check paths passed to methods of this class.
 
-    Checks whether `path` is beneath `base_dir` and normalizes it.
-    Additionally paths are converted into relative paths with respect to
-    `base_dir`, considering PWD in case of relative paths. This
-    is intended to be used in repository classes, which means that
-    `base_dir` usually will be the repository's base directory.
+def rmtree(path: str | Path, chmod_files: bool | Literal["auto"] ='auto', children_only: bool =False, *args: Any, **kwargs: Any) -> None:
+    """To remove git-annex .git it is needed to make all files and directories writable again first
 
     Parameters
     ----------
-    path: str
-        path to be normalized
-    base_dir: str
-        directory to serve as base to normalized, relative paths
+    path: Path or str
+       Path to remove
+    chmod_files : string or bool, optional
+       Whether to make files writable also before removal.  Usually it is just
+       a matter of directories to have write permissions.
+       If 'auto' it would chmod files on windows by default
+    children_only : bool, optional
+       If set, all files and subdirectories would be removed while the path
+       itself (must be a directory) would be preserved
+    `*args` :
+    `**kwargs` :
+       Passed into shutil.rmtree call
+    """
+    # Give W permissions back only to directories, no need to bother with files
+    if chmod_files == 'auto':
+        chmod_files = on_windows
+    # TODO:  yoh thinks that if we could quickly check our Flyweight for
+    #        repos if any of them is under the path, and could call .precommit
+    #        on those to possibly stop batched processes etc, we did not have
+    #        to do it on case by case
+    # Check for open files
+    assert_no_open_files(path)
+
+    # TODO the whole thing should be reimplemented with pathlib, but for now
+    # at least accept Path
+    path = str(path)
+
+    if children_only:
+        if not isdir(path):
+            raise ValueError("Can remove children only of directories")
+        for p in os.listdir(path):
+            rmtree(op.join(path, p))
+        return
+    if not (islink(path) or not isdir(path)):
+        rotree(path, ro=False, chmod_files=chmod_files)
+        if on_windows:
+            # shutil fails to remove paths that exceed 260 characters on Windows machines
+            # that did not enable long path support. A workaround to remove long paths
+            # anyway is to prepend \\?\ to the path.
+            # https://docs.microsoft.com/en-us/windows/win32/fileio/naming-a-file?redirectedfrom=MSDN#win32-file-namespaces
+            path = r'\\?\ '.strip() + path
+        _rmtree(path, *args, **kwargs)
+    else:
+        # just remove the symlink
+        unlink(path)
+
+
+def rmdir(path: str | Path, *args: Any, **kwargs: Any) -> None:
+    """os.rmdir with our optional checking for open files"""
+    assert_no_open_files(path)
+    os.rmdir(path)
+
+
+def get_open_files(path: str | Path, log_open: int = False) -> dict[str, Any]:
+    """Get open files under a path
+
+    Note: This function is very slow on Windows.
+
+    Parameters
+    ----------
+    path : str
+      File or directory to check for open files under
+    log_open : bool or int
+      If set - logger level to use
 
     Returns
     -------
-    str:
-        path, that is a relative path with respect to `base_dir`
+    dict
+      path : pid
+
     """
-    if not path:
-        return path
+    # Original idea: https://stackoverflow.com/a/11115521/1265472
+    import psutil
+    files = {}
+    # since the ones returned by psutil would not be aware of symlinks in the
+    # path we should also get realpath for path
+    # do absolute() in addition to always get an absolute path
+    # even with non-existing paths on windows
+    path = str(Path(path).resolve().absolute())
+    for proc in psutil.process_iter():
+        try:
+            open_paths = [p.path for p in proc.open_files()] + [proc.cwd()]
+            for p in open_paths:
+                # note: could be done more efficiently so we do not
+                # renormalize path over and over again etc
+                if path_startswith(p, path):
+                    files[p] = proc
+        # Catch a race condition where a process ends
+        # before we can examine its files
+        except psutil.NoSuchProcess:
+            pass
+        except psutil.AccessDenied:
+            pass
+
+    if files and log_open:
+        lgr.log(log_open, "Open files under %s: %s", path, files)
+    return files
+
+
+_assert_no_open_files_cfg = os.environ.get('DATALAD_ASSERT_NO_OPEN_FILES')
+if _assert_no_open_files_cfg:
+    def assert_no_open_files(path: str | Path) -> None:
+        files = get_open_files(path, log_open=40)
+        if _assert_no_open_files_cfg == 'assert':
+            assert not files, "Got following files still open: %s" % ','.join(files)
+        elif files:
+            if _assert_no_open_files_cfg == 'pdb':
+                import pdb
+                pdb.set_trace()
+            elif _assert_no_open_files_cfg == 'epdb':
+                import epdb  # type: ignore[import]
+                epdb.serve()
+            pass
+        # otherwise we would just issue that error message in the log
+else:
+    def assert_no_open_files(path: str | Path) -> None:
+        pass
+
 
-    base_dir = realpath(base_dir)  # realpath OK
-    # path = normpath(path)
-    # Note: disabled normpath, because it may break paths containing symlinks;
-    # But we don't want to realpath relative paths, in case cwd isn't the
-    # correct base.
-
-    if isabs(path):
-        # path might already be a symlink pointing to annex etc,
-        # so realpath only its directory, to get "inline" with
-        # realpath(base_dir) above
-        path = opj(realpath(dirname(path)), basename(path))  # realpath OK
-    # Executive decision was made to not do this kind of magic!
-    #
-    # elif commonprefix([realpath(getpwd()), base_dir]) == base_dir:
-    #     # If we are inside repository, rebuilt relative paths.
-    #     path = opj(realpath(getpwd()), path)
-    #
-    # BUT with relative curdir/pardir start it would assume relative to curdir
-    #
-    elif path.startswith(_curdirsep) or path.startswith(_pardirsep):
-        path = normpath(opj(realpath(getpwd()), path))  # realpath OK
+def rmtemp(f: str | Path, *args: Any, **kwargs: Any) -> None:
+    """Wrapper to centralize removing of temp files so we could keep them around
+
+    It will not remove the temporary file/directory if DATALAD_TESTS_TEMP_KEEP
+    environment variable is defined
+    """
+    if not os.environ.get('DATALAD_TESTS_TEMP_KEEP'):
+        if not os.path.lexists(f):
+            lgr.debug("Path %s does not exist, so can't be removed", f)
+            return
+        lgr.log(5, "Removing temp file: %s", f)
+        # Can also be a directory
+        if isdir(f):
+            rmtree(f, *args, **kwargs)
+        else:
+            unlink(f)
     else:
-        # We were called from outside the repo. Therefore relative paths
-        # are interpreted as being relative to self.path already.
-        return path
+        lgr.info("Keeping temp file: %s", f)
 
-    if commonprefix([path, base_dir]) != base_dir:
-        raise FileNotInRepositoryError(msg="Path outside repository: %s"
-                                           % path, filename=path)
 
-    return relpath(path, start=base_dir)
+@overload
+def file_basename(name: str | Path, return_ext: Literal[True]) -> tuple[str, str]:
+    ...
 
+@overload
+def file_basename(name: str | Path, return_ext: Literal[False] = False) -> str:
+    ...
 
-@optional_args
-def normalize_path(func):
-    """Decorator to provide unified path conversion for a single file
+def file_basename(name: str | Path, return_ext: bool =False) -> str | tuple[str, str]:
+    """
+    Strips up to 2 extensions of length up to 4 characters and starting with alpha
+    not a digit, so we could get rid of .tar.gz etc
+    """
+    bname = basename(name)
+    fbname = re.sub(r'(\.[a-zA-Z_]\S{1,4}){0,2}$', '', bname)
+    if return_ext:
+        return fbname, bname[len(fbname) + 1:]
+    else:
+        return fbname
 
-    Unlike normalize_paths, intended to be used for functions dealing with a
-    single filename at a time
 
-    Note
-    ----
-    This is intended to be used within the repository classes and therefore
-    returns a class method!
-
-    The decorated function is expected to take a path at
-    first positional argument (after 'self'). Additionally the class `func`
-    is a member of, is expected to have an attribute 'path'.
+# unused in -core
+def escape_filename(filename: str) -> str:
+    """Surround filename in "" and escape " in the filename
     """
+    filename = filename.replace('"', r'\"').replace('`', r'\`')
+    filename = '"%s"' % filename
+    return filename
 
-    @wraps(func)
-    def newfunc(self, file_, *args, **kwargs):
-        file_new = _normalize_path(self.path, file_)
-        return func(self, file_new, *args, **kwargs)
 
-    return newfunc
+# unused in -core
+def encode_filename(filename: str | bytes) -> bytes:
+    """Encode unicode filename
+    """
+    if isinstance(filename, str):
+        return filename.encode(sys.getfilesystemencoding())
+    else:
+        return filename
 
 
-@optional_args
-def normalize_paths(func, match_return_type=True, map_filenames_back=False,
-                    serialize=False):
-    """Decorator to provide unified path conversions.
-
-    Note
-    ----
-    This is intended to be used within the repository classes and therefore
-    returns a class method!
-
-    The decorated function is expected to take a path or a list of paths at
-    first positional argument (after 'self'). Additionally the class `func`
-    is a member of, is expected to have an attribute 'path'.
-
-    Accepts either a list of paths or a single path in a str. Passes a list
-    to decorated function either way, but would return based on the value of
-    match_return_type and possibly input argument.
-
-    If a call to the wrapped function includes normalize_path and it is False
-    no normalization happens for that function call (used for calls to wrapped
-    functions within wrapped functions, while possible CWD is within a
-    repository)
+# unused in -core
+def decode_input(s: str | bytes) -> str:
+    """Given input string/bytes, decode according to stdin codepage (or UTF-8)
+    if not defined
+
+    If fails -- issue warning and decode allowing for errors
+    being replaced
+    """
+    if isinstance(s, str):
+        return s
+    else:
+        encoding = sys.stdin.encoding or 'UTF-8'
+        try:
+            return s.decode(encoding)
+        except UnicodeDecodeError as exc:
+            lgr.warning(
+                "Failed to decode input string using %s encoding. "
+                "Decoding allowing for errors", encoding)
+            return s.decode(encoding, errors='replace')
+
+
+# unused in -core
+if on_windows:
+    def lmtime(filepath: str | Path, mtime: int | float) -> None:
+        """Set mtime for files.  On Windows a merely adapter to os.utime
+        """
+        os.utime(filepath, (time.time(), mtime))
+else:
+    def lmtime(filepath: str | Path, mtime: int | float) -> None:
+        """Set mtime for files, while not de-referencing symlinks.
+
+        To overcome absence of os.lutime
+
+        Works only on linux and OSX ATM
+        """
+        from .cmd import WitlessRunner
+
+        # convert mtime to format touch understands [[CC]YY]MMDDhhmm[.SS]
+        smtime = time.strftime("%Y%m%d%H%M.%S", time.localtime(mtime))
+        lgr.log(3, "Setting mtime for %s to %s == %s", filepath, mtime, smtime)
+        WitlessRunner().run(['touch', '-h', '-t', '%s' % smtime, str(filepath)])
+        filepath = Path(filepath)
+        rfilepath = filepath.resolve()
+        if filepath.is_symlink() and rfilepath.exists():
+            # trust no one - adjust also of the target file
+            # since it seemed like downloading under OSX (was it using curl?)
+            # didn't bother with timestamps
+            lgr.log(3, "File is a symlink to %s Setting mtime for it to %s",
+                    rfilepath, mtime)
+            os.utime(str(rfilepath), (time.time(), mtime))
+        # doesn't work on OSX
+        # Runner().run(['touch', '-h', '-d', '@%s' % mtime, filepath])
+
+
+# See <https://github.com/python/typing/discussions/1366> for a request for a
+# better way to annotate this function.
+def ensure_tuple_or_list(obj: Any) -> list | tuple:
+    """Given an object, wrap into a tuple if not list or tuple
+    """
+    if isinstance(obj, (list, tuple)):
+        return tuple(obj)
+    return (obj,)
+
+
+ListOrSet = TypeVar("ListOrSet", list, set)
+
+
+# TODO: Improve annotation:
+def ensure_iter(s: Any, cls: type[ListOrSet], copy: bool=False, iterate: bool=True) -> ListOrSet:
+    """Given not a list, would place it into a list. If None - empty list is returned
 
     Parameters
     ----------
-    match_return_type : bool, optional
-      If True, and a single string was passed in, it would return the first
-      element of the output (after verifying that it is a list of length 1).
-      It makes easier to work with single files input.
-    map_filenames_back : bool, optional
-      If True and returned value is a dictionary, it assumes to carry entries
-      one per file, and then filenames are mapped back to as provided from the
-      normalized (from the root of the repo) paths
-    serialize : bool, optional
-      Loop through files giving only a single one to the function one at a time.
-      This allows to simplify implementation and interface to annex commands
-      which do not take multiple args in the same call (e.g. checkpresentkey)
+    s: list or anything
+    cls: class
+      Which iterable class to ensure
+    copy: bool, optional
+      If correct iterable is passed, it would generate its shallow copy
+    iterate: bool, optional
+      If it is not a list, but something iterable (but not a str)
+      iterate over it.
     """
 
-    @wraps(func)
-    def newfunc(self, files, *args, **kwargs):
+    if isinstance(s, cls):
+        return s if not copy else shallow_copy(s)
+    elif isinstance(s, str):
+        return cls((s,))
+    elif iterate and hasattr(s, '__iter__'):
+        return cls(s)
+    elif s is None:
+        return cls()
+    else:
+        return cls((s,))
 
-        normalize = _normalize_path if kwargs.pop('normalize_paths', True) \
-            else lambda rpath, filepath: filepath
 
-        if files:
-            if isinstance(files, string_types) or not files:
-                files_new = [normalize(self.path, files)]
-                single_file = True
-            elif isinstance(files, list):
-                files_new = [normalize(self.path, path) for path in files]
-                single_file = False
-            else:
-                raise ValueError("_files_decorator: Don't know how to handle "
-                                 "instance of %s." % type(files))
-        else:
-            single_file = None
-            files_new = []
+# TODO: Improve annotation:
+def ensure_list(s: Any, copy: bool=False, iterate: bool=True) -> list:
+    """Given not a list, would place it into a list. If None - empty list is returned
 
-        if map_filenames_back:
-            def remap_filenames(out):
-                """Helper to map files back to non-normalized paths"""
-                if isinstance(out, dict):
-                    assert(len(out) == len(files_new))
-                    files_ = [files] if single_file else files
-                    mapped = out.__class__()
-                    for fin, fout in zip(files_, files_new):
-                        mapped[fin] = out[fout]
-                    return mapped
-                else:
-                    return out
-        else:
-            remap_filenames = lambda x: x
+    Parameters
+    ----------
+    s: list or anything
+    copy: bool, optional
+      If list is passed, it would generate a shallow copy of the list
+    iterate: bool, optional
+      If it is not a list, but something iterable (but not a str)
+      iterate over it.
+    """
+    return ensure_iter(s, list, copy=copy, iterate=iterate)
 
-        if serialize:  # and not single_file:
-            result = [
-                func(self, f, *args, **kwargs)
-                for f in files_new
-            ]
-        else:
-            result = func(self, files_new, *args, **kwargs)
 
-        if single_file is None:
-            # no files were provided, nothing we can do really
-            return result
-        elif (result is None) or not match_return_type or not single_file:
-            # If function doesn't return anything or no denormalization
-            # was requested or it was not a single file
-            return remap_filenames(result)
-        elif single_file:
-            if len(result) != 1:
-                # Magic doesn't apply
-                return remap_filenames(result)
-            elif isinstance(result, (list, tuple)):
-                return result[0]
-            elif isinstance(result, dict) and tuple(result)[0] == files_new[0]:
-                # assume that returned dictionary has files as keys.
-                return tuple(result.values())[0]
-            else:
-                # no magic can apply
-                return remap_filenames(result)
-        else:
-            return RuntimeError("should have not got here... check logic")
+# TODO: Improve annotation:
+def ensure_result_list(r: Any) -> list:
+    """Return a list of result records
 
-    return newfunc
+    Largely same as ensure_list, but special casing a single dict being passed
+    in, which a plain `ensure_list` would iterate over. Hence, this deals with
+    the three ways datalad commands return results:
+    - single dict
+    - list of dicts
+    - generator
 
+    Used for result assertion helpers.
+    """
+    return [r] if isinstance(r, dict) else ensure_list(r)
 
-def check_git_configured():
-    """Do a check if git is configured (user.name and user.email are set)
+@overload
+def ensure_list_from_str(s: str, sep: str='\n') -> Optional[list[str]]:
+    ...
+
+@overload
+def ensure_list_from_str(s: list[T], sep: str='\n') -> Optional[list[T]]:
+    ...
 
-    Raises
-    ------
-    RuntimeError if any of those two variables are not set
+def ensure_list_from_str(s: str | list[T], sep: str='\n') -> Optional[list[str]] | Optional[list[T]]:
+    """Given a multiline string convert it to a list of return None if empty
 
-    Returns
-    -------
-    dict with user.name and user.email entries
+    Parameters
+    ----------
+    s: str or list
     """
 
-    check_runner = GitRunner()
-    vals = {}
-    exc_ = ""
-    for c in 'user.name', 'user.email':
-        try:
-            v, err = check_runner.run(['git', 'config', c])
-            vals[c] = v.rstrip('\n')
-        except CommandError as exc:
-            exc_ += exc_str(exc)
-    if exc_:
-        lgr.warning(
-            "It is highly recommended to configure git first (set both "
-            "user.name and user.email) before using DataLad. Failed to "
-            "verify that git is configured: %s.  Some operations might fail or "
-            "not perform correctly." % exc_
-        )
-    return vals
+    if not s:
+        return None
+
+    if isinstance(s, list):
+        return s
+    return s.split(sep)
+
+@overload
+def ensure_dict_from_str(s: str, sep: str = '\n') -> Optional[dict[str, str]]:
+    ...
+
+@overload
+def ensure_dict_from_str(s: dict[K, V], sep: str = '\n') -> Optional[dict[K, V]]:
+    ...
+
+def ensure_dict_from_str(s: str | dict[K, V], sep: str = '\n') -> Optional[dict[str, str]] | Optional[dict[K, V]]:
+    """Given a multiline string with key=value items convert it to a dictionary
+
+    Parameters
+    ----------
+    s: str or dict
+
+    Returns None if input s is empty
+    """
+
+    if not s:
+        return None
+
+    if isinstance(s, dict):
+        return s
+
+    out: dict[str, str] = {}
+    values = ensure_list_from_str(s, sep=sep)
+    assert values is not None
+    for value_str in values:
+        if '=' not in value_str:
+            raise ValueError("{} is not in key=value format".format(repr(value_str)))
+        k, v = value_str.split('=', 1)
+        if k in out:
+            err = "key {} was already defined in {}, but new value {} was provided".format(k, out, v)
+            raise ValueError(err)
+        out[k] = v
+    return out
 
 
-def _remove_empty_items(list_):
-    """Remove empty entries from list
+def ensure_bytes(s: str | bytes, encoding: str='utf-8') -> bytes:
+    """Convert/encode unicode string to bytes.
 
-    This is needed, since some functions of GitPython may convert
-    an empty entry to '.', when used with a list of paths.
+    If `s` isn't a string, return it as is.
 
-    Parameter:
+    Parameters
     ----------
-    list_: list of str
+    encoding: str, optional
+      Encoding to use.  "utf-8" is the default
+    """
+    if not isinstance(s, str):
+        return s
+    return s.encode(encoding)
 
-    Returns
-    -------
-    list of str
+
+def ensure_unicode(s: str | bytes, encoding: Optional[str]=None, confidence: Optional[float]=None) -> str:
+    """Convert/decode bytestring to unicode.
+
+    If `s` isn't a bytestring, return it as is.
+
+    Parameters
+    ----------
+    encoding: str, optional
+      Encoding to use.  If None, "utf-8" is tried, and then if not a valid
+      UTF-8, encoding will be guessed
+    confidence: float, optional
+      A value between 0 and 1, so if guessing of encoding is of lower than
+      specified confidence, ValueError is raised
     """
-    if not isinstance(list_, list):
-        lgr.warning(
-            "_remove_empty_items() called with non-list type: %s" % type(list_))
-        return list_
-    return [file_ for file_ in list_ if file_]
+    if not isinstance(s, bytes):
+        return s
+    if encoding is None:
+        # Figure out encoding, defaulting to 'utf-8' which is our common
+        # target in contemporary digital society
+        try:
+            return s.decode('utf-8')
+        except UnicodeDecodeError as exc:
+            lgr.debug("Failed to decode a string as utf-8: %s",
+                      CapturedException(exc))
+        # And now we could try to guess
+        from chardet import detect
+        enc = detect(s)
+        denc = enc.get('encoding', None)
+        if denc:
+            denc_confidence = enc.get('confidence', 0)
+            if confidence is not None and  denc_confidence < confidence:
+                raise ValueError(
+                    "Failed to auto-detect encoding with high enough "
+                    "confidence. Highest confidence was %s for %s"
+                    % (denc_confidence, denc)
+                )
+            lgr.log(5, "Auto-detected encoding to be %s", denc)
+            return s.decode(denc)
+        else:
+            raise ValueError(
+                "Could not decode value as utf-8, or to guess its encoding: %s"
+                % repr(s)
+            )
+    else:
+        return s.decode(encoding)
+
 
+def ensure_bool(s: Any) -> bool:
+    """Convert value into boolean following convention for strings
 
-def Repo(*args, **kwargs):
-    """Factory method around gitpy.Repo to consistently initiate with different
-    backend
+    to recognize on,True,yes as True, off,False,no as False
     """
-    # TODO: This probably doesn't work as intended (or at least not as
-    #       consistently as intended). gitpy.Repo could be instantiated by
-    #       classmethods Repo.init or Repo.clone_from. In these cases 'odbt'
-    #       would be needed as a paramter to these methods instead of the
-    #       constructor.
-    if 'odbt' not in kwargs:
-        kwargs['odbt'] = default_git_odbt
-    return gitpy.Repo(*args, **kwargs)
+    if isinstance(s, str):
+        if s.isdigit():
+            return bool(int(s))
+        sl = s.lower()
+        if sl in {'y', 'yes', 'true', 'on'}:
+            return True
+        elif sl in {'n', 'no', 'false', 'off'}:
+            return False
+        else:
+            raise ValueError("Do not know how to treat %r as a boolean" % s)
+    return bool(s)
+
 
+def unique(seq: Sequence[T], key: Optional[Callable[[T], Any]]=None, reverse: bool=False) -> list[T]:
+    """Given a sequence return a list only with unique elements while maintaining order
 
-def split_remote_branch(branch):
-    """Splits a remote branch's name into the name of the remote and the name
-    of the branch.
+    This is the fastest solution.  See
+    https://www.peterbe.com/plog/uniqifiers-benchmark
+    and
+    http://stackoverflow.com/a/480227/1265472
+    for more information.
+    Enhancement -- added ability to compare for uniqueness using a key function
 
     Parameters
     ----------
-    branch: str
-      the remote branch's name to split
+    seq:
+      Sequence to analyze
+    key: callable, optional
+      Function to call on each element so we could decide not on a full
+      element, but on its member etc
+    reverse: bool, optional
+      If True, uniqueness checked in the reverse order, so that the later ones
+      will take the order
+    """
+    seen: set[T] = set()
+    seen_add = seen.add
+
+    if reverse:
+        def trans(x: Sequence[T]) -> Iterable[T]:
+            return reversed(x)
+    else:
+        def trans(x: Sequence[T]) -> Iterable[T]:
+            return x
+
+    if key is None:
+        out = [x for x in trans(seq) if not (x in seen or seen_add(x))]
+    else:
+        # OPT: could be optimized, since key is called twice, but for our cases
+        # should be just as fine
+        out = [x for x in trans(seq) if not (key(x) in seen or seen_add(key(x)))]
+
+    return out[::-1] if reverse else out
+
+
+# TODO: Annotate (would be made easier if the return value was always a dict
+# instead of doing `v.__class__(...)`)
+def map_items(func, v):
+    """A helper to apply `func` to all elements (keys and values) within dict
+
+    No type checking of values passed to func is done, so `func`
+    should be resilient to values which it should not handle
+
+    Initial usecase - apply_recursive(url_fragment, ensure_unicode)
+    """
+    # map all elements within item
+    return v.__class__(
+        item.__class__(map(func, item))
+        for item in v.items()
+    )
+
+
+def partition(items: Iterable[T], predicate: Callable[[T], Any]=bool) -> tuple[Iterator[T], Iterator[T]]:
+    """Partition `items` by `predicate`.
+
+    Parameters
+    ----------
+    items : iterable
+    predicate : callable
+        A function that will be mapped over each element in `items`. The
+        elements will partitioned based on whether the return value is false or
+        true.
 
     Returns
     -------
-    list of str
+    A tuple with two generators, the first for 'false' items and the second for
+    'true' ones.
+
+    Notes
+    -----
+    Taken from Peter Otten's snippet posted at
+    https://nedbatchelder.com/blog/201306/filter_a_list_into_two_parts.html
     """
-    assert '/' in branch, \
-        "remote branch %s must have had a /" % branch
-    assert not branch.endswith('/'), \
-        "branch name with trailing / is invalid. (%s)" % branch
-    return branch.split('/', 1)
+    a, b = tee((predicate(item), item) for item in items)
+    return ((item for pred, item in a if not pred),
+            (item for pred, item in b if pred))
 
 
-@add_metaclass(Flyweight)
-class GitRepo(RepoInterface):
-    """Representation of a git repository
+def generate_chunks(container: list[T], size: int) -> Iterator[list[T]]:
+    """Given a container, generate chunks from it with size up to `size`
+    """
+    # There could be a "smarter" solution but I think this would suffice
+    assert size > 0,  "Size should be non-0 positive"
+    while container:
+        yield container[:size]
+        container = container[size:]
 
+
+def generate_file_chunks(files: list[str], cmd: str | list[str] | None = None) -> Iterator[list[str]]:
+    """Given a list of files, generate chunks of them to avoid exceeding cmdline length
+
+    Parameters
+    ----------
+    files: list of str
+    cmd: str or list of str, optional
+      Command to account for as well
     """
+    files = ensure_list(files)
+    cmd = ensure_list(cmd)
 
-    # We use our sshrun helper
-    GIT_SSH_ENV = {'GIT_SSH_COMMAND': GIT_SSH_COMMAND}
+    maxl = max(map(len, files)) if files else 0
+    chunk_size = max(
+        1,  # should at least be 1. If blows then - not our fault
+        (CMD_MAX_ARG
+         - sum((len(x) + 3) for x in cmd)
+         - 4  # for '--' below
+         ) // (maxl + 3)  # +3 for possible quotes and a space
+    )
+    # TODO: additional treatment for "too many arguments"? although
+    # as https://github.com/datalad/datalad/issues/1883#issuecomment
+    # -436272758
+    # shows there seems to be no hardcoded limit on # of arguments,
+    # but may be we decide to go for smth like follow to be on safe side
+    # chunk_size = min(10240 - len(cmd), chunk_size)
+    file_chunks = generate_chunks(files, chunk_size)
+    return file_chunks
 
-    # Just a non-functional example:
-    # must be implemented, since abstract in RepoInterface:
-    def sth_like_file_has_content(self):
-        return "Yes, if it's in the index"
 
-    # We must check git config to have name and email set, but
-    # should do it once
-    _config_checked = False
+#
+# Generators helpers
+#
 
-    # Begin Flyweight:
+def saved_generator(gen: Iterable[T]) -> tuple[Iterator[T], Iterator[T]]:
+    """Given a generator returns two generators, where 2nd one just replays
 
-    _unique_instances = WeakValueDictionary()
+    So the first one would be going through the generated items and 2nd one
+    would be yielding saved items
+    """
+    saved = []
 
-    @classmethod
-    def _flyweight_id_from_args(cls, *args, **kwargs):
+    def gen1() -> Iterator[T]:
+        for x in gen:  # iterating over original generator
+            saved.append(x)
+            yield x
+
+    def gen2() -> Iterator[T]:
+        for x in saved:  # yielding saved entries
+            yield x
 
-        if args:
-            # to a certain degree we need to simulate an actual call to __init__
-            # and make sure, passed arguments are fitting:
-            # TODO: Figure out, whether there is a cleaner way to do this in a
-            # generic fashion
-            assert('path' not in kwargs)
-            path = args[0]
-            args = args[1:]
-        elif 'path' in kwargs:
-            path = kwargs.pop('path')
-        else:
-            raise TypeError("__init__() requires argument `path`")
+    return gen1(), gen2()
 
-        if path is None:
-            raise AttributeError
 
-        # Sanity check for argument `path`:
-        # raise if we cannot deal with `path` at all or
-        # if it is not a local thing:
-        path = RI(path).localpath
-        # resolve symlinks to make sure we have exactly one instance per
-        # physical repository at a time
-        path = realpath(path)
-        kwargs['path'] = path
-        return path, args, kwargs
-
-    @classmethod
-    def _flyweight_invalid(cls, id_):
-        return not cls.is_valid_repo(id_)
-
-    @classmethod
-    def _flyweight_reject(cls, id_, *args, **kwargs):
-        # TODO:
-        # This is a temporary approach. See PR # ...
-        # create = kwargs.pop('create', None)
-        # kwargs.pop('path', None)
-        # if create and kwargs:
-        #     # we have `create` plus options other than `path`
-        #     return "Call to {0}() with args {1} and kwargs {2} conflicts " \
-        #            "with existing instance {3}." \
-        #            "This is likely to be caused by inconsistent logic in " \
-        #            "your code." \
-        #            "".format(cls, args, kwargs, cls._unique_instances[id_])
-        pass
+#
+# Decorators
+#
 
-    # End Flyweight
+# Originally better_wraps was created to provide `wrapt`-based, instead of
+# `functools.wraps` implementation to preserve the correct signature of the
+# decorated function. By using inspect.signature in our getargspec, which
+# works fine on `functools.wraps`ed functions, we mediated this necessity.
+better_wraps = wraps
+
+
+# TODO: Annotate:
+# Borrowed from pandas
+# Copyright: 2011-2014, Lambda Foundry, Inc. and PyData Development Team
+# License: BSD-3
+def optional_args(decorator):
+    """allows a decorator to take optional positional and keyword arguments.
+        Assumes that taking a single, callable, positional argument means that
+        it is decorating a function, i.e. something like this::
+
+            @my_decorator
+            def function(): pass
+
+        Calls decorator with decorator(f, `*args`, `**kwargs`)"""
+
+    @better_wraps(decorator)
+    def wrapper(*args, **kwargs):
+        def dec(f):
+            return decorator(f, *args, **kwargs)
+
+        is_decorating = not kwargs and len(args) == 1 and isinstance(args[0], Callable)
+        if is_decorating:
+            f = args[0]
+            args = []
+            return dec(f)
+        else:
+            return dec
 
-    def __init__(self, path, url=None, runner=None, create=True,
-                 git_opts=None, repo=None, **kwargs):
-        """Creates representation of git repository at `path`.
+    return wrapper
 
-        Can also be used to create a git repository at `path`.
 
-        Parameters
-        ----------
-        path: str
-          path to the git repository; In case it's not an absolute path,
-          it's relative to PWD
-        url: str, optional
-          DEPRECATED -- use .clone() class method
-          url to the to-be-cloned repository. Requires a valid git url
-          according to:
-          http://www.kernel.org/pub/software/scm/git/docs/git-clone.html#URLS .
-        create: bool, optional
-          if true, creates a git repository at `path` if there is none. Also
-          creates `path`, if it doesn't exist.
-          If set to false, an exception is raised in case `path` doesn't exist
-          or doesn't contain a git repository.
-        repo: git.Repo, optional
-          GitPython's Repo instance to (re)use if provided
-        kwargs:
-          keyword arguments serving as additional options to the git-init
-          command. Therefore, it makes sense only if called with `create`.
-
-          Generally, this way of passing options to the git executable is
-          (or will be) used a lot in this class. It's a transformation of
-          python-style keyword arguments (or a `dict`) to command line arguments,
-          provided by GitPython.
-
-          A single character keyword will be prefixed by '-', multiple characters
-          by '--'. An underscore in the keyword becomes a dash. The value of the
-          keyword argument is used as the value for the corresponding command
-          line argument. Assigning a boolean creates a flag.
-
-          Examples:
-          no_commit=True => --no-commit
-          C='/my/path'   => -C /my/path
+# TODO: just provide decorators for tempfile.mk* functions. This is ugly!
+def get_tempfile_kwargs(tkwargs: Optional[dict[str, Any]]=None, prefix: str="", wrapped: Optional[Callable]=None) -> dict[str, Any]:
+    """Updates kwargs to be passed to tempfile. calls depending on env vars
+    """
+    if tkwargs is None:
+        tkwargs_ = {}
+    else:
+        # operate on a copy of tkwargs to avoid any side-effects
+        tkwargs_ = tkwargs.copy()
 
-        """
+    # TODO: don't remember why I had this one originally
+    # if len(targs)<2 and \
+    if 'prefix' not in tkwargs_:
+        tkwargs_['prefix'] = '_'.join(
+            ['datalad_temp'] +
+            ([prefix] if prefix else []) +
+            ([''] if (on_windows or not wrapped) else [wrapped.__name__]))
+
+    directory = os.environ.get('TMPDIR')
+    if directory and 'dir' not in tkwargs_:
+        tkwargs_['dir'] = directory
 
-        if url is not None:
-            raise DeprecatedError(
-                new=".clone() class method",
-                version="0.5.0",
-                msg="RF: url passed to init()"
-            )
+    return tkwargs_
 
-        # So that we "share" control paths with git/git-annex
-        if ssh_manager:
-            ssh_manager.assure_initialized()
-
-        if not GitRepo._config_checked:
-            check_git_configured()
-            GitRepo._config_checked = True
-
-        self.realpath = realpath(path)
-        # note: we may also want to distinguish between a path to the worktree
-        # and the actual repository
-
-        # Disable automatic garbage and autopacking
-        self._GIT_COMMON_OPTIONS = ['-c', 'receive.autogc=0', '-c', 'gc.auto=0']
-        # actually no need with default GitPython db backend not in memory
-        # default_git_odbt but still allows for faster testing etc.
-        # May be eventually we would make it switchable _GIT_COMMON_OPTIONS = []
-
-        if git_opts is None:
-            git_opts = {}
-        if kwargs:
-            git_opts.update(kwargs)
-
-        self.path = path
-        self.cmd_call_wrapper = runner or GitRunner(cwd=self.path)
-        self._repo = repo
-        self._cfg = None
-
-        _valid_repo = GitRepo.is_valid_repo(path)
-        if create and not _valid_repo:
-            if repo is not None:
-                # `repo` passed with `create`, which doesn't make sense
-                raise TypeError("argument 'repo' must not be used with 'create'")
 
-            try:
-                lgr.debug(
-                    "Initialize empty Git repository at '%s'%s",
-                    path,
-                    ' %s' % git_opts if git_opts else '')
-                self._repo = self.cmd_call_wrapper(gitpy.Repo.init, path,
-                                                   mkdir=True,
-                                                   odbt=default_git_odbt,
-                                                   **git_opts)
-            except GitCommandError as e:
-                lgr.error(exc_str(e))
-                raise
-        else:
-            # Note: We used to call gitpy.Repo(path) here, which potentially
-            # raised NoSuchPathError or InvalidGitRepositoryError. This is
-            # used by callers of GitRepo.__init__() to detect whether we have a
-            # valid repo at `path`. Now, with switching to lazy loading property
-            # `repo`, we detect those cases without instantiating a
-            # gitpy.Repo().
-
-            if not exists(path):
-                raise NoSuchPathError(path)
-            if not _valid_repo:
-                raise InvalidGitRepositoryError(path)
-
-        # inject git options into GitPython's git call wrapper:
-        # Note: `None` currently can happen, when Runner's protocol prevents
-        # calls above from being actually executed (DryRunProtocol)
-        if self._repo is not None:
-            self._repo.git._persistent_git_options = self._GIT_COMMON_OPTIONS
-
-        # with DryRunProtocol path might still not exist
-        if exists(self.realpath):
-            self.inode = os.stat(self.realpath).st_ino
-        else:
-            self.inode = None
+def line_profile(func: Callable[P, T]) -> Callable[P, T]:
+    """Q&D helper to line profile the function and spit out stats
+    """
+    import line_profiler  # type: ignore[import]
+    prof = line_profiler.LineProfiler()
 
-    @property
-    def repo(self):
-        # with DryRunProtocol path not exist
-        if exists(self.realpath):
-            inode = os.stat(self.realpath).st_ino
-        else:
-            inode = None
-        if self.inode != inode:
-            # reset background processes invoked by GitPython:
-            self._repo.git.clear_cache()
-            self.inode = inode
-
-        if self._repo is None:
-            # Note, that this may raise GitCommandError, NoSuchPathError,
-            # InvalidGitRepositoryError:
-            self._repo = self.cmd_call_wrapper(Repo, self.path)
-            lgr.log(8, "Using existing Git repository at %s", self.path)
-
-        # inject git options into GitPython's git call wrapper:
-        # Note: `None` currently can happen, when Runner's protocol prevents
-        # call of Repo(path) above from being actually executed (DryRunProtocol)
-        if self._repo is not None:
-            self._repo.git._persistent_git_options = self._GIT_COMMON_OPTIONS
-
-        return self._repo
-
-    @classmethod
-    def clone(cls, url, path, *args, **kwargs):
-        """Clone url into path
+    @wraps(func)
+    def  _wrap_line_profile(*args: P.args, **kwargs: P.kwargs) -> T:
+        try:
+            pfunc = prof(func)
+            return pfunc(*args, **kwargs)
+        finally:
+            prof.print_stats()
+    return  _wrap_line_profile
 
-        Provides workarounds for known issues (e.g.
-        https://github.com/datalad/datalad/issues/785)
 
-        Parameters
-        ----------
-        url : str
-        path : str
-        expect_fail : bool
-          Either expect that command might fail, so error should be logged then
-          at DEBUG level instead of ERROR
-        """
+# unused in -core
+@optional_args
+def collect_method_callstats(func: Callable[P, T]) -> Callable[P, T]:
+    """Figure out methods which call the method repeatedly on the same instance
 
-        if 'repo' in kwargs:
-            raise TypeError("argument 'repo' conflicts with cloning")
-            # TODO: what about 'create'?
-
-        expect_fail = kwargs.pop('expect_fail', False)
-        # fail early on non-empty target:
-        from os import listdir
-        if exists(path) and listdir(path):
-            # simulate actual GitCommandError:
-            lgr.warning("destination path '%s' already exists and is not an "
-                        "empty directory." % path)
-            raise GitCommandError(
-                ['git', 'clone', '-v', url, path],
-                128,
-                "fatal: destination path '%s' already exists and is not an "
-                "empty directory." % path)
-        else:
-            # protect against cloning into existing and obviously dangling
-            # instance for that location
-            try:
-                del cls._unique_instances[path]
-            except KeyError:
-                # didn't exist - all fine
-                pass
+    Use case(s):
+      - .repo is expensive since does all kinds of checks.
+      - .config is expensive transitively since it calls .repo each time
+
+    TODO:
+      - fancy one could look through the stack for the same id(self) to see if
+        that location is already in memo.  That would hint to the cases where object
+        is not passed into underlying functions, causing them to redo the same work
+        over and over again
+      - ATM might flood with all "1 lines" calls which are not that informative.
+        The underlying possibly suboptimal use might be coming from their callers.
+        It might or not relate to the previous TODO
+    """
+    import traceback
+    from collections import defaultdict
+    from time import time
+    memo: defaultdict[tuple[int, str], defaultdict[int, int]] = defaultdict(lambda: defaultdict(int))  # it will be a dict of lineno: count
+    # gross timing
+    times = []
+    toppath = dirname(__file__) + sep
 
-        # Massage URL
-        url_ri = RI(url) if not isinstance(url, RI) else url
-        # try to get a local path from `url`:
+    @wraps(func)
+    def _wrap_collect_method_callstats(*args: P.args, **kwargs: P.kwargs) -> T:
         try:
-            url = url_ri.localpath
-            url_ri = RI(url)
-        except ValueError:
+            self = args[0]
+            stack = traceback.extract_stack()
+            caller = stack[-2]
+            stack_sig = \
+                "{relpath}:{s.name}".format(
+                    s=caller, relpath=relpath(caller.filename, toppath))
+            sig = (id(self), stack_sig)
+            # we will count based on id(self) + wherefrom
+            if caller.lineno is not None:
+                memo[sig][caller.lineno] += 1
+            t0 = time()
+            return func(*args, **kwargs)
+        finally:
+            times.append(time() - t0)
             pass
 
-        if is_ssh(url_ri):
-            ssh_manager.get_connection(url).open()
-            # TODO: with git <= 2.3 keep old mechanism:
-            #       with rm.repo.git.custom_environment(GIT_SSH="wrapper_script"):
-            env = GitRepo.GIT_SSH_ENV
+    def print_stats() -> None:
+        print("The cost of property {}:".format(func.__name__))
+        if not memo:
+            print("None since no calls")
+            return
+        # total count
+        counts = {k: sum(v.values()) for k,v in memo.items()}
+        total = sum(counts.values())
+        ids = {self_id for (self_id, _) in memo}
+        print(" Total: {} calls from {} objects with {} contexts taking {:.2f} sec"
+              .format(total, len(ids), len(memo), sum(times)))
+        # now we need to sort by value
+        for (self_id, caller), count in sorted(counts.items(), key=lambda x: x[1], reverse=True):
+            print("  {} {}: {} from {} lines"
+                  .format(self_id, caller, count, len(memo[(self_id, caller)])))
+
+    # Upon total exit we print the stats
+    import atexit
+    atexit.register(print_stats)
+
+    return  _wrap_collect_method_callstats
+
+
+# Borrowed from duecredit to wrap duecredit-handling to guarantee failsafe
+def never_fail(f: Callable[P, T]) -> Callable[P, Optional[T]]:
+    """Assure that function never fails -- all exceptions are caught
+
+    Returns `None` if function fails internally.
+    """
+    @wraps(f)
+    def wrapped_func(*args: P.args, **kwargs: P.kwargs) -> Optional[T]:
+        try:
+            return f(*args, **kwargs)
+        except Exception as e:
+            lgr.warning(
+                "DataLad internal failure while running %s: %r. "
+                "Please report at https://github.com/datalad/datalad/issues"
+                % (f, e)
+            )
+            return None
+
+    if os.environ.get('DATALAD_ALLOW_FAIL', False):
+        return f
+    else:
+        return wrapped_func
+
+
+def shortened_repr(value: Any, l: int=30) -> str:
+    try:
+        if hasattr(value, '__repr__') and (value.__repr__ is not object.__repr__):
+            value_repr = repr(value)
+            if not value_repr.startswith('<') and len(value_repr) > l:
+                value_repr = "<<%s++%d chars++%s>>" % (
+                    value_repr[:l - 16],
+                    len(value_repr) - (l - 16 + 4),
+                    value_repr[-4:]
+                )
+            elif value_repr.startswith('<') and value_repr.endswith('>') and ' object at 0x':
+                raise ValueError("I hate those useless long reprs")
         else:
-            if isinstance(url_ri, PathRI):
-                new_url = os.path.expanduser(url)
-                if url != new_url:
-                    # TODO: remove whenever GitPython is fixed:
-                    # https://github.com/gitpython-developers/GitPython/issues/731
-                    lgr.info("Expanded source path to %s from %s", new_url, url)
-                    url = new_url
-            env = None
+            raise ValueError("gimme class")
+    except Exception as e:
+        value_repr = "<%s>" % value.__class__.__name__.split('.')[-1]
+    return value_repr
+
+
+def __auto_repr__(obj: Any, short: bool =True) -> str:
+    attr_names: tuple[str, ...] = tuple()
+    if hasattr(obj, '__dict__'):
+        attr_names += tuple(obj.__dict__.keys())
+    if hasattr(obj, '__slots__'):
+        attr_names += tuple(obj.__slots__)
+
+    items = []
+    for attr in sorted(set(attr_names)):
+        if attr.startswith('_'):
+            continue
+        value = getattr(obj, attr)
+        # TODO:  should we add this feature to minimize some talktative reprs
+        # such as of URL?
+        #if value is None:
+        #    continue
+        items.append("%s=%s" % (attr, shortened_repr(value) if short else value))
 
-        ntries = 5  # 3 is not enough for robust workaround
-        for trial in range(ntries):
-            try:
-                lgr.debug("Git clone from {0} to {1}".format(url, path))
-                repo = gitpy.Repo.clone_from(url, path, env=env,
-                                             odbt=default_git_odbt)
-                # Note/TODO: signature for clone from:
-                # (url, to_path, progress=None, env=None, **kwargs)
+    return "%s(%s)" % (obj.__class__.__name__, ', '.join(items))
 
-                lgr.debug("Git clone completed")
-                break
-            except GitCommandError as e:
-                # log here but let caller decide what to do
-                e_str = exc_str(e)
-                # see https://github.com/datalad/datalad/issues/785
-                if re.search("Request for .*aborted.*Unable to find", str(e),
-                             re.DOTALL) \
-                        and trial < ntries - 1:
-                    lgr.info(
-                        "Hit a known issue with Git (see GH#785). Trial #%d, "
-                        "retrying",
-                        trial)
-                    continue
-                    (lgr.debug if expect_fail else lgr.error)(e_str)
-                raise
-            except ValueError as e:
-                if gitpy.__version__ == '1.0.2' \
-                        and "I/O operation on closed file" in str(e):
-                    # bug https://github.com/gitpython-developers/GitPython
-                    # /issues/383
-                    raise GitCommandError(
-                        "clone has failed, telling ya",
-                        999,  # good number
-                        stdout="%s already exists" if exists(path) else "")
-                raise  # reraise original
-
-        gr = cls(path, *args, repo=repo, **kwargs)
-        return gr
-
-    def __del__(self):
-        # unbind possibly bound ConfigManager, to prevent all kinds of weird
-        # stalls etc
-        self._cfg = None
-        # Make sure to flush pending changes, especially close batch processes
-        # (internal `git cat-file --batch` by GitPython)
-        try:
-            if hasattr(self, 'repo') and exists(self.path) \
-                    and self.repo is not None:
-                # gc might be late, so the (temporary)
-                # repo doesn't exist on FS anymore
-                self.repo.git.clear_cache()
-                # We used to write out the index to flush GitPython's
-                # state... but such unconditional write is really a workaround
-                # and does not play nice with read-only operations - permission
-                # denied etc. So disabled 
-                #if exists(opj(self.path, '.git')):  # don't try to write otherwise
-                #    self.repo.index.write()
-        except InvalidGitRepositoryError:
-            # might have being removed and no longer valid
-            pass
 
-    def __repr__(self):
-        return "<GitRepo path=%s (%s)>" % (self.path, type(self))
+@optional_args
+def auto_repr(cls: type[T], short: bool=True) -> type[T]:
+    """Decorator for a class to assign it an automagic quick and dirty __repr__
 
-    def __eq__(self, obj):
-        """Decides whether or not two instances of this class are equal.
+    It uses public class attributes to prepare repr of a class
 
-        This is done by comparing the base repository path.
-        """
-        return self.realpath == obj.realpath
+    Original idea: http://stackoverflow.com/a/27799004/1265472
+    """
 
-    @classmethod
-    def is_valid_repo(cls, path):
-        """Returns if a given path points to a git repository"""
-        return exists(opj(path, '.git'))
+    cls.__repr__ = lambda obj:__auto_repr__(obj, short=short)  # type: ignore[assignment]
+    return cls
 
-    @property
-    def config(self):
-        """Get an instance of the parser for the persistent repository
-        configuration.
-
-        Note: This allows to also read/write .datalad/config,
-        not just .git/config
-
-        Returns
-        -------
-        ConfigManager
-        """
-        if self._cfg is None:
-            # associate with this dataset and read the entire config hierarchy
-            self._cfg = ConfigManager(dataset=self, dataset_only=False)
-        return self._cfg
 
-    def is_with_annex(self, only_remote=False):
-        """Return True if GitRepo (assumed) at the path has remotes with git-annex branch
+def todo_interface_for_extensions(f: T) -> T:
+    return f
 
-        Parameters
-        ----------
-        only_remote: bool, optional
-            Check only remote (no local branches) for having git-annex branch
-        """
-        return any((b.endswith('/git-annex') or
-                    'annex/direct' in b
-                    for b in self.get_remote_branches())) or \
-            ((not only_remote) and
-             any((b == 'git-annex' or 'annex/direct' in b
-                  for b in self.get_branches())))
-
-    @classmethod
-    def get_toppath(cls, path, follow_up=True, git_options=None):
-        """Return top-level of a repository given the path.
 
-        Parameters
-        -----------
-        follow_up : bool
-          If path has symlinks -- they get resolved by git.  If follow_up is
-          True, we will follow original path up until we hit the same resolved
-          path.  If no such path found, resolved one would be returned.
-        git_options: list of str
-          options to be passed to the git rev-parse call
+#
+# Context Managers
+#
 
-        Return None if no parent directory contains a git repository.
-        """
-        cmd = ['git']
-        if git_options:
-            cmd.extend(git_options)
-        cmd += ["rev-parse", "--show-toplevel"]
-        try:
-            toppath, err = GitRunner().run(
-                cmd,
-                cwd=path,
-                log_stdout=True, log_stderr=True,
-                expect_fail=True, expect_stderr=True)
-            toppath = toppath.rstrip('\n\r')
-        except CommandError:
-            return None
-        except OSError:
-            toppath = GitRepo.get_toppath(dirname(path), follow_up=follow_up,
-                                          git_options=git_options)
-
-        if follow_up:
-            path_ = path
-            path_prev = ""
-            while path_ and path_ != path_prev:  # on top /.. = /
-                if realpath(path_) == toppath:
-                    toppath = path_
-                    break
-                path_prev = path_
-                path_ = dirname(path_)
-
-        return toppath
-
-    # classmethod so behavior could be tuned in derived classes
-    @classmethod
-    def _get_added_files_commit_msg(cls, files):
-        if not files:
-            return "No files were added"
-        msg = "Added %d file" % len(files)
-        if len(files) > 1:
-            msg += "s"
-        return msg + '\n\nFiles:\n' + '\n'.join(files)
-
-    @normalize_paths
-    def add(self, files, commit=False, msg=None, git=True, git_options=None,
-            _datalad_msg=False, update=False):
-        """Adds file(s) to the repository.
 
-        Parameters
-        ----------
-        files: list
-          list of paths to add
-        commit: bool
-          whether or not to directly commit
-        msg: str
-          commit message in case `commit=True`. A default message, containing
-          the list of files that were added, is created by default.
-        git: bool
-          somewhat ugly construction to be compatible with AnnexRepo.add();
-          has to be always true.
-        update: bool
-          --update option for git-add. From git's manpage:
-           Update the index just where it already has an entry matching
-           <pathspec>. This removes as well as modifies index entries to match
-           the working tree, but adds no new files.
-
-           If no <pathspec> is given when --update option is used, all tracked
-           files in the entire working tree are updated (old versions of Git
-           used to limit the update to the current directory and its
-           subdirectories).
-        """
+# unused in -core
+@contextmanager
+def nothing_cm() -> Iterator[None]:
+    """Just a dummy cm to programmically switch context managers"""
+    yield
 
-        # TODO: git_options is used as options for the git-add here,
-        # instead of options to the git executable => rename for consistency
 
-        # needs to be True - see docstring:
-        assert(git)
+class SwallowOutputsAdapter:
+    """Little adapter to help getting out/err values
+    """
+    def __init__(self) -> None:
+        kw = get_tempfile_kwargs({}, prefix="outputs")
 
-        files = _remove_empty_items(files)
-        out = []
+        self._out = NamedTemporaryFile(delete=False, mode='w', **kw)
+        self._err = NamedTemporaryFile(delete=False, mode='w', **kw)
 
-        if files or git_options or update:
-            try:
-                # without --verbose git 2.9.3  add does not return anything
-                add_out = self._git_custom_command(
-                    files,
-                    ['git', 'add'] + assure_list(git_options) +
-                    to_options(update=update) + ['--verbose']
-                )
-                # get all the entries
-                out = self._process_git_get_output(*add_out)
-                # Note: as opposed to git cmdline, force is True by default in
-                #       gitpython, which would lead to add things, that are
-                #       ignored or excluded otherwise
-                # 2. Note: There is an issue with globbing (like adding '.'),
-                #       which apparently doesn't care for 'force' and therefore
-                #       adds '.git/...'. May be it's expanded at the wrong
-                #       point in time or sth. like that.
-                # For now, use direct call to git add.
-                #self.cmd_call_wrapper(self.repo.index.add, files, write=True,
-                #                      force=False)
-                # TODO: May be make use of 'fprogress'-option to indicate
-                # progress
-                # But then, we don't have it for git-annex add, anyway.
-                #
-                # TODO: Is write=True a reasonable way to do it?
-                # May be should not write until success of operation is
-                # confirmed?
-                # What's best in case of a list of files?
-            except OSError as e:
-                lgr.error("add: %s" % e)
-                raise
+    def _read(self, h: IO[str]) -> str:
+        with open(h.name) as f:
+            return f.read()
 
-        else:
-            lgr.warning("add was called with empty file list and no options.")
+    @property
+    def out(self) -> str:
+        if not self._out.closed:
+            self._out.flush()
+        return self._read(self._out)
 
-        if commit:
-            if msg is None:
-                msg = self._get_added_files_commit_msg(files)
-            self.commit(msg=msg, _datalad_msg=_datalad_msg)
-
-        # Make sure return value from GitRepo is consistent with AnnexRepo
-        # currently simulating similar return value, assuming success
-        # for all files:
-        # TODO: Make return values consistent across both *Repo classes!
-        return out
-
-    @staticmethod
-    def _process_git_get_output(stdout, stderr=None):
-        """Given both outputs (stderr is ignored atm) of git add - process it
+    @property
+    def err(self) -> str:
+        if not self._err.closed:
+            self._err.flush()
+        return self._read(self._err)
 
-        Primarily to centralize handling in both indirect annex and direct
-        modes when ran through proxy
-        """
-        return [{u'file': f, u'success': True}
-                for f in re.findall("'(.*)'[\n$]", stdout)]
+    @property
+    def handles(self) -> tuple[TextIO, TextIO]:
+        return (cast(TextIO, self._out), cast(TextIO, self._err))
 
-    @normalize_paths(match_return_type=False)
-    def remove(self, files, recursive=False, **kwargs):
-        """Remove files.
+    def cleanup(self) -> None:
+        self._out.close()
+        self._err.close()
+        out_name = self._out.name
+        err_name = self._err.name
+        from datalad import cfg
+        if cfg.getbool('datalad.log', 'outputs', default=False) \
+                and lgr.getEffectiveLevel() <= logging.DEBUG:
+            for s, sname in ((self.out, 'stdout'),
+                             (self.err, 'stderr')):
+                if s:
+                    pref = os.linesep + "| "
+                    lgr.debug("Swallowed %s:%s%s", sname, pref, s.replace(os.linesep, pref))
+                else:
+                    lgr.debug("Nothing was swallowed for %s", sname)
+        del self._out
+        del self._err
+        gc.collect()
+        rmtemp(out_name)
+        rmtemp(err_name)
+
+@contextmanager
+def swallow_outputs() -> Iterator[SwallowOutputsAdapter]:
+    """Context manager to help consuming both stdout and stderr, and print()
+
+    stdout is available as cm.out and stderr as cm.err whenever cm is the
+    yielded context manager.
+    Internally uses temporary files to guarantee absent side-effects of swallowing
+    into StringIO which lacks .fileno.
+
+    print mocking is necessary for some uses where sys.stdout was already bound
+    to original sys.stdout, thus mocking it later had no effect. Overriding
+    print function had desired effect
+    """
 
-        Calls git-rm.
+    def fake_print(*args: str, sep: str = ' ', end: str = "\n", file: Optional[IO[str]] = None) -> None:
+        if file is None:
+            file = sys.stdout
 
-        Parameters
-        ----------
-        files: str
-          list of paths to remove
-        recursive: False
-          either to allow recursive removal from subdirectories
-        kwargs:
-          see `__init__`
-
-        Returns
-        -------
-        [str]
-          list of successfully removed files.
-        """
+        if file in (oldout, olderr, sys.stdout, sys.stderr):
+            # we mock
+            try:
+                sys.stdout.write(sep.join(args) + end)
+            except UnicodeEncodeError as exc:
+                lgr.error(
+                    "Failed to write to mocked stdout, got %s, continue as it "
+                    "didn't happen",  exc)
+        else:
+            # must be some other file one -- leave it alone
+            oldprint(*args, sep=sep, end=end, file=file)
 
-        files = _remove_empty_items(files)
+    from .ui import ui
 
-        if recursive:
-            kwargs['r'] = True
-        stdout, stderr = self._git_custom_command(
-            files, ['git', 'rm'] + to_options(**kwargs))
+    # preserve -- they could have been mocked already
+    oldprint = getattr(builtins, 'print')
+    oldout, olderr = sys.stdout, sys.stderr
+    olduiout = ui.out
+    adapter = SwallowOutputsAdapter()
+
+    try:
+        sys.stdout, sys.stderr = adapter.handles
+        ui.out = adapter.handles[0]
+        setattr(builtins, 'print', fake_print)
+
+        yield adapter
+    finally:
+        sys.stdout, sys.stderr, ui.out = oldout, olderr, olduiout
+        setattr(builtins, 'print',  oldprint)
+        adapter.cleanup()
+
+
+# Let's log everything into a string
+# TODO: generalize with the one for swallow_outputs
+class SwallowLogsAdapter:
+    """Little adapter to help getting out values
 
-        # output per removed file is expected to be "rm 'PATH'":
-        return [line.strip()[4:-1] for line in stdout.splitlines()]
+    And to stay consistent with how swallow_outputs behaves
+    """
+    def __init__(self, file_: str | Path | None) -> None:
+        self._out: IO[str]
+        if file_ is None:
+            kw = get_tempfile_kwargs({}, prefix="logs")
+            self._out = NamedTemporaryFile(mode='a', delete=False, **kw)
+        else:
+            out_file = file_
+            # PY3 requires clearly one or another.  race condition possible
+            self._out = open(out_file, 'a')
+        self.file = file_
+        self._final_out: Optional[str] = None
+
+    def _read(self, h: IO[str]) -> str:
+        with open(h.name) as f:
+            return f.read()
 
-        #return self.repo.git.rm(files, cached=False, **kwargs)
+    @property
+    def out(self) -> str:
+        if self._final_out is not None:
+            # we closed and cleaned up already
+            return self._final_out
+        else:
+            self._out.flush()
+            return self._read(self._out)
 
-    def precommit(self):
-        """Perform pre-commit maintenance tasks
-        """
-        # All GitPython commands should take care about flushing index
-        # whenever they modify it, so we would not care to do anything
-        # if self.repo is not None and exists(opj(self.path, '.git')):  # don't try to write otherwise:
-        #     # flush possibly cached in GitPython changes to index:
-        #     # if self.repo.git:
-        #     #     sys.stderr.write("CLEARING\n")
-        #     #     self.repo.git.clear_cache()
-        #     self.repo.index.write()
-        pass
+    @property
+    def lines(self) -> list[str]:
+        return self.out.split('\n')
+
+    @property
+    def handle(self) -> IO[str]:
+        return self._out
+
+    def cleanup(self) -> None:
+        # store for access while object exists
+        self._final_out = self.out
+        self._out.close()
+        out_name = self._out.name
+        del self._out
+        gc.collect()
+        if not self.file:
+            rmtemp(out_name)
+
+    def assert_logged(self, msg: Optional[str]=None, level: Optional[str]=None, regex: bool =True, **kwargs: Any) -> None:
+        """Provide assertion on whether a msg was logged at a given level
 
-    @staticmethod
-    def _get_prefixed_commit_msg(msg):
-        DATALAD_PREFIX = "[DATALAD]"
-        return DATALAD_PREFIX if not msg else "%s %s" % (DATALAD_PREFIX, msg)
-
-    def commit(self, msg=None, options=None, _datalad_msg=False, careless=True,
-               files=None, date=None):
-        """Commit changes to git.
+        If neither `msg` nor `level` provided, checks if anything was logged
+        at all.
 
         Parameters
         ----------
         msg: str, optional
-          commit-message
-        options: list of str, optional
-          cmdline options for git-commit
-        _datalad_msg: bool, optional
-          To signal that commit is automated commit by datalad, so
-          it would carry the [DATALAD] prefix
-        careless: bool, optional
-          if False, raise when there's nothing actually committed;
-          if True, don't care
-        files: list of str, optional
-          path(s) to commit
-        date: str, optional
-          Date in one of the formats git understands
-        """
+          Message (as a regular expression, if `regex`) to be searched.
+          If no msg provided, checks if anything was logged at a given level.
+        level: str, optional
+          String representing the level to be logged
+        regex: bool, optional
+          If False, regular `assert_in` is used
+        **kwargs: str, optional
+          Passed to `assert_re_in` or `assert_in`
+        """
+        from datalad.tests.utils_pytest import (
+            assert_in,
+            assert_re_in,
+        )
 
-        self.precommit()
+        if regex:
+            match = r'\[%s\] ' % level if level else r"\[\S+\] "
+        else:
+            match = '[%s] ' % level if level else ''
 
-        if _datalad_msg:
-            msg = self._get_prefixed_commit_msg(msg)
+        if msg:
+            match += msg
 
-        options = options or []
+        if match:
+            (assert_re_in if regex else assert_in)(match, self.out, **kwargs)
+        else:
+            assert not kwargs, "no kwargs to be passed anywhere"
+            assert self.out, "Nothing was logged!?"
 
-        if not msg:
-            if options:
-                if "--allow-empty-message" not in options:
-                        options.append("--allow-empty-message")
-            else:
-                options = ["--allow-empty-message"]
 
-        if date:
-            options += ["--date", date]
-        # Note: We used to use a direct call to git only if there were options,
-        # since we can't pass all possible options to gitpython's implementation
-        # of commit.
-        # But there's an additional issue. GitPython implements commit in a way,
-        # that it might create a new commit, when a direct call wouldn't. This
-        # was discovered with a modified (but unstaged) submodule, leading to a
-        # commit, that apparently did nothing - git status still showed the very
-        # same thing afterwards. But a commit was created nevertheless:
-        # diff --git a/sub b/sub
-        # --- a/sub
-        # +++ b/sub
-        # @@ -1 +1 @@
-        # -Subproject commit d3935338a3b3735792de1078bbfb5e9913ef998f
-        # +Subproject commit d3935338a3b3735792de1078bbfb5e9913ef998f-dirty
-        #
-        # Therefore, for now always use direct call.
-        # TODO: Figure out, what exactly is going on with gitpython here
-
-        cmd = ['git', 'commit'] + (["-m", msg if msg else ""])
-        if options:
-            cmd.extend(options)
-        lgr.debug("Committing via direct call of git: %s" % cmd)
+@contextmanager
+def swallow_logs(new_level: str | int | None = None, file_ : str | Path | None = None, name: str='datalad') -> Iterator[SwallowLogsAdapter]:
+    """Context manager to consume all logs."""
+    lgr = logging.getLogger(name)
+
+    # Keep old settings
+    old_level = lgr.level
+    old_handlers = lgr.handlers
+
+    adapter = SwallowLogsAdapter(file_)
+    # TODO: it does store messages but without any formatting, i.e. even without
+    # date/time prefix etc.  IMHO it should preserve formatting in case if file_ is
+    # set
+    swallow_handler = logging.StreamHandler(adapter.handle)
+    # we want to log levelname so we could test against it
+    swallow_handler.setFormatter(
+        logging.Formatter('[%(levelname)s] %(message)s'))
+    swallow_handler.filters = sum([h.filters for h in old_handlers],
+                                  [])
+    lgr.handlers = [swallow_handler]
+    if old_level < logging.DEBUG:  # so if HEAVYDEBUG etc -- show them!
+        lgr.handlers += old_handlers
+
+    if isinstance(new_level, str):
+        new_level = getattr(logging, new_level)
+
+    if new_level is not None:
+        lgr.setLevel(new_level)
+
+    try:
+        yield adapter
+        # TODO: if file_ and there was an exception -- most probably worth logging it?
+        # although ideally it should be the next log outside added to that file_ ... oh well
+    finally:
+        lgr.handlers = old_handlers
+        lgr.setLevel(old_level)
+        adapter.cleanup()
+
+
+# TODO: May be melt in with swallow_logs at some point:
+@contextmanager
+def disable_logger(logger: Optional[logging.Logger]=None) -> Iterator[logging.Logger]:
+    """context manager to temporarily disable logging
 
-        try:
-            self._git_custom_command(files, cmd,
-                                     expect_stderr=True, expect_fail=True)
-        except CommandError as e:
-            if 'nothing to commit' in e.stdout:
-                if careless:
-                    lgr.debug("nothing to commit in {}. "
-                              "Ignored.".format(self))
-                else:
-                    raise
-            elif 'no changes added to commit' in e.stdout or \
-                    'nothing added to commit' in e.stdout:
-                if careless:
-                    lgr.debug("no changes added to commit in {}. "
-                              "Ignored.".format(self))
-                else:
-                    raise
-            elif "did not match any file(s) known to git." in e.stderr:
-                # TODO: Improve FileNotInXXXXError classes to better deal with
-                # multiple files; Also consider PathOutsideRepositoryError
-                raise FileNotInRepositoryError(cmd=e.cmd,
-                                               msg="File(s) unknown to git",
-                                               code=e.code,
-                                               filename=linesep.join(
-                                            [l for l in e.stderr.splitlines()
-                                             if l.startswith("pathspec")]))
-            else:
-                raise
+    This is to provide one of swallow_logs' purposes without unnecessarily
+    creating temp files (see gh-1865)
 
-    def get_indexed_files(self):
-        """Get a list of files in git's index
+    Parameters
+    ----------
+    logger: Logger
+        Logger whose handlers will be ordered to not log anything.
+        Default: datalad's topmost Logger ('datalad')
+    """
 
-        Returns
-        -------
-        list
-            list of paths rooting in git's base dir
+    class NullFilter(logging.Filter):
+        """Filter class to reject all records
         """
+        def filter(self, record: logging.LogRecord) -> bool:
+            return False
 
-        return [x[0] for x in self.cmd_call_wrapper(
-            self.repo.index.entries.keys)]
+    if logger is None:
+        # default: all of datalad's logging:
+        logger = logging.getLogger('datalad')
+
+    filter_ = NullFilter(logger.name)
+    for h in logger.handlers:
+        h.addFilter(filter_)
+
+    try:
+        yield logger
+    finally:
+        for h in logger.handlers:
+            h.removeFilter(filter_)
+
+
+@contextmanager
+def lock_if_required(lock_required: bool, lock: threading.Lock) -> Iterator[threading.Lock]:
+    """ Acquired and released the provided lock if indicated by a flag"""
+    if lock_required:
+        lock.acquire()
+    try:
+        yield lock
+    finally:
+        if lock_required:
+            lock.release()
 
-    def get_hexsha(self, object=None):
-        """Return a hexsha for a given object. If None - of current HEAD
 
-        Parameters
-        ----------
-        object: str, optional
-          Any type of Git object identifier. See `git show`.
-        """
-        cmd = ['git', 'show', '--no-patch', "--format=%H"]
-        if object:
-            cmd.append(object)
-        # make sure Git takes our argument as a revision
-        cmd.append('--')
-        try:
-            stdout, stderr = self._git_custom_command(
-                '', cmd, expect_stderr=True, expect_fail=True)
-        except CommandError as e:
-            if 'bad revision' in e.stderr:
-                raise ValueError("Unknown object identifier: %s" % object)
-            elif 'does not have any commits yet' in e.stderr:
-                return None
-            else:
-                raise e
-        stdout = stdout.splitlines()
-        assert(len(stdout) == 1)
-        return stdout[0]
-
-    def get_merge_base(self, treeishes):
-        """Get a merge base hexsha
+#
+# Additional handlers
+#
+def ensure_dir(*args: str) -> str:
+    """Make sure directory exists.
 
-        Parameters
-        ----------
-        treeishes: str or list of str
-          List of treeishes (branches, hexshas, etc) to determine the merge
-          base of. If a single value provided, returns merge_base with the
-          current branch.
-
-        Returns
-        -------
-        str or None
-          If no merge-base for given commits, or specified treeish doesn't
-          exist, None returned
-        """
-        if isinstance(treeishes, string_types):
-            treeishes = [treeishes]
-        if not treeishes:
-            raise ValueError("Provide at least a single value")
-        elif len(treeishes) == 1:
-            treeishes = treeishes + [self.get_active_branch()]
+    Joins the list of arguments to an os-specific path to the desired
+    directory and creates it, if it not exists yet.
+    """
+    dirname = op.join(*args)
+    if not exists(dirname):
+        os.makedirs(dirname)
+    return dirname
 
-        try:
-            bases = self.repo.merge_base(*treeishes)
-        except GitCommandError as exc:
-            if "fatal: Not a valid object name" in str(exc):
-                return None
-            raise
 
-        if not bases:
-            return None
-        assert(len(bases) == 1)  # we do not do 'all' yet
-        return bases[0].hexsha
+def updated(d: dict[K, V], update: dict[K, V]) -> dict[K, V]:
+    """Return a copy of the input with the 'update'
 
-    def get_commit_date(self, branch=None, date='authored'):
-        """Get the date stamp of the last commit (in a branch or head otherwise)
+    Primarily for updating dictionaries
+    """
+    d = d.copy()
+    d.update(update)
+    return d
+
+
+_pwd_mode: Optional[str] = None
+
+
+def _switch_to_getcwd(msg: str, *args: Any) -> None:
+    global _pwd_mode
+    _pwd_mode = 'cwd'
+    lgr.debug(
+        msg + ". From now on will be returning os.getcwd(). Directory"
+               " symlinks in the paths will be resolved",
+        *args
+    )
+    # TODO:  we might want to mitigate by going through all flywheighted
+    # repos and tuning up their .paths to be resolved?
+
+
+def getpwd() -> str:
+    """Try to return a CWD without dereferencing possible symlinks
+
+    This function will try to use PWD environment variable to provide a current
+    working directory, possibly with some directories along the path being
+    symlinks to other directories.  Unfortunately, PWD is used/set only by the
+    shell and such functions as `os.chdir` and `os.getcwd` nohow use or modify
+    it, thus `os.getcwd()` returns path with links dereferenced.
+
+    While returning current working directory based on PWD env variable we
+    verify that the directory is the same as `os.getcwd()` after resolving all
+    symlinks.  If that verification fails, we fall back to always use
+    `os.getcwd()`.
 
-        Parameters
-        ----------
-        date: {'authored', 'committed'}
-          Which date to return.  "authored" will be the date shown by "git show"
-          and the one possibly specified via --date to `git commit`
-
-        Returns
-        -------
-        int or None
-          None if no commit
-        """
+    Initial decision to either use PWD env variable or os.getcwd() is done upon
+    the first call of this function.
+    """
+    global _pwd_mode
+    if _pwd_mode is None:
+        # we need to decide!
         try:
-            if branch:
-                commit = next(self.get_branch_commits(branch))
+            pwd = os.environ['PWD']
+            if on_windows and pwd and pwd.startswith('/'):
+                # It should be a path from MSYS.
+                # - it might start with a drive letter or not
+                # - it seems to be "illegal" to have a single letter directories
+                #   under / path, i.e. if created - they aren't found
+                # - 'ln -s' does not fail to create a "symlink" but it just
+                # copies!
+                #   so we are not likely to need original PWD purpose on
+                # those systems
+                # Verdict:
+                _pwd_mode = 'cwd'
             else:
-                commit = self.repo.head.commit
-        except Exception as exc:
-            lgr.debug("Got exception while trying to get last commit: %s",
-                      exc_str(exc))
-            return None
-        return getattr(commit, "%s_date" % date)
-
-    def get_active_branch(self):
+                _pwd_mode = 'PWD'
+        except KeyError:
+            _pwd_mode = 'cwd'
+
+    if _pwd_mode == 'cwd':
+        return os.getcwd()
+    elif _pwd_mode == 'PWD':
         try:
-            branch = self.repo.active_branch.name
-        except TypeError as e:
-            if "HEAD is a detached symbolic reference" in str(e):
-                lgr.debug("detached HEAD in {0}".format(self))
-                return None
+            cwd = os.getcwd()
+        except OSError as exc:
+            if "o such file" in str(exc):
+                # directory was removed but we promised to be robust and
+                # still report the path we might know since we are still in PWD
+                # mode
+                cwd = None
             else:
                 raise
-        return branch
+        try:
+            pwd = os.environ['PWD']
+            # do absolute() in addition to always get an absolute path
+            # even with non-existing paths on windows
+            pwd_real = str(Path(pwd).resolve().absolute())
+            # This logic would fail to catch the case where chdir did happen
+            # to the directory where current PWD is pointing to, e.g.
+            # $> ls -ld $PWD
+            # lrwxrwxrwx 1 yoh yoh 5 Oct 11 13:27 /home/yoh/.tmp/tmp -> /tmp//
+            # hopa:~/.tmp/tmp
+            # $> python -c 'import os; os.chdir("/tmp"); from datalad.utils import getpwd; print(getpwd(), os.getcwd())'
+            # ('/home/yoh/.tmp/tmp', '/tmp')
+            # but I guess that should not be too harmful
+            if cwd is not None and pwd_real != cwd:
+                _switch_to_getcwd(
+                    "realpath of PWD=%s is %s whenever os.getcwd()=%s",
+                    pwd, pwd_real, cwd
+                )
+                return cwd
+            return pwd
+        except KeyError:
+            _switch_to_getcwd("PWD env variable is no longer available")
+            if cwd is not None:
+                return cwd  # Must not happen, but may be someone
+                            # evil purges PWD from environ?
+    raise RuntimeError(
+        "Must have not got here. "
+        "pwd_mode must be either cwd or PWD. And it is now %r" % (_pwd_mode,)
+    )
+
+
+class chpwd:
+    """Wrapper around os.chdir which also adjusts environ['PWD']
+
+    The reason is that otherwise PWD is simply inherited from the shell
+    and we have no ability to assess directory path without dereferencing
+    symlinks.
 
-    def get_branches(self):
-        """Get all branches of the repo.
+    If used as a context manager it allows to temporarily change directory
+    to the given path
+    """
+    def __init__(self, path: str | Path | None, mkdir: bool=False, logsuffix: str='') -> None:
 
-        Returns
-        -------
-        [str]
-            Names of all branches of this repository.
-        """
+        self._prev_pwd: Optional[str]
+        if path:
+            pwd = getpwd()
+            self._prev_pwd = pwd
+        else:
+            self._prev_pwd = None
+            return
 
-        return [branch.name for branch in self.repo.branches]
+        if not isabs(path):
+            path = normpath(op.join(pwd, path))
+        if not os.path.exists(path) and mkdir:
+            self._mkdir = True
+            os.mkdir(path)
+        else:
+            self._mkdir = False
+        lgr.debug("chdir %r -> %r %s", self._prev_pwd, path, logsuffix)
+        os.chdir(path)  # for grep people -- ok, to chdir here!
+        os.environ['PWD'] = str(path)
 
-    def get_remote_branches(self):
-        """Get all branches of all remotes of the repo.
+    def __enter__(self) -> None:
+        # nothing more to do really, chdir was in the constructor
+        pass
 
-        Returns
-        -----------
-        [str]
-            Names of all remote branches.
-        """
-        # TODO: Reconsider melting with get_branches()
+    def __exit__(self, exc_type: Optional[type[BaseException]], exc_val: Optional[BaseException], exc_tb: Optional[TracebackType]) -> None:
+        if self._prev_pwd:
+            # Need to use self.__class__ so this instance, if the entire
+            # thing mocked during the test, still would use correct chpwd
+            self.__class__(self._prev_pwd, logsuffix="(coming back)")
 
-        # TODO: treat entries like this: origin/HEAD -> origin/master'
-        # currently this is done in collection
 
-        # For some reason, this is three times faster than the version below:
-        remote_branches = list()
-        for remote in self.repo.remotes:
-            try:
-                for ref in remote.refs:
-                    remote_branches.append(ref.name)
-            except AssertionError as e:
-                if str(e).endswith("did not have any references"):
-                    # this will happen with git annex special remotes
-                    pass
-                else:
-                    raise e
-        return remote_branches
-        # return [branch.strip() for branch in
-        #         self.repo.git.branch(r=True).splitlines()]
+def dlabspath(path: str | Path, norm: bool =False) -> str:
+    """Symlinks-in-the-cwd aware abspath
 
-    def get_remotes(self, with_urls_only=False):
-        """Get known remotes of the repository
+    os.path.abspath relies on os.getcwd() which would not know about symlinks
+    in the path
 
-        Parameters
-        ----------
-        with_urls_only : bool, optional
-          return only remotes which have urls
+    TODO: we might want to norm=True by default to match behavior of
+    os .path.abspath?
+    """
+    if not isabs(path):
+        # if not absolute -- relative to pwd
+        path = op.join(getpwd(), path)
+    return normpath(path) if norm else str(path)
 
-        Returns
-        -------
-        remotes : list of str
-          List of names of the remotes
-        """
 
-        # Note: read directly from config and spare instantiation of gitpy.Repo
-        # since we need this in AnnexRepo constructor. Furthermore gitpy does it
-        # pretty much the same way and the use of a Repo instance seems to have
-        # no reason other than a nice object oriented look.
-        from datalad.utils import unique
-
-        self.config.reload()
-        remotes = unique([x[7:] for x in self.config.sections()
-                          if x.startswith("remote.")])
-
-        if with_urls_only:
-            remotes = [
-                r for r in remotes
-                if self.config.get('remote.%s.url' % r)
-            ]
-        return remotes
+def with_pathsep(path: str) -> str:
+    """Little helper to guarantee that path ends with /"""
+    return path + sep if not path.endswith(sep) else path
 
-    def get_files(self, branch=None):
-        """Get a list of files in git.
 
-        Lists the files in the (remote) branch.
+def get_path_prefix(path: str | Path, pwd: Optional[str]=None) -> str:
+    """Get path prefix (for current directory)
 
-        Parameters
-        ----------
-        branch: str
-          Name of the branch to query. Default: active branch.
+    Returns relative path to the topdir, if we are under topdir, and if not
+    absolute path to topdir.  If `pwd` is not specified - current directory
+    assumed
+    """
+    pwd = pwd or getpwd()
+    path = dlabspath(path)
+    path_ = with_pathsep(path)
+    pwd_ = with_pathsep(pwd)
+    common = commonprefix((path_, pwd_))
+    if common.endswith(sep) and common in {path_, pwd_}:
+        # we are in subdir or above the path = use relative path
+        location_prefix = relpath(path, pwd)
+        # if benign "here" - cut off
+        if location_prefix in (curdir, curdir + sep):
+            location_prefix = ''
+        return location_prefix
+    else:
+        # just return absolute path
+        return path
 
-        Returns
-        -------
-        [str]
-          list of files.
-        """
-        # TODO: RF codes base and melt get_indexed_files() in
 
-        if branch is None:
-            # active branch can be queried way faster:
-            return self.get_indexed_files()
-        else:
-            return [item.path for item in self.repo.tree(branch).traverse()
-                    if isinstance(item, Blob)]
+def _get_normalized_paths(path: str, prefix: str) -> tuple[str, str]:
+    if isabs(path) != isabs(prefix):
+        raise ValueError("Both paths must either be absolute or relative. "
+                         "Got %r and %r" % (path, prefix))
+    path = with_pathsep(path)
+    prefix = with_pathsep(prefix)
+    return path, prefix
 
-    def get_file_content(self, file_, branch='HEAD'):
-        """
 
-        Returns
-        -------
-        [str]
-          content of file_ as a list of lines.
-        """
-        content_str = self.repo.commit(branch).tree[file_].data_stream.read()
+def path_startswith(path: str, prefix: str) -> bool:
+    """Return True if path starts with prefix path
 
-        # in python3 a byte string is returned. Need to convert it:
-        from six import PY3
-        if PY3:
-            conv_str = u''
-            for b in bytes(content_str):
-                conv_str += chr(b)
-            return conv_str.splitlines()
-        else:
-            return content_str.splitlines()
-        # TODO: keep splitlines?
+    Parameters
+    ----------
+    path: str
+    prefix: str
+    """
+    path, prefix = _get_normalized_paths(path, prefix)
+    return path.startswith(prefix)
 
-    def _get_files_history(self, files, branch='HEAD'):
-        """
 
-        Parameters
-        ----------
-        files: list
-          list of files, only commits with queried files are considered
-        branch: str
-          Name of the branch to query. Default: HEAD.
-
-        Returns
-        -------
-        [iterator]
-        yielding Commit items generator from branch history associated with files
-        """
-        return gitpy.objects.commit.Commit.iter_items(self.repo, branch, paths=files)
+def path_is_subpath(path: str, prefix: str) -> bool:
+    """Return True if path is a subpath of prefix
 
-    def _get_remotes_having_commit(self, commit_hexsha, with_urls_only=True):
-        """Traverse all branches of the remote and check if commit in any of their ancestry
+    It will return False if path == prefix.
 
-        It is a generator yielding names of the remotes
-        """
-        out, err = self._git_custom_command(
-            '', 'git branch -r --contains ' + commit_hexsha
-        )
-        # sanitize a bit (all the spaces and new lines)
-        remote_branches = [
-            b  # could be origin/HEAD -> origin/master, we just skip ->
-            for b in filter(bool, out.split())
-            if b != '->'
-        ]
-        return [
-            remote
-            for remote in self.get_remotes(with_urls_only=with_urls_only)
-            if any(rb.startswith(remote + '/') for rb in remote_branches)
-        ]
-
-    def _gitpy_custom_call(self, cmd, cmd_args=None, cmd_options=None,
-                           git_options=None, env=None,
-
-                           # 'old' options for Runner; not sure yet, which of
-                           # them are actually still needed:
-                           log_stdout=True, log_stderr=True, log_online=False,
-                           expect_stderr=True, cwd=None,
-                           shell=None, expect_fail=False):
-
-        """Helper to call GitPython's wrapper for git calls.
-
-        The used instance of `gitpy.Git` is bound to the repository,
-        which determines its working directory.
-        This is used for adhoc implementation of a git command and to
-        demonstrate how to use it in more specific implementations.
-
-        Note
-        ----
-        Aims to replace the use of datalad's `Runner` class for direct git
-        calls. (Currently the `_git_custom_command()` method).
-        Therefore mimicking its behaviour during RF'ing.
+    Parameters
+    ----------
+    path: str
+    prefix: str
+    """
+    path, prefix = _get_normalized_paths(path, prefix)
+    return (len(prefix) < len(path)) and path.startswith(prefix)
 
-        Parameters
-        ----------
-        cmd: str
-          the native git command to call
-        cmd_args: list of str
-          arguments to the git command
-        cmd_options: dict
-          options for the command as key, value pair
-          (this transformation, needs some central place to document)
-        git_options: dict
-          options for the git executable as key, value pair
-          (see above)
-        env: dict
-          environment vaiables to temporarily set for this call
-
-        TODO
-        ----
-        Example
-
-        Returns
-        -------
-        (stdout, stderr)
-        """
 
-        # TODO: Reconsider when to log/stream what (stdout, stderr) and/or
-        # fully implement the behaviour of `Runner`
+def knows_annex(path: str | Path) -> bool:
+    """Returns whether at a given path there is information about an annex
 
-        if log_online:
-            raise NotImplementedError("option 'log_online' not implemented yet")
-        with_exceptions = not expect_fail
-        if cwd:
-            # the gitpy.cmd.Git instance, bound to this repository doesn't allow
-            # to explicitly set the working dir, except for using os.getcwd
-            raise NotImplementedError("working dir is a read-only property")
-
-        _tmp_shell = gitpy.cmd.Git.USE_SHELL
-        gitpy.cmd.Git.USE_SHELL = shell
-
-        if env is None:
-            env = {}
-        if git_options is None:
-            git_options = {}
-        if cmd_options is None:
-            cmd_options = {}
-        cmd_options.update({'with_exceptions': with_exceptions,
-                            'with_extended_output': True})
+    It is just a thin wrapper around GitRepo.is_with_annex() classmethod
+    which also checks for `path` to exist first.
 
-        # TODO: _GIT_COMMON_OPTIONS!
+    This includes actually present annexes, but also uninitialized ones, or
+    even the presence of a remote annex branch.
+    """
+    from os.path import exists
+    if not exists(path):
+        lgr.debug("No annex: test path %s doesn't exist", path)
+        return False
+    from datalad.support.gitrepo import GitRepo
+    return GitRepo(path, init=False, create=False).is_with_annex()
 
-        with self.repo.git.custom_environment(**env):
-            try:
-                status, std_out, std_err = \
-                    self.repo.git(**git_options).__getattr__(cmd)(
-                        cmd_args, **cmd_options)
-            except GitCommandError as e:
-                # For now just reraise. May be raise CommandError instead
-                raise
-            finally:
-                gitpy.cmd.Git.USE_SHELL = _tmp_shell
 
-        if not expect_stderr and std_err:
-            lgr.error("Unexpected output on stderr: %s" % std_err)
-            raise CommandError
-        if log_stdout:
-            for line in std_out.splitlines():
-                lgr.debug("stdout| " + line)
-        if log_stderr:
-            for line in std_err.splitlines():
-                lgr.log(level=logging.DEBUG if expect_stderr else logging.ERROR,
-                        msg="stderr| " + line)
-
-        return std_out, std_err
-
-    @normalize_paths(match_return_type=False)
-    def _git_custom_command(self, files, cmd_str,
-                            log_stdout=True, log_stderr=True, log_online=False,
-                            expect_stderr=True, cwd=None, env=None,
-                            shell=None, expect_fail=False):
-        """Allows for calling arbitrary commands.
-
-        Helper for developing purposes, i.e. to quickly implement git commands
-        for proof of concept without the need to figure out, how this is done
-        via GitPython.
+@contextmanager
+def make_tempfile(content: str | bytes | None = None, wrapped: Optional[Callable[..., Any]] = None, **tkwargs: Any) -> Iterator[str]:
+    """Helper class to provide a temporary file name and remove it at the end (context manager)
 
-        Parameters
-        ----------
-        files: list of files
-        cmd_str: str or list
-            arbitrary command str. `files` is appended to that string.
-
-        Returns
-        -------
-        stdout, stderr
-        """
+    Parameters
+    ----------
+    mkdir : bool, optional (default: False)
+        If True, temporary directory created using tempfile.mkdtemp()
+    content : str or bytes, optional
+        Content to be stored in the file created
+    wrapped : function, optional
+        If set, function name used to prefix temporary file name
+    `**tkwargs`:
+        All other arguments are passed into the call to tempfile.mk{,d}temp(),
+        and resultant temporary filename is passed as the first argument into
+        the function t.  If no 'prefix' argument is provided, it will be
+        constructed using module and function names ('.' replaced with
+        '_').
+
+    To change the used directory without providing keyword argument 'dir' set
+    DATALAD_TESTS_TEMP_DIR.
+
+    Examples
+    --------
+        >>> from os.path import exists
+        >>> from datalad.utils import make_tempfile
+        >>> with make_tempfile() as fname:
+        ...    k = open(fname, 'w').write('silly test')
+        >>> assert not exists(fname)  # was removed
+
+        >>> with make_tempfile(content="blah") as fname:
+        ...    assert open(fname).read() == "blah"
+    """
+
+    if tkwargs.get('mkdir', None) and content is not None:
+        raise ValueError("mkdir=True while providing content makes no sense")
 
-        # ensure cmd_str becomes a well-formed list:
-        if isinstance(cmd_str, string_types):
-            if files and not cmd_str.strip().endswith(" --"):
-                cmd_str += " --"
-            cmd_str = shlex.split(cmd_str, posix=not on_windows)
+    tkwargs_ = get_tempfile_kwargs(tkwargs, wrapped=wrapped)
+
+    # if DATALAD_TESTS_TEMP_DIR is set, use that as directory,
+    # let mktemp handle it otherwise. However, an explicitly provided
+    # dir=... will override this.
+    mkdir = bool(tkwargs_.pop('mkdir', False))
+
+    filename = {False: tempfile.mktemp,
+                True: tempfile.mkdtemp}[mkdir](**tkwargs_)
+    # MIH: not clear to me why we need to perform this (possibly expensive)
+    # resolve. It was already part of the original implementation
+    # 008d9ab8cc3e0170c0a9b8479e80dee9ffe6eb7f
+    filepath = Path(filename).resolve()
+
+    if content:
+        if isinstance(content, bytes):
+            filepath.write_bytes(content)
         else:
-            if files and cmd_str[-1] != '--':
-                cmd_str.append('--')
+            filepath.write_text(content)
 
-        cmd = cmd_str + files
+    # TODO globbing below can also be done with pathlib
+    filename = str(filepath)
 
-        assert(cmd[0] == 'git')
-        cmd = cmd[:1] + self._GIT_COMMON_OPTIONS + cmd[1:]
+    if __debug__:
+        lgr.debug(
+            'Created temporary %s named %s',
+            'directory' if mkdir else 'file',
+            filename)
+    try:
+        yield filename
+    finally:
+        # glob here for all files with the same name (-suffix)
+        # would be useful whenever we requested .img filename,
+        # and function creates .hdr as well
+        # MIH: this is undocumented behavior, and undesired in the general
+        # case. it should be made conditional and explicit
+        lsuffix = len(tkwargs_.get('suffix', ''))
+        filename_ = lsuffix and filename[:-lsuffix] or filename
+        filenames = glob.glob(filename_ + '*')
+        if len(filename_) < 3 or len(filenames) > 5:
+            # For paranoid yoh who stepped into this already ones ;-)
+            lgr.warning("It is unlikely that it was intended to remove all"
+                        " files matching %r. Skipping" % filename_)
+            return
+        for f in filenames:
+            try:
+                rmtemp(f)
+            except OSError:  # pragma: no cover
+                pass
 
-        from .exceptions import GitIgnoreError
 
-        try:
-            out, err = self.cmd_call_wrapper.run(
-                cmd,
-                log_stderr=log_stderr,
-                log_stdout=log_stdout,
-                log_online=log_online,
-                expect_stderr=expect_stderr,
-                cwd=cwd,
-                env=env,
-                shell=shell,
-                expect_fail=expect_fail)
-        except CommandError as e:
-            ignored = re.search(GitIgnoreError.pattern, e.stderr)
-            if ignored:
-                raise GitIgnoreError(cmd=e.cmd, msg=e.stderr,
-                                     code=e.code, stdout=e.stdout,
-                                     stderr=e.stderr,
-                                     paths=ignored.groups()[0].splitlines())
-            raise
-        return out, err
+def _path_(*p: str) -> str:
+    """Given a path in POSIX notation, regenerate one in native to the env one"""
+    if on_windows:
+        return op.join(*map(lambda x: op.join(*x.split('/')), p))
+    else:
+        # Assume that all others as POSIX compliant so nothing to be done
+        return op.join(*p)
 
-# TODO: --------------------------------------------------------------------
 
-    def add_remote(self, name, url, options=None):
-        """Register remote pointing to a url
-        """
-        cmd = ['git', 'remote', 'add']
-        if options:
-            cmd += options
-        cmd += [name, url]
-
-        result = self._git_custom_command('', cmd)
-        self.config.reload()
-        return result
+def get_timestamp_suffix(time_: int | time.struct_time | None=None, prefix: str='-') -> str:
+    """Return a time stamp (full date and time up to second)
+
+    primarily to be used for generation of log files names
+    """
+    args = []
+    if time_ is not None:
+        if isinstance(time_, int):
+            time_ = time.gmtime(time_)
+        args.append(time_)
+    return time.strftime(prefix + TIMESTAMP_FMT, *args)
+
+
+# unused in -core
+def get_logfilename(dspath: str | Path, cmd: str='datalad') -> str:
+    """Return a filename to use for logging under a dataset/repository
+
+    directory would be created if doesn't exist, but dspath must exist
+    and be a directory
+    """
+    assert(exists(dspath))
+    assert(isdir(dspath))
+    ds_logdir = ensure_dir(str(dspath), '.git', 'datalad', 'logs')  # TODO: use WEB_META_LOG whenever #789 merged
+    return op.join(ds_logdir, 'crawl-%s.log' % get_timestamp_suffix())
 
-    def remove_remote(self, name):
-        """Remove existing remote
-        """
 
-        # TODO: testing and error handling!
-        from .exceptions import RemoteNotAvailableError
+def get_trace(edges: Sequence[tuple[T, T]], start: T, end: T, trace: Optional[list[T]]=None) -> Optional[list[T]]:
+    """Return the trace/path to reach a node in a tree.
+
+    Parameters
+    ----------
+    edges : sequence(2-tuple)
+      The tree given by a sequence of edges (parent, child) tuples. The
+      nodes can be identified by any value and data type that supports
+      the '==' operation.
+    start :
+      Identifier of the start node. Must be present as a value in the parent
+      location of an edge tuple in order to be found.
+    end :
+      Identifier of the target/end node. Must be present as a value in the child
+      location of an edge tuple in order to be found.
+    trace : list
+      Mostly useful for recursive calls, and used internally.
+
+    Returns
+    -------
+    None or list
+      Returns a list with the trace to the target (the starts and the target
+      are not included in the trace, hence if start and end are directly connected
+      an empty list is returned), or None when no trace to the target can be found,
+      or start and end are identical.
+    """
+    # the term trace is used to avoid confusion with a path in the sense
+    # of a filesystem path, but the analogy fits and nodes can be paths
+    if trace is None:
+        trace = []
+    if not edges:
+        raise ValueError("no edges given")
+    for cand in edges:
+        cand_super, cand_sub = cand
+        if cand_sub in trace:
+            # only DAGs, skip any cyclic traces
+            continue
+        if trace and cand_super != trace[-1]:
+            # only consider edges that lead off the end of the trace
+            continue
+        if not trace and cand_super != start:
+            # we got nothing yet, and this edges is not matching the start
+            continue
+        if cand_sub == end:
+            return trace
+        # dive into potential subnodes
+        cand_trace = get_trace(
+            edges,
+            start,
+            end,
+            trace + [cand_sub])
+        if cand_trace:
+            return cand_trace
+    return None
+
+
+def get_dataset_root(path: str | Path) -> Optional[str]:
+    """Return the root of an existent dataset containing a given path
+
+    The root path is returned in the same absolute or relative form
+    as the input argument. If no associated dataset exists, or the
+    input path doesn't exist, None is returned.
+
+    If `path` is a symlink or something other than a directory, its
+    the root dataset containing its parent directory will be reported.
+    If none can be found, at a symlink at `path` is pointing to a
+    dataset, `path` itself will be reported as the root.
+
+    Parameters
+    ----------
+    path : Path-like
+
+    Returns
+    -------
+    str or None
+    """
+
+    # NOTE: path = "" is effectively "."
+
+    path = str(path)
+    suffix = '.git'
+    altered = None
+    if islink(path) or not isdir(path):
+        altered = path
+        path = dirname(path)
+    apath = abspath(path)
+    # while we can still go up
+    while split(apath)[1]:
+        if exists(op.join(path, suffix)):
+            return path
+        # new test path in the format we got it
+        path = normpath(op.join(path, os.pardir))
+        # no luck, next round
+        apath = abspath(path)
+    # if we applied dirname() at the top, we give it another go with
+    # the actual path, if it was itself a symlink, it could be the
+    # top-level dataset itself
+    if altered and exists(op.join(altered, suffix)):
+        return altered
+
+    return None
+
+
+# ATM used in datalad_crawler extension, so do not remove yet
+def try_multiple(ntrials: int, exception: type[BaseException], base: float, f: Callable[P, T], *args: P.args, **kwargs: P.kwargs) -> T:
+    """Call f multiple times making exponentially growing delay between the calls"""
+    for trial in range(1, ntrials+1):
         try:
-            out, err = self._git_custom_command(
-                '', ['git', 'remote', 'remove', name])
-        except CommandError as e:
-            if 'fatal: No such remote' in e.stderr:
-                raise RemoteNotAvailableError(name,
-                                              cmd="git remote remove",
-                                              msg="No such remote",
-                                              stdout=out,
-                                              stderr=err)
-            else:
-                raise e
+            return f(*args, **kwargs)
+        except exception as exc:
+            if trial == ntrials:
+                raise  # just reraise on the last trial
+            t = base ** trial
+            lgr.warning("Caught %s on trial #%d. Sleeping %f and retrying",
+                        CapturedException(exc), trial, t)
+            sleep(t)
+    raise ValueError("ntrials must be > 0")
 
-        # TODO: config.reload necessary?
-        self.config.reload()
-        return
 
-    def update_remote(self, name=None, verbose=False):
-        """
-        """
-        options = ["-v"] if verbose else []
-        name = [name] if name else []
-        self._git_custom_command(
-            '', ['git', 'remote'] + name + ['update'] + options,
-            expect_stderr=True
-        )
+@optional_args
+def try_multiple_dec(
+        f: Callable[P, T],
+        ntrials: Optional[int] = None,
+        duration: float = 0.1,
+        exceptions: type[BaseException] | tuple[type[BaseException], ...] | None = None,
+        increment_type: Literal["exponential"] | None = None,
+        exceptions_filter: Optional[Callable[[BaseException], Any]] = None,
+        logger: Optional[Callable] = None,
+) -> Callable[P, T]:
+    """Decorator to try function multiple times.
+
+    Main purpose is to decorate functions dealing with removal of files/directories
+    and which might need a few seconds to work correctly on Windows which takes
+    its time to release files/directories.
 
-    # TODO: centralize all the c&p code in fetch, pull, push
-    # TODO: document **kwargs passed to gitpython
-    def fetch(self, remote=None, refspec=None, progress=None, all_=False,
-              **kwargs):
-        """Fetches changes from a remote (or all_ remotes).
+    Parameters
+    ----------
+    ntrials: int, optional
+    duration: float, optional
+      Seconds to sleep before retrying.
+    increment_type: {None, 'exponential'}
+      Note that if it is exponential, duration should typically be > 1.0
+      so it grows with higher power
+    exceptions: Exception or tuple of Exceptions, optional
+      Exception or a tuple of multiple exceptions, on which to retry
+    exceptions_filter: callable, optional
+      If provided, this function will be called with a caught exception
+      instance.  If function returns True - we will re-try, if False - exception
+      will be re-raised without retrying.
+    logger: callable, optional
+      Logger to log upon failure.  If not provided, will use stock logger
+      at the level of 5 (heavy debug).
+    """
+    # We need to bind these to new names so that mypy doesn't complain about
+    # the values possibly being `None` inside the inner function:
+    exceptions_: type[BaseException] | tuple[type[BaseException], ...]
+    if not exceptions:
+        exceptions_ = (OSError, PermissionError) if on_windows else OSError
+    else:
+        exceptions_ = exceptions
+    if not ntrials:
+        # Life goes fast on proper systems, no need to delay it much
+        ntrials_ = 100 if on_windows else 10
+    else:
+        ntrials_ = ntrials
+    if logger is None:
+        def logger_(*args: Any, **kwargs: Any) -> None:
+            return lgr.log(5, *args, **kwargs)
+    else:
+        logger_ = logger
+    assert increment_type in {None, 'exponential'}
 
-        Parameters
-        ----------
-        remote: str
-          (optional) name of the remote to fetch from. If no remote is given and
-          `all_` is not set, the tracking branch is fetched.
-        refspec: str
-          (optional) refspec to fetch.
-        progress:
-          passed to gitpython. TODO: Figure it out, make consistent use of it
-          and document it.
-        all_: bool
-          fetch all_ remotes (and all_ of their branches).
-          Fails if `remote` was given.
-        kwargs:
-          passed to gitpython. TODO: Figure it out, make consistent use of it
-          and document it.
-
-        Returns
-        -------
-        list
-            FetchInfo objects of the items fetched from remote
-        """
-        # TODO: options=> **kwargs):
-        # Note: Apparently there is no explicit (fetch --all) in gitpython,
-        #       but fetch is always bound to a certain remote instead.
-        #       Therefore implement it on our own:
-        if remote is None:
-            if refspec is not None:
-                # conflicts with using tracking branch or fetch all remotes
-                # For now: Just fail.
-                # TODO: May be check whether it fits to tracking branch
-                raise ValueError("refspec specified without a remote. (%s)" %
-                                 refspec)
-            if all_:
-                remotes_to_fetch = [
-                    self.repo.remote(r)
-                    for r in self.get_remotes(with_urls_only=True)
-                ]
-            else:
-                # No explicit remote to fetch.
-                # => get tracking branch:
-                tb_remote, refspec = self.get_tracking_branch()
-                if tb_remote is not None:
-                    remotes_to_fetch = [self.repo.remote(tb_remote)]
+    @wraps(f)
+    def _wrap_try_multiple_dec(*args: P.args, **kwargs: P.kwargs) -> T:
+        t = duration
+        for trial in range(ntrials_):
+            try:
+                return f(*args, **kwargs)
+            except exceptions_ as exc:
+                if exceptions_filter and not exceptions_filter(exc):
+                    raise
+                if trial < ntrials_ - 1:
+                    if increment_type == 'exponential':
+                        t = duration ** (trial + 1)
+                    logger_(
+                        "Caught %s on trial #%d. Sleeping %f and retrying",
+                        CapturedException(exc), trial, t)
+                    sleep(t)
                 else:
-                    # No remote, no tracking branch
-                    # => fail
-                    raise ValueError("Neither a remote is specified to fetch "
-                                     "from nor a tracking branch is set up.")
-        else:
-            remotes_to_fetch = [self.repo.remote(remote)]
+                    raise
+        raise ValueError("ntrials must be > 0")
 
-        fi_list = []
-        for rm in remotes_to_fetch:
-            fetch_url = \
-                self.config.get('remote.%s.fetchurl' % rm.name,
-                                self.config.get('remote.%s.url' % rm.name,
-                                                None))
-            if fetch_url is None:
-                lgr.debug("Remote %s has no URL", rm)
-                return []
-
-            if is_ssh(fetch_url):
-                ssh_manager.get_connection(fetch_url).open()
-                # TODO: with git <= 2.3 keep old mechanism:
-                #       with rm.repo.git.custom_environment(GIT_SSH="wrapper_script"):
-                with rm.repo.git.custom_environment(**GitRepo.GIT_SSH_ENV):
-                    fi_list += rm.fetch(refspec=refspec, progress=progress, **kwargs)
-                    # TODO: progress +kwargs
-            else:
-                fi_list += rm.fetch(refspec=refspec, progress=progress, **kwargs)
-                # TODO: progress +kwargs
+    return _wrap_try_multiple_dec
 
-        # TODO: fetch returns a list of FetchInfo instances. Make use of it.
-        return fi_list
 
-    def pull(self, remote=None, refspec=None, progress=None, **kwargs):
-        """See fetch
-        """
-        if remote is None:
-            if refspec is not None:
-                # conflicts with using tracking branch or fetch all remotes
-                # For now: Just fail.
-                # TODO: May be check whether it fits to tracking branch
-                raise ValueError("refspec specified without a remote. (%s)" %
-                                 refspec)
-            # No explicit remote to pull from.
-            # => get tracking branch:
-            tb_remote, refspec = self.get_tracking_branch()
-            if tb_remote is not None:
-                remote = self.repo.remote(tb_remote)
-            else:
-                # No remote, no tracking branch
-                # => fail
-                raise ValueError("No remote specified to pull from nor a "
-                                 "tracking branch is set up.")
+@try_multiple_dec
+def unlink(f: str | Path) -> None:
+    """'Robust' unlink.  Would try multiple times
 
-        else:
-            remote = self.repo.remote(remote)
+    On windows boxes there is evidence for a latency of more than a second
+    until a file is considered no longer "in-use".
+    WindowsError is not known on Linux, and if IOError or any other
+    exception
+    is thrown then if except statement has WindowsError in it -- NameError
+    also see gh-2533
+    """
+    # Check for open files
+    assert_no_open_files(f)
+    return os.unlink(f)
 
-        fetch_url = \
-            remote.config_reader.get(
-                'fetchurl' if remote.config_reader.has_option('fetchurl')
-                else 'url')
-        if is_ssh(fetch_url):
-            ssh_manager.get_connection(fetch_url).open()
-            # TODO: with git <= 2.3 keep old mechanism:
-            #       with remote.repo.git.custom_environment(GIT_SSH="wrapper_script"):
-            with remote.repo.git.custom_environment(**GitRepo.GIT_SSH_ENV):
-                return remote.pull(refspec=refspec, progress=progress, **kwargs)
-                # TODO: progress +kwargs
-        else:
-            return remote.pull(refspec=refspec, progress=progress, **kwargs)
-            # TODO: progress +kwargs
 
-    def push(self, remote=None, refspec=None, progress=None, all_remotes=False,
-             **kwargs):
-        """Push to remote repository
+@try_multiple_dec
+def _rmtree(*args: Any, **kwargs: Any) -> None:
+    """Just a helper to decorate shutil.rmtree.
+
+    rmtree defined above does more and ideally should not itself be decorated
+    since a recursive definition and does checks for open files inside etc -
+    might be too runtime expensive
+    """
+    shutil.rmtree(*args, **kwargs)
 
-        Parameters
-        ----------
-        remote: str
-          name of the remote to push to
-        refspec: str
-          specify what to push
-        progress:
-          TODO
-        all_remotes: bool
-          if set to True push to all remotes. Conflicts with `remote` not being
-          None.
-        kwargs: dict
-          options to pass to `git push`
-
-        Returns
-        -------
-        list
-            PushInfo objects of the items pushed to remote
-        """
 
-        if remote is None:
-            if refspec is not None:
-                # conflicts with using tracking branch or fetch all remotes
-                # For now: Just fail.
-                # TODO: May be check whether it fits to tracking branch
-                raise ValueError("refspec specified without a remote. (%s)" %
-                                 refspec)
-            if all_remotes:
-                remotes_to_push = self.repo.remotes
-            else:
-                # Nothing explicitly specified. Just call `git push` and let git
-                # decide what to do would be an option. But:
-                # - without knowing the remote and its URL we cannot provide
-                #   shared SSH connection
-                # - we lose ability to use GitPython's progress info and return
-                #   values
-                #   (the latter would be solvable:
-                #    Provide a Repo.push() method for GitPython, copying
-                #    Remote.push() for similar return value and progress
-                #    (also: fetch, pull)
-
-                # Do what git would do:
-                # 1. branch.*.remote for current branch or 'origin' as default
-                #    if config is missing
-                # 2. remote.*.push or push.default
-
-                # TODO: check out "same procedure" for fetch/pull
-
-                tb_remote, refspec = self.get_tracking_branch()
-                if tb_remote is None:
-                    tb_remote = 'origin'
-                remotes_to_push = [self.repo.remote(tb_remote)]
-                # use no refspec; let git find remote.*.push or push.default on
-                # its own
+def slash_join(base: Optional[str], extension: Optional[str]) -> Optional[str]:
+    """Join two strings with a '/', avoiding duplicate slashes
 
-        else:
-            if all_remotes:
-                lgr.warning("Option 'all_remotes' conflicts with specified "
-                            "remote '%s'. Option ignored.")
-            remotes_to_push = [self.repo.remote(remote)]
-
-        pi_list = []
-        for rm in remotes_to_push:
-            push_url = \
-                rm.config_reader.get('pushurl'
-                                     if rm.config_reader.has_option('pushurl')
-                                     else 'url')
-            if is_ssh(push_url):
-                ssh_manager.get_connection(push_url).open()
-                # TODO: with git <= 2.3 keep old mechanism:
-                #       with rm.repo.git.custom_environment(GIT_SSH="wrapper_script"):
-                with rm.repo.git.custom_environment(**GitRepo.GIT_SSH_ENV):
-                    pi_list += rm.push(refspec=refspec, progress=progress, **kwargs)
-                    # TODO: progress +kwargs
-            else:
-                pi_list += rm.push(refspec=refspec, progress=progress, **kwargs)
-                # TODO: progress +kwargs
-        return pi_list
+    If any of the strings is None the other is returned as is.
+    """
+    if extension is None:
+        return base
+    if base is None:
+        return extension
+    return '/'.join(
+        (base.rstrip('/'),
+         extension.lstrip('/')))
 
-    def get_remote_url(self, name, push=False):
-        """Get the url of a remote.
 
-        Reads the configuration of remote `name` and returns its url or None,
-        if there is no url configured.
+#
+# IO Helpers
+#
 
-        Parameters
-        ----------
-        name: str
-          name of the remote
-        push: bool
-          if True, get the pushurl instead of the fetch url.
-        """
+# unused in -core
+def open_r_encdetect(fname: str | Path, readahead: int=1000) -> IO[str]:
+    """Return a file object in read mode with auto-detected encoding
 
-        var = 'remote.{0}.{1}'.format(name, 'pushurl' if push else 'url')
-        return self.config.get(var, None)
+    This is helpful when dealing with files of unknown encoding.
 
-    def set_remote_url(self, name, url, push=False):
-        """Set the URL a remote is pointing to
+    Parameters
+    ----------
+    readahead: int, optional
+      How many bytes to read for guessing the encoding type.  If
+      negative - full file will be read
+    """
+    import io
 
-        Sets the URL of the remote `name`. Requires the remote to already exist.
+    from chardet import detect
 
-        Parameters
-        ----------
-        name: str
-          name of the remote
-        url: str
-        push: bool
-          if True, set the push URL, otherwise the fetch URL
-        """
+    # read some bytes from the file
+    with open(fname, 'rb') as f:
+        head = f.read(readahead)
+    enc = detect(head)
+    denc = enc.get('encoding', None)
+    lgr.debug("Auto-detected encoding %s for file %s (confidence: %s)",
+              denc,
+              fname,
+              enc.get('confidence', 'unknown'))
+    return io.open(fname, encoding=denc)
+
+
+@overload
+def read_file(fname: str | Path, decode: Literal[True] =True) -> str:
+    ...
+
+@overload
+def read_file(fname: str | Path, decode: Literal[False]) -> bytes:
+    ...
 
-        var = 'remote.{0}.{1}'.format(name, 'pushurl' if push else 'url')
-        self.config.set(var, url, where='local', reload=True)
+def read_file(fname: str | Path, decode: Literal[True, False] =True) -> str | bytes:
+    """A helper to read file passing content via ensure_unicode
 
-    def get_branch_commits(self, branch=None, limit=None, stop=None, value=None):
-        """Return GitPython's commits for the branch
+    Parameters
+    ----------
+    decode: bool, optional
+      if False, no ensure_unicode and file content returned as bytes
+    """
+    with open(fname, 'rb') as f:
+        content = f.read()
+    return ensure_unicode(content) if decode else content
 
-        Pretty much similar to what 'git log <branch>' does.
-        It is a generator which returns top commits first
 
-        Parameters
-        ----------
-        branch: str, optional
-          If not provided, assumes current branch
-        limit: None | 'left-only', optional
-          Limit which commits to report.  If None -- all commits (merged or not),
-          if 'left-only' -- only the commits from the left side of the tree upon
-          merges
-        stop: str, optional
-          hexsha of the commit at which stop reporting (matched one is not
-          reported either)
-        value: None | 'hexsha', optional
-          What to yield.  If None - entire commit object is yielded, if 'hexsha'
-          only its hexsha
-        """
+def read_csv_lines(fname: str | Path, dialect: Optional[str] = None, readahead: int=16384, **kwargs: Any) -> Iterator[dict[str, str]]:
+    """A generator of dict records from a CSV/TSV
 
-        if not branch:
-            branch = self.get_active_branch()
-
-        try:
-            _branch = self.repo.branches[branch]
-        except IndexError:
-            raise MissingBranchError(self, branch,
-                                     [b.name for b in self.repo.branches])
-
-        fvalue = {None: lambda x: x, 'hexsha': lambda x: x.hexsha}[value]
-
-        if not limit:
-            def gen():
-                # traverse doesn't yield original commit
-                co = _branch.commit
-                yield co
-                for co_ in co.traverse():
-                    yield co_
-        elif limit == 'left-only':
-            # we need a custom implementation since couldn't figure out how to
-            # do with .traversal
-            def gen():
-                co = _branch.commit
-                while co:
-                    yield co
-                    co = co.parents[0] if co.parents else None
-        else:
-            raise ValueError(limit)
+    Automatically guesses the encoding for each record to convert to UTF-8
 
-        for c in gen():
-            if stop and c.hexsha == stop:
-                return
-            yield fvalue(c)
+    Parameters
+    ----------
+    fname: str
+      Filename
+    dialect: str, optional
+      Dialect to specify to csv.reader. If not specified -- guessed from
+      the file, if fails to guess, "excel-tab" is assumed
+    readahead: int, optional
+      How many bytes to read from the file to guess the type
+    **kwargs
+      Passed to `csv.reader`
+    """
+    import csv
+    csv_dialect: str | type[csv.Dialect]
+    if dialect is None:
+        with open(fname) as tsvfile:
+            # add robustness, use a sniffer
+            try:
+                csv_dialect = csv.Sniffer().sniff(tsvfile.read(readahead))
+            except Exception as exc:
+                lgr.warning(
+                    'Could not determine file-format, assuming TSV: %s',
+                    CapturedException(exc)
+                )
+                csv_dialect = 'excel-tab'
+    else:
+        csv_dialect = dialect
 
-    def checkout(self, name, options=None):
-        """
-        """
-        # TODO: May be check for the need of -b options herein?
-        cmd = ['git', 'checkout']
-        if options:
-            cmd += options
-        cmd += [str(name)]
-
-        self._git_custom_command('', cmd, expect_stderr=True)
-
-    # TODO: Before implementing annex merge, find usages and check for a needed
-    # change to call super().merge
-    def merge(self, name, options=None, msg=None, allow_unrelated=False, **kwargs):
-        if options is None:
-            options = []
-        if msg:
-            options = options + ["-m", msg]
-        if allow_unrelated and external_versions['cmd:git'] >= '2.9':
-            options += ['--allow-unrelated-histories']
-        self._git_custom_command(
-            '', ['git', 'merge'] + options + [name],
+    with open(fname, 'r', encoding="utf-8") as tsvfile:
+        csv_reader = csv.reader(
+            tsvfile,
+            dialect=csv_dialect,
             **kwargs
         )
+        header: Optional[list[str]] = None
+        for row in csv_reader:
+            if header is None:
+                header = row
+            else:
+                yield dict(zip(header, row))
 
-    def remove_branch(self, branch):
-        self._git_custom_command(
-            '', ['git', 'branch', '-D', branch]
-        )
 
-    def ls_remote(self, remote, options=None):
-        if options is None:
-            options = []
-        self._git_custom_command(
-            '', ['git', 'ls-remote'] + options + [remote]
-        )
-        # TODO: Return values?
+def import_modules(modnames: Iterable[str], pkg: str, msg: str="Failed to import {module}", log: Callable[[str], Any]=lgr.debug) -> list[ModuleType]:
+    """Helper to import a list of modules without failing if N/A
 
-    def is_dirty(self, index=True, working_tree=True, untracked_files=True,
-                 submodules=True, path=None):
-        """Returns true if the repo is considered to be dirty
+    Parameters
+    ----------
+    modnames: list of str
+      List of module names to import
+    pkg: str
+      Package under which to import
+    msg: str, optional
+      Message template for .format() to log at DEBUG level if import fails.
+      Keys {module} and {package} will be provided and ': {exception}' appended
+    log: callable, optional
+      Logger call to use for logging messages
+    """
+    from importlib import import_module
+    _globals = globals()
+    mods_loaded = []
+    if pkg and not pkg in sys.modules:
+        # with python 3.5.1 (ok with 3.5.5) somehow kept running into
+        #  Failed to import dlsub1: Parent module 'dltestm1' not loaded
+        # while running the test. Preloading pkg resolved the issue
+        import_module(pkg)
+    for modname in modnames:
+        try:
+            _globals[modname] = mod = import_module(
+                '.{}'.format(modname),
+                pkg)
+            mods_loaded.append(mod)
+        except Exception as exc:
+            from datalad.support.exceptions import CapturedException
+            ce = CapturedException(exc)
+            log((msg + ': {exception}').format(
+                module=modname, package=pkg, exception=ce.message))
+    return mods_loaded
 
-        Parameters
-        ----------
-        index: bool
-          if True, consider changes to the index
-        working_tree: bool
-          if True, consider changes to the working tree
-        untracked_files: bool
-          if True, consider untracked files
-        submodules: bool
-          if True, consider submodules
-        path: str or list of str
-          path(s) to consider only
-        Returns
-        -------
-          bool
-        """
 
-        return self.repo.is_dirty(index=index, working_tree=working_tree,
-                                  untracked_files=untracked_files,
-                                  submodules=submodules, path=path)
+def import_module_from_file(modpath: str, pkg: Optional[ModuleType]=None, log: Callable[[str], Any]=lgr.debug) -> ModuleType:
+    """Import provided module given a path
+
+    TODO:
+    - RF/make use of it in pipeline.py which has similar logic
+    - join with import_modules above?
 
-    @property
-    def dirty(self):
-        return self.is_dirty()
+    Parameters
+    ----------
+    pkg: module, optional
+       If provided, and modpath is under pkg.__path__, relative import will be
+       used
+    """
+    assert(modpath.endswith('.py'))  # for now just for .py files
 
-    @property
-    def untracked_files(self):
-        return self.repo.untracked_files
+    log("Importing %s" % modpath)
 
-    def gc(self, allow_background=False, auto=False):
-        """Perform house keeping (garbage collection, repacking)"""
-        cmd_options = ['git']
-        if not allow_background:
-            cmd_options += ['-c', 'gc.autodetach=0']
-        cmd_options += ['gc', '--aggressive']
-        if auto:
-            cmd_options += ['--auto']
-        self._git_custom_command('', cmd_options)
-
-    def get_submodules(self, sorted_=True):
-        """Return a list of git.Submodule instances for all submodules"""
-        # check whether we have anything in the repo. if not go home early
-        if not self.repo.head.is_valid():
-            return []
-        submodules = self.repo.submodules
-        if sorted_:
-            submodules = sorted(submodules, key=lambda x: x.path)
-        return submodules
-
-    def is_submodule_modified(self, name, options=[]):
-        """Whether a submodule has new commits
-
-        Note: This is an adhoc method. It parses output of
-        'git submodule summary' and currently is not able to distinguish whether
-        or not this change is staged in `self` and whether this would be
-        reported 'added' or 'modified' by 'git status'.
-        Parsing isn't heavily tested yet.
+    modname = basename(modpath)[:-3]
+    relmodpath = None
+    if pkg:
+        for pkgpath in pkg.__path__:
+            if path_is_subpath(modpath, pkgpath):
+                # for now relying on having .py extension -- assertion above
+                relmodpath = '.' + relpath(modpath[:-3], pkgpath).replace(sep, '.')
+                break
+
+    try:
+        if relmodpath:
+            from importlib import import_module
+            mod = import_module(relmodpath, pkg.__name__ if pkg is not None else None)
+        else:
+            dirname_ = dirname(modpath)
+            try:
+                sys.path.insert(0, dirname_)
+                mod = __import__(modname, level=0)
+            finally:
+                if dirname_ in sys.path:
+                    sys.path.pop(sys.path.index(dirname_))
+                else:
+                    log("Expected path %s to be within sys.path, but it was gone!" % dirname_)
+    except Exception as e:
+        raise RuntimeError(
+            "Failed to import module from %s" % modpath) from e
+
+    return mod
+
+
+def get_encoding_info() -> dict[str, str]:
+    """Return a dictionary with various encoding/locale information"""
+    import locale
+    import sys
+    return dict([
+        ('default', sys.getdefaultencoding()),
+        ('filesystem', sys.getfilesystemencoding()),
+        ('locale.prefered', locale.getpreferredencoding()),
+    ])
+
+
+def get_envvars_info() -> dict[str, str]:
+    envs = []
+    for var, val in os.environ.items():
+        if (
+                var.startswith('PYTHON') or
+                var.startswith('LC_') or
+                var.startswith('GIT_') or
+                var in ('LANG', 'LANGUAGE', 'PATH')
+        ):
+            envs.append((var, val))
+    return dict(envs)
+
+
+# This class is modified from Snakemake (v5.1.4)
+class SequenceFormatter(string.Formatter):
+    """string.Formatter subclass with special behavior for sequences.
+
+    This class delegates formatting of individual elements to another
+    formatter object. Non-list objects are formatted by calling the
+    delegate formatter's "format_field" method. List-like objects
+    (list, tuple, set, frozenset) are formatted by formatting each
+    element of the list according to the specified format spec using
+    the delegate formatter and then joining the resulting strings with
+    a separator (space by default).
+    """
+
+    def __init__(self, separator: str=" ", element_formatter: string.Formatter =string.Formatter(),
+                 *args: Any, **kwargs: Any) -> None:
+        self.separator = separator
+        self.element_formatter = element_formatter
+
+    def format_element(self, elem: Any, format_spec: str) -> Any:
+        """Format a single element
+
+        For sequences, this is called once for each element in a
+        sequence. For anything else, it is called on the entire
+        object. It is intended to be overridden in subclases.
+        """
+        return self.element_formatter.format_field(elem, format_spec)
+
+    def format_field(self, value: Any, format_spec: str) -> Any:
+        if isinstance(value, (list, tuple, set, frozenset)):
+            return self.separator.join(self.format_element(v, format_spec)
+                                       for v in value)
+        else:
+            return self.format_element(value, format_spec)
 
-        Parameters
-        ----------
-        name: str
-          the submodule's name
-        options: list
-          options to pass to 'git submodule summary'
-        Returns
-        -------
-        bool
-          True if there are commits in the submodule, differing from
-          what is registered in `self`
-        --------
-        """
 
-        out, err = self._git_custom_command('',
-                                            ['git', 'submodule', 'summary'] + \
-                                            options + ['--', name])
-        return any([line.split()[1] == name
-                    for line in out.splitlines()
-                    if line and len(line.split()) > 1])
-
-    def add_submodule(self, path, name=None, url=None, branch=None):
-        """Add a new submodule to the repository.
-
-        This will alter the index as well as the .gitmodules file, but will not
-        create a new commit.  If the submodule already exists, no matter if the
-        configuration differs from the one provided, the existing submodule
-        is considered as already added and no further action is performed.
+# TODO: eventually we might want to make use of attr module
+class File:
+    """Helper for a file entry in the create_tree/@with_tree
+
+    It allows to define additional settings for entries
+    """
+    def __init__(self, name: str, executable: bool=False) -> None:
+        """
 
         Parameters
         ----------
-        path : str
-          repository-relative path at which the submodule should be located, and
-          which will be created as required during the repository initialization.
-        name : str or None
-          name/identifier for the submodule. If `None`, the `path` will be used
-          as name.
-        url : str or None
-          git-clone compatible URL. If `None`, the repository is assumed to
-          exist, and the url of the first remote is taken instead. This is
-          useful if you want to make an existing repository a submodule of
-          another one.
-        branch : str or None
-          name of branch to be checked out in the submodule. The given branch
-          must exist in the remote repository, and will be checked out locally
-          as a tracking branch. If `None`, remote HEAD will be checked out.
+        name : str
+          Name of the file
+        executable: bool, optional
+          Make it executable
         """
-        if name is None:
-            name = path
-        # XXX the following should do it, but GitPython will refuse to add a submodule
-        # unless you specify a URL that is configured as one of its remotes, or you
-        # specify no URL, but the repo has at least one remote.
-        # this is stupid, as for us it is valid to not have any remote, because we can
-        # still obtain the submodule from a future publication location, based on the
-        # parent
-        # gitpy.Submodule.add(self.repo, name, path, url=url, branch=branch)
-        # going git native instead
-        cmd = ['git', 'submodule', 'add', '--name', name]
-        if branch is not None:
-            cmd += ['-b', branch]
-        if url is None:
-            if not isabs(path):
-                # need to recode into a relative path "URL" in POSIX
-                # style, even on windows
-                url = posixpath.join(curdir, posix_relpath(path))
+        self.name = name
+        self.executable = executable
+
+    def __str__(self) -> str:
+        return self.name
+
+
+TreeSpec = Union[
+    Tuple[Tuple[Union[str, File], "Load"], ...],
+    List[Tuple[Union[str, File], "Load"]],
+    Dict[Union[str, File], "Load"],
+]
+
+Load = Union[str, bytes, "TreeSpec"]
+
+
+def create_tree_archive(path: str, name: str, load: TreeSpec, overwrite: bool=False, archives_leading_dir: bool=True) -> None:
+    """Given an archive `name`, create under `path` with specified `load` tree
+    """
+    from datalad.support.archives import compress_files
+    dirname = file_basename(name)
+    full_dirname = op.join(path, dirname)
+    os.makedirs(full_dirname)
+    create_tree(full_dirname, load, archives_leading_dir=archives_leading_dir)
+    # create archive
+    if archives_leading_dir:
+        compress_files([dirname], name, path=path, overwrite=overwrite)
+    else:
+        compress_files(
+            # <https://github.com/python/mypy/issues/9864>
+            list(map(basename, glob.glob(op.join(full_dirname, '*')))),  # type: ignore[arg-type]
+                       op.join(pardir, name),
+                       path=op.join(path, dirname),
+                       overwrite=overwrite)
+    # remove original tree
+    rmtree(full_dirname)
+
+
+def create_tree(path: str, tree: TreeSpec, archives_leading_dir: bool =True, remove_existing: bool =False) -> None:
+    """Given a list of tuples (name, load) create such a tree
+
+    if load is a tuple itself -- that would create either a subtree or an archive
+    with that content and place it into the tree if name ends with .tar.gz
+    """
+    lgr.log(5, "Creating a tree under %s", path)
+    if not exists(path):
+        os.makedirs(path)
+
+    if isinstance(tree, dict):
+        tree = list(tree.items())
+
+    for file_, load in tree:
+        if isinstance(file_, File):
+            executable = file_.executable
+            name = file_.name
+        else:
+            executable = False
+            name = file_
+        full_name = op.join(path, name)
+        if remove_existing and lexists(full_name):
+            rmtree(full_name, chmod_files=True)
+        if isinstance(load, (tuple, list, dict)):
+            if name.endswith('.tar.gz') or name.endswith('.tar') or name.endswith('.zip'):
+                create_tree_archive(
+                    path, name, load,
+                    archives_leading_dir=archives_leading_dir)
+            else:
+                create_tree(
+                    full_name, load,
+                    archives_leading_dir=archives_leading_dir,
+                    remove_existing=remove_existing)
+        else:
+            if full_name.endswith('.gz'):
+                def open_func() -> IO[bytes]:
+                    return gzip.open(full_name, "wb")  # type: ignore[return-value]
+            elif full_name.split('.')[-1] in ('xz', 'lzma'):
+                import lzma
+                def open_func() -> IO[bytes]:
+                    return lzma.open(full_name, "wb")
             else:
-                url = path
-        cmd += [url, path]
-        self._git_custom_command('', cmd)
-        # TODO: return value
+                def open_func() -> IO[bytes]:
+                    return open(full_name, "wb")
+            with open_func() as f:
+                f.write(ensure_bytes(load, 'utf-8'))
+        if executable:
+            os.chmod(full_name, os.stat(full_name).st_mode | stat.S_IEXEC)
 
-    def deinit_submodule(self, path, **kwargs):
-        """Deinit a submodule
 
-        Parameters
-        ----------
-        path: str
-            path to the submodule; relative to `self.path`
-        kwargs:
-            see `__init__`
-        """
+def get_suggestions_msg(values: Optional[str | Iterable[str]], known: str, sep: str="\n        ") -> str:
+    """Return a formatted string with suggestions for values given the known ones
+    """
+    import difflib
+    suggestions = []
+    if not values:
+        values = []
+    elif isinstance(values, str):
+        values = [values]
+    for value in values:  # might not want to do it if we change presentation below
+        suggestions += difflib.get_close_matches(value, known)
+    suggestions = unique(suggestions)
+    msg = "Did you mean any of these?"
+    if suggestions:
+        if '\n' in sep:
+            # if separator includes new line - we add entire separator right away
+            msg += sep
+        else:
+            msg += ' '
+        return msg + "%s\n" % sep.join(suggestions)
+    return ''
 
-        self._git_custom_command(path,
-                                 ['git', 'submodule', 'deinit'] +
-                                 to_options(**kwargs))
-        # TODO: return value
-
-    def update_submodule(self, path, mode='checkout', init=False):
-        """Update a registered submodule.
-
-        This will make the submodule match what the superproject expects by
-        cloning missing submodules and updating the working tree of the
-        submodules. The "updating" can be done in several ways depending
-        on the value of submodule.<name>.update configuration variable, or
-        the `mode` argument.
 
-        Parameters
-        ----------
-        path : str
-          Identifies which submodule to operate on by it's repository-relative
-          path.
-        mode : {checkout, rebase, merge}
-          Update procedure to perform. 'checkout': the commit recorded in the
-          superproject will be checked out in the submodule on a detached HEAD;
-          'rebase': the current branch of the submodule will be rebased onto
-          the commit recorded in the superproject; 'merge': the commit recorded
-          in the superproject will be merged into the current branch in the
-          submodule.
-        init : bool
-          If True, initialize all submodules for which "git submodule init" has
-          not been called so far before updating.
-          Primarily provided for internal purposes and should not be used directly
-          since would result in not so annex-friendly .git symlinks/references
-          instead of full featured .git/ directories in the submodules
-        """
-        cmd = ['git', 'submodule', 'update', '--%s' % mode]
-        if init:
-            cmd.append('--init')
-            subgitpath = opj(self.path, path, '.git')
-            if not exists(subgitpath):
-                # TODO:  wouldn't with --init we get all those symlink'ed .git/?
-                # At least let's warn
-                lgr.warning(
-                    "Do not use update_submodule with init=True to avoid git creating "
-                    "symlinked .git/ directories in submodules"
-                )
-            #  yoh: I thought I saw one recently but thought it was some kind of
-            #  an artifact from running submodule update --init manually at
-            #  some point, but looking at this code now I worry that it was not
-        self._git_custom_command(path, cmd)
-        # TODO: return value
+def bytes2human(n: int | float, format: str ='%(value).1f %(symbol)sB') -> str:
+    """
+    Convert n bytes into a human readable string based on format.
+    symbols can be either "customary", "customary_ext", "iec" or "iec_ext",
+    see: http://goo.gl/kTQMs
+
+      >>> from datalad.utils import bytes2human
+      >>> bytes2human(1)
+      '1.0 B'
+      >>> bytes2human(1024)
+      '1.0 KB'
+      >>> bytes2human(1048576)
+      '1.0 MB'
+      >>> bytes2human(1099511627776127398123789121)
+      '909.5 YB'
+
+      >>> bytes2human(10000, "%(value).1f %(symbol)s/sec")
+      '9.8 K/sec'
+
+      >>> # precision can be adjusted by playing with %f operator
+      >>> bytes2human(10000, format="%(value).5f %(symbol)s")
+      '9.76562 K'
+
+    Taken from: http://goo.gl/kTQMs and subsequently simplified
+    Original Author: Giampaolo Rodola' <g.rodola [AT] gmail [DOT] com>
+    License: MIT
+    """
+    n = int(n)
+    if n < 0:
+        raise ValueError("n < 0")
+    symbols = ('', 'K', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y')
+    prefix = {}
+    for i, s in enumerate(symbols[1:]):
+        prefix[s] = 1 << (i + 1) * 10
+    for symbol in reversed(symbols[1:]):
+        if n >= prefix[symbol]:
+            value = float(n) / prefix[symbol]
+            return format % locals()
+    return format % dict(symbol=symbols[0], value=n)
+
+
+def quote_cmdlinearg(arg: str) -> str:
+    """Perform platform-appropriate argument quoting"""
+    # https://stackoverflow.com/a/15262019
+    return '"{}"'.format(
+        arg.replace('"', '""')
+    ) if on_windows else shlex_quote(arg)
+
+
+def guard_for_format(arg: str) -> str:
+    """Replace { and } with {{ and }}
+
+    To be used in cases if arg is not expected to have provided
+    by user .format() placeholders, but 'arg' might become a part
+    of a composite passed to .format(), e.g. via 'Run'
+    """
+    return arg.replace('{', '{{').replace('}', '}}')
 
-    def update_ref(self, ref, value, symbolic=False):
-        """Update the object name stored in a ref "safely".
 
-        Just a shim for `git update-ref` call if not symbolic, and
-        `git symbolic-ref` if symbolic
+def join_cmdline(args: Iterable[str]) -> str:
+    """Join command line args into a string using quote_cmdlinearg
+    """
+    return ' '.join(map(quote_cmdlinearg, args))
 
-        Parameters
-        ----------
-        ref : str
-          Reference, such as `ref/heads/BRANCHNAME` or HEAD.
-        value : str
-          Value to update to, e.g. hexsha of a commit when updating for a
-          branch ref, or branch ref if updating HEAD
-        symbolic : None
-          To instruct if ref is symbolic, e.g. should be used in case of
-          ref=HEAD
-        """
-        self._git_custom_command(
-            '',
-            ['git', 'symbolic-ref' if symbolic else 'update-ref', ref, value]
-        )
 
-    def tag(self, tag, message=None):
-        """Assign a tag to current commit
+def split_cmdline(s: str) -> list[str]:
+    """Perform platform-appropriate command line splitting.
 
-        Parameters
-        ----------
-        tag : str
-          Custom tag label.
-        message : str, optional
-          If provided, would create an annotated tag with that message
-        """
-        # TODO later to be extended with tagging particular commits and signing
-        # TODO: call in save.py complains about extensive logging. When does it
-        # happen in what way? Figure out, whether to just silence it or raise or
-        # whatever else.
-        options = []
-        if message:
-            options += ['-m', message]
-        self._git_custom_command(
-            '', ['git', 'tag'] + options + [str(tag)]
-        )
+    Identical to `shlex.split()` on non-windows platforms.
 
-    def get_tags(self, output=None):
-        """Get list of tags
+    Modified from https://stackoverflow.com/a/35900070
+    """
+    if not on_windows:
+        return shlex_split(s)
 
-        Parameters
-        ----------
-        output : str, optional
-          If given, limit the return value to a list of values matching that
-          particular key of the tag properties.
-
-        Returns
-        -------
-        list
-          Each item is a dictionary with information on a tag. At present
-          this includes 'hexsha', and 'name', where the latter is the string
-          label of the tag, and the format the hexsha of the object the tag
-          is attached to. The list is sorted by commit date, with the most
-          recent commit being the last element.
-        """
-        tag_objs = sorted(
-            self.repo.tags,
-            key=lambda t: t.commit.committed_date
-        )
-        tags = [
-            {
-                'name': t.name,
-                'hexsha': t.commit.hexsha
-             }
-            for t in tag_objs
-        ]
-        if output:
-            return [t[output] for t in tags]
+    # the rest is for windows
+    RE_CMD_LEX = r'''"((?:""|\\["\\]|[^"])*)"?()|(\\\\(?=\\*")|\\")|(&&?|\|\|?|\d?>|[<])|([^\s"&|<>]+)|(\s+)|(.)'''
+
+    args = []
+    accu = None   # collects pieces of one arg
+    for qs, qss, esc, pipe, word, white, fail in re.findall(RE_CMD_LEX, s):
+        if word:
+            pass   # most frequent
+        elif esc:
+            word = esc[1]
+        elif white or pipe:
+            if accu is not None:
+                args.append(accu)
+            if pipe:
+                args.append(pipe)
+            accu = None
+            continue
+        elif fail:
+            raise ValueError("invalid or incomplete shell string")
+        elif qs:
+            word = qs.replace('\\"', '"').replace('\\\\', '\\')
+            if platform == 0:
+                word = word.replace('""', '"')
         else:
-            return tags
+            word = qss   # may be even empty; must be last
 
-    def describe(self, **kwargs):
-        """ Quick and dirty implementation to call git-describe
+        accu = (accu or '') + word
 
-        Parameters:
-        -----------
-        kwargs:
-            transformed to cmdline options for git-describe;
-            see __init__ for description of the transformation
-        """
-        # TODO: be more precise what failure to expect when and raise actual
-        # errors
-        try:
-            describe, outerr = self._git_custom_command(
-                [],
-                ['git', 'describe'] + to_options(**kwargs),
-                expect_fail=True)
-            return describe.strip()
-        # TODO: WTF "catch everything"?
-        except:
-            return None
+    if accu is not None:
+        args.append(accu)
 
-    def get_tracking_branch(self, branch=None):
-        """Get the tracking branch for `branch` if there is any.
+    return args
 
-        Parameters
-        ----------
-        branch: str
-            local branch to look up. If none is given, active branch is used.
 
-        Returns
-        -------
-        tuple
-            (remote or None, refspec or None) of the tracking branch
-        """
-        if branch is None:
-            branch = self.get_active_branch()
-            if branch is None:
-                return None, None
-
-        track_remote = self.config.get('branch.{0}.remote'.format(branch), None)
-        track_branch = self.config.get('branch.{0}.merge'.format(branch), None)
-        return track_remote, track_branch
+def get_wrapped_class(wrapped: Callable) -> type:
+    """Determine the command class a wrapped __call__ belongs to"""
+    mod = sys.modules[wrapped.__module__]
+    command_class_name = wrapped.__qualname__.split('.')[-2]
+    _func_class = mod.__dict__[command_class_name]
+    lgr.debug("Determined class of decorated function: %s", _func_class)
+    return _func_class
 
-    @property
-    def count_objects(self):
-        """return dictionary with count, size(in KiB) information of git objects
-        """
 
-        count_cmd = ['git', 'count-objects', '-v']
-        count_str, err = self._git_custom_command('', count_cmd)
-        count = {key: int(value)
-                 for key, value in [item.split(': ')
-                                    for item in count_str.split('\n')
-                                    if len(item.split(': ')) == 2]}
-        return count
-
-    def get_missing_files(self):
-        """Return a list of paths with missing files (and no staged deletion)"""
-        return [f.split('\t')[1]
-                for f in self.repo.git.diff('--raw', '--name-status').split('\n')
-                if f.split('\t')[0] == 'D']
-
-    def get_deleted_files(self):
-        """Return a list of paths with deleted files (staged deletion)"""
-        return [f.split('\t')[1]
-                for f in self.repo.git.diff('--raw', '--name-status', '--staged').split('\n')
-                if f.split('\t')[0] == 'D']
+def _make_assure_kludge(fn: Callable[P, T]) -> Callable[P, T]:
+    old_name = fn.__name__.replace("ensure", "assure")
 
-    def get_git_attributes(self):
-        """Check git attribute for the current repository (not per-file support for now)
+    @wraps(fn)
+    def compat_fn(*args: P.args, **kwargs: P.kwargs) -> T:
+        warnings.warn(
+            "{} is deprecated and will be removed in a future release. "
+            "Use {} instead."
+            .format(old_name, fn.__name__),
+            DeprecationWarning)
+        return fn(*args, **kwargs)
 
-        Parameters
-        ----------
-        all_: bool
-          Adds --all to git check-attr call
+    compat_fn.__doc__ = ("Note: This function is deprecated. Use {} instead."
+                         .format(fn.__name__))
+    return compat_fn
 
-        Returns
-        -------
-        dict:
-          attribute: value pairs
-        """
-        out, err = self._git_custom_command(["."], ["git", "check-attr", "--all"])
-        assert not err, "no stderr output is expected"
-        out_split = [
-            # splitting by : would leave leading space(s)
-            [e.lstrip(' ') for e in l.split(':', 2)]
-            for l in out.split('\n') if l
-        ]
-        assert all(o[0] == '.' for o in out_split)  # for paranoid
-        return dict(o[1:] for o in out_split)
 
+assure_tuple_or_list = _make_assure_kludge(ensure_tuple_or_list)
+assure_iter = _make_assure_kludge(ensure_iter)
+assure_list = _make_assure_kludge(ensure_list)
+assure_list_from_str = _make_assure_kludge(ensure_list_from_str)
+assure_dict_from_str = _make_assure_kludge(ensure_dict_from_str)
+assure_bytes = _make_assure_kludge(ensure_bytes)
+assure_unicode = _make_assure_kludge(ensure_unicode)
+assure_bool = _make_assure_kludge(ensure_bool)
+assure_dir = _make_assure_kludge(ensure_dir)
+
+
+lgr.log(5, "Done importing datalad.utils")
+
+
+def check_symlink_capability(path: Path, target: Path) -> bool:
+    """helper similar to datalad.tests.utils_pytest.has_symlink_capability
+
+    However, for use in a datalad command context, we shouldn't
+    assume to be able to write to tmpfile and also not import a whole lot from
+    datalad's test machinery. Finally, we want to know, whether we can create a
+    symlink at a specific location, not just somewhere. Therefore use
+    arbitrary path to test-build a symlink and delete afterwards. Suitable
+    location can therefore be determined by high lever code.
+
+    Parameters
+    ----------
+    path: Path
+    target: Path
+
+    Returns
+    -------
+    bool
+    """
+
+    try:
+        target.touch()
+        path.symlink_to(target)
+        return True
+    except Exception:
+        return False
+    finally:
+        if path.exists():
+            path.unlink()
+        if target.exists():
+            target.unlink()
+
+
+def obtain_write_permission(path: Path) -> Optional[int]:
+    """Obtains write permission for `path` and returns previous mode if a
+    change was actually made.
+
+    Parameters
+    ----------
+    path: Path
+      path to try to obtain write permission for
+
+    Returns
+    -------
+    int or None
+      previous mode of `path` as return by stat().st_mode if a change in
+      permission was actually necessary, `None` otherwise.
+    """
 
-# TODO
-# remove submodule: nope, this is just deinit_submodule + remove
-# status?
+    mode = path.stat().st_mode
+    # only IWRITE works on Windows, in principle
+    if not mode & stat.S_IWRITE:
+        path.chmod(mode | stat.S_IWRITE)
+        return mode
+    else:
+        return None
+
+
+@contextmanager
+def ensure_write_permission(path: Path) -> Iterator[None]:
+    """Context manager to get write permission on `path` and
+    restore original mode afterwards.
+
+    Parameters
+    ----------
+    path: Path
+      path to the target file
+
+    Raises
+    ------
+    PermissionError
+       if write permission could not be obtained
+    """
+
+    restore = None
+    try:
+        restore = obtain_write_permission(path)
+        yield
+    finally:
+        if restore is not None:
+            try:
+                path.chmod(restore)
+            except FileNotFoundError:
+                # If `path` was deleted within the context block, there's
+                # nothing to do. Don't test exists(), though - asking for
+                # forgiveness to save a call.
+                pass
```

### Comparing `datalad-0.9.3/datalad/support/status.py` & `datalad-1.0.0/datalad/support/status.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 """(comparable) descriptors of the file status
```

### Comparing `datalad-0.9.3/datalad/support/third/nda_aws_token_generator.py` & `datalad-1.0.0/datalad/support/third/nda_aws_token_generator.py`

 * *Files 9% similar despite different names*

```diff
@@ -18,37 +18,37 @@
 
 class NDATokenGenerator(object):
     __schemas = {
         'soap': 'http://schemas.xmlsoap.org/soap/envelope/',
         'data': 'http://gov/nih/ndar/ws/datamanager/server/bean/jaxb'
     }
 
-    def __init__(self, url='https://ndar.nih.gov/DataManager/dataManager'):
+    def __init__(self, url):
         assert url is not None
         self.url = url
-        logging.debug('constructed with url %s' % url)
+        logging.debug('constructed with url %s', url)
 
     def generate_token(self, username, password):
         logging.info('request to generate AWS token')
         encoded_password = self.__encode_password(password)
         request_xml = self.__construct_request_xml(username, encoded_password)
         return self.__make_request(request_xml)
 
     def __encode_password(self, password):
         logging.debug('encoding password')
         hasher = hashlib.sha1()
         hasher.update(password.encode('utf-8'))
         digest_bytes = hasher.digest()
         byte_string = binascii.hexlify(digest_bytes)
         output = byte_string.decode('utf-8')
-        logging.debug('encoded password hash: %s' % output)
+        logging.debug('encoded password hash: %s', output)
         return output
 
     def __construct_request_xml(self, username, encoded_password):
-        logging.debug('constructing request with %s - %s' % (username, encoded_password))
+        logging.debug('constructing request with %s - %s', username, encoded_password)
         soap_schema = self.__schemas['soap']
         datamanager_schema = self.__schemas['data']
 
         element = etree.Element('{%s}Envelope' % soap_schema)
         body = etree.SubElement(element, '{%s}Body' % soap_schema)
         userelement = etree.SubElement(body, '{%s}UserElement' % datamanager_schema)
 
@@ -65,15 +65,15 @@
         uid = etree.SubElement(user, "threshold")
         uid.text = '0'
 
         logging.debug(etree.tostring(element))
         return etree.tostring(element)
 
     def __make_request(self, request_message):
-        logging.debug('making post request to %s' % self.url)
+        logging.debug('making post request to %s', self.url)
 
         headers = {
             'SOAPAction': '"generateToken"',
             'Content-Type': 'text/xml; charset=utf-8'
         }
 
         request = urllib_request.Request(self.url, data=request_message, headers=headers)
@@ -84,15 +84,15 @@
     def __parse_response(self, response):
         logging.debug('parsing response')
         tree = etree.fromstring(response)
 
         error = tree.find('.//errorMessage')
         if error is not None:
             error_msg = error.text
-            logging.error('response had error message: %s' % error_msg)
+            logging.error('response had error message: %s', error_msg)
             raise Exception(error_msg)
         generated_token = tree[0][0]
         token_elements = [e.text for e in generated_token[0:4]]
         token = Token(*token_elements)
         return token
```

### Comparing `datalad-0.9.3/datalad/support/nda_.py` & `datalad-1.0.0/datalad/support/nda_.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 """Various supporting utilities to interface with NIMH Data Archive (NDA)
```

### Comparing `datalad-0.9.3/datalad/support/sshconnector.py` & `datalad-1.0.0/datalad/support/external_versions.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,480 +1,435 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""Interface to an ssh connection.
-
-Allows for connecting via ssh and keeping the connection open
-(by using a controlmaster), in order to perform several ssh commands or
-git calls to a ssh remote without the need to reauthenticate.
+"""Module to help maintain a registry of versions for external modules etc
 """
-
-import logging
-from socket import gethostname
-from hashlib import md5
-from os import remove
-from os.path import exists
-from os.path import join as opj
-from subprocess import Popen
-# importing the quote function here so it can always be imported from this
-# module
-try:
-    # from Python 3.3 onwards
-    from shlex import quote as sh_quote
-except ImportError:
-    # deprecated since Python 2.7
-    from pipes import quote as sh_quote
-
-# !!! Do not import network here -- delay import, allows to shave off 50ms or so
-# on initial import datalad time
-# from datalad.support.network import RI, is_ssh
-
-from datalad.support.exceptions import CommandError
-from datalad.dochelpers import exc_str
-from datalad.utils import not_supported_on_windows
-from datalad.utils import assure_dir
-from datalad.utils import auto_repr
-from datalad.cmd import Runner
-
-lgr = logging.getLogger('datalad.support.sshconnector')
-
-
-def get_connection_hash(hostname, port='', username=''):
-    """Generate a hash based on SSH connection properties
-
-    This can be used for generating filenames that are unique
-    to a connection from and to a particular machine (with
-    port and login username). The hash also contains the local
-    host name.
-    """
-    # returning only first 8 characters to minimize our chance
-    # of hitting a limit on the max path length for the Unix socket.
-    # Collisions would be very unlikely even if we used less than 8.
-    # References:
-    #  https://github.com/ansible/ansible/issues/11536#issuecomment-153030743
-    #  https://github.com/datalad/datalad/pull/1377
-    return md5(
-        '{lhost}{rhost}{port}{username}'.format(
-            lhost=gethostname(),
-            rhost=hostname,
-            port=port,
-            username=username).encode('utf-8')).hexdigest()[:8]
-
-
-@auto_repr
-class SSHConnection(object):
-    """Representation of a (shared) ssh connection.
+import os.path as op
+import re
+import sys
+from itertools import chain
+from os import linesep
+
+from looseversion import LooseVersion
+
+# import version helper from config to have only one implementation
+# config needs this to avoid circular imports
+from datalad.config import get_git_version as __get_git_version
+from datalad.log import lgr
+
+from .exceptions import (
+    CapturedException,
+    CommandError,
+)
+
+__all__ = ['UnknownVersion', 'ExternalVersions', 'external_versions']
+
+
+# To depict an unknown version, which can't be compared by mistake etc
+class UnknownVersion:
+    """For internal use
     """
 
-    def __init__(self, ctrl_path, sshri):
-        """Create a connection handler
+    def __str__(self):
+        return "UNKNOWN"
 
-        The actual opening of the connection is performed on-demand.
+    def __cmp__(self, other):
+        if other is self:
+            return 0
+        raise TypeError("UNKNOWN version is not comparable")
 
-        Parameters
-        ----------
-        ctrl_path: str
-          path to SSH controlmaster
-        sshri: SSHRI
-          SSH resource identifier (contains all connection-relevant info),
-          or another resource identifier that can be converted into an SSHRI.
-        """
-        self._runner = None
 
-        from datalad.support.network import SSHRI, is_ssh
-        if not is_ssh(sshri):
-            raise ValueError(
-                "Non-SSH resource identifiers are not supported for SSH "
-                "connections: {}".format(sshri))
-        self.sshri = SSHRI(**{k: v for k, v in sshri.fields.items()
-                              if k in ('username', 'hostname', 'port')})
-        self.ctrl_path = ctrl_path
-        self._ctrl_options = ["-o", "ControlPath=\"%s\"" % self.ctrl_path]
-        if self.sshri.port:
-            self._ctrl_options += ['-p', '{}'.format(self.sshri.port)]
-
-        # essential properties of the remote system
-        self._remote_props = {}
-        self._opened_by_us = False
-
-    def __call__(self, cmd, stdin=None, log_output=True):
-        """Executes a command on the remote.
-
-        It is the callers responsibility to properly quote commands
-        for remote execution (e.g. filename with spaces of other special
-        characters). Use the `sh_quote()` from the module for this purpose.
+#
+# Custom handlers
+#
+from datalad.cmd import (
+    GitWitlessRunner,
+    StdOutErrCapture,
+    WitlessRunner,
+)
+from datalad.support.exceptions import (
+    MissingExternalDependency,
+    OutdatedExternalDependency,
+)
+
+_runner = WitlessRunner()
+_git_runner = GitWitlessRunner()
+
+
+def _get_annex_version():
+    """Return version of available git-annex"""
+    try:
+        return _runner.run(
+            'git annex version --raw'.split(),
+            protocol=StdOutErrCapture)['stdout']
+    except CommandError:
+        # fall back on method that could work with older installations
+        out = _runner.run(
+            ['git', 'annex', 'version'],
+            protocol=StdOutErrCapture)
+        return out['stdout'].splitlines()[0].split(':')[1].strip()
+
+
+def _get_git_version():
+    """Return version of git we use (might be bundled)"""
+    return __get_git_version()
 
-        Parameters
-        ----------
-        cmd: str
-          command to run on the remote
 
-        Returns
-        -------
-        tuple of str
-          stdout, stderr of the command run.
-        """
+def _get_system_git_version():
+    """Return version of git available system-wide
 
-        # TODO:  do not do all those checks for every invocation!!
-        # TODO: check for annex location once, check for open socket once
-        #       and provide roll back if fails to run and was not explicitly
-        #       checked first
-        if not self.is_open():
-            if not self.open():
-                raise RuntimeError(
-                    'Cannot open SSH connection to {}'.format(
-                        self.sshri))
-
-        # locate annex and set the bundled vs. system Git machinery in motion
-        remote_annex_installdir = self.get_annex_installdir()
-        if remote_annex_installdir:
-            # make sure to use the bundled git version if any exists
-            cmd = '{}; {}'.format(
-                'export "PATH={}:$PATH"'.format(remote_annex_installdir),
-                cmd)
-
-        # build SSH call, feed remote command as a single last argument
-        # whatever it contains will go to the remote machine for execution
-        # we cannot perform any sort of escaping, because it will limit
-        # what we can do on the remote, e.g. concatenate commands with '&&'
-        ssh_cmd = ["ssh"] + self._ctrl_options
-        ssh_cmd += [self.sshri.as_str()] \
-            + [cmd]
-
-        kwargs = dict(
-            log_stdout=log_output, log_stderr=log_output,
-            log_online=not log_output
-        )
-
-        # TODO: pass expect parameters from above?
-        # Hard to explain to toplevel users ... So for now, just set True
-        return self.runner.run(
-            ssh_cmd,
-            expect_fail=True,
-            expect_stderr=True,
-            stdin=stdin,
-            **kwargs)
+    Might be different from the one we are using, which might be
+    bundled with git-annex
+    """
+    return __get_git_version(_runner)
 
-    @property
-    def runner(self):
-        if self._runner is None:
-            self._runner = Runner()
-        return self._runner
-
-    def is_open(self):
-        if not exists(self.ctrl_path):
-            lgr.log(
-                5,
-                "Not opening %s for checking since %s does not exist",
-                self, self.ctrl_path
-            )
-            return False
-        # check whether controlmaster is still running:
-        cmd = ["ssh", "-O", "check"] + self._ctrl_options + [self.sshri.as_str()]
-        lgr.debug("Checking %s by calling %s" % (self, cmd))
-        null = open('/dev/null')
-        try:
-            # expect_stderr since ssh would announce to stderr
-            # "Master is running" and that is normal, not worthy warning about
-            # etc -- we are doing the check here for successful operation
-            out, err = self.runner.run(cmd, stdin=null, expect_stderr=True)
-            res = True
-        except CommandError as e:
-            if e.code != 255:
-                # this is not a normal SSH error, whine ...
-                raise e
-            # SSH died and left socket behind, or server closed connection
-            self.close()
-            res = False
-        finally:
-            null.close()
-        lgr.debug("Check of %s has %s", self, {True: 'succeeded', False: 'failed'}[res])
-        return res
-
-    def open(self):
-        """Opens the connection.
-
-        In other words: Creates the SSH controlmaster to be used by this
-        connection, if it is not there already.
-
-        Returns
-        -------
-        bool
-          Whether SSH reports success opening the connection
-        """
-        if self.is_open():
-            return
 
-        # set control options
-        ctrl_options = ["-fN",
-                        "-o", "ControlMaster=auto",
-                        "-o", "ControlPersist=15m"] + self._ctrl_options
-        # create ssh control master command
-        cmd = ["ssh"] + ctrl_options + [self.sshri.as_str()]
-
-        # start control master:
-        lgr.debug("Opening %s by calling %s" % (self, cmd))
-        proc = Popen(cmd)
-        stdout, stderr = proc.communicate(input="\n")  # why the f.. this is necessary?
-
-        # wait till the command exits, connection is conclusively
-        # open or not at this point
-        exit_code = proc.wait()
-        ret = exit_code == 0
-
-        if not ret:
-            lgr.warning(
-                "Failed to run cmd %s. Exit code=%s\nstdout: %s\nstderr: %s",
-                cmd, exit_code, stdout, stderr
-            )
-        else:
-            self._opened_by_us = True
-        return ret
+def _get_bundled_git_version():
+    """Return version of git bundled with git-annex.
+    """
+    path = _git_runner._get_bundled_path()
+    if path:
+        out = _runner.run(
+            [op.join(path, "git"), "version"],
+            protocol=StdOutErrCapture)['stdout']
+        # format: git version 2.22.0
+        return out.split()[2]
+
+
+def _get_ssh_version(exe=None):
+    """Return version of ssh
+
+    Annex prior 20170302 was using bundled version, then across all systems
+    we used system one if installed, and then switched to the one defined in
+    configuration, with system-wide (not default in PATH e.g. from conda)
+    "forced" on Windows.  If no specific executable provided in `exe`, we will
+    use the one in configuration
+    """
+    if exe is None:
+        from datalad import cfg
+        exe = cfg.obtain("datalad.ssh.executable")
+    out = _runner.run(
+        [exe, '-V'],
+        protocol=StdOutErrCapture)
+    # apparently spits out to err but I wouldn't trust it blindly
+    stdout = out['stdout']
+    if out['stderr'].startswith('OpenSSH'):
+        stdout = out['stderr']
+    match = re.match(
+        "OpenSSH.*_([0-9][0-9]*)\\.([0-9][0-9]*)(p([0-9][0-9]*))?",
+        stdout)
+    if match:
+        return "{}.{}p{}".format(
+            match.groups()[0],
+            match.groups()[1],
+            match.groups()[3])
+    raise AssertionError(f"no OpenSSH client found: {stdout}")
 
-    def close(self):
-        """Closes the connection.
-        """
-        if not self._opened_by_us:
-            lgr.debug("Not closing %s since was not opened by itself", self)
-            return
-        # stop controlmaster:
-        cmd = ["ssh", "-O", "stop"] + self._ctrl_options + [self.sshri.as_str()]
-        lgr.debug("Closing %s by calling %s", self, cmd)
-        try:
-            self.runner.run(cmd, expect_stderr=True, expect_fail=True)
-        except CommandError as e:
-            lgr.debug("Failed to run close command")
-            if exists(self.ctrl_path):
-                lgr.debug("Removing existing control path %s", self.ctrl_path)
-                # socket need to go in any case
-                remove(self.ctrl_path)
-            if e.code != 255:
-                # not a "normal" SSH error
-                raise e
 
-    def copy(self, source, destination, recursive=False, preserve_attrs=False):
-        """Copies source file/folder to destination on the remote.
+def _get_system_ssh_version():
+    """Return version of the default on the system (in the PATH) ssh
+    """
+    return _get_ssh_version("ssh")
 
-        Parameters
-        ----------
-        source: str or list
-          file/folder path(s) to copy from on local
-        destination: str
-          file/folder path to copy to on remote
-
-        Returns
-        -------
-        str
-          stdout, stderr of the copy operation.
-        """
 
-        # add recursive, preserve_attributes flag if recursive, preserve_attrs set and create scp command
-        scp_options = self._ctrl_options + ["-r"] if recursive else self._ctrl_options
-        scp_options += ["-p"] if preserve_attrs else []
-        scp_cmd = ["scp"] + scp_options
-
-        # add source filepath(s) to scp command
-        scp_cmd += source if isinstance(source, list) \
-            else [source]
-
-        # add destination path
-        scp_cmd += ['%s:"%s"' % (self.sshri.hostname, destination)]
-        return self.runner.run(scp_cmd)
-
-    def get_annex_installdir(self):
-        key = 'installdir:annex'
-        if key in self._remote_props:
-            return self._remote_props[key]
-        annex_install_dir = None
-        # already set here to avoid any sort of recursion until we know
-        # more
-        self._remote_props[key] = annex_install_dir
-        try:
-            with open('/dev/null') as null:
-                annex_install_dir = self(
-                    # use sh -e to be able to fail at each stage of the process
-                    "sh -e -c 'dirname $(readlink -f $(which git-annex-shell))'"
-                    , stdin=null
-                )[0].strip()
-        except CommandError as e:
-            lgr.debug('Failed to locate remote git-annex installation: %s',
-                      exc_str(e))
-        self._remote_props[key] = annex_install_dir
-        return annex_install_dir
-
-    def get_annex_version(self):
-        key = 'cmd:annex'
-        if key in self._remote_props:
-            return self._remote_props[key]
-        try:
-            # modern annex versions
-            version = self('git annex version --raw')[0]
-        except CommandError:
-            # either no annex, or old version
-            try:
-                # fall back on method that could work with older installations
-                out, err = self('git annex version')
-                version = out.split('\n')[0].split(':')[1].strip()
-            except CommandError as e:
-                lgr.debug('Failed to determine remote git-annex version: %s',
-                          exc_str(e))
-                version = None
-        self._remote_props[key] = version
-        return version
-
-    def get_git_version(self):
-        key = 'cmd:git'
-        if key in self._remote_props:
-            return self._remote_props[key]
-        git_version = None
-        try:
-            git_version = self('git version')[0].split()[2]
-        except CommandError as e:
-            lgr.debug('Failed to determine Git version: %s',
-                      exc_str(e))
-        self._remote_props[key] = git_version
-        return git_version
-
-
-@auto_repr
-class SSHManager(object):
-    """Keeps ssh connections to share. Serves singleton representation
-    per connection.
+def _get_system_7z_version():
+    """Return version of 7-Zip"""
+    out = _runner.run(
+        ['7z'],
+        protocol=StdOutErrCapture)
+    # reporting in variable order across platforms
+    # Linux: 7-Zip [64] 16.02
+    # Windows: 7-Zip 19.00 (x86)
+    pieces = out['stdout'].strip().split(':', maxsplit=1)[0].strip().split()
+    for p in pieces:
+        # the one with the dot is the version
+        if '.' in p:
+            return p
+    lgr.debug("Could not determine version of 7z from stdout. %s", out)
+
+
+def get_rsync_version():
+
+    # This does intentionally not query the version of rsync itself, but
+    # that of the debian package it's installed with. Reason is in gh-7320,
+    # which results in the need to detect a patched-by-ubuntu version of rsync
+    # and therefore the package version, not the result of `rsync --version`.
+    from datalad.utils import (
+        get_linux_distribution,
+        on_linux,
+    )
+    if on_linux:
+        dist = get_linux_distribution()[0]
+        if dist in ['debian', 'ubuntu']:
+            out = _runner.run(['apt-cache', 'policy', 'rsync'],
+                              protocol=StdOutErrCapture)
+            for line in out['stdout'].splitlines():
+                parts = line.split()
+                if parts[0] == 'Installed:':
+                    ver = LooseVersion(parts[1])
+                    break
+        # If we have a debian package version, use this as rsync version.
+        # Otherwise report what `rsync --version` itself has to say.
+        if ver:
+            return ver
+    out = _runner.run(['rsync', '--version'], protocol=StdOutErrCapture)
+    # Expected first line:
+    # rsync version x protocol version y
+    return LooseVersion(out['stdout'].splitlines()[0].split()[2])
+
+
+class ExternalVersions(object):
+    """Helper to figure out/use versions of the externals (modules, cmdline tools, etc).
+
+    To avoid collision between names of python modules and command line tools,
+    prepend names for command line tools with `cmd:`.
+
+    It maintains a dictionary of `distuil.version.LooseVersion`s to make
+    comparisons easy. Note that even if version string conform the StrictVersion
+    "standard", LooseVersion will be used.  If version can't be deduced for the
+    external, `UnknownVersion()` is assigned.  If external is not present (can't
+    be imported, or custom check throws exception), None is returned without
+    storing it, so later call will re-evaluate fully.
     """
 
+    UNKNOWN = UnknownVersion()
+
+    _CUSTOM = {
+        'cmd:annex': _get_annex_version,
+        'cmd:git': _get_git_version,
+        'cmd:bundled-git': _get_bundled_git_version,
+        'cmd:system-git': _get_system_git_version,
+        'cmd:ssh': _get_ssh_version,
+        'cmd:system-ssh': _get_system_ssh_version,
+        'cmd:7z': _get_system_7z_version,
+    }
+    # ad-hoc hardcoded map for relevant Python packages which do not provide
+    # __version__ and are shipped by a differently named pypi package
+    _PYTHON_PACKAGES = {  # Python package -> distribution package
+        'github': 'pygithub',
+    }
+    _INTERESTING = (
+        'annexremote',
+        'platformdirs',
+        'boto',
+        'git',
+        'gitdb',
+        'humanize',
+        'iso8601',
+        'keyring',
+        'keyrings.alt',
+        'msgpack',
+        'patool',
+        'cmd:7z',
+        'requests',
+        'scrapy',
+    )
+
     def __init__(self):
-        not_supported_on_windows("TODO: Make this an abstraction to "
-                                 "interface platform dependent SSH")
+        self._versions = {}
+        self.CUSTOM = self._CUSTOM.copy()
+        self.INTERESTING = list(self._INTERESTING)  # make mutable for `add`
+
+    @classmethod
+    def _deduce_version(klass, value):
+        version = None
+
+        # see if it is something containing a version
+        for attr in ('__version__', 'version'):
+            if hasattr(value, attr):
+                version = getattr(value, attr)
+                break
+
+        # try importlib.metadata
+        if version is None and hasattr(value, '__name__'):
+            pkg_name = klass._PYTHON_PACKAGES.get(value.__name__, value.__name__)
+            try:
+                if sys.version_info < (3, 10):
+                    import importlib_metadata as im
+                else:
+                    import importlib.metadata as im
 
-        self._socket_dir = None
-        self._connections = dict()
-        # Initialization of prev_connections is happening during initial
-        # handling of socket_dir, so we do not define them here explicitly
-        # to an empty list to fail if logic is violated
-        self._prev_connections = None
-        # and no explicit initialization in the constructor
-        # self.assure_initialized()
+                version = im.version(pkg_name)
+            except Exception:
+                pass
+
+        # assume that value is the version
+        if version is None:
+            version = value
+
+        # do type analysis
+        if isinstance(version, (tuple, list)):
+            #  Generate string representation
+            version = ".".join(str(x) for x in version)
+        elif isinstance(version, bytes):
+            version = version.decode()
+        elif isinstance(version, str):
+            pass
+        else:
+            version = None
 
-    @property
-    def socket_dir(self):
-        """Return socket_dir, and if was not defined before,
-        and also pick up all previous connections (if any)
+        if version:
+            return LooseVersion(version)
+        else:
+            return klass.UNKNOWN
+
+    def __getitem__(self, module):
+        # when ran straight in its source code -- fails to discover nipy's version.. TODO
+        #if module == 'nipy':
+        #    import pdb; pdb.set_trace()
+        if not isinstance(module, str):
+            modname = module.__name__
+        else:
+            modname = module
+            module = None
+
+        lgr.log(5, "Requested to provide version for %s", modname)
+        # Early returns None so we do not store prev result for  them
+        # and allow users to install things at run time, so later check
+        # doesn't pick it up from the _versions
+        if modname not in self._versions:
+            version = None   # by default -- not present
+            if modname in self.CUSTOM:
+                try:
+                    version = self.CUSTOM[modname]()
+                    version = self._deduce_version(version)
+                except Exception as exc:
+                    lgr.debug("Failed to deduce version of %s due to %s"
+                              % (modname, CapturedException(exc)))
+                    return None
+            else:
+                if module is None:
+                    if modname not in sys.modules:
+                        try:
+                            module = __import__(modname)
+                        except ImportError:
+                            lgr.debug("Module %s seems to be not present", modname)
+                            return None
+                        except Exception as exc:
+                            lgr.warning("Failed to import module %s due to %s",
+                                        modname, CapturedException(exc))
+                            return None
+                    else:
+                        module = sys.modules[modname]
+                if module:
+                    version = self._deduce_version(module)
+            self._versions[modname] = version
+
+        return self._versions.get(modname, self.UNKNOWN)
+
+    def keys(self, query=False):
+        """Return names of the known modules
+
+        Parameters
+        ----------
+        query: bool, optional
+          If True, we will first query all CUSTOM and INTERESTING entries
+          to make sure we have them known.
         """
-        self.assure_initialized()
-        return self._socket_dir
+        if query:
+            [self[k] for k in chain(self.CUSTOM, self.INTERESTING)]
+        return self._versions.keys()
+
+    def __contains__(self, item):
+        return bool(self[item])
+
+    def add(self, name, func=None):
+        """Add a version checker
+
+        This method allows third-party libraries to define additional checks.
+        It will not add `name` if already exists.  If `name` exists and `func`
+        is different - it will override with a new `func`.  Added entries will
+        be included in the output of `dumps(query=True)`.
 
-    def assure_initialized(self):
-        """Assures that manager is initialized - knows socket_dir, previous connections
+        Parameters
+        ----------
+        name: str
+          Name of the check (usually a name of the Python module, or an
+          external command prefixed with "cmd:")
+        func: callable, optional
+          Function to be called to obtain version information. This should be
+          defined when checking the version of something that is not a Python
+          module or when this class's method for determining the version of a
+          Python module isn't sufficient.
         """
-        if self._socket_dir is not None:
-            return
-        from ..config import ConfigManager
-        from os import chmod
-        cfg = ConfigManager()
-        self._socket_dir = opj(cfg.obtain('datalad.locations.cache'),
-                               'sockets')
-        assure_dir(self._socket_dir)
-        try:
-            chmod(self._socket_dir, 0o700)
-        except OSError as exc:
-            lgr.warning(
-                "Failed to (re)set permissions on the %s. "
-                "Most likely future communications would be impaired or fail. "
-                "Original exception: %s",
-                self._socket_dir, exc_str(exc)
-            )
-
-        from os import listdir
-        from os.path import isdir
-        try:
-            self._prev_connections = [opj(self.socket_dir, p)
-                                      for p in listdir(self.socket_dir)
-                                      if not isdir(opj(self.socket_dir, p))]
-        except OSError as exc:
-            self._prev_connections = []
-            lgr.warning(
-                "Failed to list %s for existing sockets. "
-                "Most likely future communications would be impaired or fail. "
-                "Original exception: %s",
-                self._socket_dir, exc_str(exc)
-            )
-
-        lgr.log(5,
-                "Found %d previous connections",
-                len(self._prev_connections))
+        if func:
+            func_existing = self.CUSTOM.get(name, None)
+            was_known = False
+            if func_existing and func_existing is not func:
+                lgr.debug(
+                    "Adding a new custom version checker %s for %s, "
+                    "old one: %s", func, name, func_existing)
+                was_known = name in self._versions
+            self.CUSTOM[name] = func
+            if was_known:
+                # pop and query it again right away to possibly replace with a new value
+                self._versions.pop(name)
+                _ = self[name]
+        elif name not in self.INTERESTING:
+            self.INTERESTING.append(name)
+
+    @property
+    def versions(self):
+        """Return dictionary (copy) of versions"""
+        return self._versions.copy()
 
-    def get_connection(self, url):
-        """Get a singleton, representing a shared ssh connection to `url`
+    def dumps(self, indent=None, preamble="Versions:", query=False):
+        """Return listing of versions as a string
 
         Parameters
         ----------
-        url: str
-          ssh url
-
-        Returns
-        -------
-        SSHConnection
+        indent: bool or str, optional
+          If set would instruct on how to indent entries (if just True, ' '
+          is used). Otherwise returned in a single line
+        preamble: str, optional
+          What preamble to the listing to use
+        query : bool, optional
+          To query for versions of all "registered" custom externals, so to
+          get those which weren't queried for yet
         """
-        # parse url:
-        from datalad.support.network import RI, is_ssh
-        if isinstance(url, RI):
-            sshri = url
-        else:
-            if ':' not in url and '/' not in url:
-                # it is just a hostname
-                lgr.debug("Assuming %r is just a hostname for ssh connection",
-                          url)
-                url += ':'
-            sshri = RI(url)
-
-        if not is_ssh(sshri):
-            raise ValueError("Unsupported SSH URL: '{0}', use "
-                             "ssh://host/path or host:path syntax".format(url))
-
-        conhash = get_connection_hash(
-            sshri.hostname,
-            port=sshri.port,
-            username=sshri.username)
-        # determine control master:
-        ctrl_path = "%s/%s" % (self.socket_dir, conhash)
-
-        # do we know it already?
-        if ctrl_path in self._connections:
-            return self._connections[ctrl_path]
+        if indent and (indent is True):
+            indent = ' '
+        items = ["%s=%s" % (k, self._versions[k]) for k in sorted(self.keys(query=query))]
+        out = "%s" % preamble if preamble else ''
+        if indent is not None:
+            if preamble:
+                preamble += linesep
+            indent = ' ' if indent is True else str(indent)
+            out += (linesep + indent).join(items) + linesep
         else:
-            c = SSHConnection(ctrl_path, sshri)
-            self._connections[ctrl_path] = c
-            return c
+            out += " " + ' '.join(items)
+        return out
 
-    def close(self, allow_fail=True):
-        """Closes all connections, known to this instance.
+    def check(self, name, min_version=None, msg=""):
+        """Check if an external (optionally of specified min version) present
 
         Parameters
         ----------
-        allow_fail: bool, optional
-          If True, swallow exceptions which might be thrown during
-          connection.close, and just log them at DEBUG level
+        name: str
+          Name of the external (typically a Python module)
+        min_version: str or version, optional
+          Minimal version to satisfy
+        msg: str, optional
+          An additional message to include into the exception message
+
+        Raises
+        ------
+        MissingExternalDependency
+          if the external is completely missing
+        OutdatedExternalDependency
+          if the external is present but does not satisfy the min_version
         """
-        if self._connections:
-            to_close = [c for c in self._connections
-                        # don't close if connection wasn't opened by SSHManager
-                        if self._connections[c].ctrl_path
-                        not in self._prev_connections and
-                        exists(self._connections[c].ctrl_path)]
-            if to_close:
-                lgr.debug("Closing %d SSH connections..." % len(to_close))
-            for cnct in to_close:
-                f = self._connections[cnct].close
-                if allow_fail:
-                    f()
-                else:
-                    try:
-                        f()
-                    except Exception as exc:
-                        lgr.debug("Failed to close a connection: "
-                                  "%s", exc_str(exc))
-            self._connections = dict()
+        ver_present = self[name]
+        if ver_present is None:
+            raise MissingExternalDependency(
+                name, ver=min_version, msg=msg)
+        elif min_version and ver_present < min_version:
+            raise OutdatedExternalDependency(
+                name, ver=min_version, ver_present=ver_present, msg=msg)
+
+
+external_versions = ExternalVersions()
```

### Comparing `datalad-0.9.3/datalad/support/strings.py` & `datalad-1.0.0/datalad/support/strings.py`

 * *Files 12% similar despite different names*

```diff
@@ -4,55 +4,56 @@
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##g
 """Variety of helpers to deal with strings"""
 
-__docformat__ = 'restructuredtext'
+from __future__ import annotations
 
-from collections import OrderedDict
-from six import binary_type, text_type
+__docformat__ = 'restructuredtext'
 import re
+from typing import AnyStr
 
 
-def get_replacement_dict(rules):
+def get_replacement_dict(rules: AnyStr | list[AnyStr | list[AnyStr] | tuple[AnyStr, AnyStr]]) -> dict[AnyStr, AnyStr]:
     """Given a string with replacement rules, produces a dict of from: to"""
 
-    if isinstance(rules, (binary_type, text_type)):
+    if isinstance(rules, (bytes, str)):
         rules = [rules]
 
-    pairs = OrderedDict()
+    pairs = dict()
     for rule in rules:
         if isinstance(rule, (list, tuple)):
             if len(rule) == 2:
-                pairs.append(rule)
+                pairs[rule[0]] = rule[1]
             else:
                 raise ValueError("Got a rule %s which is not a string or a pair of values (from, to)"
                                  % repr(rule))
-        if len(rule) <= 2:
+        elif len(rule) <= 2:
             raise ValueError("")
-        rule_split = rule[1:].split(rule[0])
-        if len(rule_split) != 2:
-            raise ValueError(
-                "Rename string must be of format '/pat1/replacement', "
-                "where / is an arbitrary character to decide replacement. "
-                "Got %s when trying to separate %s" % (rule_split, rule)
-            )
-        pairs[rule_split[0]] = rule_split[1]
+        else:
+            rule_split = rule[1:].split(rule[0:1])
+            if len(rule_split) != 2:
+                raise ValueError(
+                    "Rename string must be of format '/pat1/replacement', "
+                    "where / is an arbitrary character to decide replacement. "
+                    "Got %r when trying to separate %r" % (rule_split, rule)
+                )
+            pairs[rule_split[0]] = rule_split[1]
     return pairs
 
 
-def apply_replacement_rules(rules, s):
-    """Apply replacement rules specified as a single string
+def apply_replacement_rules(rules: AnyStr | list[AnyStr | list[AnyStr] | tuple[AnyStr, AnyStr]], s: AnyStr) -> AnyStr:
+    r"""Apply replacement rules specified as a single string
 
     Examples
     --------
 
-    >>> apply_replacement_rules(r'/my_(.*)\.dat/your_\\1.dat.gz', 'd/my_pony.dat')
+    >>> apply_replacement_rules(r'/my_(.*)\.dat/your_\1.dat.gz', 'd/my_pony.dat')
     'd/your_pony.dat.gz'
 
     Parameters
     ----------
     rules : str, list of str
       Rules of the format '/pat1/replacement', where / is an arbitrary
       character to decide replacement.
```

### Comparing `datalad-0.9.3/datalad/support/network.py` & `datalad-1.0.0/datalad/downloaders/http.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,945 +1,693 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
+"""Provide access to stuff (html, data files) via HTTP and HTTPS
 
-import logging
-lgr = logging.getLogger('datalad.network')
-
-lgr.log(5, "Importing support.network")
-import calendar
-import email.utils
-import os
-import pickle
+"""
 import re
-import time
-import iso8601
-
-from hashlib import md5
-from collections import OrderedDict
-from os.path import abspath, isabs
-from os.path import join as opj
-from os.path import dirname
-from ntpath import splitdrive as win_splitdrive
-
-from six import string_types
-from six import iteritems
-from six.moves.urllib.parse import urlsplit
-from six.moves.urllib.request import Request
-from six.moves.urllib.parse import quote as urlquote, unquote as urlunquote
-from six.moves.urllib.parse import urljoin, urlparse, urlsplit, urlunparse, ParseResult
-from six.moves.urllib.parse import parse_qsl
-from six.moves.urllib.parse import urlencode
-from six.moves.urllib.error import URLError
-
-from datalad.dochelpers import exc_str
-from datalad.utils import on_windows
-from datalad.utils import assure_dir
-from datalad import consts
-from datalad import cfg
-from datalad.support.cache import lru_cache
-
-# TODO not sure what needs to use `six` here yet
-# !!! Lazily import requests where needed -- needs 30ms or so
-# import requests
-
-
-def get_response_disposition_filename(s):
-    """Given a string s as from HTTP Content-Disposition field in the response
-    return possibly present filename if any
-    """
-    if not s:
-        return None
-    # If the response has Content-Disposition, try to get filename from it
-    cd = map(
-        lambda x: x.strip().split('=', 1) if '=' in x else [x.strip(), ''],
-        s.split(';')
-    )
-    # unify the key to be lower case and make it into a dict
-    cd = dict([[x[0].lower()] + x[1:] for x in cd])
-    if 'filename' in cd:
-        filename = cd['filename'].strip("\"'")
-        return filename
-    return None
-
-
-def get_url_disposition_filename(url, headers=None):
-    """Get filename as possibly provided by the server in Content-Disposition
-    """
-    if headers is None:
-        request = Request(url)
-        r = retry_urlopen(request)
-        # things are different in requests
-        if 'requests.' in str(r.__class__):
-            headers = r.headers
-        else:
-            headers = r.info()
-    else:
-        r = None
-    try:
-        return get_response_disposition_filename(headers.get('Content-Disposition', ''))
-    finally:
-        if r:
-            r.close()
-
-
-def get_url_straight_filename(url, strip=None, allowdir=False):
-    """Get file/dir name of the last path component of the URL
-
-    Parameters
-    ----------
-    strip: list, optional
-      If provided, listed names will not be considered and their
-      parent directory will be selected
-    allowdir: bool, optional
-      If url points to a "directory" (ends with /), empty string
-      would be returned unless allowdir is True, in which case the
-      name of the directory would be returned
-    """
-    path = urlunquote(urlsplit(url).path)
-    path_parts = path.split('/')
-
-    if allowdir:
-        # strip empty ones
-        while len(path_parts) > 1 and not path_parts[-1]:
-            path_parts = path_parts[:-1]
-
-    if strip:
-        while path_parts and path_parts[-1] in strip:
-            path_parts = path_parts[:-1]
-
-    if path_parts:
-        return path_parts[-1]
+import requests
+import requests.auth
+from requests.utils import parse_dict_header
+
+# at some point was trying to be too specific about which exceptions to
+# catch for a retry of a download.
+# from urllib3.exceptions import MaxRetryError, NewConnectionError
+
+import io
+from time import sleep
+
+from .. import __version__
+from ..utils import (
+    ensure_list_from_str,
+    ensure_dict_from_str,
+    ensure_bytes,
+)
+from ..dochelpers import borrowkwargs
+
+from ..ui import ui
+from ..utils import auto_repr
+from ..support.network import get_url_filename
+from ..support.network import get_response_disposition_filename
+from ..support.network import rfc2822_to_epoch
+from ..support.cookies import cookies_db
+from ..support.status import FileStatus
+from ..support.exceptions import (
+    AccessDeniedError,
+    AccessFailedError,
+    CapturedException,
+    DownloadError,
+    UnhandledRedirectError,
+)
+
+from .base import Authenticator
+from .base import BaseDownloader, DownloaderSession
+
+from logging import getLogger
+from ..log import LoggerHelper
+lgr = getLogger('datalad.http')
+
+# Following https://meta.wikimedia.org/wiki/User-Agent_policy to provide
+# extended and informative User-Agent string
+DEFAULT_USER_AGENT = \
+    f'DataLad/{__version__} ' \
+    '(https://datalad.org; team@datalad.org) ' \
+    f'python-requests/{requests.__version__}'
+
+try:
+    import requests_ftp
+    _FTP_SUPPORT = True
+    requests_ftp.monkeypatch_session()
+except ImportError as e:
+    ce = CapturedException(e)
+    lgr.debug("Failed to import requests_ftp, thus no ftp support: %s", ce)
+    _FTP_SUPPORT = False
+
+if lgr.getEffectiveLevel() <= 1:
+    # Let's also enable requests etc debugging
+
+    # These two lines enable debugging at httplib level (requests->urllib3->http.client)
+    # You will see the REQUEST, including HEADERS and DATA, and RESPONSE with HEADERS but without DATA.
+    # The only thing missing will be the response.body which is not logged.
+    import http.client
+    # TODO: nohow wrapped with logging, plain prints (heh heh), so formatting will not be consistent
+    http.client.HTTPConnection.debuglevel = 1
+
+    # for requests we can define logging properly
+    requests_log = LoggerHelper(logtarget="requests.packages.urllib3").get_initialized_logger()
+    requests_log.setLevel(lgr.getEffectiveLevel())
+    requests_log.propagate = True
+
+__docformat__ = 'restructuredtext'
+
+
+def process_www_authenticate(v):
+    if not v:
+        return []
+    # TODO: provide proper parsing/handling of this custom format and wider support:
+    #   <type> realm=<realm>[, charset="UTF-8"]
+    # More notes: https://github.com/datalad/datalad/issues/5846#issuecomment-890221053
+    # The most complete solution is from 2018 on https://stackoverflow.com/a/52462292/1265472
+    # relying on parsing it using pyparsing.
+    supported_type = v.split(' ')[0].lower()
+    our_type = {
+        'basic': 'http_basic_auth',
+        'digest': 'http_digest_auth',
+        # TODO: bearer_token_anon ?
+    }.get(supported_type)
+    return [our_type] if our_type else []
+
+
+def check_response_status(response, err_prefix="", session=None):
+    """Check if response's status_code signals problem with authentication etc
+
+    ATM succeeds only if response code was 200
+    """
+    if not err_prefix:
+        err_prefix = "Access to %s has failed: " % response.url
+    # 401 would be for digest authentication mechanism, or if we first ask which mechanisms are
+    # supported.... must be linked into the logic if we decide to automagically detect which
+    # mechanism or to give more sensible error message
+    err_msg = err_prefix + "status code %d" % response.status_code
+    if response.status_code in {404}:
+        # It could have been that form_url is wrong, so let's just say that
+        # TODO: actually may be that is where we could use tagid and actually determine the form submission url
+        raise DownloadError(err_prefix + "not found")
+    elif 400 <= response.status_code < 500:
+        raise AccessDeniedError(
+            err_msg,
+            supported_types=process_www_authenticate(
+                response.headers.get('WWW-Authenticate')),
+            status=response.status_code)
+    elif response.status_code in {200}:
+        pass
+    elif response.status_code in {301, 302, 307}:
+        # TODO: apparently tests do not exercise this one yet
+        if session is None:
+            raise AccessFailedError(err_msg + " no session was provided",
+                                    status=response.status_code)
+        redirs = list(session.resolve_redirects(response, response.request))
+        if len(redirs) > 1:
+            lgr.warning("Multiple redirects aren't supported yet.  Taking first")
+        elif len(redirs) == 0:
+            raise AccessFailedError("No redirects were resolved",
+                                    status=response.status_code)
+        raise UnhandledRedirectError(err_msg, url=redirs[0].url,
+                                     status=response.status_code)
     else:
-        return None
-
-
-def get_url_filename(url, headers=None, strip=None):
-    """Get filename from the url, first consulting server about Content-Disposition
-    """
-    filename = get_url_disposition_filename(url, headers)
-    if filename:
-        return filename
-    return get_url_straight_filename(url, strip=strip)
-
-
-def get_url_response_stamp(url, response_info):
-    size, mtime = None, None
-    if 'Content-length' in response_info:
-        size = int(response_info['Content-length'])
-    if 'Last-modified' in response_info:
-        mtime = calendar.timegm(email.utils.parsedate(
-            response_info['Last-modified']))
-    return dict(size=size, mtime=mtime, url=url)
-
-
-def get_tld(url):
-    """Return top level domain from a url
-
-    Parameters
-    ----------
-    url : str
-    """
-    # maybe use this instead to be safe:  https://pypi.python.org/pypi/tld
-    if not url.strip():
-        raise ValueError("Empty URL has no TLD")
-    rec = urlsplit(url)
-    if not rec.netloc:
-        if not rec.scheme:
-            # There were no scheme provided thus netloc was empty -- must have been a simple 'path like'
-            return url.split('/', 1)[0]
-        else:
-            raise ValueError("It seems that only the scheme was provided without the net location/TLD")
-    return rec.netloc
-
-
-from email.utils import parsedate_tz, mktime_tz
-
-
-def rfc2822_to_epoch(datestr):
-    """Given rfc2822 date/time format, return seconds since epoch"""
-    return mktime_tz(parsedate_tz(datestr))
+        raise AccessFailedError(err_msg, status=response.status_code)
 
 
-def iso8601_to_epoch(datestr):
-    """Given ISO 8601 date/time format, return in seconds since epoch
-
-    iso8601 is used to parse properly the time zone information, which
-    can't be parsed with standard datetime strptime
+@auto_repr
+class HTTPBaseAuthenticator(Authenticator):
+    """Base class for html_form and http_auth authenticators
     """
-    return calendar.timegm(iso8601.parse_date(datestr).timetuple())
-
-
-def __urlopen_requests(url):
-    # XXX Workaround for now for ... broken code
-    if isinstance(url, Request):
-        url = url.get_full_url()
-    from requests import Session
-    return Session().get(url)
-
-
-def retry_urlopen(url, retries=3):
-    for t in range(retries):
-        try:
-            return __urlopen_requests(url)
-        except URLError as e:
-            lgr.warn("Received exception while reading %s: %s" % (url, e))
-            if t == retries - 1:
-                # if we have reached allowed number of retries -- reraise
-                raise
-
-
-def is_url_quoted(url):
-    """Return either URL looks being already quoted
-    """
-    try:
-        url_ = urlunquote(url)
-        return url != url_
-    except:  # problem with unquoting -- then it must be wasn't quoted (correctly)
-        # MIH: ValueError?
-        return False
-
-
-def same_website(url_rec, u_rec):
-    """Decide either a link leads to external site
-
-    Parameters
-    ----------
-    url_rec: ParseResult
-      record for original url
-    u_rec: ParseResult
-      record for new url
-    """
-    if isinstance(url_rec, string_types):
-        url_rec = urlparse(url_rec)
-    if isinstance(u_rec, string_types):
-        u_rec = urlparse(u_rec)
-    return (url_rec.netloc == u_rec.netloc)
-    # todo: collect more of sample cases.
-    # disabled below check while working on ratholeradio, since links
-    # could go to the parent and that is ok.  Figure out when it was
-    # desired not to go to the parent -- we might need explicit option
-    # and u_rec.path.startswith(url_rec.path)):
-
-
-def dlurljoin(u_path, url):
-    url_rec = urlparse(url)  # probably duplicating parsing :-/ TODO
-    if url_rec.scheme:
-        # independent full url, so just return it
-        return url
-    if u_path.endswith('/'):  # should here be also a scheme use?
-        if url.startswith('/'):  # jump to the root
-            u_path_rec = urlparse(u_path)
-            return urljoin(urlunparse(
-                (u_path_rec.scheme, u_path_rec.netloc, '', '', '', '')), url)
-        else:
-            return os.path.join(u_path, url)
-    # TODO: recall where all this dirname came from and bring into the test
-    return urljoin(os.path.dirname(u_path) + '/', url)
-
-
-# TODO should it be a node maybe?
-class SimpleURLStamper(object):
-    """Gets a simple stamp about the URL: {url, time, size} of whatever was provided in the header
-    """
-    def __init__(self, mode='full'):
-        self.mode = mode
-
-    def __call__(self, url):
-        # Extracted from above madness
-        # TODO: add mode alike to 'relaxed' where we would not
-        # care about content-disposition filename
-        # http://stackoverflow.com/questions/862173/how-to-download-a-file-using-python-in-a-smarter-way
-        request = Request(url)
-
-        # No traffic compression since we do not know how to identify
-        # exactly either it has to be decompressed
-        # request.add_header('Accept-encoding', 'gzip,deflate')
-        #
-        # TODO: think about stamping etc -- we seems to be redoing
-        # what git-annex does for us already... not really
-        r = retry_urlopen(request)
+    def __init__(self, url=None, failure_re=None, success_re=None,
+                 session_cookies=None, **kwargs):
+        """
+        Parameters
+        ----------
+        url : str, optional
+          URL where to find the form/login to authenticate.  If not provided, an original query url
+          which will be provided to the __call__ of the authenticator will be used
+        failure_re : str or list of str, optional
+        success_re : str or list of str, optional
+          Regular expressions to determine if login has failed or succeeded.
+          TODO: we might condition when it gets ran
+        session_cookies : str or list of str, optional
+          Session cookies to store (besides auth response cookies)
+        """
+        super(HTTPBaseAuthenticator, self).__init__(**kwargs)
+        self.url = url
+        self.failure_re = ensure_list_from_str(failure_re)
+        self.success_re = ensure_list_from_str(success_re)
+        self.session_cookies = ensure_list_from_str(session_cookies)
+
+    def authenticate(self, url, credential, session, update=False):
+        # we should use specified URL for this authentication first
+        lgr.info("http session: Authenticating into session for %s", url)
+        post_url = self.url if self.url else url
+        credential.set_context(auth_url=post_url)
+        credentials = credential()
+
+        # The whole thing relies on server first spitting out 401
+        # and client getting again with 'Authentication:' header
+        # So we need custom handling for those, while keeping track not
+        # of cookies per se, but of 'Authentication:' header which is
+        # to be used in subsequent GETs
+        response = self._post_credential(credentials, post_url, session)
+        if response is None:
+            # authentication did not involve any interaction, nothing to
+            # check at this point
+            return
+
+        # Handle responses if there was initial authentication exchange,
+        # e.g. posting to a form and getting a cookie etc
+        err_prefix = "Authentication to %s failed: " % post_url
         try:
-            r_info = r.info()
-            r_stamp = get_url_response_stamp(url, r_info)
-
-            return dict(mtime=r_stamp['mtime'], size=r_stamp['size'], url=url)
-        finally:
-            r.close()
-
-
-# TODO:  make it consistent/clear at what stage % encoding/decoding happens!
-# now it is a mix!
-
-#
-# Useful functionality in requests.models
-#  utils.requote_uri -- quote/unquote cycle to guarantee consistent appearance
-#  RequestEncodingMixin._encode_params -- Will successfully encode parameters when passed as a dict or a list of ...
-#  PreparedRequest().prepare_url(url, params) -- nicely cares about url encodings etc
-#
-
-@lru_cache(maxsize=100)
-def _guess_ri_cls(ri):
-    """Factory function which would determine which type of a ri a provided string is"""
-    TYPES = {
-        'url': URL,
-        'ssh':  SSHRI,
-        'file': PathRI,
-        'datalad': DataLadRI
-    }
-    # go in exotic mode if this is an absolute windows path
-    win_split = win_splitdrive(ri)
-    # we need a drive and a path, otherwise this could be a false positive
-    if win_split[0] and win_split[1]:
-        # OMG we got something from windows
-        lgr.log(5, "Detected file ri")
-        return TYPES['file']
-
-    # We assume that it is a URL and parse it. Depending on the result
-    # we might decide that it was something else ;)
-    fields = URL._pr_to_fields(urlparse(ri))
-    lgr.log(5, "Parsed ri %s into fields %s" % (ri, fields))
-    type_ = 'url'
-    # Special treatments
-    # file:///path should stay file:
-    if fields['scheme'] and fields['scheme'] not in {'file'} \
-            and not fields['hostname']:
-        # dl+archive:... or just for ssh   hostname:path/p1
-        if '+' not in fields['scheme']:
-            type_ = 'ssh'
-            lgr.log(5, "Assuming ssh style ri, adjusted: %s" % (fields,))
-
-    if not fields['scheme'] and not fields['hostname']:
-        parts = _split_colon(ri)
-        if fields['path'] and '@' in fields['path'] or len(parts) > 1:
-            # user@host:path/sp1
-            # or host_name: (hence parts check)
-            # TODO: we need a regex to catch those really, parts check is not suff
-            type_ = 'ssh'
-        elif ri.startswith('//'):
-            # e.g. // or ///path
-            type_ = 'datalad'
-        else:
-            type_ = 'file'
-
-    if not fields['scheme'] and fields['hostname']:
-        # e.g. //a/path
-        type_ = 'datalad'
-
-    cls = TYPES[type_]
-    # just parse the ri according to regex matchint ssh "ri" specs
-    lgr.log(5, "Detected %s ri" % type_)
-    return cls
-
-
-class RI(object):
-    """Resource Identifier - base class and a factory for URL, SSHRI, etc
-
-    Intended to be a R/O object (i.e. no fields should be changed in-place).
-    Subclasses define specific collections of fields they care about in _FIELDS
-    class variable.
-    The idea is that this class should help to break apart a URL, while being
-    able to rebuild itself into a string representation for reuse
-
-    `RI` could be used as factory, whenever type of the resource is unknown and
-    must be guessed from the string representation.  One of the subclasses will be
-    provided as output, e.g.
-
-    >>> RI('http://example.com')
-    URL(hostname='example.com', scheme='http')
-    >>> RI('example.com:path')
-    SSHRI(hostname='example.com', path='path')
+            check_response_status(response, err_prefix, session=session)
+        except DownloadError:
+            # It might have happened that the return code was 'incorrect'
+            # and we did get some feedback, which we could analyze to
+            # figure out actual problem.  E.g. in case of nersc of crcns
+            # it returns 404 (not found) with text in the html
+            if response is not None and response.text:
+                self.check_for_auth_failure(response.text, err_prefix)
+            raise
+
+        response_text = response.text
+        self.check_for_auth_failure(response_text, err_prefix)
+
+        if self.success_re:
+            # the one which must be used to verify success
+            # verify that we actually logged in
+            for success_re in self.success_re:
+                if not re.search(success_re, response_text):
+                    raise AccessDeniedError(
+                        err_prefix + " returned output did not match 'success' regular expression %s" % success_re
+                    )
+
+        cookies_dict = {}
+        if response.cookies:
+            cookies_dict = requests.utils.dict_from_cookiejar(response.cookies)
+        if self.session_cookies:
+            # any session cookies to store
+            cookies_dict.update({k: session.cookies[k] for k in self.session_cookies})
+
+        if cookies_dict:
+            if (url in cookies_db) and update:
+                cookies_db[url].update(cookies_dict)
+            else:
+                cookies_db[url] = cookies_dict
+            # assign cookies for this session
+            for c, v in cookies_dict.items():
+                if c not in session.cookies or session.cookies[c] != v:
+                    session.cookies[c] = v  # .update(cookies_dict)
+
+        return response
+
+    def _post_credential(self, credentials, post_url, session):
+        raise NotImplementedError("Must be implemented in subclass")
+
+    def check_for_auth_failure(self, content, err_prefix=""):
+        if self.failure_re:
+            content_is_bytes = isinstance(content, bytes)
+            # verify that we actually logged in
+            for failure_re in self.failure_re:
+                if content_is_bytes:
+                    # content could be not in utf-8. But I do not think that
+                    # it is worth ATM messing around with guessing encoding
+                    # of the content to figure out what to encode it into
+                    # since typically returned "auth failed" should be in
+                    # utf-8 or plain ascii
+                    failure_re = ensure_bytes(failure_re)
+                if re.search(failure_re, content):
+                    raise AccessDeniedError(
+                        err_prefix + "returned output which matches regular expression %s" % failure_re
+                    )
+
+
+@auto_repr
+class HTMLFormAuthenticator(HTTPBaseAuthenticator):
+    """Authenticate by opening a session via POSTing to HTML form
     """
 
-    # All of the subclasses will provide path
-    _FIELDS = (
-        'path',
-    )
-
-    __slots__ = _FIELDS + ('_fields', '_str')
-
-    def __new__(cls, ri=None, **kwargs):
-        """Used as a possible factory for known RI types
-
-        Returns
-        -------
-        RI
-           uninitialized RI object of appropriate class with _str
-           set to string representation if was provided
-
+    def __init__(self, fields, tagid=None, **kwargs):
         """
-        if cls is RI and ri is not None:
-            # RI class was used as a factory
-            cls = _guess_ri_cls(ri)
-
-        if cls is RI:
-            # should we fail or just pretend we are nothing??? ;-) XXX
-            raise ValueError("Could not deduce RI type for %r" % (ri,))
-
-        ri_obj = super(RI, cls).__new__(cls)
-        # Store internally original str
-        ri_obj._str = ri
-        return ri_obj
 
-    def __init__(self, ri=None, **fields):
-        """
+        Example specification in the .ini config file
+        [provider:crcns]
+        ...
+        credential = crcns ; is not given to authenticator as is
+        authentication_type = html_form
+        # TODO: may be rename into post_url
+        html_form_url = https://crcns.org/login_form
+        # probably not needed actually since form_url
+        # html_form_tagid = login_form
+        html_form_fields = __ac_name={user}
+                   __ac_password={password}
+                   submit=Log in
+                   form.submitted=1
+                   js_enabled=0
+                   cookies_enabled=
+        html_form_failure_re = (Login failed|Please log in)
+        html_form_success_re = You are now logged in
+
         Parameters
         ----------
-        ri: str, optional
-          String version of a resource specific for this class.  If you would like
-          a type of the resource be deduced, use RI(ri)
-        **fields: dict, optional
-          The values for the fields defined in _FIELDS class variable.
+        fields : str or dict
+          String or a dictionary, which will be used (along with credential) information
+          to feed into the form
+        tagid : str, optional
+          id of the HTML <form> in the document to use. If None, and page contains a single form,
+          that one will be used.  If multiple forms -- error will be raise
+        **kwargs : dict, optional
+          Passed to super class HTTPBaseAuthenticator
         """
-        if ri and (bool(ri) == bool(fields)):
-            raise ValueError(
-                "Specify either ri or breakdown from the fields, not both. "
-                "Got ri=%r, fields=%r" % (ri, fields))
-
-        self._fields = self._get_blank_fields()
-        if ri is not None:
-            fields = self._str_to_fields(ri)
-        self._set_from_fields(**fields)
-
-        # If was initialized from a string representation
-        if self._str is not None:
-            # well -- some ris might not unparse identically back
-            # strictly speaking, but let's assume they do
-            ri_ = self.as_str()
-            if ri != ri_:
-                lgr.debug("Parsed version of %s %r differs from original %r",
-                          self.__class__.__name__, ri_, ri)
-
-    @classmethod
-    def _get_blank_fields(cls, **fields):
-        return OrderedDict(((f, fields.get(f, '')) for f in cls._FIELDS))
-
-    @property
-    def fields(self):
-        """Returns shallow copy of fields to ease manipulations"""
-        return self._fields.copy()
-
-    def __repr__(self):
-        # since auto_repr doesn't support "non-0" values atm
-        return "%s(%s)" % (
-            self.__class__.__name__,
-            ", ".join(["%s=%r" % (k, v)
-                       for k, v in sorted(self._fields.items())
-                       if v]))
-
-    # Lazily evaluated if _str was not set
-    def __str__(self):
-        if self._str is None:
-            self._str = self.as_str()
-        return self._str
-
-    @classmethod
-    def from_str(cls, ri_str):
-        obj = cls(**cls._str_to_fields(ri_str))
-        obj._str = ri_str
-        return obj
-
-    @property
-    def localpath(self):
-        # by default RIs point to remote locations
-        raise ValueError("%s points to remote location" % self)
-
-    # Apparently doesn't quite play nicely with multiple inheritence for MixIn'
-    # of regexp based URLs
-    #@abstractmethod
-    #@classmethod
-    #def _str_to_fields(cls, ri_str):
-    #    raise NotImplementedError
-
-    #
-    # If any field is specified, URL is not considered 'False', i.e.
-    # non-existing, although may be we could/shout omit having only
-    # scheme or port specified since it doesn't point to any useful
-    # location
-    #
-
-    def __nonzero__(self):
-        fields = self._fields
-        return any(fields.values())
-
-    # for PY3
-    __bool__ = __nonzero__
-
-    #
-    # Helpers to deal with internal structures and conversions
-    #
-
-    def _set_from_fields(self, **fields):
-        unknown_fields = set(fields).difference(self._FIELDS)
-        if unknown_fields:
-            raise ValueError("Do not know about %s. Known fields for %s are: %s"
-                             % (unknown_fields, self.__class__, self._FIELDS))
-
-        # encode dicts for query or fragment into
-        for f in {'query', 'fragment'}:
-            v = fields.get(f)
-            if isinstance(v, dict):
-
-                ev = urlencode(v)
-                # / is reserved char within query
-                if f == 'fragment' and '%2F' not in str(v):
-                    # but seems to be ok'ish within the fragment which is
-                    # the last element of URI and anyways used only by the
-                    # client (i.e. by us here if used to compose the URL)
-                    # so let's return / back for clarity if there were no
-                    # awkward %2F to startswith
-                    ev = ev.replace('%2F', '/')
-                fields[f] = ev
-
-        self._fields.update(fields)
-
-    #
-    # Quick comparators
-    #
-
-    def __eq__(self, other):
-        if not isinstance(other, RI):
-            other = RI(other)
-        return isinstance(other, self.__class__) and dict(other._fields) == dict(self._fields)
-
-    def __ne__(self, other):
-        return not (self == other)
-
-    def __getattribute__(self, item):
-        if item.startswith('_') or item not in self._FIELDS:
-            return super(RI, self).__getattribute__(item)
-        else:
-            return self._fields[item]
-
-    def __setattr__(self, item, value):
-        if item.startswith('_') or item not in self._FIELDS:
-            super(RI, self).__setattr__(item, value)
-        else:
-            self._fields[item] = value
-            self._str = None
-
-
-class URL(RI):
-    """Universal resource locator
-
-    Although largely decorating urlparse.ParseResult, it
-    - doesn't mandate providing all parts of the URL
-    - doesn't require netloc but rather asks for separate username, password, and hostname
-    """
-
-    _FIELDS = RI._FIELDS + (
-        'scheme',
-        'username',
-        'password',
-        'hostname', 'port',
-        'query',
-        'fragment',
-    )
-
-    def as_str(self):
-        """Render URL as a string"""
-        return urlunparse(self.to_pr())
-
-    @classmethod
-    def _str_to_fields(cls, url_str):
-        fields = URL._pr_to_fields(urlparse(url_str))
-        fields['path'] = urlunquote(fields['path'])
-        return fields
-
-    def to_pr(self):
-        """Convert URL to urlparse.ParseResults namedtuple"""
-        return self._fields_to_pr(self._fields)
-
-    @classmethod
-    def _fields_to_pr(cls, fields):
-        """Recompose back fields dict to ParseResult"""
-        netloc = fields['username'] or ''
-        if fields['password']:
-            netloc += ':' + fields['password']
-        if netloc:
-            netloc += '@'
-        netloc += fields['hostname']
-        if fields['port']:
-            if fields['hostname'].count(':') >= 2:
-                # ipv6 -- need to enclose in []
-                netloc = '[%s]:%s' % (netloc, fields['port'])
-            else:
-                netloc += ':%s' % fields['port']
-
-        pr_fields = {
-            f: fields[f]
-            for f in cls._FIELDS
-            if f not in ('hostname', 'password', 'username', 'port')
+        super(HTMLFormAuthenticator, self).__init__(**kwargs)
+        self.fields = ensure_dict_from_str(fields)
+        self.tagid = tagid
+
+    def _post_credential(self, credentials, post_url, session):
+        post_fields = {
+            k: v.format(**credentials)
+            for k, v in self.fields.items()
         }
-        pr_fields['netloc'] = netloc
-        pr_fields['params'] = ''
-        # We need to quote the path
-        pr_fields['path'] = urlquote(pr_fields['path'])
-        # TODO: figure out what to do with query/fragment... one step at a time
-        return ParseResult(**pr_fields)
 
-    @classmethod
-    def _pr_to_fields(cls, pr):
-        """ParseResult is a tuple so immutable, which complicates adjusting it
-
-        This function converts ParseResult into dict"""
+        response = session.post(post_url, data=post_fields)
+        lgr.debug("Posted to %s fields %s, got response %s with headers %s",
+                  post_url, list(post_fields.keys()), response,
+                  list(response.headers.keys()))
+        return response
 
-        if pr.params:
-            lgr.warning("ParseResults contains params %r, which will be ignored"
-                        % (pr.params,))
-
-        hostname_port = pr.netloc.split('@')[-1]
-        is_ipv6 = hostname_port.count(':') >= 2
-        # can't use just pr._asdict since we care to ask those properties
-        # such as .port , .hostname etc
-        # Forcing '' instead of None since those properties (.hostname), .password,
-        # .username return None if not available and we decided to uniformize
-        if is_ipv6:
-            rem = re.match('\[(?P<hostname>.*)\]:(?P<port>\d+)', hostname_port)
-            if rem:
-                hostname, port = rem.groups()
-                port = int(port)
-            else:
-                hostname, port = hostname_port, ''
 
-            def _getattr(pr, f):
-                """Helper for custom handling in case of ipv6 addresses which blows
-                stock ParseResults logic"""
-                if f == 'port':
-                    # for now not supported at all, so
-                    return port
-                elif f == 'hostname':
-                    return hostname
-                else:
-                    return getattr(pr, f)
-        else:
-            _getattr = getattr
+@auto_repr
+class HTTPRequestsAuthenticator(HTTPBaseAuthenticator):
+    """Base class for various authenticators using requests pre-crafted ones
 
-        return {f: (_getattr(pr, f) or '') for f in cls._FIELDS}
-
-    #
-    # Access helpers
-    #
-
-    def _parse_qs(self, s, auto_delist=True):
-        """Helper around parse_qs to strip unneeded 'list'ing etc and return a dict of key=values"""
-        if not s:
-            return {}
-        out = OrderedDict(parse_qsl(s, 1))
-        if not auto_delist:
-            return out
-        for k in out:
-            v = out[k]
-            if isinstance(v, list) and len(v) == 1:
-                v = v[0]
-                out[k] = None if v == '' else v
-        return out
-
-    @property
-    def query_dict(self):
-        return self._parse_qs(self.query)
-
-    @property
-    def fragment_dict(self):
-        return self._parse_qs(self.fragment)
-
-    @property
-    def localpath(self):
-        if self.scheme != 'file':
-            raise ValueError(
-                "Non 'file://' URL cannot be resolved to a local path")
-        hostname = self.hostname
-        if not (hostname in (None, '', 'localhost', '::1')
-                or hostname.startswith('127.')):
-            raise ValueError("file:// URL does not point to 'localhost'")
-        return self.path
-
-
-class PathRI(RI):
-    """RI pointing to a (local) file/directory"""
-    def as_str(self):
-        return self.path
-
-    @classmethod
-    def _str_to_fields(cls, url_str):
-        return dict(path=url_str)
-
-    @property
-    def localpath(self):
-        return self.path
 
+    Note, that current implementation assumes REQUESTS_FIELDS to be identical to
+    the keys of a `Credential` object's FIELDS.
+    """
 
-class RegexBasedURLMixin(object):
-    """Base class for URLs which we could simple parse using regular expressions"""
+    REQUESTS_AUTHENTICATOR = None
+    REQUESTS_FIELDS = ('user', 'password')
 
-    _REGEX = None
+    def __init__(self, **kwargs):
+        # so we have __init__ solely for a custom docstring
+        super(HTTPRequestsAuthenticator, self).__init__(**kwargs)
 
-    # not used ATM but possible ;)
-    # @classmethod
-    # def is_str_matches(cls, url_str):
-    #     return bool(cls._REGEX.match(url_str))
+    def _post_credential(self, credentials, post_url, session):
+        authenticator = self.REQUESTS_AUTHENTICATOR(
+            *[credentials[f] for f in self.REQUESTS_FIELDS])
+        session.auth = authenticator
 
-    @classmethod
-    def _str_to_fields(cls, url_str):
-        re_match = cls._REGEX.match(url_str)
-        if not re_match:
-            # TODO: custom error?
-            raise ValueError(
-                "Cannot handle URL '%s': categorized as %r, but does not match syntax.%s"
-                % (url_str,
-                   cls,
-                   " Did you intent to use '///'?" if url_str.startswith('//') else '')
-            )
-        fields = cls._get_blank_fields()
-        fields.update({k: v for k, v in iteritems(re_match.groupdict()) if v})
-        cls._normalize_fields(fields)
-        return fields
 
-    @classmethod
-    def _normalize_fields(self, fields):
-        """Helper to be ran if any of the fields need to be normalized after parsing"""
-        pass
+@auto_repr
+class HTTPBasicAuthAuthenticator(HTTPRequestsAuthenticator):
+    """Authenticate via basic HTTP authentication
 
+    Example specification in the .ini config file
+    [provider:hcp-db]
+    ...
+    credential = hcp-db
+    authentication_type = http_auth
 
-class SSHRI(RI, RegexBasedURLMixin):
-    """RI pointing to a remote location reachable via SSH"""
+    Parameters
+    ----------
+    **kwargs : dict, optional
+      Passed to super class HTTPBaseAuthenticator
+    """
 
-    _FIELDS = RI._FIELDS + (
-        'username',
-        'hostname',
-        'port',
-    )
+    REQUESTS_AUTHENTICATOR = requests.auth.HTTPBasicAuth
 
-    _REGEX = re.compile(r'((?P<username>\S*)@)?(?P<hostname>[^:]+)(\:(?P<path>.*))?$')
 
-    @classmethod
-    def _normalize_fields(cls, fields):
-        if fields['path'] and fields['path'].startswith('//'):
-            # Let's normalize for now to avoid multiple leading slashes
-            fields['path'] = '/' + fields['path'].lstrip('/')
-        # escape path so we have direct representation of the path to work with
-        fields['path'] = unescape_ssh_path(fields['path'])
-
-    def as_str(self, escape=False):
-        fields = self.fields  # copy so we could escape symbols
-        url_fmt = '{hostname}'
-        if fields['username']:
-            url_fmt = "{username}@" + url_fmt
-        if fields['path']:
-            url_fmt += ':{path}'
-        if escape:
-            fields['path'] = escape_ssh_path(fields['path'])
-        return url_fmt.format(**fields)
-
-    # TODO:
-    # we can "support" localhost:path as localpaths
-
-
-class DataLadRI(RI, RegexBasedURLMixin):
-    """RI pointing to datasets within central DataLad super-dataset"""
-
-    _FIELDS = RI._FIELDS + (
-        'remote',
-    )
-
-    # For now or forever we don't deal with any fragments or other special stuff
-    _REGEX = re.compile(r'//(?P<remote>[^\s/]*)/(?P<path>.*)$')
+@auto_repr
+class HTTPAuthAuthenticator(HTTPRequestsAuthenticator):
+    """Authenticate via Basic authentication to some other post url
 
-    # do they need to be normalized??? loosing track ...
+    TODO:  actually this is some remnants which might later were RFed
+    into the form authenticator since otherwise they make little sense
+    """
 
-    def as_str(self):
-        return "//{remote}/{path}".format(**self._fields)
+    REQUESTS_AUTHENTICATOR = requests.auth.HTTPBasicAuth
 
-    def as_git_url(self):
-        """Dereference /// into original URLs which could be used by git for cloning
+    def _post_credential(self, credentials, post_url, session):
+        authenticator = self.REQUESTS_AUTHENTICATOR(
+            *[credentials[f] for f in self.REQUESTS_FIELDS])
+        session.auth = authenticator
+        response = session.post(post_url, data={},
+                                auth=authenticator)
+        auth_request = response.headers.get('www-authenticate')
+        if response.status_code == 401 and auth_request:
+            if auth_request.lower().split(' ', 1)[0] == 'basic':
+                if response.url != post_url:
+                    # was instructed to authenticate elsewhere
+                    # TODO: do we need to loop may be??
+                    response2 = session.get(response.url, auth=authenticator)
+                    return response2
+            else:
+                lgr.warning(
+                    f"{self} received response with www-authenticate={auth_request!r} "
+                    "which is not Basic, and thus it cannot handle ATM.")
+        return response
 
-        Returns
-        -------
-        str
-          URL string to reference the DataLadRI from its /// form
-        """
-        if self.remote:
-            raise NotImplementedError("not supported ATM to reference additional remotes")
-        return "{}{}".format(consts.DATASETS_TOPURL, urlquote(self.path))
 
+@auto_repr
+class HTTPDigestAuthAuthenticator(HTTPRequestsAuthenticator):
+    """Authenticate via HTTP digest authentication
+    """
 
-def _split_colon(s, maxsplit=1):
-    """Split on unescaped colon"""
-    return re.compile(r'(?<!\\):').split(s, maxsplit=maxsplit)
+    REQUESTS_AUTHENTICATOR = requests.auth.HTTPDigestAuth
 
-# \ should be first to deal with
-_SSH_ESCAPED_CHARACTERS = '\\#&;`|*?~<>^()[]{}$\'" '
 
+@auto_repr
+class HTTPBearerTokenAuthenticator(HTTPRequestsAuthenticator):
+    """Authenticate via HTTP 'Authorization: Bearer TOKEN' header
 
-# TODO: RF using re.sub
-def escape_ssh_path(path):
-    """Escape all special characters present in the path"""
-    for c in _SSH_ESCAPED_CHARACTERS:
-        if c in path:
-            path = path.replace(c, '\\' + c)
-    return path
+    E.g. as defined for OAuth2 in RFC 6750
+    https://datatracker.ietf.org/doc/html/rfc6750
+    """
 
+    DEFAULT_CREDENTIAL_TYPE = 'token'
+    AUTH_KEYWORD = 'Bearer'
 
-def unescape_ssh_path(path):
-    """Un-escape all special characters present in the path"""
-    for c in _SSH_ESCAPED_CHARACTERS[::-1]:
-        if c in path:
-            path = path.replace('\\' + c, c)
-    return path
+    def __init__(self, **kwargs):
+        # so we have __init__ solely for a custom docstring
+        super(HTTPBearerTokenAuthenticator, self).__init__(**kwargs)
 
+    def _post_credential(self, credentials, post_url, session):
+        # we do not need to post anything, just inject token into the session
+        session.headers['Authorization'] = f"{self.AUTH_KEYWORD} {credentials['token']}"
 
-def parse_url_opts(url):
-    """Given a string with url-style query, split into content before # and options as dict"""
-    url = URL(url)
-    # we need to filter out query and fragment to get the base url
-    fields = url.fields
-    fields.pop('query')
-    fields.pop('fragment')
-    opts = url.query_dict
-    return str(URL(**fields)), opts
 
+class HTTPTokenAuthenticator(HTTPBearerTokenAuthenticator):
+    """Authenticate via HTTP 'Authorization: Token TOKEN' header
 
-# TODO: should we just define URL.good_for_git or smth like that? ;)
-# although git also understands regular paths
-def is_url(ri):
-    """Returns whether argument is a resource identifier what datalad should treat as a URL
+    It is pretty much the "Bearer TOKEN" method but which uses different keyword
+    "Token".  It is e.g. the one provided by Django REST Framework.
+    GitHub allows for both 'Bearer' and 'Token' keywords:
+    https://docs.github.com/en/rest/authentication/authenticating-to-the-rest-api?apiVersion=2022-11-28
+    """
+    AUTH_KEYWORD = 'Token'
 
-    This includes ssh "urls" which git understands.
 
-    Parameters
-    ----------
-    ri : str or RI
-      The resource identifier (as a string or RI) to "analyze"
+@auto_repr
+class HTTPAnonBearerTokenAuthenticator(HTTPBearerTokenAuthenticator):
+    """Retrieve token via 401 response and add Authorization: Bearer header.
     """
-    if not isinstance(ri, RI):
-        try:
-            ri = RI(ri)
-        except:  # MIH: MemoryError?
-            return False
-    return isinstance(ri, (URL, SSHRI))
 
+    allows_anonymous = True
 
-# TODO: RF to remove duplication
-def is_datalad_compat_ri(ri):
-    """Returns whether argument is a resource identifier what datalad should treat as a URL
+    def authenticate(self, url, credential, session, update=False):
+        if credential:
+            lgr.warning(
+                "Argument 'credential' specified, but it will be ignored: %s",
+                credential)
+        response = session.head(url)
+        status = response.status_code
+        if status == 200:
+            lgr.debug("No authorization needed for %s", url)
+            return
+        if status != 401:
+            raise DownloadError(
+                "Expected 200 or 401 but got {} from {}"
+                .format(status, url))
 
-    including its own DataLadRI
-    """
-    if not isinstance(ri, RI):
+        lgr.debug("Requesting authorization token for %s", url)
+        # TODO: it is not RFC 2068 Section 2 format, but a custom
+        # <type> realm=<realm>[, charset="UTF-8"]
+        # see TODO/harmonize with  process_www_authenticate
+        auth_parts = parse_dict_header(response.headers["www-authenticate"])
+        auth_url = ("{}?service={}&scope={}"
+                    .format(auth_parts["Bearer realm"],
+                            auth_parts["service"],
+                            auth_parts["scope"]))
+        auth_response = session.get(auth_url)
         try:
-            ri = RI(ri)
-        except:  # MIH: MemoryError?
-            return False
-    return isinstance(ri, (URL, SSHRI, DataLadRI))
+            auth_info = auth_response.json()
+        except ValueError as e:
+            raise DownloadError(
+                "Failed to get information from {}"
+                .format(auth_url)) from e
+        session.headers['Authorization'] = "Bearer " + auth_info["token"]
+
+
+@auto_repr
+class HTTPDownloaderSession(DownloaderSession):
+    def __init__(self, size=None, filename=None,  url=None, headers=None,
+                 response=None, chunk_size=1024 ** 2):
+        super(HTTPDownloaderSession, self).__init__(
+            size=size, filename=filename, url=url, headers=headers,
+        )
+        self.chunk_size = chunk_size
+        self.response = response
+
+    def download(self, f=None, pbar=None, size=None):
+        response = self.response
+        # content_gzipped = 'gzip' in response.headers.get('content-encoding', '').split(',')
+        # if content_gzipped:
+        #     raise NotImplemented("We do not support (yet) gzipped content")
+        #     # see https://rationalpie.wordpress.com/2010/06/02/python-streaming-gzip-decompression/
+        #     # for ways to implement in python 2 and 3.2's gzip is working better with streams
+
+        total = 0
+        return_content = f is None
+        if f is None:
+            # no file to download to
+            # TODO: actually strange since it should have been decoded then...
+            f = io.BytesIO()
+
+        # must use .raw to be able avoiding decoding/decompression while downloading
+        # to a file
+        chunk_size_ = min(self.chunk_size, size) if size is not None else self.chunk_size
+
+        # XXX With requests_ftp BytesIO is provided as response.raw for ftp urls,
+        # which has no .stream, so let's do ducktyping and provide our custom stream
+        # via BufferedReader for such cases, while maintaining the rest of code
+        # intact.  TODO: figure it all out, since doesn't scale for any sizeable download
+        # This code is tested by tests/test_http.py:test_download_ftp BUT
+        # it causes 503 on travis,  but not always so we allow to skip that test
+        # in such cases. That causes fluctuating coverage
+        if not hasattr(response.raw, 'stream'):  # pragma: no cover
+            def _stream():
+                buf = io.BufferedReader(response.raw)
+                v = True
+                while v:
+                    v = buf.read(chunk_size_)
+                    yield v
 
+            stream = _stream()
+        else:
+            # XXX TODO -- it must be just a dirty workaround
+            # As we discovered with downloads from NITRC all headers come with
+            # Content-Encoding: gzip which leads  requests to decode them.  But the point
+            # is that ftp links (yoh doesn't think) are gzip compressed for the transfer
+            decode_content = not response.url.startswith('ftp://')
+            stream = response.raw.stream(chunk_size_, decode_content=decode_content)
+
+        for chunk in stream:
+            if chunk:  # filter out keep-alive new chunks
+                chunk_len = len(chunk)
+                if size is not None and total + chunk_len > size:
+                    # trim the download to match target size
+                    chunk = chunk[:size - total]
+                    chunk_len = len(chunk)
+                total += chunk_len
+                f.write(chunk)
+                try:
+                    # TODO: pbar is not robust ATM against > 100% performance ;)
+                    if pbar:
+                        pbar.update(total)
+                except Exception as e:
+                    ce = CapturedException(e)
+                    lgr.warning("Failed to update progressbar: %s", ce)
+                # TEMP
+                # see https://github.com/niltonvolpato/python-progressbar/pull/44
+                ui.out.flush()
+                if size is not None and total >= size:  # pragma: no cover
+                    break  # we have done as much as we were asked
 
-# TODO: better name? additionally may be move to SSHRI.is_valid() or sth.
-def is_ssh(ri):
-    """helper to determine, whether `ri` requires an SSH connection
+        if return_content:
+            out = f.getvalue()
+            return out
 
-    Parameters
-    ----------
-    ri: str or RI
 
-    Returns
-    -------
-    bool
+@auto_repr
+class HTTPDownloader(BaseDownloader):
+    """A stateful downloader to maintain a session to the website
     """
 
-    # not exactly fitting the doc, but we actually can deal not necessarily with
-    # string or RI only, but with everything RI itself can deal with:
-    _ri = RI(ri) if not isinstance(ri, RI) else ri
-
-    return isinstance(_ri, SSHRI) \
-        or (isinstance(_ri, URL) and _ri.scheme == 'ssh')
+    @borrowkwargs(BaseDownloader)
+    def __init__(self, headers=None, **kwargs):
+        """
 
+        Parameters
+        ----------
+        headers: dict, optional
+          Header fields to be provided to the session. Unless User-Agent provided, a custom
+          one, available in `DEFAULT_USER_AGENT` constant of this module will be used.
+        """
+        super(HTTPDownloader, self).__init__(**kwargs)
+        self._session = None
+        headers = headers.copy() if headers else {}
+        if 'user-agent' not in map(str.lower, headers):
+            headers['User-Agent'] = DEFAULT_USER_AGENT
+        self._headers = headers
 
-#### windows workaround ###
-# TODO: There should be a better way
-def get_local_file_url(fname):
-    """Return OS specific URL pointing to a local file
+    def _establish_session(self, url, allow_old=True):
+        """
 
-    Parameters
-    ----------
-    fname : string
-        Filename.  If not absolute, abspath is used
-    """
-    fname = fname if isabs(fname) else abspath(fname)
-    if on_windows:
-        fname_rep = fname.replace('\\', '/')
-        furl = "file:///%s" % urlquote(fname_rep)
-        lgr.debug("Replaced '\\' in file\'s url: %s" % furl)
-    else:
-        # TODO:  need to fix for all the encoding etc
-        furl = str(URL(scheme='file', path=fname))
-    return furl
-
-
-def get_url_cache_filename(url, name=None):
-    """Return a filename where to cache online doc from a url"""
-    if not name:
-        name = "misc"
-    cache_dir = opj(cfg.obtain('datalad.locations.cache'), name)
-    doc_fname = opj(
-        cache_dir,
-        '{}-{}.p{}'.format(
-            urlsplit(url).netloc,
-            md5(url.encode('utf-8')).hexdigest(),
-            pickle.HIGHEST_PROTOCOL)
-    )
-    return doc_fname
-
-
-def get_cached_url_content(url, name=None, fetcher=None, maxage=None):
-    """Loader of a document from a url, which caches loaded instance on disk
-
-    Doesn't do anything smart about http headers etc which could provide
-    information for cache/proxy servers for how long to retain etc
-
-    TODO: theoretically it is not network specific at all -- and just a memoize
-    pattern, but may be some time we would make it treat headers etc correctly.
-    And ATM would support any URL we support via providers/downloaders
+        Parameters
+        ----------
+        allow_old: bool, optional
+          If a Downloader allows for persistent sessions by some means -- flag
+          instructs whether to use previous session, or establish a new one
 
-    Parameters
-    ----------
-    fetcher: callable, optional
-       Function to call with url if needed to be refetched
-    maxage: float, optional
-       Age in days to retain valid for.  <0 - would retain forever.  If None -
-       would consult the config, 0 - would force to reload
-    """
-    doc_fname = get_url_cache_filename(url, name)
-    if maxage is None:
-        maxage = float(cfg.get('datalad.locations.cache-maxage'))
+        Returns
+        -------
+        bool
+          To state if old instance of a session/authentication was used
+        """
+        if allow_old:
+            if self._session:
+                lgr.debug("http session: Reusing previous")
+                return True  # we used old
+            elif url in cookies_db:
+                cookie_dict = cookies_db[url]
+                lgr.debug("http session: Creating new with old cookies %s", list(cookie_dict.keys()))
+                self._session = requests.Session()
+                # not sure what happens if cookie is expired (need check to that or exception will prolly get thrown)
+
+                # TODO dict_to_cookiejar doesn't preserve all fields when reversed
+                self._session.cookies = requests.utils.cookiejar_from_dict(cookie_dict)
+                # TODO cookie could be expired w/ something like (but docs say it should be expired automatically):
+                # http://docs.python-requests.org/en/latest/api/#requests.cookies.RequestsCookieJar.clear_expired_cookies
+                # self._session.cookies.clear_expired_cookies()
+                return True
+
+        lgr.debug("http session: Creating brand new session")
+        self._session = requests.Session()
+        self._session.headers.update(self._headers)
+        if self.authenticator:
+            self.authenticator.authenticate(url, self.credential, self._session)
 
-    doc = None
-    if os.path.exists(doc_fname) and maxage != 0:
+        return False
 
-        fage = (time.time() - os.stat(doc_fname).st_mtime)/(24. * 3600)
-        if maxage < 0 or fage < maxage:
+    def get_downloader_session(self, url,
+                               allow_redirects=True,
+                               use_redirected_url=True,
+                               headers=None):
+        # TODO: possibly make chunk size adaptive
+        # TODO: make it not this ugly -- but at the moment we are testing end-file size
+        # while can't know for sure if content was gunziped and whether it all went ok.
+        # So safer option -- just request to not have it gzipped
+        if headers is None:
+            headers = {}
+        if 'Accept-Encoding' not in headers:
+            headers['Accept-Encoding'] = ''
+
+        # TODO: our tests ATM aren't ready for retries, thus altogether disabled for now
+        nretries = 1
+        for retry in range(1, nretries+1):
             try:
-                lgr.debug("use cached request result to '%s' from %s", url, doc_fname)
-                doc = pickle.load(open(doc_fname, 'rb'))
-            except Exception as e:  # it is OK to ignore any error and fall back on the true source
+                response = self._session.get(
+                    url, stream=True, allow_redirects=allow_redirects,
+                    headers=headers)
+            #except (MaxRetryError, NewConnectionError) as exc:
+            except Exception as exc:
+                ce = CapturedException(exc)
+                # happen to run into those with urls pointing to Amazon,
+                # so let's rest and try again
+                if retry >= nretries:
+                    #import epdb; epdb.serve()
+                    if not _FTP_SUPPORT and url.startswith("ftp://"):
+                        msg_ftp = "For ftp:// support, install requests_ftp. "
+                    else:
+                        msg_ftp = ""
+
+                    raise AccessFailedError(
+                        "Failed to establish a new session %d times. %s"
+                        % (nretries, msg_ftp)) from exc
                 lgr.warning(
-                    "cannot load cache from '%s', fall back to download: %s",
-                    doc_fname, exc_str(e))
+                    "Caught exception %s. Will retry %d out of %d times",
+                    ce, retry + 1, nretries)
+                sleep(2**retry)
+
+        check_response_status(response, session=self._session)
+        headers = response.headers
+        lgr.debug("Establishing session for url %s, response headers: %s",
+                  url, headers)
+        target_size = int(headers.get('Content-Length', '0').strip()) or None
+        if use_redirected_url and response.url and response.url != url:
+            lgr.debug("URL %s was redirected to %s and thus the later will be used"
+                      % (url, response.url))
+            url = response.url
+        # Consult about filename.  Since we already have headers,
+        # should not result in an additional request
+        url_filename = get_url_filename(url, headers=headers)
+
+        headers['Url-Filename'] = url_filename
+        return HTTPDownloaderSession(
+            size=target_size,
+            url=response.url,
+            filename=url_filename,
+            headers=headers,
+            response=response
+        )
 
-    if doc is None:
-        if fetcher is None:
-            from datalad.downloaders.providers import Providers
-            providers = Providers.from_config_files()
-            fetcher = providers.fetch
-
-        doc = fetcher(url)
-        assure_dir(dirname(doc_fname))
-        # use pickle to store the entire request result dict
-        pickle.dump(doc, open(doc_fname, 'wb'))
-        lgr.debug("stored result of request to '{}' in {}".format(url, doc_fname))
-    return doc
-
-lgr.log(5, "Done importing support.network")
+    @classmethod
+    def get_status_from_headers(cls, headers):
+        """Given HTTP headers, return 'status' record to assess later if link content was changed
+        """
+        # used for quick checks for HTTP or S3?
+        # TODO:  So we will base all statuses on this set? e.g. for Last-Modified if to be
+        # mapping from field to its type converter
+        HTTP_HEADERS_TO_STATUS = {
+            'Content-Length': int,
+            'Content-Disposition': str,
+            'Last-Modified': rfc2822_to_epoch,
+            'Url-Filename': str,
+        }
+        # Allow for webserver to return them in other casing
+        HTTP_HEADERS_TO_STATUS_lower = {s.lower(): (s, t) for s, t in HTTP_HEADERS_TO_STATUS.items()}
+        status = {}
+        if headers:
+            for header_key in headers:
+                try:
+                    k, t = HTTP_HEADERS_TO_STATUS_lower[header_key.lower()]
+                except KeyError:
+                    continue
+                status[k] = t(headers[header_key])
+
+        # convert to FileStatus
+        return FileStatus(
+            size=status.get('Content-Length'),
+            mtime=status.get('Last-Modified'),
+            filename=get_response_disposition_filename(
+                status.get('Content-Disposition')) or status.get('Url-Filename')
+        )
```

### Comparing `datalad-0.9.3/datalad/support/repo.py` & `datalad-1.0.0/datalad/support/archive_utils_patool.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,188 +1,213 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-""" utility classes for repositories
+"""patool based implementation for datalad.support.archives utilities"""
 
-"""
+import patoolib
+from .external_versions import external_versions
+# There were issues, so let's stay consistently with recent version
+assert(external_versions["patoolib"] >= "1.7")
+
+import os
+from .exceptions import MissingExternalDependency
+from .path import (
+    basename,
+    join as opj,
+)
+
+from datalad.utils import (
+    ensure_bytes,
+    chpwd,
+)
 
 import logging
+lgr = logging.getLogger('datalad.support.archive_utils_patool')
 
-from.exceptions import InvalidInstanceRequestError
-
-lgr = logging.getLogger('datalad.repo')
-
-
-class Flyweight(type):
-    """Metaclass providing an implementation of the flyweight pattern.
-
-    Since the flyweight is very similar to a singleton, we occasionally use this
-    term to make clear there's only one instance (at a time).
-    This integrates the "factory" into the actual classes, which need
-    to have a class attribute `_unique_instances` (WeakValueDictionary).
-    By providing an implementation of __call__, you don't need to call a
-    factory's get_xy_repo() method to get a singleton. Instead this is called
-    when you simply instantiate via MyClass(). So, you basically don't even need
-    to know there were singletons. Therefore it is also less likely to sabotage
-    the concept by not being aware of how to get an appropriate object.
-
-    Multiple instances, pointing to the same physical repository can cause a
-    lot of trouble. This is why this class exists. You should be very aware of
-    the implications, if you want to circumvent that mechanism.
-
-    To use this pattern, you need to add this class as a metaclass to the class
-    you want to use it with. Additionally there needs to be a class attribute
-    `_unique_instances`, which should be a `WeakValueDictionary`. Furthermore
-    implement `_flyweight_id_from_args` method to determine, what should be the
-    identifying criteria to consider two requested instances the same.
-
-    Example:
-
-    from weakref import WeakValueDictionary
-    from six import add_metaclass
-
-    @add_metaclass(Flyweight)
-    class MyFlyweightClass(object):
-
-        _unique_instances = WeakValueDictionary()
-
-        @classmethod
-        def _flyweight_id_from_args(cls, *args, **kwargs):
+# Monkey-patch patoolib's logging, so it logs coherently with the rest of
+# datalad
+import patoolib.util
+#
+# Seems have managed with swallow_outputs
+#
+# def _patool_log(level, msg):
+#     lgr.log(level, "patool: %s", msg)
+#
+# def _patool_log_info(msg, *args, **kwargs):
+#     _patool_log(logging.DEBUG, msg)
+#
+# def _patool_log_error(msg, *args, **kwargs):
+#     _patool_log(logging.ERROR, msg)
+#
+# patoolib.util.log_info = _patool_log_info
+# patoolib.util.log_error = _patool_log_error
+# patoolib.util.log_internal_error = _patool_log_error
+
+# we need to decorate patool.util.run
+# because otherwise it just lets processes to spit out everything to std and we
+# do want to use it at "verbosity>=0" so we could get idea on what is going on.
+# And I don't want to mock for every invocation
+from ..support.exceptions import CommandError
+from ..utils import swallow_outputs
+from datalad.cmd import (
+    WitlessRunner,
+    StdOutErrCapture,
+)
+from ..utils import ensure_unicode
+
+from ..utils import on_windows
+
+_runner = WitlessRunner()
+
+
+def _patool_run(cmd, verbosity=0, **kwargs):
+    """Decorated runner for patool so it doesn't spit out outputs to stdout"""
+    # use our runner
+    try:
+        # kwargs_ = kwargs[:];         kwargs_['shell'] = True
+        # Any debug/progress output could be spit out to stderr so let's
+        # "expect" it.
+        #
+        if isinstance(cmd, (list, tuple)) and kwargs.pop('shell', None):
+            # patool (as far as I see it) takes care about quoting args
+            cmd = ' '.join(cmd)
+        out = _runner.run(
+            cmd,
+            protocol=StdOutErrCapture,
+            **kwargs)
+        lgr.debug("Finished running for patool. stdout=%s, stderr=%s",
+                  out['stdout'], out['stderr'])
+        return 0
+    except CommandError as e:
+        return e.code
+    except Exception as e:
+        lgr.error("While invoking runner caught unexpected exception: %s", e)
+        return 100  # unknown beast
+patoolib.util.run = _patool_run
+
+
+# yoh: only keys are used atm, logic in decompress_file is replaced to use
+# patool
+
+DECOMPRESSORS = {
+    r'\.(tar\.bz|tbz)$': 'tar -xjvf %(file)s -C %(dir)s',
+    r'\.(tar\.xz)$': 'tar -xJvf %(file)s -C %(dir)s',
+    r'\.(tar\.gz|tgz)$': 'tar -xzvf %(file)s -C %(dir)s',
+    r'\.(zip)$': 'unzip %(file)s -d %(dir)s',
+}
 
-            id = kwargs.pop('id')
-            return id, args, kwargs
 
-        def __init__(self, some, someother=None):
-            pass
+def unixify_path(path):
+    r"""On windows convert paths from drive:\d\file to /drive/d/file
 
-    a = MyFlyweightClass('bla', id=1)
-    b = MyFlyweightClass('blubb', id=1)
-    assert a is b
-    c = MyFlyweightClass('whatever', id=2)
-    assert c is not a
+    This overcomes problems with various cmdline tools we are to use,
+    such as tar etc
     """
-
-    def _flyweight_id_from_args(cls, *args, **kwargs):
-        """create an ID from arguments passed to `__call__`
-
-        Subclasses need to implement this method. The ID it returns is used to
-        determine whether or not there already is an instance of that kind and
-        as key in the `_unique_instances` dictionary.
-
-        Besides the ID this should return args and kwargs, which can be modified
-        herein and will be passed on to the constructor of a requested instance.
-
-        Parameters
-        ----------
-        args:
-         positional arguments passed to __call__
-        kwargs:
-         keyword arguments passed to __call__
-
-        Returns
-        -------
-        hashable, args, kwargs
-          id, optionally manipulated args and kwargs to be passed to __init__
-        """
-        pass
-
-    def _flyweight_invalid(cls, id):
-        """determines whether or not an instance with `id` became invalid and
-        therefore has to be instantiated again.
-
-        Subclasses can implement this method to provide an additional condition
-        on when to create a new instance besides there is none yet.
-
-        Parameter
-        ---------
-        id: hashable
-          ID of the requested instance
-
-        Returns
-        -------
-        bool
-          whether to consider an existing instance with that ID invalid and
-          therefore create a new instance. Default implementation always returns
-          False.
-        """
-        return False
-
-    def _flyweight_reject(cls, id, *args, **kwargs):
-        """decides whether to reject a request for an instance
-
-        This gives the opportunity to detect a conflict of an instance request
-        with an already existing instance, that is not invalidated by
-        `_flyweight_invalid`. In case the return value is not `None`, it will be
-        used as the message for an `InvalidInstanceRequestError`,
-        raised by `__call__`
-
-        Parameters
-        ----------
-        id: hashable
-          the ID of the instance in question as calculated by
-          `_flyweight_id_from_args`
-        args:
-        kwargs:
-          (keyword) arguments to the original call
-
-        Returns:
-        --------
-        None or str
-        """
-        return None
-
-    def __call__(cls, *args, **kwargs):
-
-        id_, new_args, new_kwargs = cls._flyweight_id_from_args(*args, **kwargs)
-        instance = cls._unique_instances.get(id_, None)
-
-        if instance is None or cls._flyweight_invalid(id_):
-            # we have no such instance yet or the existing one is invalidated,
-            # so we instantiate:
-            instance = type.__call__(cls, *new_args, **new_kwargs)
-            cls._unique_instances[id_] = instance
+    if on_windows:
+        drive, path_ = os.path.splitdrive(path)
+        path_ = path_.split(os.sep)
+        path_ = '/'.join(path_)
+        if drive:
+            # last one must be :
+            assert(drive[-1] == ":")
+            return '/%s%s' % (drive[:-1], path_)
         else:
-            # we have an instance already that is not invalid itself; check
-            # whether there is a conflict, otherwise return existing one:
-            # TODO
-            # Note, that this might (and probably should) go away, when we
-            # decide how to deal with currently possible invalid constructor
-            # calls for the repo classes. In particular this is about calling
-            # it with different options than before, that might lead to
-            # fundamental changes in the repository (like annex repo version
-            # change or re-init of git)
-
-            # force? may not mean the same thing
-            msg = cls._flyweight_reject(id_, *new_args, **new_kwargs)
-            if msg is not None:
-                raise InvalidInstanceRequestError(id_, msg)
-
-        return instance
-
-
-# TODO: see issue #1100
-class RepoInterface(object):
-    """common operations for annex and plain git repositories
+            return path_
+    else:
+        return path
 
-    Especially provides "annex operations" on plain git repos, that just do
-    (or return) the "right thing"
-    """
-
-    # Note: Didn't find a way yet, to force GitRepo as well as AnnexRepo to
-    # implement a method defined herein, since AnnexRepo inherits from GitRepo.
-    # Would be much nicer, but still - I'd prefer to have a central place for
-    # these anyway.
-
-    # Note 2: Seems possible. There is MRO magic:
-    # http://pybites.blogspot.de/2009/01/mro-magic.html
-    # http://stackoverflow.com/questions/20822850/change-python-mro-at-runtime
-
-    # Test!
-
-    def sth_like_file_has_content(self):
-        raise NotImplementedError # the real thing in case of annex and True in case of git
 
+def decompress_file(archive, dir_):
+    """Decompress `archive` into a directory `dir_`
+
+    Parameters
+    ----------
+    archive: str
+    dir_: str
+    """
+    with swallow_outputs() as cmo:
+        archive = ensure_bytes(archive)
+        dir_ = ensure_bytes(dir_)
+        patoolib.util.check_existing_filename(archive)
+        patoolib.util.check_existing_filename(dir_, onlyfiles=False)
+        # Call protected one to avoid the checks on existence on unixified path
+        outdir = unixify_path(dir_)
+        # should be supplied in PY3 to avoid b''
+        outdir = ensure_unicode(outdir)
+        archive = ensure_unicode(archive)
+
+        format_compression = patoolib.get_archive_format(archive)
+        if format_compression == ('gzip', None):
+            # Yarik fell into the trap of being lazy and not providing proper
+            # support for .gz .xz etc "stream archivers" formats in handling
+            # of archives. ATM out support for .gz relies on behavior of 7z while
+            # extracting them and respecting possibly present .gz filename
+            # header field.
+            # See more https://github.com/datalad/datalad/pull/3176#issuecomment-466819861
+            # TODO: provide proper handling of all those archives without
+            # relying on any filename been stored in the header
+            program = patoolib.find_archive_program(
+                format_compression[0], 'extract')
+            if basename(program) != '7z':
+                raise MissingExternalDependency(
+                    "cmd:7z",
+                    msg="(Not) Funny enough but ATM we need p7zip installation "
+                        "to handle .gz files extraction 'correctly'"
+                )
+
+        patoolib._extract_archive(unixify_path(archive),
+                                  outdir=outdir,
+                                  verbosity=100)
+        if cmo.out:
+            lgr.debug("patool gave stdout:\n%s", cmo.out)
+        if cmo.err:
+            lgr.debug("patool gave stderr:\n%s", cmo.err)
+
+    # Note: (ben) Experienced issue, where extracted tarball
+    # lacked execution bit of directories, leading to not being
+    # able to delete them while having write permission.
+    # Can't imagine a situation, where we would want to fail on
+    # that kind of mess. So, to be sure set it.
+
+    if not on_windows:
+        os.chmod(dir_,
+                 os.stat(dir_).st_mode |
+                 os.path.stat.S_IEXEC)
+        for root, dirs, files in os.walk(dir_, followlinks=False):
+            for d in dirs:
+                subdir = opj(root, d)
+                os.chmod(subdir,
+                         os.stat(subdir).st_mode |
+                         os.path.stat.S_IEXEC)
+
+
+def compress_files(files, archive, path=None, overwrite=True):
+    """Compress `files` into an `archive` file
+
+    Parameters
+    ----------
+    files : list of str
+    archive : str
+    path : str
+      Alternative directory under which compressor will be invoked, to e.g.
+      take into account relative paths of files and/or archive
+    overwrite : bool
+      Whether to allow overwriting the target archive file if one already exists
+    """
+    with swallow_outputs() as cmo:
+        with chpwd(path):
+            if not overwrite:
+                patoolib.util.check_new_filename(archive)
+            patoolib.util.check_archive_filelist(files)
+            # Call protected one to avoid the checks on existence on unixified path
+            patoolib._create_archive(unixify_path(archive),
+                                     [unixify_path(f) for f in files],
+                                     verbosity=100)
+        if cmo.out:
+            lgr.debug("patool gave stdout:\n%s", cmo.out)
+        if cmo.err:
+            lgr.debug("patool gave stderr:\n%s", cmo.err)
```

### Comparing `datalad-0.9.3/datalad/support/vcr_.py` & `datalad-1.0.0/datalad/support/vcr_.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,26 +1,26 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 """Adapters and decorators for vcr
 """
 
 import logging
 
 from functools import wraps
 from os.path import isabs
-from os.path import realpath
 from contextlib import contextmanager
 
-from datalad.dochelpers import exc_str
+from datalad.utils import Path
+from datalad.support.exceptions import CapturedException
 
 lgr = logging.getLogger("datalad.support.vcr")
 
 
 def _get_cassette_path(path):
     """Return a path to the cassette within our unified 'storage'"""
     if not isabs(path):  # so it was given as a name
@@ -37,24 +37,28 @@
     except ImportError:
         def returnnothing(*args, **kwargs):
             return()
         _vcrp.CassettePatcherBuilder._requests = returnnothing
 
     from vcr import use_cassette as _use_cassette, VCR as _VCR
 
-    def use_cassette(path, return_body=None, **kwargs):
+    def use_cassette(path, return_body=None, skip_if_no_vcr=False, **kwargs):
         """Adapter so we could create/use custom use_cassette with custom parameters
 
         Parameters
         ----------
         path : str
           If not absolute path, treated as a name for a cassette under fixtures/vcr_cassettes/
+        skip_if_no_vcr : bool
+          Rather than running without VCR it would throw unittest.SkipTest
+          exception.  Of effect only if vcr import fails (so not in this
+          implementation but the one below)
         """
         path = _get_cassette_path(path)
-        lgr.debug("Using cassette %s" % path)
+        lgr.debug("Using cassette %s", path)
         if return_body is not None:
             my_vcr = _VCR(
                 before_record_response=lambda r: dict(r, body={'string': return_body.encode()}))
             return my_vcr.use_cassette(path, **kwargs)  # with a custom response
         else:
             return _use_cassette(path, **kwargs)  # just a straight one
 
@@ -62,31 +66,40 @@
     vcr_lgr = logging.getLogger('vcr')
     if lgr.getEffectiveLevel() > logging.DEBUG:
         vcr_lgr.setLevel(logging.WARN)
 except Exception as exc:
     if not isinstance(exc, ImportError):
         # something else went hairy (e.g. vcr failed to import boto due to some syntax error)
         lgr.warning("Failed to import vcr, no cassettes will be available: %s",
-                    exc_str(exc, limit=10))
+                    CapturedException(exc))
     # If there is no vcr.py -- provide a do nothing decorator for use_cassette
 
-    def use_cassette(*args, **kwargs):
-        def do_nothing_decorator(t):
-            @wraps(t)
-            def wrapper(*args, **kwargs):
-                lgr.debug("Not using vcr cassette")
-                return t(*args, **kwargs)
-            return wrapper
-        return do_nothing_decorator
+    def use_cassette(path, return_body=None, skip_if_no_vcr=False, **kwargs):
+        if skip_if_no_vcr:
+            def skip_decorator(t):
+                @wraps(t)
+                def wrapper(*args, **kwargs):
+                    from unittest import SkipTest
+                    raise SkipTest("No vcr")
+                return wrapper
+            return skip_decorator
+        else:
+            def do_nothing_decorator(t):
+                @wraps(t)
+                def wrapper(*args, **kwargs):
+                    lgr.debug("Not using vcr cassette")
+                    return t(*args, **kwargs)
+                return wrapper
+            return do_nothing_decorator
 
 
 @contextmanager
 def externals_use_cassette(name):
     """Helper to pass instruction via env variables to use specified cassette
 
     For instance whenever we are testing custom special remotes invoked by the annex
     but want to minimize their network traffic by using vcr.py
     """
-    from mock import patch
-    cassette_path = realpath(_get_cassette_path(name))  # realpath OK
+    from unittest.mock import patch
+    cassette_path = str(Path(_get_cassette_path(name)).resolve())  # realpath OK
     with patch.dict('os.environ', {'DATALAD_TESTS_USECASSETTE': cassette_path}):
         yield
```

### Comparing `datalad-0.9.3/datalad/support/s3.py` & `datalad-1.0.0/datalad/support/s3.py`

 * *Files 16% similar despite different names*

```diff
@@ -14,35 +14,44 @@
 """
 
 __docformat__ = 'restructuredtext'
 
 import mimetypes
 
 from os.path import splitext
-from datalad.support.network import urlquote
+import re
+
+from datalad.support.network import urlquote, URL
 
 import logging
 import datalad.log  # Just to have lgr setup happen this one used a script
 lgr = logging.getLogger('datalad.s3')
 
-from datalad.dochelpers import exc_str
-from datalad.support.exceptions import DownloadError, AccessDeniedError
+from datalad.support.exceptions import (
+    CapturedException,
+    DownloadError,
+    AccessDeniedError,
+    AccessPermissionExpiredError,
+    AnonymousAccessDeniedError,
+)
+from datalad.utils import try_multiple_dec
 
-from six.moves.urllib.request import urlopen, Request
-from six.moves.urllib.parse import urlparse, urlunparse
+from urllib.request import urlopen, Request
 
 
 try:
     import boto
     from boto.s3.key import Key
     from boto.exception import S3ResponseError
     from boto.s3.connection import OrdinaryCallingFormat
 except Exception as e:
     if not isinstance(e, ImportError):
-        lgr.warning("boto module failed to import although available: %s" % exc_str(e))
+        lgr.warning(
+            "boto module failed to import although available: %s",
+            CapturedException(e))
     boto = Key = S3ResponseError = OrdinaryCallingFormat = None
 
 
 # TODO: should become a config option and managed along with the rest
 S3_ADMIN_CREDENTIAL = "datalad-datalad-admin-s3"
 S3_TEST_CREDENTIAL = "datalad-datalad-test-s3"
 
@@ -62,46 +71,107 @@
 
 def _handle_exception(e, bucket_name):
     """Helper to handle S3 connection exception"""
     raise (
         AccessDeniedError
         if e.error_code == 'AccessDenied'
         else DownloadError)(
-            "Cannot connect to %s S3 bucket. Exception: %s"
-            % (bucket_name, exc_str(e))
-        )
+            "Cannot connect to %s S3 bucket."
+            % (bucket_name)
+        ) from e
+
+
+def _check_S3ResponseError(e):
+    """Returns True if should be retried.
+
+    raises ... if token has expired"""
+    # https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html#ErrorCodeList
+    if e.status in (
+                    307,  # MovedTemporarily -- DNS updates etc
+                    503,  # Slow down -- too many requests, so perfect fit to sleep a bit
+                    ):
+        return True
+    if e.status == 400:
+        # Generic Bad Request -- could be many things! generally -- we retry, but
+        # some times provide more directed reaction
+        # ATM, as used, many requests we send with boto might be just HEAD requests
+        # (if I got it right) and we would not receive BODY back with the detailed
+        # error_code.  Then we will allow to retry until we get something we know how to
+        # handle it more specifically
+        if e.error_code == 'ExpiredToken':
+            raise AccessPermissionExpiredError(
+                "Used token to access S3 has expired") from e
+        elif not e.error_code:
+            lgr.log(5, "Empty error_code in %s", e)
+        return True
+    return False
+
+
+def try_multiple_dec_s3(func):
+    """An S3 specific adapter to @try_multiple_dec
+
+    To decorate func to try multiple times after some sleep upon encountering
+    some intermittent error from S3
+    """
+    return try_multiple_dec(
+                ntrials=4,
+                duration=2.,
+                increment_type='exponential',
+                exceptions=S3ResponseError,
+                exceptions_filter=_check_S3ResponseError,
+                logger=lgr.debug,
+    )(func)
 
 
 def get_bucket(conn, bucket_name):
     """A helper to get a bucket
 
     Parameters
     ----------
     bucket_name: str
         Name of the bucket to connect to
     """
     try:
-        bucket = conn.get_bucket(bucket_name)
+        return try_multiple_dec_s3(conn.get_bucket)(bucket_name)
     except S3ResponseError as e:
+        ce = CapturedException(e)
         # can initially deny or error to connect to the specific bucket by name,
         # and we would need to list which buckets are available under following
         # credentials:
-        lgr.debug("Cannot access bucket %s by name: %s", bucket_name, exc_str(e))
+        lgr.debug("Cannot access bucket %s by name with validation: %s",
+                  bucket_name, ce)
+        if conn.anon:
+            raise AnonymousAccessDeniedError(
+                "Access to the bucket %s did not succeed.  Requesting "
+                "'all buckets' for anonymous S3 connection makes "
+                "little sense and thus not supported." % bucket_name,
+                supported_types=['aws-s3']
+            )
+
+        if e.reason == "Forbidden":
+            # Could be just HEAD call boto issues is not allowed, and we should not
+            # try to verify that bucket is "reachable".  Just carry on
+            try:
+                return try_multiple_dec_s3(conn.get_bucket)(bucket_name, validate=False)
+            except S3ResponseError as e2:
+                lgr.debug("Cannot access bucket %s even without validation: %s",
+                          bucket_name, CapturedException(e2))
+                _handle_exception(e, bucket_name)
+
         try:
-            all_buckets = conn.get_all_buckets()
+            all_buckets = try_multiple_dec_s3(conn.get_all_buckets)()
+            all_bucket_names = [b.name for b in all_buckets]
+            lgr.debug("Found following buckets %s", ', '.join(all_bucket_names))
+            if bucket_name in all_bucket_names:
+                return all_buckets[all_bucket_names.index(bucket_name)]
         except S3ResponseError as e2:
-            lgr.debug("Cannot access all buckets: %s", exc_str(e2))
+            lgr.debug("Cannot access all buckets: %s", CapturedException(e2))
             _handle_exception(e, 'any (originally requested %s)' % bucket_name)
-        all_bucket_names = [b.name for b in all_buckets]
-        lgr.debug("Found following buckets %s", ', '.join(all_bucket_names))
-        if bucket_name in all_bucket_names:
-            bucket = all_buckets[all_bucket_names.index(bucket_name)]
         else:
             _handle_exception(e, bucket_name)
-    return bucket
 
 
 class VersionedFilesPool(object):
     """Just a helper which would help to create versioned files in the bucket"""
     def __init__(self, bucket):
         self._versions = {}
         self._bucket = bucket
@@ -134,42 +204,45 @@
 
     def reset_version(self, filename):
         self._versions[filename] = 0
 
 
 def get_key_url(e, schema='http', versioned=True):
     """Generate an s3:// or http:// url given a key
+
+    if versioned url is requested but version_id is None, no versionId suffix
+    will be added
     """
     # TODO: here we would need to encode the name since urlquote actually
     # can't do that on its own... but then we should get a copy of the thing
     # so we could still do the .format....
     # ... = e.name.encode('utf-8')  # unicode isn't advised in URLs
     e.name_urlquoted = urlquote(e.name)
     if schema == 'http':
         fmt = "http://{e.bucket.name}.s3.amazonaws.com/{e.name_urlquoted}"
     elif schema == 's3':
         fmt = "s3://{e.bucket.name}/{e.name_urlquoted}"
     else:
         raise ValueError(schema)
-    if versioned:
+    if versioned and e.version_id is not None:
         fmt += "?versionId={e.version_id}"
     return fmt.format(e=e)
 
 
 def prune_and_delete_bucket(bucket):
     """Deletes all the content and then deletes bucket
 
     Should be used with care -- no confirmation requested
     """
     bucket.delete_keys(bucket.list_versions(''))
     # this one doesn't work since it generates DeleteMarkers instead ;)
     #for key in b.list_versions(''):
     #    b.delete_key(key)
     bucket.delete()
-    lgr.info("Bucket %s was removed" % bucket.name)
+    lgr.info("Bucket %s was removed", bucket.name)
 
 
 def set_bucket_public_access_policy(bucket):
     # we need to enable permissions for making content available
     bucket.set_policy("""{
       "Version":"2012-10-17",
       "Statement":[{
@@ -184,15 +257,15 @@
 
 
 def gen_test_bucket(bucket_name):
     conn = _get_bucket_connection(S3_ADMIN_CREDENTIAL)
     # assure we have none
     try:
         bucket = conn.get_bucket(bucket_name)
-        lgr.info("Deleting existing bucket %s" % bucket.name)
+        lgr.info("Deleting existing bucket %s", bucket.name)
         prune_and_delete_bucket(bucket)
     except:  # MIH: MemoryError?
         # so nothing to worry about
         pass
     finally:
         pass
 
@@ -231,15 +304,15 @@
     files("2versions-removed-recreated.txt_sameprefix")
 
     # File which was created/removed/recreated (with new content)
     f = "1version-removed-recreated.txt"
     bucket.delete_key(files(f))
     files.reset_version(f)
     files(f)
-    lgr.info("Bucket %s was generated and populated" % bucket_name)
+    lgr.info("Bucket %s was generated and populated", bucket_name)
 
     return bucket
 
 
 def gen_bucket_test0_versioned():
     return _gen_bucket_test0('datalad-test0-versioned', versioned=True)
 
@@ -287,16 +360,75 @@
     #files(u"юни/код")
     # all fancy ones at once
     files("f!-_.*'( )")
     # the super-fancy which aren't guaranteed to be good idea (as well as [] above)
     files("f &$=@:+,?;")
 
 
+def gen_bucket_test1_manydirs():
+    # to test crawling with flexible subdatasets making decisions
+    bucket_name = 'datalad-test1-manydirs-versioned'
+    bucket = gen_test_bucket(bucket_name)
+    bucket.configure_versioning(True)
+
+    # Enable web access to that bucket to everyone
+    bucket.configure_website('index.html')
+    set_bucket_public_access_policy(bucket)
+
+    files = VersionedFilesPool(bucket)
+
+    files("d1", load="")  # creating an empty file
+    # then we would like to remove that d1 as a file and make a directory out of it
+    files("d1/file1.txt")
+    files("d1/sd1/file1.txt")
+    files("d1/sd2/file3.txt", load="a")
+    files("d1/sd2/ssd1/file4.txt")
+    files("d2/file1.txt")
+    files("d2/sd1/file1.txt")
+    files("d2/sd1/ssd/sssd/file1.txt")
+
+
+def add_version_to_url(url, version, replace=False):
+    """Add a version ID to `url`.
+
+    Parameters
+    ----------
+    url : datalad.support.network.URL
+        A URL.
+    version : str
+        The value of 'versionId='.
+    replace : boolean, optional
+        If a versionID is already present in `url`, replace it.
+
+    Returns
+    -------
+    A versioned URL (str)
+    """
+    version_id = "versionId={}".format(version)
+    if not url.query:
+        query = version_id
+    else:
+        ver_match = re.match("(?P<pre>.*&)?"
+                             "(?P<vers>versionId=[^&]+)"
+                             "(?P<post>&.*)?",
+                             url.query)
+        if ver_match:
+            if replace:
+                query = "".join([ver_match.group("pre") or "",
+                                 version_id,
+                                 ver_match.group("post") or ""])
+            else:
+                query = url.query
+        else:
+            query = url.query + "&" + version_id
+    return URL(**dict(url.fields, query=query)).as_str()
+
+
 def get_versioned_url(url, guarantee_versioned=False, return_all=False, verify=False,
-                      s3conn=None):
+                      s3conn=None, update=False):
     """Given a url return a versioned URL
 
     Originally targeting AWS S3 buckets with versioning enabled
 
     Parameters
     ----------
     url : string
@@ -307,55 +439,68 @@
     return_all: bool, optional
       If True, would return a list with URLs for all the versions of this
       file, sorted chronologically with latest first (when possible, e.g.
       for S3).  Remove markers get ignored
     verify: bool, optional
       Verify that URL is accessible. As discovered some versioned keys might
       be denied access to
+    update : bool, optional
+      If the URL already contains a version ID, update it to the latest version
+      ID.  This option has no effect if return_all is true.
 
     Returns
     -------
     string or list of string
     """
-    url_rec = urlparse(url)
+    url_rec = URL(url)
 
     s3_bucket, fpath = None, url_rec.path.lstrip('/')
 
-    if url_rec.netloc.endswith('.s3.amazonaws.com'):
+    was_versioned = False
+    all_versions = []
+
+    if url_rec.hostname.endswith('.s3.amazonaws.com'):
         if url_rec.scheme not in ('http', 'https'):
             raise ValueError("Do not know how to handle %s scheme" % url_rec.scheme)
-        # we know how to slice this cat
-        s3_bucket = url_rec.netloc.split('.', 1)[0]
-    elif url_rec.netloc == 's3.amazonaws.com':
+        # bucket name could have . in it, e.g. openneuro.org
+        s3_bucket = url_rec.hostname[:-len('.s3.amazonaws.com')]
+    elif url_rec.hostname == 's3.amazonaws.com':
         if url_rec.scheme not in ('http', 'https'):
             raise ValueError("Do not know how to handle %s scheme" % url_rec.scheme)
         # url is s3.amazonaws.com/bucket/PATH
         s3_bucket, fpath = fpath.split('/', 1)
     elif url_rec.scheme == 's3':
-        s3_bucket = url_rec.netloc  # must be
-        # and for now implement magical conversion to URL
-        # TODO: wouldn't work if needs special permissions etc
-        # actually for now
-        raise NotImplementedError
+        s3_bucket = url_rec.hostname  # must be
+        if url_rec.query and 'versionId=' in url_rec.query:
+            was_versioned = True
+            all_versions.append(url)
+        else:
+            # and for now implement magical conversion to URL
+            # TODO: wouldn't work if needs special permissions etc
+            # actually for now
+            raise NotImplementedError
 
-    was_versioned = False
-    all_versions = []
     if s3_bucket:
         # TODO: cache
         if s3conn is None:
             # we need to reuse our providers
             from ..downloaders.providers import Providers
             providers = Providers.from_config_files()
             s3url = "s3://%s/" % s3_bucket
             s3provider = providers.get_provider(s3url)
-            if s3provider.authenticator.bucket is not None and s3provider.authenticator.bucket.name == s3_bucket:
+            authenticator = s3provider.authenticator
+            if not authenticator:
+                # We will use anonymous one
+                from ..downloaders.s3 import S3Authenticator
+                authenticator = S3Authenticator()
+            if authenticator.bucket is not None and authenticator.bucket.name == s3_bucket:
                 # we have established connection before, so let's just reuse
-                bucket = s3provider.authenticator.bucket
+                bucket = authenticator.bucket
             else:
-                bucket = s3provider.authenticator.authenticate(s3_bucket, s3provider.credential)  # s3conn or _get_bucket_connection(S3_TEST_CREDENTIAL)
+                bucket = authenticator.authenticate(s3_bucket, s3provider.credential)  # s3conn or _get_bucket_connection(S3_TEST_CREDENTIAL)
         else:
             bucket = s3conn.get_bucket(s3_bucket)
 
         supports_versioning = True  # assume that it does
         try:
             supports_versioning = bucket.get_versioning_status()  # TODO cache
         except S3ResponseError as e:
@@ -372,32 +517,31 @@
             # our current assumptions
             assert(all_keys[0].is_latest)
             # and now filter out delete markers etc
             all_keys = [x for x in all_keys if isinstance(x, Key)]  # ignore DeleteMarkers
             assert(all_keys)
 
             for key in all_keys:
-                version_id = key.version_id
-                query = ((url_rec.query + "&") if url_rec.query else "") \
-                    + "versionId=%s" % version_id
-                url_versioned = urlunparse(url_rec._replace(query=query))
+                url_versioned = add_version_to_url(
+                    url_rec, key.version_id, replace=update and not return_all)
+
                 all_versions.append(url_versioned)
                 if verify:
                     # it would throw HTTPError exception if not accessible
                     _ = urlopen(Request(url_versioned))
                 was_versioned = True
                 if not return_all:
                     break
 
     if guarantee_versioned and not was_versioned:
         raise RuntimeError("Could not version %s" % url)
 
     if not all_versions:
         # we didn't get a chance
-        all_versions = [urlunparse(url_rec)]
+        all_versions = [url_rec.as_str()]
 
     if return_all:
         return all_versions
     else:
         return all_versions[0]
```

### Comparing `datalad-0.9.3/datalad/support/archives.py` & `datalad-1.0.0/datalad/support/archives.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,122 +1,71 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 """Various handlers/functionality for different types of files (e.g. for archives)
 
 """
 
 import hashlib
-import patoolib
-from .external_versions import external_versions
-# There were issues, so let's stay consistently with recent version
-assert(external_versions["patoolib"] >= "1.7")
-
+import logging
 import os
-import tempfile
-from os.path import join as opj, exists, abspath, isabs, normpath, relpath, pardir, isdir
-from os.path import sep as opsep
-from os.path import realpath
-from six import next
-from six.moves.urllib.parse import unquote as urlunquote
-
-import string
 import random
+import string
+import tempfile
 
-from .locking import lock_if_check_fails
-from ..utils import any_re_search
-
-import logging
-lgr = logging.getLogger('datalad.files')
-
-# Monkey-patch patoolib's logging, so it logs coherently with the rest of
-# datalad
-import patoolib.util
-#
-# Seems have managed with swallow_outputs
-#
-# def _patool_log(level, msg):
-#     lgr.log(level, "patool: %s" % msg)
-#
-# def _patool_log_info(msg, *args, **kwargs):
-#     _patool_log(logging.DEBUG, msg)
-#
-# def _patool_log_error(msg, *args, **kwargs):
-#     _patool_log(logging.ERROR, msg)
-#
-# patoolib.util.log_info = _patool_log_info
-# patoolib.util.log_error = _patool_log_error
-# patoolib.util.log_internal_error = _patool_log_error
-
-# we need to decorate patool.util.run
-# because otherwise it just lets processes to spit out everything to std and we
-# do want to use it at "verbosity>=0" so we could get idea on what is going on.
-# And I don't want to mock for every invocation
-from ..support.exceptions import CommandError
-from ..utils import swallow_outputs
-from ..utils import rmtemp
-from ..cmd import Runner
-from ..consts import ARCHIVES_TEMP_DIR
-from ..utils import rmtree
-from ..utils import get_tempfile_kwargs
-
-from ..utils import on_windows
-
-_runner = Runner()
-
-
-def _patool_run(cmd, verbosity=0, **kwargs):
-    """Decorated runner for patool so it doesn't spit out outputs to stdout"""
-    # use our runner
-    try:
-        # kwargs_ = kwargs[:];         kwargs_['shell'] = True
-        _runner.run(cmd, **kwargs)
-        return 0
-    except CommandError as e:
-        return e.code
-    except Exception as e:
-        lgr.error("While invoking runner caught unexpected exception: %s" % e)
-        return 100  # unknown beast
-patoolib.util.run = _patool_run
-
-
-# yoh: only keys are used atm, logic in decompress_file is replaced to use
-# patool
-
-DECOMPRESSORS = {
-    '\.(tar\.bz|tbz)$': 'tar -xjvf %(file)s -C %(dir)s',
-    '\.(tar\.xz)$': 'tar -xJvf %(file)s -C %(dir)s',
-    '\.(tar\.gz|tgz)$': 'tar -xzvf %(file)s -C %(dir)s',
-    '\.(zip)$': 'unzip %(file)s -d %(dir)s',
-}
-
-
-def unixify_path(path):
-    """On windows convert paths from drive:\d\file to /drive/d/file
+from datalad import cfg
+from datalad.config import anything2bool
+from datalad.consts import ARCHIVES_TEMP_DIR
+from datalad.support.external_versions import external_versions
+from datalad.support.locking import lock_if_check_fails
+from datalad.support.path import (
+    abspath,
+    exists,
+    isabs,
+    isdir,
+)
+from datalad.support.path import join as opj
+from datalad.support.path import (
+    normpath,
+    pardir,
+    relpath,
+)
+from datalad.support.path import sep as opsep
+from datalad.utils import (
+    Path,
+    any_re_search,
+    ensure_bytes,
+    ensure_unicode,
+    get_tempfile_kwargs,
+    on_windows,
+    rmtemp,
+    rmtree,
+    unlink,
+)
+
+# fall back on patool, if requested, or 7z is not found
+if (cfg.obtain('datalad.runtime.use-patool', default=False,
+               valtype=anything2bool)
+        or not external_versions['cmd:7z']):
+    from datalad.support.archive_utils_patool import compress_files
+    from datalad.support.archive_utils_patool import \
+        decompress_file as \
+        _decompress_file  # other code expects this to be here
+else:
+    from datalad.support.archive_utils_7z import compress_files
+    from datalad.support.archive_utils_7z import \
+        decompress_file as \
+        _decompress_file  # other code expects this to be here
 
-    This overcomes problems with various cmdline tools we are to use,
-    such as tar etc
-    """
-    if on_windows:
-        drive, path_ = os.path.splitdrive(path)
-        path_ = path_.split(os.sep)
-        path_ = '/'.join(path_)
-        if drive:
-            # last one must be :
-            assert(drive[-1] == ":")
-            return '/%s%s' % (drive[:-1], path_)
-        else:
-            return path_
-    else:
-        return path
+lgr = logging.getLogger('datalad.support.archives')
 
 
 def decompress_file(archive, dir_, leading_directories='strip'):
     """Decompress `archive` into a directory `dir_`
 
     Parameters
     ----------
@@ -124,112 +73,43 @@
     dir_: str
     leading_directories: {'strip', None}
       If `strip`, and archive contains a single leading directory under which
       all content is stored, all the content will be moved one directory up
       and that leading directory will be removed.
     """
     if not exists(dir_):
-        lgr.debug("Creating directory %s to extract archive into" % dir_)
+        lgr.debug("Creating directory %s to extract archive into", dir_)
         os.makedirs(dir_)
 
-    with swallow_outputs() as cmo:
-        patoolib.util.check_existing_filename(archive)
-        patoolib.util.check_existing_filename(dir_, onlyfiles=False)
-        # Call protected one to avoid the checks on existence on unixified path
-        patoolib._extract_archive(unixify_path(archive),
-                                  outdir=unixify_path(dir_),
-                                  verbosity=100)
-        if cmo.out:
-            lgr.debug("patool gave stdout:\n%s" % cmo.out)
-        if cmo.err:
-            lgr.debug("patool gave stderr:\n%s" % cmo.err)
-
-    # Note: (ben) Experienced issue, where extracted tarball
-    # lacked execution bit of directories, leading to not being
-    # able to delete them while having write permission.
-    # Can't imagine a situation, where we would want to fail on
-    # that kind of mess. So, to be sure set it.
-
-    if not on_windows:
-        os.chmod(dir_,
-                 os.stat(dir_).st_mode |
-                 os.path.stat.S_IEXEC)
-        for root, dirs, files in os.walk(dir_, followlinks=False):
-            for d in dirs:
-                subdir = opj(root, d)
-                os.chmod(subdir,
-                         os.stat(subdir).st_mode |
-                         os.path.stat.S_IEXEC)
+    _decompress_file(archive, dir_)
 
     if leading_directories == 'strip':
         _, dirs, files = next(os.walk(dir_))
         if not len(files) and len(dirs) == 1:
             # move all the content under dirs[0] up 1 level
             widow_dir = opj(dir_, dirs[0])
-            lgr.debug("Moving content within %s upstairs" % widow_dir)
+            lgr.debug("Moving content within %s upstairs", widow_dir)
             subdir, subdirs_, files_ = next(os.walk(opj(dir_, dirs[0])))
             for f in subdirs_ + files_:
                 os.rename(opj(subdir, f), opj(dir_, f))
-            os.rmdir(widow_dir)
+            # NFS might hold it victim so use rmtree so it tries a few times
+            rmtree(widow_dir)
     elif leading_directories is None:
         pass   # really do nothing
     else:
         raise NotImplementedError("Not supported %s" % leading_directories)
 
 
-def compress_files(files, archive, path=None, overwrite=True):
-    """Compress `files` into an `archive` file
-
-    Parameters
-    ----------
-    files : list of str
-    archive : str
-    path : str
-      Alternative directory under which compressor will be invoked, to e.g.
-      take into account relative paths of files and/or archive
-    overwrite : bool
-      Either to allow overwriting the target archive file if one already exists
-    """
-
-    with swallow_outputs() as cmo:
-        # to test filenames, if path is not None, we should join:
-        if path:
-            opj_path = lambda p: opj(path, p)
-        else:
-            opj_path = lambda p: p
-        if not overwrite:
-            patoolib.util.check_new_filename(opj_path(archive))
-        patoolib.util.check_archive_filelist([opj_path(f) for f in files])
-
-        # ugly but what can you do? ;-) we might wrap it all into a class
-        # at some point. TODO
-        old_cwd = _runner.cwd
-        if path is not None:
-            _runner.cwd = path
-        try:
-            # Call protected one to avoid the checks on existence on unixified path
-            patoolib._create_archive(unixify_path(archive),
-                                     [unixify_path(f) for f in files],
-                                     verbosity=100)
-        finally:
-            _runner.cwd = old_cwd
-
-        if cmo.out:
-            lgr.debug("patool gave stdout:\n%s" % cmo.out)
-        if cmo.err:
-            lgr.debug("patool gave stderr:\n%s" % cmo.err)
-
-
 def _get_cached_filename(archive):
     """A helper to generate a filename which has original filename and additional suffix
     which wouldn't collide across files with the same name from different locations
     """
     #return "%s_%s" % (basename(archive), hashlib.md5(archive).hexdigest()[:5])
     # per se there is no reason to maintain any long original name here.
-    archive_cached = hashlib.md5(realpath(archive).encode()).hexdigest()[:10]
+    archive_cached = hashlib.md5(ensure_bytes(str(Path(archive).resolve()))).hexdigest()[:10]
     lgr.debug("Cached directory for archive %s is %s", archive, archive_cached)
     return archive_cached
 
 
 def _get_random_id(size=6, chars=string.ascii_uppercase + string.digits):
     """Return a random ID composed from digits and uppercase letters
 
@@ -249,62 +129,81 @@
     persistent : bool, optional
       Passed over into generated ExtractedArchives
     """
     # TODO: make caching persistent across sessions/runs, with cleanup
     # IDEA: extract under .git/annex/tmp so later on annex unused could clean it
     #       all up
     def __init__(self, toppath=None, persistent=False):
-
         self._toppath = toppath
         if toppath:
             path = opj(toppath, ARCHIVES_TEMP_DIR)
             if not persistent:
                 tempsuffix = "-" + _get_random_id()
                 lgr.debug("For non-persistent archives using %s suffix for path %s",
                           tempsuffix, path)
                 path += tempsuffix
+            # TODO: begging for a race condition
+            if not exists(path):
+                lgr.debug("Initiating clean cache for the archives under %s",
+                          path)
+                try:
+                    self._made_path = True
+                    os.makedirs(path)
+                    lgr.debug("Cache initialized")
+                except Exception:
+                    lgr.error("Failed to initialize cached under %s", path)
+                    raise
+            else:
+                lgr.debug(
+                    "Not initiating existing cache for the archives under %s",
+                    path)
+                self._made_path = False
         else:
             if persistent:
-                raise ValueError("%s cannot be persistent since no toppath was provided" % self)
-            path = tempfile.mktemp(**get_tempfile_kwargs())
+                raise ValueError(
+                    "%s cannot be persistent, because no toppath was provided"
+                    % self)
+            path = tempfile.mkdtemp(**get_tempfile_kwargs())
+            self._made_path = True
+
         self._path = path
         self.persistent = persistent
-        # TODO?  assure that it is absent or we should allow for it to persist a bit?
+        # TODO?  ensure that it is absent or we should allow for it to persist a bit?
         #if exists(path):
         #    self._clean_cache()
         self._archives = {}
 
         # TODO: begging for a race condition
         if not exists(path):
-            lgr.debug("Initiating clean cache for the archives under %s" % self.path)
+            lgr.debug("Initiating clean cache for the archives under %s", self.path)
             try:
                 self._made_path = True
                 os.makedirs(path)
                 lgr.debug("Cache initialized")
-            except:
-                lgr.error("Failed to initialize cached under %s" % path)
+            except Exception as e:
+                lgr.error("Failed to initialize cached under %s", path)
                 raise
         else:
-            lgr.debug("Not initiating existing cache for the archives under %s" % self.path)
+            lgr.debug("Not initiating existing cache for the archives under %s", self.path)
             self._made_path = False
 
     @property
     def path(self):
         return self._path
 
     def clean(self, force=False):
         for aname, a in list(self._archives.items()):
             a.clean(force=force)
             del self._archives[aname]
         # Probably we should not rely on _made_path and not bother if persistent removing it
         # if ((not self.persistent) or force) and self._made_path:
-        #     lgr.debug("Removing the entire archives cache under %s" % self.path)
+        #     lgr.debug("Removing the entire archives cache under %s", self.path)
         #     rmtemp(self.path)
         if (not self.persistent) or force:
-            lgr.debug("Removing the entire archives cache under %s" % self.path)
+            lgr.debug("Removing the entire archives cache under %s", self.path)
             rmtemp(self.path)
 
     def _get_normalized_archive_path(self, archive):
         """Return full path to archive
 
         So we have consistent operation from different subdirs,
         while referencing archives from the topdir
@@ -383,15 +282,15 @@
             (self.stamp_path, 'stamp file')
         ]:
             if exists(path):
                 if (not self._persistent) or force:
                     lgr.debug("Cleaning up the %s for %s under %s", name, self._archive, path)
                     # TODO:  we must be careful here -- to not modify permissions of files
                     #        only of directories
-                    (rmtree if isdir(path) else os.unlink)(path)
+                    (rmtree if isdir(path) else unlink)(path)
 
     @property
     def path(self):
         """Given an archive -- return full path to it within cache (extracted)
         """
         return self._path
 
@@ -419,15 +318,15 @@
                 self._extract_archive(path)
         return path
 
     def _extract_archive(self, path):
         # we need to extract the archive
         # TODO: extract to _tmp and then move in a single command so we
         # don't end up picking up broken pieces
-        lgr.debug("Extracting {self._archive} under {path}".format(**locals()))
+        lgr.debug("Extracting %s under %s", self._archive, path)
         if exists(path):
             lgr.debug(
                 "Previous extracted (but probably not fully) cached archive "
                 "found. Removing %s",
                 path)
             rmtree(path)
         os.makedirs(path)
@@ -438,39 +337,39 @@
         decompress_file(self._archive, path, leading_directories=None)
         # TODO: must optional since we might to use this content, move it
         # into the tree etc
         # lgr.debug("Adjusting permissions to R/O for the extracted content")
         # rotree(path)
         assert (exists(path))
         # create a stamp
-        with open(self.stamp_path, 'w') as f:
-            f.write(self._archive)
+        with open(self.stamp_path, 'wb') as f:
+            f.write(ensure_bytes(self._archive))
         # assert that stamp mtime is not older than archive's directory
         assert (self.is_extracted)
 
     # TODO: remove?
     #def has_file_ready(self, afile):
-    #    lgr.debug("Checking file {afile} from archive {archive}".format(**locals()))
+    #    lgr.debug(u"Checking file {afile} from archive {archive}".format(**locals()))
     #    return exists(self.get_extracted_filename(afile))
 
     def get_extracted_filename(self, afile):
         """Return full path to the `afile` within extracted `archive`
 
         It does not actually extract any archive
         """
-        return opj(self.path, urlunquote(afile))
+        return opj(self.path, afile)
 
     def get_extracted_files(self):
         """Generator to provide filenames which are available under extracted archive
         """
         path = self.assure_extracted()
         path_len = len(path) + (len(os.sep) if not path.endswith(os.sep) else 0)
         for root, dirs, files in os.walk(path):  # TEMP
             for name in files:
-                yield opj(root, name)[path_len:]
+                yield ensure_unicode(opj(root, name)[path_len:])
 
     def get_leading_directory(self, depth=None, consider=None, exclude=None):
         """Return leading directory of the content within archive
 
         Parameters
         ----------
         depth: int or None, optional
@@ -512,25 +411,25 @@
                     leading = leading_
             if not len(leading):
                 # no common leading - ready to exit
                 return None
         return leading if leading is None else opj(*leading)
 
     def get_extracted_file(self, afile):
-        lgr.debug("Requested file {afile} from archive {self._archive}".format(**locals()))
+        lgr.debug("Requested file %s from archive %s", afile, self._archive)
         # TODO: That could be a good place to provide "compatibility" layer if
         # filenames within archive are too obscure for local file system.
         # We could somehow adjust them while extracting and here channel back
         # "fixed" up names since they are only to point to the load
         self.assure_extracted()
         path = self.get_extracted_filename(afile)
         # TODO: make robust
-        lgr.log(2, "Verifying that %s exists" % abspath(path))
+        lgr.log(2, "Verifying that %s exists", abspath(path))
         assert exists(path), "%s must exist" % path
         return path
 
     def __del__(self):
         try:
             if self._persistent:
                 self.clean()
-        except:  # MIH: IOError?
+        except Exception as e:  # MIH: IOError?
             pass
```

### Comparing `datalad-0.9.3/datalad/support/stats.py` & `datalad-1.0.0/datalad/support/stats.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 """A helper for collecting stats on carried out actions
@@ -181,12 +181,12 @@
         if mode == 'full':
             return '\n'.join(out)
         elif mode == 'line':
             for i, o in enumerate(out):
                 if o[0] != ' ':
                     out[i] = '  ' + o
             return ','.join(out).lstrip()
-            return "{files} files (git/annex: {add_git}/{add_annex}), " \
-                   "{skipped} skipped, {renamed} renamed, {overwritten} overwritten".format(
-                       **entries)
+            #return "{files} files (git/annex: {add_git}/{add_annex}), " \
+            #       "{skipped} skipped, {renamed} renamed, {overwritten} overwritten".format(
+            #           **entries)
         else:
             raise ValueError("Unknown mode %s" % mode)
```

### Comparing `datalad-0.9.3/datalad/support/cache.py` & `datalad-1.0.0/datalad/support/cache.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,25 +1,21 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 """Simple constructs to be used as caches
 """
 
 from collections import OrderedDict
-from six import PY2
 
-if PY2:
-    from ._lru_cache2 import lru_cache
-else:
-    from functools import lru_cache
+from functools import lru_cache
 
 
 # based on http://stackoverflow.com/a/2437645/1265472
 class DictCache(OrderedDict):
     """A simple cache (dictionary) with limited size which expunges oldest entries
     """
     def __init__(self, *args, **kwds):
```

### Comparing `datalad-0.9.3/datalad/crawler/dbs/versions.py` & `datalad-1.0.0/datalad/tests/test_version.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,101 +1,77 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""Simple helper to store/retrieve information about the version of the last scraped
 
-"""
+import re
 
-from collections import OrderedDict
-from six import iteritems
+from packaging.version import Version
 
-from ...utils import auto_repr
-from ...consts import CRAWLER_META_VERSIONS_DIR
-
-from .base import JsonBaseDB
-
-import logging
-lgr = logging.getLogger('datalad.crawler.dbs')
-
-__docformat__ = 'restructuredtext'
-
-
-@auto_repr
-class SingleVersionDB(JsonBaseDB):
-    """
-    Simple helper to store/retrieve information about the last scraped version
-
-    Since we do not expect many changes being done to this DB, it also
-    saves its state into the file upon any change
-    """
-    __version__ = 1
-    __crawler_subdir__ = CRAWLER_META_VERSIONS_DIR
-
-    #
-    # Defining abstract methods implementations
-    #
-    def _get_empty_db(self):
-        return {'version': None,
-                'versions': OrderedDict()}
-
-    def _get_loaded_db(self, db):
-        """Given a DB loaded from a file, prepare it for being used
-        """
-        assert (set(db.keys()) == {'db_version', 'version', 'versions'})
-        # no compatibility layers for now
-        assert (db['db_version'] == self.__class__.__version__)
-        db['versions'] = OrderedDict(db['versions'])
-        return db
-
-    def _get_db_to_save(self):
-        """Return DB to be saved as JSON file
-        """
-        db = self._db.copy()
-        # since we have it ordered, let's store it as a list of items
-        db['versions'] = list(db['versions'].items())
-        return db
-
-    #
-    # Custom properties and methods
-    #
-    @property
-    def version(self):
-        return self._db['version']
-
-    @version.setter
-    def version(self, v):
-        self._db['version'] = v
-        self.save()
-
-
-    @property
-    def versions(self):
-        return self._db['versions']
-
-    # @versions.setter
-    # def versions(self, v):
-    #     self._db['versions'] = v
-    #     self.save()
-
-    def update_versions(self, new_versions):
-        """Update known versions with new information
-        """
-        versions = self._db['versions']
-        for new_version, new_fpaths in iteritems(new_versions):
-            if new_version not in versions:
-                # TODO: check that it is newer!?
-                versions[new_version] = {}
-            fpaths = versions[new_version]
-            for new_fpath, entry in iteritems(new_fpaths):
-                if new_fpath not in fpaths:
-                    # new new_fpath
-                    fpaths[new_fpath] = {}
-                if entry in fpaths[new_fpath] and entry != fpaths[new_fpath]:
-                    raise NotImplementedError("conflict resolutions for when new item added for the same entry")
+from datalad.support import path as op
+from datalad.tests.utils_pytest import (
+    SkipTest,
+    assert_equal,
+    assert_greater,
+    assert_in,
+    assert_not_in,
+    ok_startswith,
+)
+from datalad.utils import ensure_unicode
+
+from .. import __version__
+
+
+def test__version__():
+    # in released stage, version in the last CHANGELOG entry
+    # should correspond to the one in datalad
+    CHANGELOG_filename = op.join(
+        op.dirname(__file__), op.pardir, op.pardir, 'CHANGELOG.md')
+    if not op.exists(CHANGELOG_filename):
+        raise SkipTest("no %s found" % CHANGELOG_filename)
+    regex = re.compile(r'^# '
+                       r'(?P<version>[0-9]+\.[0-9.abcrc~]+)\s+'
+                       r'\((?P<date>.*)\)'
+                       )
+    with open(CHANGELOG_filename, 'rb') as f:
+        for line in f:
+            line = line.rstrip()
+            if not line.startswith(b'# '):
+                # The first section header we hit, must be our changelog entry
+                continue
+            reg = regex.match(ensure_unicode(line))
+            if not reg:  # first one at that level is the one
+                raise AssertionError(
+                    "Following line must have matched our regex: %r" % line)
+            regd = reg.groupdict()
+            changelog_version = regd['version']
+            lv_changelog_version = Version(changelog_version)
+            # we might have a suffix - sanitize
+            san__version__ = __version__.rstrip('.dirty')
+            lv__version__ = Version(san__version__)
+            if '???' in regd['date'] and 'will be better than ever' in regd['codename']:
+                # we only have our template
+                # we can only assert that its version should be higher than
+                # the one we have now
+                assert_greater(lv_changelog_version, lv__version__)
+            else:
+                # should be a "release" record
+                assert_not_in('???', regd['date'])
+                ok_startswith(__version__, changelog_version)
+                if lv__version__ != lv_changelog_version:
+                    # It was not tagged yet and Changelog has no new records
+                    # (they are composed by auto upon release)
+                    assert_greater(lv__version__, lv_changelog_version)
+                    assert_in('+', san__version__)  # we have build suffix
                 else:
-                    fpaths[new_fpath] = entry
-        self.save()
+                    # all is good, tagged etc
+                    assert_equal(lv_changelog_version, lv__version__)
+                    assert_equal(changelog_version, san__version__)
+            return
+
+    raise AssertionError(
+        "No log line matching our regex found in %s" % CHANGELOG_filename
+    )
```

### Comparing `datalad-0.9.3/datalad/crawler/oldconfig/__init__.py` & `datalad-1.0.0/datalad/support/itertools.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,15 +1,19 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""Old configuration handling for the crawler
+"""Auxiliary itertools"""
 
-"""
+import itertools
 
-__docformat__ = 'restructuredtext'
 
-from .base import EnhancedConfigParser
+def groupby_sorted(iter, key=None):
+    """A little helper which first sorts iterable by the same key
+
+    Since groupby expects sorted entries
+    """
+    yield from itertools.groupby(sorted(iter, key=key), key=key)
```

### Comparing `datalad-0.9.3/datalad/crawler/nodes/annex.py` & `datalad-1.0.0/datalad/distributed/ora_remote.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,1445 +1,1659 @@
-# emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
-# ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-#
-#   See COPYING file distributed along with the datalad package for the
-#   copyright and license terms.
-#
-# ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""Nodes to interact with annex -- initiate a new dataset or operate with existing one
+import functools
+import os
+import stat
+import sys
+from pathlib import (
+    Path,
+    PurePosixPath
+)
+from contextlib import contextmanager
+import requests
+import shutil
+from shlex import quote as sh_quote
+import subprocess
+import logging
+from functools import wraps
+
+from datalad import ssh_manager
+from datalad.config import anything2bool
+from datalad.customremotes import (
+    ProtocolError,
+    RemoteError,
+    SpecialRemote,
+)
+from datalad.customremotes.main import main as super_main
+from datalad.support.annex_utils import _sanitize_key
+from datalad.support.annexrepo import AnnexRepo
+from datalad.support.exceptions import (
+    AccessDeniedError,
+    AccessFailedError,
+    CapturedException,
+    DownloadError
+)
+from datalad.support.network import url_path2local_path
+from datalad.customremotes.ria_utils import (
+    get_layout_locations,
+    UnknownLayoutVersion,
+    verify_ria_url,
+)
+from datalad.utils import (
+    ensure_write_permission,
+    on_osx
+)
+
+
+lgr = logging.getLogger('datalad.customremotes.ria_remote')
+
+DEFAULT_BUFFER_SIZE = 65536
+
+# TODO
+# - make archive check optional
+
+
+# only use by _get_datalad_id
+def _get_gitcfg(gitdir, key, cfgargs=None, regex=False):
+    cmd = [
+        'git',
+        '--git-dir', gitdir,
+        'config',
+    ]
+    if cfgargs:
+        cmd += cfgargs
+
+    cmd += ['--get-regexp'] if regex else ['--get']
+    cmd += [key]
+    try:
+        return subprocess.check_output(
+            cmd,
+            # yield text
+            universal_newlines=True).strip()
+    except Exception:
+        lgr.debug(
+            "Failed to obtain config '%s' at %s",
+            key, gitdir,
+        )
+        return None
 
-via Annexificator class, which could be used to add files, checkout branches, etc
-"""
 
-import os
-import re
-import time
-from os import listdir
-from os.path import expanduser, join as opj, exists, isabs, lexists, curdir, realpath
-from os.path import split as ops
-from os.path import isdir, islink
-from os.path import relpath
-from os import unlink, makedirs
-from collections import OrderedDict
-from humanize import naturalsize
-from six import iteritems
-from six import string_types
-from distutils.version import LooseVersion
-from functools import partial
-
-from git import Repo
-
-from ...version import __version__
-from ...api import add_archive_content
-from ...api import clean
-from ...consts import CRAWLER_META_DIR, CRAWLER_META_CONFIG_FILENAME
-from ...utils import rmtree, updated
-from ...utils import lmtime
-from ...utils import find_files
-from ...utils import auto_repr
-from ...utils import _path_
-from ...utils import getpwd
-from ...utils import try_multiple
-from ...utils import assure_list
-from ...tests.utils import put_file_under_git
-
-from ...downloaders.providers import Providers
-from ...distribution.dataset import Dataset
-from ...api import create
-from ...support.configparserinc import SafeConfigParserWithIncludes
-from ...support.gitrepo import GitRepo, _normalize_path
-from ...support.annexrepo import AnnexRepo
-from ...support.stats import ActivityStats
-from ...support.versions import get_versions
-from ...support.exceptions import AnnexBatchCommandError
-from ...support.external_versions import external_versions
-from ...support.network import get_url_straight_filename, get_url_disposition_filename
-
-from ... import cfg
-from ...cmd import get_runner
-
-from ..pipeline import CRAWLER_PIPELINE_SECTION
-from ..pipeline import initiate_pipeline_config
-from ..dbs.files import PhysicalFileStatusesDB, JsonFileStatusesDB
-from ..dbs.versions import SingleVersionDB
-from datalad.customremotes.base import init_datalad_remote
-from datalad.dochelpers import exc_str
-
-from logging import getLogger
-
-lgr = getLogger('datalad.crawl.annex')
-
-_runner = get_runner()
-_call = _runner.call
-_run = _runner.run
-
-
-# TODO: make use of datalad_stats
-@auto_repr
-class initiate_dataset(object):
-    """Action to initiate a dataset following one of the known templates
+# cannot be replaced until https://github.com/datalad/datalad/issues/6264
+# is fixed
+def _get_datalad_id(gitdir):
+    """Attempt to determine a DataLad dataset ID for a given repo
+
+    Returns
+    -------
+    str or None
+      None in case no ID was found
     """
+    dsid = _get_gitcfg(
+        gitdir, 'datalad.dataset.id', ['--blob', ':.datalad/config']
+    )
+    if dsid is None:
+        lgr.debug(
+            "Cannot determine a DataLad ID for repository: %s",
+            gitdir,
+        )
+    else:
+        dsid = dsid.strip()
+    return dsid
 
-    def __init__(self, template, dataset_name=None,
-                 path=None, branch=None, backend=None,
-                 template_func=None, template_kwargs=None,
-                 add_to_super='auto',
-                 data_fields=[], add_fields={}, existing=None):
-        """
-        Parameters
-        ----------
-        template : str
-          Which template (probably matching the superdataset name) to use.
-          TODO: refer to specs of template that it might understand some
-          arguments encoded, such as #func=custom_pipeline
-        template_func : str, optional
-          Explicitly specify the function name within template module
-        template_kwargs: dict, optional
-          Keyword arguments to pass into the `template_func`.
-        dataset_name : str, optional
-          Name of the dataset. If None, reacts on 'dataset_name' in data
-        path : str, optional
-          Path were to initiate the dataset.  If not specified, would use
-          default path for all new datasets (DATALAD_CRAWL_COLLECTIONSPATH)
-        branch : str, optional
-          Which branch to initialize
-        backend : str, optional
-          Supported by git-annex backend.  By default (if None specified),
-          it is MD5E backend to improve compatibility with filesystems
-          having a relatively small limit for a maximum path size
-        add_to_super : bool or 'auto', optional
-          Add to super-dataset
-        data_fields : list or tuple of str, optional
-          Additional fields from data to store into configuration for
-          the dataset crawling options -- would be passed into the corresponding
-          crawler template
-        add_fields : dict, optional
-          Dictionary of additional fields to store in the crawler configuration
-          to be passed into the template
-        existing : ('skip', 'raise', 'adjust', 'replace', 'crawl'), optional
-          Behavior if encountering existing dataset
-        """
-        # TODO: add_fields might not be flexible enough for storing more elaborate
-        # configurations for e.g. "basic" template
 
-        self.template = template
-        self.template_func = template_func
-        self.template_kwargs = template_kwargs
-        self.dataset_name = dataset_name
-        self.data_fields = data_fields
-        self.add_fields = add_fields
-        self.existing = existing
-        self.path = path
-        self.branch = branch
-        # TODO: backend -> backends (https://github.com/datalad/datalad/issues/358)
-        self.backend = backend
-        self.add_to_super = add_to_super
-
-    def _initiate_dataset(self, path, name):
-        lgr.info("Initiating dataset %s" % name)
-
-        if self.branch is not None:
-            raise NotImplementedError("Disabled for now")
-            # because all the 'create' magic is stuffed into the constructor ATM
-            # we need first to initiate a git repository
-            git_repo = GitRepo(path, create=True)
-            # since we are initiating, that branch shouldn't exist yet, thus --orphan
-            git_repo.checkout(self.branch, options=["--orphan"])
-            # TODO: RF whenevever create becomes a dedicated factory/method
-            # and/or branch becomes an option for the "creator"
-
-        backend = self.backend or cfg.obtain('datalad.crawl.default_backend', default='MD5E')
-        direct = cfg.obtain('datalad.crawl.init_direct', default=False)
-
-        if direct:
-            raise NotImplementedError("Disabled for now to init direct mode ones")
-
-        ds = create(
-                path=path,
-                force=False,
-                # no_annex=False,  # TODO: add as an arg
-                # Passing save arg based on backend was that we need to save only if
-                #  custom backend was specified, but now with dataset id -- should always save
-                # save=not bool(backend),
-                # annex_version=None,
-                annex_backend=backend,
-                #git_opts=None,
-                #annex_opts=None,
-                #annex_init_opts=None
-        )
-        if self.add_to_super:
-            # place hack from 'add-to-super' times here
-            sds = ds.get_superdataset()
-            if sds is not None:
-                lgr.debug("Adding %s as a subdataset to %s", ds, sds)
-                sds.add(ds.path, save=False)
-                # this leaves the subdataset staged in the parent
-            elif str(self.add_to_super) != 'auto':
-                raise ValueError(
-                    "Was instructed to add to super dataset but no super dataset "
-                    "was found for %s" % ds
-                )
+class RemoteCommandFailedError(Exception):
+    pass
 
-        return ds
 
-    def _save_crawl_config(self, dataset_path, data):
-        kwargs = self.template_kwargs or {}
-        # update with those from data
-        kwargs.update({f: data[f] for f in self.data_fields})
-        # additional options given as a dictionary
-        kwargs.update(self.add_fields)
-        return initiate_pipeline_config(
-            template=self.template,
-            template_func=self.template_func,
-            template_kwargs=kwargs,
-            path=dataset_path,
-            commit=True
-        )
+class RIARemoteError(RemoteError):
+    pass
 
-    def __call__(self, data={}):
-        # figure out directory where create such a dataset
-        dataset_name = self.dataset_name or data.get('dataset_name', None)
-        dataset_path = opj(os.curdir, dataset_name) \
-            if self.path is None \
-            else self.path
-
-        data_updated = updated(data, {'dataset_path': dataset_path,
-                                      'dataset_name': dataset_name})
-        lgr.debug("Request to initialize a dataset %s at %s", dataset_name, dataset_path)
-        init = True
-        if exists(dataset_path):
-            # TODO: config crawl.subdataset.existing = skip|raise|replace|crawl|adjust
-            # TODO: config crawl.subdataset.crawl_new = false|true
-            existing = self.existing or 'skip'
-            if existing == 'skip':
-                lgr.info("Skipping dataset %s since already exists" % dataset_name)
-                yield data_updated
-                return
-            elif existing == 'raise':
-                raise RuntimeError("%s already exists" % dataset_path)
-            elif existing == 'replace':
-                _call(rmtree, dataset_path)
-            elif existing == 'adjust':
-                # E.g. just regenerate configs/meta
-                init = False
-            else:  # TODO: 'crawl'  ;)
-                raise ValueError(self.existing)
-        if init:
-            _call(self._initiate_dataset, dataset_path, dataset_name)
-        _call(self._save_crawl_config, dataset_path, data)
-
-        yield data_updated
 
+class IOBase(object):
+    """Abstract class with the desired API for local/remote operations"""
 
-class Annexificator(object):
-    """A helper which would encapsulate the operation of adding new content to git/annex repo
+    def get_7z(self):
+        raise NotImplementedError
 
-    If 'filename' field was not found in the data, filename from the URL
-    gets taken.
+    def mkdir(self, path):
+        raise NotImplementedError
 
-    'path' field of data (if present) is used to define path within the subdirectory.
-    Should be relative. If absolute found -- ValueError is raised
-    """
+    def symlink(self, target, link_name):
+        raise NotImplementedError
+
+    def put(self, src, dst, progress_cb):
+        raise NotImplementedError
+
+    def get(self, src, dst, progress_cb):
+        raise NotImplementedError
+
+    def rename(self, src, dst):
+        raise NotImplementedError
+
+    def remove(self, path):
+        raise NotImplementedError
+
+    def exists(self, path):
+        raise NotImplementedError
 
-    def __init__(self, path=None,
-                 no_annex=False,
-                 mode='full', options=None,
-                 special_remotes=[],
-                 allow_dirty=False, yield_non_updated=False,
-                 auto_finalize=True,
-                 statusdb=None,
-                 skip_problematic=False,
-                 largefiles=None,
-                 **kwargs):
+    def get_from_archive(self, archive, src, dst, progress_cb):
+        """Get a file from an archive
+
+        Parameters
+        ----------
+        archive_path : Path or str
+          Must be an absolute path and point to an existing supported archive
+        file_path : Path or str
+          Must be a relative Path (relative to the root
+          of the archive)
         """
+        raise NotImplementedError
 
-        Note that always_commit=False for the used AnnexRepo to minimize number
-        of unnecessary commits
+    def in_archive(self, archive_path, file_path):
+        """Test whether a file is in an archive
 
         Parameters
         ----------
-        mode : str of {'full', 'fast', 'relaxed'}
-          What mode of download to use for the content.  In "full" content gets downloaded
-          and checksummed (according to the backend), 'fast' and 'relaxed' are just original
-          annex modes where no actual download is performed and the files' keys are their URLs
-        no_annex : bool
-          Assume/create a simple Git repository, without git-annex
-        special_remotes : list, optional
-          List of custom special remotes to initialize and enable by default
-        yield_non_updated : bool, optional
-          Either to yield original data (with filepath) if load was not updated in annex
-        auto_finalize : bool, optional
-          In some cases, if e.g. adding a file in place of an existing directory or placing
-          a file under a directory for which there is a file atm, we would 'finalize' before
-          carrying out the operation
-        statusdb : {'json', 'fileattr'}, optional
-          DB of file statuses which will be used to figure out if remote load has changed.
-          If None, no statusdb will be used so Annexificator will process every given URL
-          as if it leads to new content.  'json' -- JsonFileStatusesDB will
-          be used which will store information about each provided file/url into a JSON file.
-          'fileattr' -- PhysicalFileStatusesDB will be used to decide based on information in
-          annex and file(s) mtime on the disk.
-          Note that statusdb "lives" within the branch, so switch_branch would drop existing DB (which
-          should get committed within the branch) and would create a new one if DB is requested
-          again.
-        skip_problematic: bool, optional
-          If True, it would not raise an exception if e.g. url is 404 or forbidden -- then just
-          nothing is yielded, and effectively that entry is skipped
-        largefiles: str, optional
-          A setting to pass as '-c annex.largefiles=' option to all git annex calls
-          in case largefiles setting is not yet defined in "git attributes"
-        **kwargs : dict, optional
-          to be passed into AnnexRepo
+        archive_path : Path or str
+          Must be an absolute path and point to an existing supported archive
+        file_path : Path or str
+          Must be a relative Path (relative to the root
+          of the archive)
         """
-        if path is None:
-            path = realpath(curdir)
-        # TODO: commented out to ease developing for now
-        # self.repo = _call(AnnexRepo, path, **kwargs)
-        # TODO: backend -- should be fetched from the config I guess... or should we
-        # give that duty to the dataset initialization routine to change default backend?
-        # Well -- different annexifiers might have different ideas for the backend, but
-        # then those could be overriden via options
-
-        if exists(path):
-            if not exists(opj(path, '.git')):
-                if (len(listdir(path))) and (not allow_dirty):
-                    raise RuntimeError("Directory %s is not empty." % path)
-
-        self.repo = (GitRepo if no_annex else AnnexRepo)(path, always_commit=False, **kwargs)
-
-        git_remotes = self.repo.get_remotes()
-        if special_remotes:
-            if no_annex: # isinstance(self.repo, GitRepo):
-                raise ValueError("Cannot have special remotes in a simple git repo")
-
-            # TODO: move under AnnexRepo with proper testing etc
-            repo_info_repos = [v for k, v in self.repo.repo_info().items()
-                               if k.endswith(' repositories')]
-            annex_remotes = {r['description']: r for r in sum(repo_info_repos, [])}
-
-            for remote in special_remotes:
-                if remote not in git_remotes:
-                    if remote in annex_remotes:
-                        # Already known - needs only enabling
-                        lgr.info("Enabling existing special remote %s" % remote)
-                        self.repo.enable_remote(remote)
-                    else:
-                        init_datalad_remote(self.repo, remote, autoenable=True)
-
-        self.mode = mode
-        self.options = assure_list(options, copy=True)
-        self.auto_finalize = auto_finalize
-        self._states = set()
-        # TODO: may be should be a lazy centralized instance?
-        self._providers = Providers.from_config_files()
-        self.yield_non_updated = yield_non_updated
+        raise NotImplementedError
 
-        if largefiles:
-            repo_largefiles = self.repo.get_git_attributes().get('annex.largefiles', None)
-            if repo_largefiles is not None:
-                lgr.info(
-                    "Not adding annex.largefiles=%s to git annex calls because "
-                    "already defined to be %s", largefiles, repo_largefiles
-                )
-            else:
-                self.options += ["-c", "annex.largefiles=%s" % largefiles]
+    def read_file(self, file_path):
+        """Read a remote file's content
 
-        if (not allow_dirty) and self.repo.dirty:
-            raise RuntimeError("Repository %s is dirty.  Finalize your changes before running this pipeline" % path)
+        Parameters
+        ----------
+        file_path : Path or str
+          Must be an absolute path
 
-        self.statusdb = statusdb
-        self._statusdb = None  # actual DB to be instantiated later
-        self.skip_problematic = skip_problematic
-
-    # def add(self, filename, url=None):
-    #     # TODO: modes
-    #     self.repo.add_url_to_file(filename, url, batch=True #, TODO  backend
-    #                                    )
-    #     raise NotImplementedError()
-    #
-    # def addurl(self, url, filename):
-    #     raise NotImplementedError()
-    #     # TODO: register url within "The DB" after it was added
-    #     self.register_url_in_db(url, filename)
-    #
-    def register_url_in_db(self, url, filename):
-        # might need to go outside -- since has nothing to do with self
-        raise NotImplementedError()
-
-    def reset(self):
-        if self._statusdb:
-            self._statusdb.reset()
+        Returns
+        -------
+        string
+        """
 
-    @staticmethod
-    def _get_filename_from_url(url):
-        if url is None:
-            return None
-        filename = get_url_straight_filename(url)
-        # if still no filename
-        if not filename:
-            filename = get_url_disposition_filename(url)
-        if not filename:
-            raise ValueError("No filename was figured out for %s. "
-                             "Please adjust pipeline to provide one" % url)
-        return filename
-
-    def _get_url_status(self, data, url):
-        """A helper to return url_status if url_status is present in data
-         otherwise request a new one
-         """
-        if 'url_status' in data:
-            return data['url_status']
-        else:
-            downloader = self._providers.get_provider(url).get_downloader(url)
-            return downloader.get_status(url)
+        raise NotImplementedError
 
-    def __call__(self, data):  # filename=None, get_disposition_filename=False):
-        # some checks
-        assert (self.mode is not None)
-        stats = data.get('datalad_stats', ActivityStats())
-
-        url = data.get('url')
-        if url:
-            stats.urls += 1
-
-        fpath = self._get_fpath(data, stats, return_None=True)
-        url_status = None
-        if url:
-            try:
-                url_status = self._get_url_status(data, url)
-            except Exception:
-                if self.skip_problematic:
-                    stats.skipped += 1
-                    lgr.debug("Failed to obtain status for url %s" % url)
-                    return
-                raise
-            if fpath is None:
-                # pick it from url_status and give for "reprocessing"
-                fpath = self._get_fpath(data, stats, filename=url_status.filename)
-
-        if not fpath:
-            if self.skip_problematic:
-                stats.skipped += 1
-                lgr.debug("Failed to figure out filename for url %s" % url)
-                return
-            raise ValueError("No filename was provided")
-
-        filepath = opj(self.repo.path, fpath)
-
-        lgr.debug("Request to annex %(url)s to %(fpath)s", locals())
-        # since filename could have come from url -- let's update with it
-        updated_data = updated(data, {'filename': ops(fpath)[1],
-                                      # TODO? 'filepath': filepath
-                                      })
-
-        if self.statusdb is not None and self._statusdb is None:
-            if isinstance(self.statusdb, string_types):
-                # initiate the DB
-                self._statusdb = {
-                    'json': JsonFileStatusesDB,
-                    'fileattr': PhysicalFileStatusesDB}[self.statusdb](annex=self.repo)
-            else:
-                # use provided persistent instance
-                self._statusdb = self.statusdb
+    def write_file(self, file_path, content, mode='w'):
+        """Write a remote file
 
-        statusdb = self._statusdb
-        if url:
-            if lexists(filepath):
-                # check if URL provides us updated content.  If not -- we should do nothing
-                # APP1:  in this one it would depend on local_status being asked first BUT
-                #        may be for S3 support where statusdb would only record most recent
-                #        modification time
-                # local_status = self.statusdb.get(fpath)
-                # remote_status = downloader.get_status(url, old_status=local_status)
-                # if remote_status == local_status:  # WIP TODO not is_status_different(local_status, remote_status):
-                # APP2:  no explicit local_status
-                # if self.mode != 'full' and fpath == '1-copy.dat':
-                #     import pdb; pdb.set_trace()
-
-                # TODO: what if the file came from another url bearing the same mtime and size????
-                #       unlikely but possible.  We would need to provide URL for comparison(s)
-                if self.mode == 'relaxed' or (
-                        statusdb is not None and not statusdb.is_different(fpath, url_status, url)):
-                    # TODO:  check if remote_status < local_status, i.e. mtime jumped back
-                    # and if so -- warning or may be according to some config setting -- skip
-                    # e.g. config.get('crawl', 'new_older_files') == 'skip'
-                    lgr.debug("Skipping download. URL %s doesn't provide new content for %s.  New status: %s",
-                              url, filepath, url_status)
-                    stats.skipped += 1
-                    if self.yield_non_updated:
-                        yield updated_data  # there might be more to it!
-                    return
-        else:
-            # just to mark file as still of interest to us so it doesn't get wiped out later
-            # as it should have happened if we removed creation/tracking of that file intentionally
-            if statusdb:
-                statusdb.get(fpath)
-
-        if not url:
-            lgr.debug("Adding %s to annex without url being provided" % filepath)
-            # so we have only filename
-            assert fpath
-            # just add into git directly for now
-            # TODO: tune  add so we could use its json output, and may be even batch it
-            out_json = _call(self.repo.add, fpath, options=self.options)
-        # elif self.mode == 'full':
-        #     # since addurl ignores annex.largefiles we need first to download that file and then
-        #     # annex add it
-        #     # see http://git-annex.branchable.com/todo/make_addurl_respect_annex.largefiles_option
-        #     lgr.debug("Downloading %s into %s and adding to annex", url, filepath)
-        #     def _download_and_git_annex_add(url, fpath):
-        #         # Just to feed into _call for dry-run
-        #         filepath_downloaded = downloader.download(url, filepath, overwrite=True, stats=stats)
-        #         assert filepath_downloaded == filepath
-        #         self.repo.add(fpath, options=self.options)
-        #         # and if the file ended up under annex, and not directly under git -- addurl
-        #         # TODO: better function which explicitly checks if file is under annex or either under git
-        #         if self.repo.file_has_content(fpath):
-        #             stats.add_annex += 1
-        #             self.repo.add_url_to_file(fpath, url, batch=True)
-        #         else:
-        #             stats.add_git += 1
-        #     _call(_download_and_git_annex_add, url, fpath)
-        else:
-            # !!!! If file shouldn't get under annex due to largefile setting -- we must download it!!!
-            # TODO: http://git-annex.branchable.com/todo/make_addurl_respect_annex.largefiles_option/#comment-b43ef555564cc78c6dee2092f7eb9bac
-            # we should make use of   matchexpression   command, but that might reincarnated
-            # above code so just left it commented out for now
-            annex_options = self.options
-            if self.mode == 'full':
-                lgr.debug("Downloading %s into %s and adding to annex" % (url, filepath))
-            else:
-                annex_options = annex_options + ["--%s" % self.mode]
-                lgr.debug("Pointing %s to %s within annex in %s mode" % (url, filepath, self.mode))
+        Parameters
+        ----------
+        file_path : Path or str
+          Must be an absolute path
+        content : str
+        """
 
-            if lexists(filepath):
-                lgr.debug("Removing %s since it exists before fetching a new copy" % filepath)
-                if isdir(filepath):
-                    # if directory - tricky, since we would want then to check if no
-                    # staged changes under
-                    _call(self._check_no_staged_changes_under_dir, filepath, stats=stats)
-                    _call(rmtree, filepath)
-                else:
-                    _call(unlink, filepath)
-                _call(stats.increment, 'overwritten')
-            else:
-                _call(self._check_non_existing_filepath, filepath, stats=stats)
-            # TODO: We need to implement our special remote here since no downloaders used
-            if self.mode == 'full' and url_status and url_status.size:  # > 1024**2:
-                lgr.info("Need to download %s from %s. No progress indication will be reported"
-                         % (naturalsize(url_status.size), url))
-            try:
-                out_json = try_multiple(
-                    6, AnnexBatchCommandError, 3,  # up to 3**5=243 sec sleep
-                    _call,
-                    self.repo.add_url_to_file, fpath, url,
-                    options=annex_options, batch=True)
-            except AnnexBatchCommandError as exc:
-                if self.skip_problematic:
-                    lgr.warning("Skipping %s due to %s", url, exc_str(exc))
-                    return
-                else:
-                    raise
-            added_to_annex = 'key' in out_json
+        raise NotImplementedError
 
-            if self.mode == 'full' or not added_to_annex:
-                # we need to adjust our download stats since addurl doesn't do that and we do
-                # not use our downloaders here
-                _call(stats.increment, 'downloaded')
-                _call(stats.increment, 'downloaded_size', _call(lambda: os.stat(filepath).st_size))
-
-        # file might have been added but really not changed anything (e.g. the same README was generated)
-        # TODO:
-        # if out_json:  # if not try -- should be here!
-        # File might have been not modified at all, so let's check its status first
-        changed = set().union(*self._get_status(args=[fpath]))
-        if fpath in changed:
-            _call(stats.increment,
-                  'add_annex'
-                    if ('key' in out_json and out_json['key'] is not None)
-                    else 'add_git'
-                  )
-        else:
-            _call(stats.increment, 'skipped')
 
-        # TODO!!:  sanity check that no large files are added to git directly!
+class LocalIO(IOBase):
+    """IO operation if the object tree is local (e.g. NFS-mounted)"""
 
-        # so we have downloaded the beast
-        # since annex doesn't care to set mtime for the symlink itself, we better set it ourselves
-        if lexists(filepath):  # and islink(filepath):
-            if url_status:
-                # set mtime of the symlink or git-added file itself
-                # utime dereferences!
-                # _call(os.utime, filepath, (time.time(), remote_status.mtime))
-                # *nix only!  TODO
-                if url_status.mtime:
-                    _call(lmtime, filepath, url_status.mtime)
-                if statusdb:
-                    _call(statusdb.set, filepath, url_status)
-            else:
-                # we still need to inform DB about this file so later it would signal to remove it
-                # if we no longer care about it
-                if statusdb:
-                    _call(statusdb.set, filepath)
-
-        self._states.add("Updated git/annex from a remote location")
-
-        # WiP: commented out to do testing before merge
-        # db_filename = self.db.get_filename(url)
-        # if filename is not None and filename != db_filename:
-        #     # need to download new
-        #     self.repo.add_urls
-        #     # remove old
-        #     self.repo.remove([db_filename])
-        #     self.db.set_filename(url, filename)
-        # # figure out if we need to download it
-        # #if self.mode in ('relaxed', 'fast'):
-        # git annex addurl --pathdepth=-1 --backend=SHA256E '-c' 'annex.alwayscommit=false' URL
-        # with subsequent "drop" leaves no record that it ever was here
-        yield updated_data  # There might be more to it!
+    ensure_writeable = staticmethod(ensure_write_permission)
 
-    def _check_no_staged_changes_under_dir(self, dirpath, stats=None):
-        """Helper to verify that we can "safely" remove a directory
-        """
-        dirty_files = self._get_status()
-        dirty_files = sum(dirty_files, [])
-        dirpath_normalized = _normalize_path(self.repo.path, dirpath)
-        for dirty_file in dirty_files:
-            if stats:
-                _call(stats.increment, 'removed')
-            if dirty_file.startswith(dirpath_normalized):
-                if self.auto_finalize:
-                    self.finalize()({'datalad_stats': stats})
-                    return
-                else:
-                    raise RuntimeError(
-                        "We need to save some file instead of directory %(dirpath)s "
-                        "but there are uncommitted changes (%(dirty_file)s) under "
-                        "that directory.  Please commit them first" % locals())
+    def mkdir(self, path):
+        path.mkdir(
+            parents=True,
+            exist_ok=True,
+        )
 
-    def _check_non_existing_filepath(self, filepath, stats=None):
-        """Helper to verify that we can safely save into the target path
+    def symlink(self, target, link_name):
+        os.symlink(target, link_name)
 
-        For instance, we can't save into a file d/file if d is a file, not
-        a directory
-        """
-        # if file doesn't exist we need to verify that there are no conflicts
-        dirpath, name = ops(filepath)
-        if dirpath:
-            # we need to assure that either the directory exists or directories
-            # on the way to it exist and are not a file by some chance
-            while dirpath:
-                if lexists(dirpath):
-                    if not isdir(dirpath):
-                        # we have got a problem
-                        # HANDLE THE SITUATION
-                        # check if given file is not staged for a commit or dirty
-                        dirty_files = self._get_status()
-                        # it was a tuple of 3
-                        dirty_files = sum(dirty_files, [])
-                        dirpath_normalized = _normalize_path(self.repo.path, dirpath)
-                        if dirpath_normalized in dirty_files:
-                            if self.auto_finalize:
-                                self.finalize()({'datalad_stats': stats})
-                            else:
-                                raise RuntimeError(
-                                    "We need to annex file %(filepath)s but there is a file "
-                                    "%(dirpath)s in its path which destiny wasn't yet decided "
-                                    "within git.  Please commit or remove it before trying "
-                                    "to annex this new file" % locals())
-                        lgr.debug("Removing %s as it is in the path of %s" % (dirpath, filepath))
-                        _call(os.unlink, dirpath)
-                        if stats:
-                            _call(stats.increment, 'overwritten')
-                    break  # in any case -- we are done!
-
-                dirpath, _ = ops(dirpath)
-                if not dirpath.startswith(self.repo.path):
-                    # shouldn't happen!
-                    raise RuntimeError("We escaped the border of the repository itself. "
-                                       "path: %s  repo: %s" % (dirpath, self.repo.path))
-
-    def _get_fpath(self, data, stats, filename=None, return_None=False):
-        """Return relative path (fpath) to the file based on information in data
-
-        Raises
-        ------
-        ValueError if no filename field was provided
-        """
-        # figure out the filename. If disposition one was needed, pipeline should
-        # have had it explicitly
-        if filename is None:
-            filename = data['filename'] if 'filename' in data else None
-            stats.files += 1
-        fpath = filename
-
-        if filename is None:
-            if return_None:
-                return None
-            stats.skipped += 1
-            raise ValueError("No filename were provided")
-        elif isabs(filename):
-            stats.skipped += 1
-            raise ValueError("Got absolute filename %r" % filename)
-
-        path_ = data.get('path', None)
-        if path_:
-            # TODO: test all this handling of provided paths
-            if isabs(path_):
-                stats.skipped += 1
-                raise ValueError("Absolute path %s was provided" % path_)
-            fpath = opj(path_, fpath)
+    def put(self, src, dst, progress_cb):
+        shutil.copy(
+            str(src),
+            str(dst),
+        )
 
-        return fpath
+    def get(self, src, dst, progress_cb):
+        shutil.copy(
+            str(src),
+            str(dst),
+        )
+
+    def get_from_archive(self, archive, src, dst, progress_cb):
+        # Upfront check to avoid cryptic error output
+        # https://github.com/datalad/datalad/issues/4336
+        if not self.exists(archive):
+            raise RIARemoteError("archive {arc} does not exist."
+                                 "".format(arc=archive))
+
+        # this requires python 3.5
+        with open(dst, 'wb') as target_file:
+            subprocess.run([
+                '7z', 'x', '-so',
+                str(archive), str(src)],
+                stdout=target_file,
+            )
+        # Note for progress reporting:
+        # man 7z:
+        #
+        # -bs{o|e|p}{0|1|2}
+        #         Set output stream for output/error/progress line
+
+    def rename(self, src, dst):
+        with self.ensure_writeable(dst.parent):
+            src.rename(dst)
+
+    def remove(self, path):
+        try:
+            with self.ensure_writeable(path.parent):
+                path.unlink()
+        except PermissionError as e:
+            raise RIARemoteError(f"Unable to remove {path}. Could not "
+                                 "obtain write permission for containing"
+                                 "directory.") from e
+
+    def remove_dir(self, path):
+        with self.ensure_writeable(path.parent):
+            path.rmdir()
+
+    def exists(self, path):
+        return path.exists()
+
+    def in_archive(self, archive_path, file_path):
+        if not archive_path.exists():
+            # no archive, not file
+            return False
+        loc = str(file_path)
+        from datalad.cmd import (
+            StdOutErrCapture,
+            WitlessRunner,
+        )
+        runner = WitlessRunner()
+        # query 7z for the specific object location, keeps the output
+        # lean, even for big archives
+        out = runner.run(
+            ['7z', 'l', str(archive_path),
+             loc],
+            protocol=StdOutErrCapture,
+        )
+        return loc in out['stdout']
 
-    def switch_branch(self, branch, parent=None, must_exist=None, allow_remote=True):
-        """Node generator to switch branches, returns actual node
+    def read_file(self, file_path):
 
+        with open(str(file_path), 'r') as f:
+            content = f.read()
+        return content
+
+    def write_file(self, file_path, content, mode='w'):
+        if not content.endswith('\n'):
+            content += '\n'
+        with open(str(file_path), mode) as f:
+            f.write(content)
+
+    def get_7z(self):
+        from datalad.cmd import CommandError, StdOutErrCapture, WitlessRunner
+        # from datalad.utils import on_windows
+
+        runner = WitlessRunner()
+        # TODO: To not rely on availability in PATH we might want to use `which`
+        #       (`where` on windows) and get the actual path to 7z to reuse in
+        #       in_archive() and get().
+        #       Note: `command -v XXX` or `type` might be cross-platform
+        #       solution!
+        #       However, for availability probing only, it would be sufficient
+        #       to just call 7z and see whether it returns zero.
+
+        # cmd = 'where' if on_windows else 'which'
+        # try:
+        #     out = runner.run([cmd, '7z'], protocol=StdOutErrCapture)
+        #     return out['stdout']
+        # except CommandError:
+        #     return None
+
+        try:
+            runner.run('7z', protocol=StdOutErrCapture)
+            return True
+        except (FileNotFoundError, CommandError):
+            return False
+
+
+class SSHRemoteIO(IOBase):
+    """IO operation if the object tree is SSH-accessible
+
+    It doesn't even think about a windows server.
+    """
+
+    # output markers to detect possible command failure as well as end of output
+    # from a particular command:
+    REMOTE_CMD_FAIL = "ora-remote: end - fail"
+    REMOTE_CMD_OK = "ora-remote: end - ok"
+
+    def __init__(self, host, buffer_size=DEFAULT_BUFFER_SIZE):
+        """
         Parameters
         ----------
-        branch : str
-          Name of the branch
-        parent : str or None, optional
-          If parent is provided, it will serve as a parent of the branch. If None,
-          detached new branch will be created
-        must_exist : bool or None, optional
-          If None, doesn't matter.  If True, would fail if branch does not exist.  If
-          False, would fail if branch already exists
-        allow_remote : bool, optional
-          If not exists locally, will try to find one among remote ones
+        host : str
+          SSH-accessible host(name) to perform remote IO operations
+          on.
         """
 
-        def switch_branch(data):
-            """Switches to the branch %s""" % branch
-            # if self.repo.dirty
-            list(self.finalize()(data))
-            # statusdb is valid only within the same branch
-            self._statusdb = None
-            existing_branches = self.repo.get_branches()
-            if must_exist is not None:
-                assert must_exist == (branch in existing_branches)
-
-            # TODO: this should be a part of the gitrepo logic
-            if branch not in existing_branches and allow_remote:
-                remote_branches = self.repo.get_remote_branches()
-                remotes = sorted(set([b.split('/', 1)[0] for b in remote_branches]))
-                for r in ['origin'] + remotes:  # ok if origin tested twice
-                    remote_branch = "%s/%s" % (r, branch)
-                    if remote_branch in remote_branches:
-                        lgr.info("Did not find branch %r locally. Checking out remote one %r"
-                                 % (branch, remote_branch))
-                        self.repo.checkout(remote_branch, options=['--track'])
-                        # refresh the list -- same check will come again
-                        existing_branches = self.repo.get_branches()
-                        break
-
-            if branch not in existing_branches:
-                if parent is None:
-                    # new detached branch
-                    lgr.info("Checking out a new detached branch %s" % (branch))
-                    self.repo.checkout(branch, options=["--orphan"])
-                    if self.repo.dirty:
-                        self.repo.remove('.', r=True, f=True)  # TODO: might be insufficient if directories etc TEST/fix
-                else:
-                    if parent not in existing_branches:
-                        raise RuntimeError("Parent branch %s does not exist" % parent)
-                    lgr.info("Checking out %s into a new branch %s" % (parent, branch))
-                    self.repo.checkout(parent, options=["-b", branch])
+        # the connection to the remote
+        # we don't open it yet, not yet clear if needed
+        self.ssh = ssh_manager.get_connection(
+            host,
+            use_remote_annex_bundle=False,
+        )
+        self.ssh.open()
+        # open a remote shell
+        cmd = ['ssh'] + self.ssh._ssh_args + [self.ssh.sshri.as_str()]
+        self.shell = subprocess.Popen(cmd,
+                                      stderr=subprocess.DEVNULL,
+                                      stdout=subprocess.PIPE,
+                                      stdin=subprocess.PIPE)
+        # swallow login message(s):
+        self.shell.stdin.write(b"echo RIA-REMOTE-LOGIN-END\n")
+        self.shell.stdin.flush()
+        while True:
+            line = self.shell.stdout.readline()
+            if line == b"RIA-REMOTE-LOGIN-END\n":
+                break
+        # TODO: Same for stderr?
+
+        # make sure default is used when None was passed, too.
+        self.buffer_size = buffer_size if buffer_size else DEFAULT_BUFFER_SIZE
+
+    def close(self):
+        # try exiting shell clean first
+        self.shell.stdin.write(b"exit\n")
+        self.shell.stdin.flush()
+        exitcode = self.shell.wait(timeout=0.5)
+        # be more brutal if it doesn't work
+        if exitcode is None:  # timed out
+            # TODO: Theoretically terminate() can raise if not successful.
+            #       How to deal with that?
+            self.shell.terminate()
+
+    def _append_end_markers(self, cmd):
+        """Append end markers to remote command"""
+
+        return cmd + " && printf '%s\\n' {} || printf '%s\\n' {}\n".format(
+            sh_quote(self.REMOTE_CMD_OK),
+            sh_quote(self.REMOTE_CMD_FAIL))
+
+    def _get_download_size_from_key(self, key):
+        """Get the size of an annex object file from it's key
+
+        Note, that this is not necessarily the size of the annexed file, but
+        possibly only a chunk of it.
+
+        Parameter
+        ---------
+        key: str
+          annex key of the file
+
+        Returns
+        -------
+        int
+          size in bytes
+        """
+        # TODO: datalad's AnnexRepo.get_size_from_key() is not correct/not
+        #       fitting. Incorporate the wisdom there, too.
+        #       We prob. don't want to actually move this method there, since
+        #       AnnexRepo would be quite an expensive import. Startup time for
+        #       special remote matters.
+        # TODO: this method can be more compact. we don't need particularly
+        #       elaborated error distinction
+
+        # see: https://git-annex.branchable.com/internals/key_format/
+        key_parts = key.split('--')
+        key_fields = key_parts[0].split('-')
+
+        s = S = C = None
+
+        for field in key_fields[1:]:  # note: first has to be backend -> ignore
+            if field.startswith('s'):
+                # size of the annexed file content:
+                s = int(field[1:]) if field[1:].isdigit() else None
+            elif field.startswith('S'):
+                # we have a chunk and that's the chunksize:
+                S = int(field[1:]) if field[1:].isdigit() else None
+            elif field.startswith('C'):
+                # we have a chunk, this is it's number:
+                C = int(field[1:]) if field[1:].isdigit() else None
+
+        if s is None:
+            return None
+        elif S is None and C is None:
+            return s
+        elif S and C:
+            if C <= int(s / S):
+                return S
             else:
-                lgr.info("Checking out an existing branch %s" % (branch))
-                self.repo.checkout(branch)
-            yield updated(data, {"git_branch": branch})
-
-        return switch_branch
-
-    def merge_branch(self, branch, target_branch=None,
-                     strategy=None, commit=True, one_commit_at_a_time=False,
-                     skip_no_changes=None, **merge_kwargs):
-        """Merge a branch into the current branch
+                return s % S
+        else:
+            raise RIARemoteError("invalid key: {}".format(key))
+
+    def _run(self, cmd, no_output=True, check=False):
+
+        # TODO: we might want to redirect stderr to stdout here (or have
+        #       additional end marker in stderr) otherwise we can't empty stderr
+        #       to be ready for next command. We also can't read stderr for
+        #       better error messages (RemoteError) without making sure there's
+        #       something to read in any case (it's blocking!).
+        #       However, if we are sure stderr can only ever happen if we would
+        #       raise RemoteError anyway, it might be okay.
+        call = self._append_end_markers(cmd)
+        self.shell.stdin.write(call.encode())
+        self.shell.stdin.flush()
+
+        lines = []
+        while True:
+            line = self.shell.stdout.readline().decode()
+            lines.append(line)
+            if line == self.REMOTE_CMD_OK + '\n':
+                # end reading
+                break
+            elif line == self.REMOTE_CMD_FAIL + '\n':
+                if check:
+                    raise RemoteCommandFailedError(
+                        "{cmd} failed: {msg}".format(cmd=cmd,
+                                                     msg="".join(lines[:-1]))
+                    )
+                else:
+                    break
+        if no_output and len(lines) > 1:
+            raise RIARemoteError("{}: {}".format(call, "".join(lines)))
+        return "".join(lines[:-1])
+
+    @contextmanager
+    def ensure_writeable(self, path):
+        """Context manager to get write permission on `path` and restore
+        original mode afterwards.
+
+        If git-annex ever touched the key store, the keys will be in mode 444
+        directories, and we need to obtain permission first.
 
         Parameters
         ----------
-        branch: str
-          Branch to be merged
-        target_branch: str, optional
-          Into which branch to merge. If not None, it will be checked out first.
-          At the end we will return into original branch
-        strategy: None or 'theirs', optional
-          With 'theirs' strategy remote branch content is used 100% as is.
-          'theirs' with commit=False can be used to prepare data from that branch for
-          processing by further steps in the pipeline
-        commit: bool, optional
-          Either to commit when merge is complete or not
-        one_commit_at_at_time: bool, optional
-          Either to generate or not
-        skip_no_changes: None or bool, optional
-          Either to not perform any action if there are no changes from previous merge
-          point. If None, config TODO will be consulted with default of being True (i.e. skip
-          if no changes)
+        path: Path
+          path to the target file
         """
-        # TODO: support merge of multiple branches at once
-        assert (strategy in (None, 'theirs'))
 
-        def merge_branch(data):
+        path = sh_quote(str(path))
+        # remember original mode -- better than to prescribe a fixed mode
 
-            if target_branch is not None:
-                orig_branch = self.repo.get_active_branch()
-                target_branch_ = target_branch
-                list(self.switch_branch(target_branch_)(data))
+        if on_osx:
+            format_option = "-f%Dp"
+            # on macOS this would return decimal representation of mode (same
+            # as python's stat().st_mode
+            conversion = int
+        else:  # win is currently ignored anyway
+            format_option = "--format=\"%f\""
+            # in opposition to the above form for macOS, on debian this would
+            # yield the hexadecimal representation of the mode; hence conversion
+            # needed.
+            conversion = functools.partial(int, base=16)
+
+        output = self._run(f"stat {format_option} {path}",
+                           no_output=False, check=True)
+        mode = conversion(output)
+        if not mode & stat.S_IWRITE:
+            new_mode = oct(mode | stat.S_IWRITE)[-3:]
+            self._run(f"chmod {new_mode} {path}")
+            changed = True
+        else:
+            changed = False
+        try:
+            yield
+        finally:
+            if changed:
+                # restore original mode
+                self._run("chmod {mode} {file}".format(mode=oct(mode)[-3:],
+                                                       file=path),
+                          check=False)  # don't fail if path doesn't exist
+                                        # anymore
+
+    def mkdir(self, path):
+        self._run('mkdir -p {}'.format(sh_quote(str(path))))
+
+    def symlink(self, target, link_name):
+        self._run('ln -s {} {}'.format(sh_quote(str(target)), sh_quote(str(link_name))))
+
+    def put(self, src, dst, progress_cb):
+        self.ssh.put(str(src), str(dst))
+
+    def get(self, src, dst, progress_cb):
+
+        # Note, that as we are in blocking mode, we can't easily fail on the
+        # actual get (that is 'cat').
+        # Therefore check beforehand.
+        if not self.exists(src):
+            raise RIARemoteError("annex object {src} does not exist."
+                                 "".format(src=src))
+
+        from os.path import basename
+        key = basename(str(src))
+        try:
+            size = self._get_download_size_from_key(key)
+        except RemoteError as e:
+            raise RemoteError(f"src: {src}") from e
+
+        if size is None:
+            # rely on SCP for now
+            self.ssh.get(str(src), str(dst))
+            return
+
+        # TODO: see get_from_archive()
+
+        # TODO: Currently we will hang forever if the file isn't readable and
+        #       it's supposed size is bigger than whatever cat spits out on
+        #       stdout. This is because we don't notice that cat has exited
+        #       non-zero. We could have end marker on stderr instead, but then
+        #       we need to empty stderr beforehand to not act upon output from
+        #       earlier calls. This is a problem with blocking reading, since we
+        #       need to make sure there's actually something to read in any
+        #       case.
+        cmd = 'cat {}'.format(sh_quote(str(src)))
+        self.shell.stdin.write(cmd.encode())
+        self.shell.stdin.write(b"\n")
+        self.shell.stdin.flush()
+
+        with open(dst, 'wb') as target_file:
+            bytes_received = 0
+            while bytes_received < size:
+                # TODO: some additional abortion criteria? check stderr in
+                #       addition?
+                c = self.shell.stdout.read1(self.buffer_size)
+                # no idea yet, whether or not there's sth to gain by a
+                # sophisticated determination of how many bytes to read at once
+                # (like size - bytes_received)
+                if c:
+                    bytes_received += len(c)
+                    target_file.write(c)
+                    progress_cb(bytes_received)
+
+    def rename(self, src, dst):
+        with self.ensure_writeable(dst.parent):
+            self._run('mv {} {}'.format(sh_quote(str(src)), sh_quote(str(dst))))
+
+    def remove(self, path):
+        try:
+            with self.ensure_writeable(path.parent):
+                self._run('rm {}'.format(sh_quote(str(path))), check=True)
+        except RemoteCommandFailedError as e:
+            raise RIARemoteError(f"Unable to remove {path} "
+                                 "or to obtain write permission in parent directory.") from e
+
+    def remove_dir(self, path):
+        with self.ensure_writeable(path.parent):
+            self._run('rmdir {}'.format(sh_quote(str(path))))
+
+    def exists(self, path):
+        try:
+            self._run('test -e {}'.format(sh_quote(str(path))), check=True)
+            return True
+        except RemoteCommandFailedError:
+            return False
+
+    def in_archive(self, archive_path, file_path):
+
+        if not self.exists(archive_path):
+            return False
+
+        loc = str(file_path)
+        # query 7z for the specific object location, keeps the output
+        # lean, even for big archives
+        cmd = '7z l {} {}'.format(
+            sh_quote(str(archive_path)),
+            sh_quote(loc))
+
+        # Note: Currently relies on file_path not showing up in case of failure
+        # including non-existent archive. If need be could be more sophisticated
+        # and called with check=True + catch RemoteCommandFailedError
+        out = self._run(cmd, no_output=False, check=False)
+
+        return loc in out
+
+    def get_from_archive(self, archive, src, dst, progress_cb):
+
+        # Note, that as we are in blocking mode, we can't easily fail on the
+        # actual get (that is 'cat'). Therefore check beforehand.
+        if not self.exists(archive):
+            raise RIARemoteError("archive {arc} does not exist."
+                                 "".format(arc=archive))
+
+        # TODO: We probably need to check exitcode on stderr (via marker). If
+        #       archive or content is missing we will otherwise hang forever
+        #       waiting for stdout to fill `size`.
+
+        cmd = '7z x -so {} {}\n'.format(
+            sh_quote(str(archive)),
+            sh_quote(str(src)))
+        self.shell.stdin.write(cmd.encode())
+        self.shell.stdin.flush()
+
+        # TODO: - size needs double-check and some robustness
+        #       - can we assume src to be a posixpath?
+        #       - RF: Apart from the executed command this should be pretty much
+        #         identical to self.get(), so move that code into a common
+        #         function
+
+        from os.path import basename
+        size = self._get_download_size_from_key(basename(str(src)))
+
+        with open(dst, 'wb') as target_file:
+            bytes_received = 0
+            while bytes_received < size:
+                c = self.shell.stdout.read1(self.buffer_size)
+                if c:
+                    bytes_received += len(c)
+                    target_file.write(c)
+                    progress_cb(bytes_received)
+
+    def read_file(self, file_path):
+
+        cmd = "cat  {}".format(sh_quote(str(file_path)))
+        try:
+            out = self._run(cmd, no_output=False, check=True)
+        except RemoteCommandFailedError as e:
+            # Currently we don't read stderr. All we know is, we couldn't read.
+            # Try narrowing it down by calling a subsequent exists()
+            if not self.exists(file_path):
+                raise FileNotFoundError(f"{str(file_path)} not found.") from e
             else:
-                orig_branch = None
-                target_branch_ = self.repo.get_active_branch()
+                raise RuntimeError(f"Could not read {file_path}") from e
 
-            if self.repo.dirty:
-                raise RuntimeError("Requested to merge another branch while current state is dirty")
+        return out
 
-            last_merged_checksum = self.repo.get_merge_base([target_branch_, branch])
-            skip_no_changes_ = skip_no_changes
-            if skip_no_changes is None:
-                # TODO: skip_no_changes = config.getboolean('crawl', 'skip_merge_if_no_changes', default=True)
-                skip_no_changes_ = True
-
-            if last_merged_checksum == self.repo.get_hexsha(branch):
-                lgr.debug("Branch %s doesn't provide any new commits for current HEAD" % branch)
-                if skip_no_changes_:
-                    lgr.debug("Skipping the merge")
-                    return
-
-            if one_commit_at_a_time:
-                all_to_merge = list(
-                    self.repo.get_branch_commits(
-                        branch,
-                        limit='left-only',
-                        stop=last_merged_checksum,
-                        value='hexsha'))[::-1]
-            else:
-                all_to_merge = [branch]
+    def write_file(self, file_path, content, mode='w'):
 
-            nmerges = len(all_to_merge)
+        if mode == 'w':
+            mode = ">"
+        elif mode == 'a':
+            mode = ">>"
+        else:
+            raise ValueError("Unknown mode '{}'".format(mode))
+        if not content.endswith('\n'):
+            content += '\n'
+
+        cmd = "printf '%s' {} {} {}".format(
+            sh_quote(content),
+            mode,
+            sh_quote(str(file_path)))
+        try:
+            self._run(cmd, check=True)
+        except RemoteCommandFailedError as e:
+            raise RIARemoteError(f"Could not write to {file_path}") from e
+
+    def get_7z(self):
+        # TODO: To not rely on availability in PATH we might want to use `which`
+        #       (`where` on windows) and get the actual path to 7z to reuse in
+        #       in_archive() and get().
+        #       Note: `command -v XXX` or `type` might be cross-platform
+        #       solution!
+        #       However, for availability probing only, it would be sufficient
+        #       to just call 7z and see whether it returns zero.
+
+        try:
+            self._run("7z", check=True, no_output=False)
+            return True
+        except RemoteCommandFailedError:
+            return False
+
+        # try:
+        #     out = self._run("which 7z", check=True, no_output=False)
+        #     return out
+        # except RemoteCommandFailedError:
+        #     return None
+
+
+class HTTPRemoteIO(object):
+    # !!!!
+    # This is not actually an IO class like SSHRemoteIO and LocalIO and needs
+    # respective RF'ing of special remote implementation eventually.
+    # We want ORA over HTTP, but with a server side CGI to talk to in order to
+    # reduce the number of requests. Implementing this as such an IO class would
+    # mean to have separate requests for all server side executions, which is
+    # what we do not want. As a consequence ORARemote class implementation needs
+    # to treat HTTP as a special case until refactoring to a design that fits
+    # both approaches.
+
+    # NOTE: For now read-only. Not sure yet whether an IO class is the right
+    # approach.
+
+    def __init__(self, url, buffer_size=DEFAULT_BUFFER_SIZE):
+        from datalad.downloaders.providers import Providers
+        if not url.startswith("http"):
+            raise RIARemoteError("Expected HTTP URL, but got {}".format(url))
 
-            # There were no merges, but we were instructed to not skip
-            if not nmerges and skip_no_changes_ is False:
-                # so we will try to merge it nevertheless
-                lgr.info("There was nothing to merge but we were instructed to merge due to skip_no_changes=False")
-                all_to_merge = [branch]
-                nmerges = 1
-
-            plmerges = "s" if nmerges > 1 else ""
-            lgr.info("Initiating %(nmerges)d merge%(plmerges)s of %(branch)s using strategy %(strategy)s", locals())
-            options = ['--no-commit'] if not commit else []
-
-            for to_merge in all_to_merge:
-                # we might have switched away to orig_branch
-                if self.repo.get_active_branch() != target_branch_:
-                    self.repo.checkout(target_branch_)
-                if strategy is None:
-                    self.repo.merge(to_merge, options=options, **merge_kwargs)
-                elif strategy == 'theirs':
-                    self.repo.merge(to_merge, options=["-s", "ours", "--no-commit"],
-                                    expect_stderr=True, **merge_kwargs)
-                    self.repo._git_custom_command([], "git read-tree -m -u %s" % to_merge)
-                    self.repo.add('.', options=self.options)  # so everything is staged to be committed
-                else:
-                    raise NotImplementedError(strategy)
+        self.store_url = url.rstrip('/')
 
-                if commit:
-                    if strategy is not None:
-                        msg = branch if (nmerges == 1) else ("%s (%s)" % (branch, to_merge))
-                        self._commit("Merged %s using strategy %s" % (msg, strategy), options=["-a"])
-                else:
-                    # record into our activity stats
-                    stats = data.get('datalad_stats', None)
-                    if stats:
-                        stats.merges.append([branch, target_branch_])
-                if orig_branch is not None:
-                    self.repo.checkout(orig_branch)
-                yield data
-
-        return merge_branch
-
-    def _precommit(self):
-        self.repo.precommit()  # so that all batched annexes stop
-        if self._statusdb:
-            self._statusdb.save()
-        # there is something to commit and backends was set but no .gitattributes yet
-        path = self.repo.path
-        if self.repo.dirty and \
-                'annex.backend' not in self.repo.get_git_attributes() and \
-                isinstance(self.repo, AnnexRepo):
-            backends = self.repo.default_backends
-            if backends:
-                self.repo.set_default_backend(backends[0], commit=False)
-
-    # at least use repo._git_custom_command
-    def _commit(self, msg=None, options=[]):
-        # we need a custom commit due to "fancy" merges and GitPython
-        # not supporting that ATM
-        # https://github.com/gitpython-developers/GitPython/issues/361
-        # and apparently not actively developed
-        msg = str(msg).strip()
-        if not msg:
-            # we need to provide some commit msg, could may be deduced from current status
-            # TODO
-            msg = "a commit"
-        msg = GitRepo._get_prefixed_commit_msg(msg)
-        if msg is not None:
-            options = options + ["-m", msg]
-        self._precommit()  # so that all batched annexes stop
-        self.repo._git_custom_command([], ["git", "commit"] + options)
-        # self.repo.commit(msg)
-        # self.repo.repo.git.commit(options)
-
-    def _unstage(self, fpaths):
-        # self.repo.cmd_call_wrapper.run(["git", "reset"] + fpaths)
-        self.repo._git_custom_command(fpaths, ["git", "reset"])
-
-    def _stage(self, fpaths):
-        self.repo.add(fpaths, git=True)
-        # self.repo.cmd_call_wrapper.run(["git", "add"] + fpaths)
-
-    def _get_status(self, args=[]):
-        """Custom check of status to see what files were staged, untracked etc
-        until
-        https://github.com/gitpython-developers/GitPython/issues/379#issuecomment-180101921
-        is resolved
-        """
-        # out, err = self.repo.cmd_call_wrapper.run(["git", "status", "--porcelain"])
-        cmd_args = ["git", "status", "--porcelain"] + args
-        staged, notstaged, untracked, deleted = [], [], [], []
-        statuses = {
-            '??': untracked,
-            'A ': staged,
-            'M ': staged,
-            ' M': notstaged,
-            ' D': deleted,  #     rm-ed  smth committed before
-            'D ': deleted,  # git rm-ed  smth committed before
-            'AD': (staged, deleted)  # so we added, but then removed before committing
-                                     # generaly shouldn't happen but in some tricky S3 cases crawling did happen :-/
-                                     # TODO: handle "properly" by committing before D happens
-        }
+        # make sure default is used when None was passed, too.
+        self.buffer_size = buffer_size if buffer_size else DEFAULT_BUFFER_SIZE
+        self._providers = Providers.from_config_files()
 
-        if isinstance(self.repo, AnnexRepo) and self.repo.is_direct_mode():
-            statuses['AD'] = staged
-            out, err = self.repo.proxy(cmd_args)
-        else:
-            out, err = self.repo._git_custom_command([], cmd_args)
-            assert not err
+    def checkpresent(self, key_path):
+        # Note, that we need the path with hash dirs, since we don't have access
+        # to annexremote.dirhash from within IO classes
+
+        return self.exists(key_path)
+
+    def get(self, key_path, filename, progress_cb):
+        # Note, that we need the path with hash dirs, since we don't have access
+        # to annexremote.dirhash from within IO classes
+
+        url = self.store_url + str(key_path)
+        self._providers.download(url, path=filename, overwrite=True)
+
+    def exists(self, path):
+        # use same signature as in SSH and Local IO, although validity is
+        # limited in case of HTTP.
+        url = self.store_url + path.as_posix()
+        try:
+            response = requests.head(url, allow_redirects=True)
+        except Exception as e:
+            raise RIARemoteError from e
+
+        return response.status_code == 200
+
+    def read_file(self, file_path):
+
+        from datalad.support.network import download_url
+        url = self.store_url + file_path.as_posix()
+        try:
+            content = download_url(url)
+
+            # NOTE re Exception handling:
+            # We reraise here to:
+            #   1. Unify exceptions across IO classes
+            #   2. Get cleaner user messages. ATM what we get from the
+            #   Downloaders are exceptions, that have their cause-chain baked
+            #   into their string rather than being e proper exception chain.
+            #   Hence, we can't generically extract the ultimate cause.
+            #   RemoteError will eventually pass the entire chain string to
+            #   annex. If we add our own exception here on top, this is what is
+            #   displayed first to the user, rather than being buried deep into
+            #   a hard to parse message.
+        except AccessDeniedError as exc:
+            raise PermissionError(f"Permission denied: '{url}'") from exc
+
+        except DownloadError as exc:
+            # Note: This comes from the downloader. `check_response_status`
+            # in downloaders/http.py does not currently use
+            # `raise_from_status`, hence we don't get a proper HTTPError to
+            # check for a 404 and thereby distinguish from connection issues.
+            # When this is addressed in the downloader code, we need to
+            # adjust here.
+            if "not found" in str(exc):
+                # Raise uniform exception across IO classes:
+                raise FileNotFoundError(f"{url} not found.") from exc
+            else:
+                # Note: There's AccessFailedError(DownloadError) as well.
+                # However, we can't really tell them meaningfully apart,
+                # since possible underlying HTTPErrors, etc. are baked into
+                # their strings. Hence, "Failed to access" is what we can
+                # tell here in either case.
+                raise RuntimeError(f"Failed to access {url}") from exc
+        return content
+
+
+def handle_errors(func):
+    """Decorator to convert and log errors
+
+    Intended to use with every method of RiaRemote class, facing the outside
+    world. In particular, that is about everything, that may be called via
+    annex' special remote protocol, since a non-RemoteError will simply result
+    in a broken pipe by default handling.
+    """
+
+    # TODO: configurable on remote end (flag within layout_version!)
+
+    @wraps(func)
+    def _wrap_handle_errors(self, *args, **kwargs):
+        try:
+            return func(self, *args, **kwargs)
+        except Exception as e:
+            if self.remote_log_enabled:
+                try:
+                    from datetime import datetime
+                    from traceback import format_exc
+                    exc_str = format_exc()
+                    entry = "{time}: Error:\n{exc_str}\n" \
+                            "".format(time=datetime.now(),
+                                      exc_str=exc_str)
+                    # ensure base path is platform path
+                    log_target = (
+                        url_path2local_path(self.store_base_path)
+                        / "error_logs"
+                        / "{dsid}.{uuid}.log".format(
+                            dsid=self.archive_id,
+                            uuid=self._repo.uuid))
+                    self.io.write_file(log_target, entry, mode='a')
+                except Exception:
+                    # If logging of the exception does fail itself, there's
+                    # nothing we can do about it. Hence, don't log and report
+                    # the original issue only.
+                    # TODO: With a logger that doesn't sabotage the
+                    #  communication with git-annex, we should be abe to use
+                    #  CapturedException here, in order to get an informative
+                    #  traceback in a debug message.
+                    pass
 
-        for l in out.split('\n'):
-            if not l:
-                continue
-            act = l[:2]  # first two characters is what is happening to the file
-            fname = l[3:]
             try:
-                act_list = statuses[act]
-                if isinstance(act_list, tuple):  # like in case of AD
-                    for l in act_list:
-                        l.append(fname)
-                else:
-                    act_list.append(fname)
-                # for the purpose of this use, we don't even want MM or anything else
-            except KeyError:
-                raise RuntimeError("git status %r not yet supported. TODO" % act)
-        return staged, notstaged, untracked, deleted
-
-    def commit_versions(self,
-                        regex,
-                        dirs=True,  # either match directory names
-                        rename=False,
-                        **kwargs):
-        """Generate multiple commits if multiple versions were staged
+                # We're done using io, so let it perform any needed cleanup. At
+                # the moment, this is only relevant for SSHRemoteIO, in which
+                # case it cleans up the SSH socket and prevents a hang with
+                # git-annex 8.20201103 and later.
+                from atexit import unregister
+                if self._io:
+                    self._io.close()
+                    unregister(self._io.close)
+                if self._push_io:
+                    self._push_io.close()
+                    unregister(self._push_io.close)
+            except AttributeError:
+                # seems like things are already being cleaned up -> a good
+                pass
+            except Exception:
+                # anything else: Not a problem. We are about to exit anyway
+                pass
 
-        Parameters
-        ----------
-        TODO
-        **kwargs: dict, optional
-          Passed to get_versions
-        """
+            if not isinstance(e, RIARemoteError):
+                raise RIARemoteError from e
+            else:
+                raise e
 
-        def _commit_versions(data):
-            self._precommit()  # so that all batched annexes stop
+    return _wrap_handle_errors
 
-            # figure out versions for all files (so we could dataset conflicts with existing
-            # non versioned)
-            # TODO:  we need to care only about staged (and unstaged?) files ATM!
-            # So let's do it.  And use separate/new Git repo since we are doing manual commits through
-            # calls to git.  TODO: RF to avoid this
-            # Not usable for us ATM due to
-            # https://github.com/gitpython-developers/GitPython/issues/379
-            # repo = Repo(self.repo.path)
-            #
-            # def process_diff(diff):
-            #     """returns full paths for files in the diff"""
-            #     out = []
-            #     for obj in diff:
-            #         assert(not obj.renamed)  # not handling atm
-            #         assert(not obj.deleted_file)  # not handling atm
-            #         assert(obj.a_path == obj.b_path)  # not handling atm
-            #         out.append(opj(self.repo.path, obj.a_path))
-            #     return out
-            #
-            # staged = process_diff(repo.index.diff('HEAD'))#repo.head.commit))
-            # notstaged = process_diff(repo.index.diff(None))
-            staged, notstaged, untracked, deleted = self._get_status()
-
-            # verify that everything is under control!
-            assert (not notstaged)  # not handling atm, although should be safe I guess just needs logic
-            # to not unstage them
-            assert (not untracked)  # not handling atm
-            assert (not deleted)  # not handling atm
-            if not staged:
-                return  # nothing to be done -- so we wash our hands off entirely
-
-            if not dirs:
-                raise NotImplementedError("ATM matching will happen to dirnames as well")
-
-            versions = get_versions(staged, regex, **kwargs)
-
-            if not versions:
-                # no versioned files were added, nothing to do really
-                for d in self.finalize()(data):
-                    yield d
-                return
-
-            # we don't really care about unversioned ones... overlay and all that ;)
-            if None in versions:
-                versions.pop(None)
-
-            # take only new versions to deal with
-            versions_db = SingleVersionDB(self.repo)
-            prev_version = versions_db.version
 
-            if prev_version is None:
-                new_versions = versions  # consider all!
-            else:
-                version_keys = list(versions.keys())
-                if prev_version not in versions_db.versions:
-                    # shouldn't happen
-                    raise RuntimeError(
-                        "previous version %s not found among known to DB: %s" % (prev_version, versions_db.versions.keys()))
-                # all new versions must be greater than the previous version
-                # since otherwise it would mean that we are complementing previous version and it might be
-                # a sign of a problem
-                # Well -- so far in the single use-case with openfmri it was that they added
-                # derivatives for the same version, so I guess we will allow for that, thus allowing =
-                assert (all((LooseVersion(prev_version) <= LooseVersion(v)) for v in versions))
-                # old implementation when we didn't have entire versions db stored
-                # new_versions = OrderedDict(versions.items()[version_keys.index(prev_version) + 1:])
-                new_versions = versions
-                # if we have "new_versions" smallest one smaller than previous -- we got a problem!
-                # TODO: how to dataset ==? which could be legit if more stuff was added for the same
-                # version?  but then if we already tagged with that -- we would need special handling
-
-            if new_versions:
-                smallest_new_version = next(iter(new_versions))
-                if prev_version:
-                    if LooseVersion(smallest_new_version) < LooseVersion(prev_version):
-                        raise ValueError("Smallest new version %s is < prev_version %s"
-                                         % (smallest_new_version, prev_version))
-
-            versions_db.update_versions(versions)  # store all new known versions
-
-            # early return if no special treatment is needed
-            nnew_versions = len(new_versions)
-            if nnew_versions <= 1:
-                # if a single new version -- no special treatment is needed, but we need to
-                # inform db about this new version
-                if nnew_versions == 1:
-                    _call(setattr, versions_db, 'version', smallest_new_version)
-                # we can't return a generator here
-                for d in self.finalize()(data):
-                    yield d
-                return
-
-            # unstage all versioned files from the index
-            nunstaged = 0
-            for version, fpaths in iteritems(versions):
-                nfpaths = len(fpaths)
-                lgr.debug("Unstaging %d files for version %s", nfpaths, version)
-                nunstaged += nfpaths
-                _call(self._unstage, list(fpaths.values()))
-
-            stats = data.get('datalad_stats', None)
-            stats_str = ('\n\n' + stats.as_str(mode='full')) if stats else ''
-
-            for iversion, (version, fpaths) in enumerate(iteritems(new_versions)):  # for all versions past previous
-                # stage/add files of that version to index
-                if rename:
-                    # we need to rename and create a new vfpaths
-                    vfpaths = []
-                    for fpath, vfpath in iteritems(fpaths):
-                        # ATM we do not allow unversioned -- should have failed earlier, if not HERE!
-                        # assert(not lexists(fpath))
-                        # nope!  it must be there from previous commit of a versioned file!
-                        # so rely on logic before
-                        lgr.debug("Renaming %s into %s" % (vfpath, fpath))
-                        os.rename(vfpath, fpath)
-                        vfpaths.append(fpath)
-                else:
-                    # so far we didn't bother about status, so just values would be sufficient
-                    vfpaths = list(fpaths.values())
-                nfpaths = len(vfpaths)
-                lgr.debug("Staging %d files for version %s", nfpaths, version)
-                nunstaged -= nfpaths
-                assert (nfpaths >= 0)
-                assert (nunstaged >= 0)
-                _call(self._stage, vfpaths)
-
-                # RF: with .finalize() to avoid code duplication etc
-                # ??? what to do about stats and states?  reset them or somehow tune/figure it out?
-                vmsg = "Multi-version commit #%d/%d: %s. Remaining unstaged: %d" % (
-                iversion + 1, nnew_versions, version, nunstaged)
-
-                if stats:
-                    _call(stats.reset)
-
-                if version:
-                    _call(setattr, versions_db, 'version', version)
-                _call(self._commit, "%s (%s)%s" % (', '.join(self._states), vmsg, stats_str), options=[])
-                # unless we update data, no need to yield multiple times I guess
-                # but shouldn't hurt
-                yield data
-            assert (nunstaged == 0)  # we at the end committed all of them!
-
-        return _commit_versions
-
-    def remove_other_versions(self, name=None, db=None,
-                              overlay=None, remove_unversioned=False,
-                              fpath_subs=None,
-                              exclude=None):
-        """Remove other (non-current) versions of the files
+class NoLayoutVersion(Exception):
+    pass
 
-        Pretty much to be used in tandem with commit_versions
 
-        Parameters
-        ----------
-        name : str, optional
-          Name of the SingleVersionDB to consult (e.g., original name of the branch)
-        db : SingleVersionDB, optional
-          If provided, `name` must be None.
-        overlay : int or callable, optional
-          Overlay files of the next version to only replace files from the
-          previous version.  If specified as `int`, value will determine how
-          many leading levels of .-separated (e.g., of major.minor.patch)
-          semantic version format will be used to identify "unique"
-          non-overlayable version. E.g. overlay=2, would overlay all .patch
-          levels, while starting without overlay for any new major.minor version.
-          If a callable, it would be used to augment versions before identifying
-          non-overlayable version component.  So in other words `overlay=2`
-          should be identical to `overlay=lambda v: '.'.join(v.split('.')[:2])`
-        fpath_subs : list of (from, to), optional
-          Regex substitutions to apply to (versioned but with version part removed)
-          filenames before considering.  To be used whenever file names at some point
-          were changed
-          (e.g., `ds001_R1.0.1.tgz` one lucky day became `ds000001_R1.0.2.zip`)
-        remove_unversioned: bool, optional
-          If there is a version defined now, remove those files which are unversioned
-          i.e. not listed associated with any version
-        exclude : basestring, optional
-          Regexp to search to exclude files from considering to remove them if
-          `remove_unversioned`.  Passed to `find_files`.  E.g. `README.*` which
-          could have been generated in `incoming` branch
+class ORARemote(SpecialRemote):
+    """This is the class of RIA remotes.
+    """
+
+    dataset_tree_version = '1'
+    object_tree_version = '2'
+    # TODO: Move known versions. Needed by creation routines as well.
+    known_versions_objt = ['1', '2']
+    known_versions_dst = ['1']
+
+    @handle_errors
+    def __init__(self, annex):
+        super(ORARemote, self).__init__(annex)
+        if hasattr(self, 'configs'):
+            # introduced in annexremote 1.4.2 to support LISTCONFIGS
+            self.configs['url'] = "RIA store to use"
+            self.configs['push-url'] = "URL for pushing to the RIA store. " \
+                                       "Optional."
+            self.configs['archive-id'] = "Dataset ID (fallback: annex uuid. " \
+                                         "Should be set automatically by " \
+                                         "datalad"
+        # the local repo
+        self._repo = None
+        self.gitdir = None
+        self.name = None  # name of the special remote
+        self.gitcfg_name = None  # name in respective git remote
+
+        self.ria_store_url = None
+        self.ria_store_pushurl = None
+        # machine to SSH-log-in to access/store the data
+        # subclass must set this
+        self.storage_host = None
+        self.storage_host_push = None
+        # must be absolute, and POSIX (will be instance of PurePosixPath)
+        # subclass must set this
+        self.store_base_path = None
+        self.store_base_path_push = None
+        # by default we can read and write
+        self.read_only = False
+        self.force_write = None
+        self.ignore_remote_config = None
+        self.remote_log_enabled = None
+        self.remote_dataset_tree_version = None
+        self.remote_object_tree_version = None
+
+        # for caching the remote's layout locations:
+        self.remote_git_dir = None
+        self.remote_archive_dir = None
+        self.remote_obj_dir = None
+        # lazy IO:
+        self._io = None
+        self._push_io = None
+
+        # cache obj_locations:
+        self._last_archive_path = None
+        self._last_keypath = (None, None)
+
+        # SSH "streaming" buffer
+        self.buffer_size = DEFAULT_BUFFER_SIZE
+
+    def verify_store(self):
+        """Check whether the store exists and reports a layout version we
+        know
+
+        The layout of the store is recorded in base_path/ria-layout-version.
+        If the version found on the remote end isn't supported and `force-write`
+        isn't configured, sets the remote to read-only operation.
         """
 
-        if overlay is None:
-            overlay_version_func = lambda x: x
-        elif isinstance(overlay, int) and not isinstance(overlay, bool):
-            overlay_version_func = lambda v: '.'.join(v.split('.')[:overlay])
-        elif hasattr(overlay, '__call__'):
-            overlay_version_func = overlay
-        else:
-            raise TypeError("overlay  must be an int or a callable. Got %s"
-                            % repr(overlay))
+        # ensure base path is platform path
+        dataset_tree_version_file = \
+            url_path2local_path(self.store_base_path) / 'ria-layout-version'
+
+        # check dataset tree version
+        try:
+            self.remote_dataset_tree_version = \
+                self._get_version_config(dataset_tree_version_file)
+        except Exception as exc:
+            raise RIARemoteError("RIA store unavailable.") from exc
+        if self.remote_dataset_tree_version not in self.known_versions_dst:
+            # Note: In later versions, condition might change in order to
+            # deal with older versions.
+            raise UnknownLayoutVersion(f"RIA store layout version unknown: "
+                                       f"{self.remote_dataset_tree_version}")
+
+    def verify_ds_in_store(self):
+        """Check whether the dataset exists in store and reports a layout
+        version we know
+
+        The layout is recorded in
+        'dataset_somewhere_beneath_base_path/ria-layout-version.'
+        If the version found on the remote end isn't supported and `force-write`
+        isn't configured, sets the remote to read-only operation.
+        """
 
-        if db is not None and name is not None:
-            raise ValueError(
-                "Must have specified either name or version_db, not both"
-            )
+        object_tree_version_file = self.remote_git_dir / 'ria-layout-version'
 
-        def _remove_other_versions(data):
-            stats = data.get('datalad_stats', None)
-            versions_db = SingleVersionDB(self.repo, name=name) \
-                if db is None \
-                else db
-
-            current_version = versions_db.version
-
-            if not current_version:
-                lgr.info("No version information was found, skipping remove_other_versions")
-                yield data
-                return
-
-            current_overlay_version = overlay_version_func(current_version)
-            prev_version = None
-            tracked_files = {}  # track files in case of overlaying
-            for version, fpaths in iteritems(versions_db.versions):
-                # sanity check since we now will have assumption that versions
-                # are sorted
-                if prev_version is not None:
-                    assert(prev_version < LooseVersion(version))
-                prev_version = LooseVersion(version)
-                overlay_version = overlay_version_func(version)
-                # we do not care about non-versioned or current "overlay" version
-                #import pdb; pdb.set_trace()
-                if version is None:
-                    continue
-
-                files_to_remove = []
-                if current_overlay_version == overlay_version and \
-                    LooseVersion(version) <= LooseVersion(current_version):
-                    # the same overlay but before current version
-                    # we need to track the last known within overlay and if
-                    # current updates, remove older version
-                    fpaths_considered = {}
-                    for fpath, vfpath in fpaths.items():
-                        if fpath_subs:
-                            fpath_orig = fpath
-                            for from_, to_ in fpath_subs:
-                                fpath = re.sub(from_, to_, fpath)
-                            if fpath in fpaths_considered:
-                                # May be it is not that severe, but for now we will
-                                # crash if there is a collision
-                                raise ValueError(
-                                    "Multiple files (%s, %s) collided into the same name %s",
-                                    fpaths_considered[fpath], fpath_orig, fpath
-                                )
-                            fpaths_considered[fpath] = fpath_orig
-                        if fpath in tracked_files:
-                            files_to_remove.append(tracked_files[fpath])
-                        tracked_files[fpath] = vfpath  # replace with current one
-
-                    if version == current_version:
-                        # just clean out those tracked_files with the most recent versions
-                        # and remove nothing
-                        tracked_files = {}
+        # check (annex) object tree version
+        try:
+            self.remote_object_tree_version =\
+                self._get_version_config(object_tree_version_file)
+        except Exception as e:
+            raise RIARemoteError("Dataset unavailable from RIA store.")
+        if self.remote_object_tree_version not in self.known_versions_objt:
+            raise UnknownLayoutVersion(f"RIA dataset layout version unknown: "
+                                       f"{self.remote_object_tree_version}")
+
+    def _load_local_cfg(self):
+
+        # this will work, even when this is not a bare repo
+        # but it is not capable of reading out dataset/branch config
+        self._repo = AnnexRepo(self.gitdir)
+
+        cfg_map = {"ora-force-write": "force_write",
+                   "ora-ignore-ria-config": "ignore_remote_config",
+                   "ora-buffer-size": "buffer_size",
+                   "ora-url": "ria_store_url",
+                   "ora-push-url": "ria_store_pushurl"
+                   }
+
+        # in initremote we may not have a reliable name of the git remote config
+        # yet. Go with the default.
+        gitcfg_name = self.gitcfg_name or self.name
+        if gitcfg_name:
+            for cfg, att in cfg_map.items():
+                value = self._repo.config.get(f"remote.{gitcfg_name}.{cfg}")
+                if value is not None:
+                    self.__setattr__(cfg_map[cfg], value)
+                    if cfg == "ora-url":
+                        self.ria_store_url_source = 'local'
+                    elif cfg == "ora-push-url":
+                        self.ria_store_pushurl_source = 'local'
+            if self.buffer_size:
+                try:
+                    self.buffer_size = int(self.buffer_size)
+                except ValueError:
+                    self.message(f"Invalid value of config "
+                                 f"'remote.{gitcfg_name}."
+                                 f"ora-buffer-size': {self.buffer_size}")
+                    self.buffer_size = DEFAULT_BUFFER_SIZE
+
+        if self.name:
+            # Consider deprecated configs if there's no value yet
+            if self.force_write is None:
+                self.force_write = self._repo.config.get(
+                    f'annex.ora-remote.{self.name}.force-write')
+                if self.force_write:
+                    self.message("WARNING: config "
+                                 "'annex.ora-remote.{}.force-write' is "
+                                 "deprecated. Use 'remote.{}.ora-force-write' "
+                                 "instead.".format(self.name, self.gitcfg_name))
+                    try:
+                        self.force_write = anything2bool(self.force_write)
+                    except TypeError:
+                        raise RIARemoteError("Invalid value of config "
+                                             "'annex.ora-remote.{}.force-write'"
+                                             ": {}".format(self.name,
+                                                           self.force_write))
+
+            if self.ignore_remote_config is None:
+                self.ignore_remote_config = self._repo.config.get(
+                    f"annex.ora-remote.{self.name}.ignore-remote-config")
+                if self.ignore_remote_config:
+                    self.message("WARNING: config "
+                                 "'annex.ora-remote.{}.ignore-remote-config' is"
+                                 " deprecated. Use "
+                                 "'remote.{}.ora-ignore-ria-config' instead."
+                                 "".format(self.name, self.gitcfg_name))
+                    try:
+                        self.ignore_remote_config = \
+                            anything2bool(self.ignore_remote_config)
+                    except TypeError:
+                        raise RIARemoteError(
+                            "Invalid value of config "
+                            "'annex.ora-remote.{}.ignore-remote-config': {}"
+                            "".format(self.name, self.ignore_remote_config))
+
+    def _load_committed_cfg(self, fail_noid=True):
+
+        # which repo are we talking about
+        self.gitdir = self.annex.getgitdir()
+
+        # go look for an ID
+        self.archive_id = self.annex.getconfig('archive-id')
+        if fail_noid and not self.archive_id:
+            # TODO: Message! "archive ID" is confusing. dl-id or annex-uuid
+            raise RIARemoteError(
+                "No archive ID configured. This should not happen.")
+
+        # what is our uuid?
+        self.uuid = self.annex.getuuid()
+
+        # RIA store URL(s)
+        self.ria_store_url = self.annex.getconfig('url')
+        if self.ria_store_url:
+            self.ria_store_url_source = 'annex'
+        self.ria_store_pushurl = self.annex.getconfig('push-url')
+        if self.ria_store_pushurl:
+            self.ria_store_pushurl_source = 'annex'
+
+        # TODO: This should prob. not be done! Would only have an effect if
+        #       force-write was committed annex-special-remote-config and this
+        #       is likely a bad idea.
+        self.force_write = self.annex.getconfig('force-write')
+        if self.force_write == "":
+            self.force_write = None
+
+        # Get the special remote name
+        # TODO: Make 'name' a property of `SpecialRemote`;
+        #       Same for `gitcfg_name`, `_repo`?
+        self.name = self.annex.getconfig('name')
+        if not self.name:
+            self.name = self.annex.getconfig('sameas-name')
+        if not self.name:
+            # TODO: Do we need to crash? Not necessarily, I think. We could
+            #       still find configs and if not - might work out.
+            raise RIARemoteError(
+                "Cannot determine special remote name, got: {}".format(
+                    repr(self.name)))
+        # Get the name of the remote entry in .git/config.
+        # Note, that this by default is the same as the stored name of the
+        # special remote, but can be different (for example after
+        # git-remote-rename). The actual connection is the uuid of the special
+        # remote, not the name.
+        try:
+            self.gitcfg_name = self.annex.getgitremotename()
+        except (ProtocolError, AttributeError):
+            # GETGITREMOTENAME not supported by annex version or by annexremote
+            # version.
+            # Lets try to find ourselves: Find remote with matching annex uuid
+            response = _get_gitcfg(self.gitdir,
+                                   r"^remote\..*\.annex-uuid",
+                                   regex=True)
+            response = response.splitlines() if response else []
+            candidates = set()
+            for line in response:
+                k, v = line.split()
+                if v == self.annex.getuuid():  # TODO: Where else? self.uuid?
+                    candidates.add(''.join(k.split('.')[1:-1]))
+            num_candidates = len(candidates)
+            if num_candidates == 1:
+                self.gitcfg_name = candidates.pop()
+            elif num_candidates > 1:
+                self.message("Found multiple used remote names in git "
+                             "config: %s" % str(candidates))
+                # try same name:
+                if self.name in candidates:
+                    self.gitcfg_name = self.name
+                    self.message("Choose '%s'" % self.name)
                 else:
-                    files_to_remove = fpaths.values()
+                    self.gitcfg_name = None
+                    self.message("Ignore git config")
+            else:
+                # No entry found.
+                # Possible if we are in "initremote".
+                self.gitcfg_name = None
+
+    def _load_cfg(self, gitdir, name):
+        # Whether or not to force writing to the remote. Currently used to
+        # overrule write protection due to layout version mismatch.
+        self.force_write = self._repo.config.get(
+            f'annex.ora-remote.{name}.force-write')
+
+        # whether to ignore config flags set at the remote end
+        self.ignore_remote_config = \
+            self._repo.config.get(
+                f'annex.ora-remote.{name}.ignore-remote-config')
+
+        # buffer size for reading files over HTTP and SSH
+        self.buffer_size = self._repo.config.get(
+            f"remote.{name}.ora-buffer-size")
+
+        if self.buffer_size:
+            self.buffer_size = int(self.buffer_size)
+
+    def _verify_config(self, fail_noid=True):
+        # try loading all needed info from (git) config
+
+        # first load committed config
+        self._load_committed_cfg(fail_noid=fail_noid)
+        # now local configs (possible overwrite of committed)
+        self._load_local_cfg()
+
+        # get URL rewriting config
+        url_cfgs = {k: v for k, v in self._repo.config.items()
+                    if k.startswith('url.')}
+
+        if self.ria_store_url:
+            self.storage_host, self.store_base_path, self.ria_store_url = \
+                verify_ria_url(self.ria_store_url, url_cfgs)
 
-                for vfpath in files_to_remove:
-                    vfpathfull = opj(self.repo.path, vfpath)
-                    if os.path.lexists(vfpathfull):
-                        lgr.debug(
-                            "Removing %s of version %s (overlay %s). "
-                            "Current one %s (overlay %s)",
-                            vfpathfull, version, overlay_version,
-                            current_version, current_overlay_version
-                        )
-                        os.unlink(vfpathfull)
-
-            assert not tracked_files, "we must not end up having tracked files"
-            if remove_unversioned:
-                # it might be that we haven't 'recorded' unversioned ones at all
-                # and now got an explicit version, so we would just need to remove them all
-                # For that we need to get all files which left, and remove them unless they
-                # were a versioned file (considered above) for any version
-                all_versioned_files = set()
-                for versioned_files_ in versions_db.versions.values():
-                    all_versioned_files.update(versioned_files_.values())
-                for fpath in find_files(
-                        '.*',
-                        topdir=self.repo.path,
-                        exclude=exclude, exclude_datalad=True, exclude_vcs=True
-                    ):
-                    fpath = relpath(fpath, self.repo.path)  # ./bla -> bla
-                    if fpath in all_versioned_files:
-                        lgr.log(
-                            5, "Not removing %s file since it was versioned",
-                            fpath)
-                        continue
-                    lgr.log(5, "Removing unversioned %s file", fpath)
-                    os.unlink(fpath)
-            elif exclude:
-                lgr.warning("`exclude=%r` was specified whenever remove_unversioned is False", exclude)
-
-            if stats:
-                stats.versions.append(current_version)
+        else:
+            # There's one exception to the precedence of local configs:
+            # Age-old "ssh-host" + "base-path" configs are only considered,
+            # if there was no RIA URL (local or committed). However, issue
+            # deprecation warning, if that situation is encountered:
+            host = None
+            path = None
+
+            if self.name:
+                host = self._repo.config.get(
+                    f'annex.ora-remote.{self.name}.ssh-host') or \
+                       self.annex.getconfig('ssh-host')
+                # Note: Special value '0' is replaced by None only after checking
+                # the repository's annex config. This is to uniformly handle '0' and
+                # None later on, but let a user's config '0' overrule what's
+                # stored by git-annex.
+                self.storage_host = None if host == '0' else host
+                path = self._repo.config.get(
+                    f'annex.ora-remote.{self.name}.base-path') or \
+                       self.annex.getconfig('base-path')
+                self.store_base_path = path.strip() if path else path
+
+            if path or host:
+                self.message("WARNING: base-path + ssh-host configs are "
+                             "deprecated and won't be considered in the future."
+                             " Use 'git annex enableremote {} "
+                             "url=<RIA-URL-TO-STORE>' to store a ria+<scheme>:"
+                             "//... URL in the special remote's config."
+                             "".format(self.name),
+                             type='info')
+
+
+        if not self.store_base_path:
+            raise RIARemoteError(
+                "No base path configured for RIA store. Specify a proper "
+                "ria+<scheme>://... URL.")
+
+        # the base path is ultimately derived from a URL, always treat as POSIX
+        self.store_base_path = PurePosixPath(self.store_base_path)
+        if not self.store_base_path.is_absolute():
+            raise RIARemoteError(
+                'Non-absolute RIA store base path configuration: %s'
+                '' % str(self.store_base_path))
+
+        if self.ria_store_pushurl:
+            if self.ria_store_pushurl.startswith("ria+http"):
+                raise RIARemoteError("Invalid push-url: {}. Pushing over HTTP "
+                                     "not implemented."
+                                     "".format(self.ria_store_pushurl))
+            self.storage_host_push, \
+            self.store_base_path_push, \
+            self.ria_store_pushurl = \
+                verify_ria_url(self.ria_store_pushurl, url_cfgs)
+            self.store_base_path_push = PurePosixPath(self.store_base_path_push)
 
-            yield data
+    def _get_version_config(self, path):
+        """ Get version and config flags from RIA store's layout file
+        """
 
-        return _remove_other_versions
+        if self.ria_store_url:
+            # construct path to ria_layout_version file for reporting
+            local_store_base_path = url_path2local_path(self.store_base_path)
+            target_ri = (
+                self.ria_store_url[4:]
+                + "/"
+                + path.relative_to(local_store_base_path).as_posix()
+            )
+        elif self.storage_host:
+            target_ri = "ssh://{}{}".format(self.storage_host, path.as_posix())
+        else:
+            target_ri = path.as_uri()
 
-    # TODO: @borrow_kwargs from api_add_...
-    def add_archive_content(self, commit=False, **aac_kwargs):
-        """
+        try:
+            file_content = self.io.read_file(path).strip().split('|')
 
-        Parameters
-        ----------
-        aac_kwargs: dict, optional
-           Options to pass into api.add_archive_content
+        # Note, that we enhance the reporting here, as the IO classes don't
+        # uniformly operate on that kind of RI (which is more informative
+        # as it includes the store base address including the access
+        # method).
+        except FileNotFoundError as exc:
+            raise NoLayoutVersion(
+                f"{target_ri} not found, "
+                f"self.ria_store_url: {self.ria_store_url}, "
+                f"self.store_base_pass: {self.store_base_path}, "
+                f"self.store_base_pass_push: {self.store_base_path_push}, "
+                f"path: {type(path)} {path}") from exc
+        except PermissionError as exc:
+            raise PermissionError(f"Permission denied: {target_ri}") from exc
+        except Exception as exc:
+            raise RuntimeError(f"Failed to access {target_ri}") from exc
+
+        if not (1 <= len(file_content) <= 2):
+            self.message("invalid version file {}".format(path),
+                         type='info')
+            return None
+
+        remote_version = file_content[0]
+        remote_config_flags = file_content[1] \
+            if len(file_content) == 2 else None
+        if not self.ignore_remote_config and remote_config_flags:
+            # Note: 'or', since config flags can come from toplevel
+            #       (dataset-tree-root) as well as from dataset-level.
+            #       toplevel is supposed flag the entire tree.
+            self.remote_log_enabled = self.remote_log_enabled or \
+                                      'l' in remote_config_flags
+
+        return remote_version
+
+    def get_store(self):
+        """checks the remote end for an existing store and dataset
+
+        Furthermore reads and stores version and config flags, layout
+        locations, etc.
+        If this doesn't raise, the remote end should be fine to work with.
         """
+        # make sure the base path is a platform path when doing local IO
+        # the incoming Path object is a PurePosixPath
+        # XXX this else branch is wrong: Incoming is PurePosixPath
+        # but it is subsequently assumed to be a platform path, by
+        # get_layout_locations() etc. Hence it must be converted
+        # to match the *remote* platform, not the local client
+        store_base_path = (
+            url_path2local_path(self.store_base_path)
+            if self._local_io
+            else self.store_base_path)
+
+        # cache remote layout directories
+        self.remote_git_dir, self.remote_archive_dir, self.remote_obj_dir = \
+            self.get_layout_locations(store_base_path, self.archive_id)
+
+        read_only_msg = "Treating remote as read-only in order to " \
+                        "prevent damage by putting things into an unknown " \
+                        "version of the target layout. You can overrule this " \
+                        "by setting 'annex.ora-remote.<name>.force-write=true'."
+        try:
+            self.verify_store()
+        except UnknownLayoutVersion:
+            reason = "Remote dataset tree reports version {}. Supported " \
+                     "versions are: {}. Consider upgrading datalad or " \
+                     "fix the 'ria-layout-version' file at the RIA store's " \
+                     "root. ".format(self.remote_dataset_tree_version,
+                                     self.known_versions_dst)
+            self._set_read_only(reason + read_only_msg)
+        except NoLayoutVersion:
+            reason = "Remote doesn't report any dataset tree version. " \
+                     "Consider upgrading datalad or add a fitting " \
+                     "'ria-layout-version' file at the RIA store's " \
+                     "root."
+            self._set_read_only(reason + read_only_msg)
+
+        try:
+            self.verify_ds_in_store()
+        except UnknownLayoutVersion:
+            reason = "Remote object tree reports version {}. Supported" \
+                     "versions are {}. Consider upgrading datalad or " \
+                     "fix the 'ria-layout-version' file at the remote " \
+                     "dataset root. " \
+                     "".format(self.remote_object_tree_version,
+                               self.known_versions_objt)
+            self._set_read_only(reason + read_only_msg)
+        except NoLayoutVersion:
+            reason = "Remote doesn't report any object tree version. " \
+                     "Consider upgrading datalad or add a fitting " \
+                     "'ria-layout-version' file at the remote " \
+                     "dataset root. "
+            self._set_read_only(reason + read_only_msg)
+
+    @handle_errors
+    def initremote(self):
+        self._verify_config(fail_noid=False)
+        if not self.archive_id:
+            self.archive_id = _get_datalad_id(self.gitdir)
+            if not self.archive_id:
+                # fall back on the UUID for the annex remote
+                self.archive_id = self.annex.getuuid()
+
+        self.get_store()
+
+        self.annex.setconfig('archive-id', self.archive_id)
+        # Make sure, we store the potentially rewritten URL. But only, if the
+        # source was annex as opposed to a local config.
+        if self.ria_store_url and self.ria_store_url_source == 'annex':
+            self.annex.setconfig('url', self.ria_store_url)
+        if self.ria_store_pushurl and self.ria_store_pushurl_source == 'annex':
+            self.annex.setconfig('push-url', self.ria_store_pushurl)
+
+    def _local_io(self):
+        """Are we doing local operations?"""
+        # let's not make this decision dependent on the existence
+        # of a directory the matches the name of the configured
+        # store tree base dir. Such a match could be pure
+        # coincidence. Instead, let's do remote whenever there
+        # is a remote host configured
+        #return self.store_base_path.is_dir()
+
+        # TODO: Isn't that wrong with HTTP anyway?
+        #       + just isinstance(LocalIO)?
+        # XXX isinstance(LocalIO) would not work, this method is used
+        # before LocalIO is instantiated
+        return not self.storage_host
+
+    def _set_read_only(self, msg):
+
+        if not self.force_write:
+            self.read_only = True
+            self.message(msg, type='info')
+        else:
+            self.message("Was instructed to force write", type='info')
 
-        def _add_archive_content(data):
-            # if no stats -- they will be brand new each time :-/
-            stats = data.get('datalad_stats', ActivityStats())
-            archive = self._get_fpath(data, stats)
-            # TODO: may be adjust annex_options
-            annex = add_archive_content(
-                archive, annex=self.repo,
-                key=False, commit=commit, allow_dirty=True,
-                annex_options=self.options,
-                stats=stats,
-                **aac_kwargs
-            )
-            self._states.add("Added files from extracted archives")
-            assert (annex is self.repo)  # must be the same annex, and no new one created
-            # to propagate statistics from this call into commit msg since we commit=False here
-            # we update data with stats which gets a new instance if wasn't present
-            yield updated(data, {'datalad_stats': stats})
-
-        return _add_archive_content
-
-    # TODO: either separate out commit or allow to pass a custom commit msg?
-    def finalize(self, tag=False, existing_tag=None, cleanup=False, aggregate=False):
-        """Finalize operations -- commit uncommited, prune abandoned? etc
+    def _ensure_writeable(self):
+        if self.read_only:
+            raise RIARemoteError("Remote is treated as read-only. "
+                                 "Set 'ora-remote.<name>.force-write=true' to "
+                                 "overrule this.")
+        if isinstance(self.push_io, HTTPRemoteIO):
+            raise RIARemoteError("Write access via HTTP not implemented")
+
+    @property
+    def io(self):
+        if not self._io:
+            if self._local_io():
+                self._io = LocalIO()
+            elif self.ria_store_url.startswith("ria+http"):
+                # TODO: That construction of "http(s)://host/" should probably
+                #       be moved, so that we get that when we determine
+                #       self.storage_host. In other words: Get the parsed URL
+                #       instead and let HTTPRemoteIO + SSHRemoteIO deal with it
+                #       uniformly. Also: Don't forget about a possible port.
+
+                url_parts = self.ria_store_url[4:].split('/')
+                # we expect parts: ("http(s):", "", host:port, path)
+                self._io = HTTPRemoteIO(
+                    url_parts[0] + "//" + url_parts[2],
+                    self.buffer_size
+                )
+            elif self.storage_host:
+                self._io = SSHRemoteIO(self.storage_host, self.buffer_size)
+                from atexit import register
+                register(self._io.close)
+            else:
+                raise RIARemoteError(
+                    "Local object tree base path does not exist, and no SSH"
+                    "host configuration found.")
+        return self._io
+
+    @property
+    def push_io(self):
+        # Instance of an IOBase subclass for execution based on configured
+        # 'push-url' if such exists. Otherwise identical to `self.io`.
+        # Note, that once we discover we need to use the push-url (that is on
+        # TRANSFER_STORE and REMOVE), we should switch all operations to that IO
+        # instance instead of using different connections for read and write
+        # operations. Ultimately this is due to the design of annex' special
+        # remote protocol - we don't know which annex command is running and
+        # therefore we don't know whether to use fetch or push URL during
+        # PREPARE.
+
+        if not self._push_io:
+            if self.ria_store_pushurl:
+                self.message("switching ORA to push-url")
+                # Not-implemented-push-HTTP is ruled out already when reading
+                # push-url, so either local or SSH:
+                if not self.storage_host_push:
+                    # local operation
+                    self._push_io = LocalIO()
+                else:
+                    self._push_io = SSHRemoteIO(self.storage_host_push,
+                                                self.buffer_size)
 
-        Parameters
-        ----------
-        tag: bool or str, optional
-          If set, information in datalad_stats and data can be used to tag release if
-          versions is non-empty.
-          If True, simply the last version to be used.  If str, it is .format'ed
-          using datalad_stats, so something like "r{stats.versions[0]}" can be used.
-          Also `last_version` is provided as the last one from stats.versions (None
-          if empty)
-        existing_tag: None or '+suffix', optional
-          What to do if tag already exists, if None, warning is issued. If `+suffix`,
-          +0, +1, +2 ... are tried until available one is found.
-        cleanup: bool, optional
-          Either to perform cleanup operations, such as 'git gc' and 'datalad clean'
-        aggregate: bool, optional
-          Aggregate meta-data (ATM no recursion, guessing the type)
-        """
+                # We have a new instance. Kill the existing one and replace.
+                from atexit import register, unregister
+                if hasattr(self.io, 'close'):
+                    unregister(self.io.close)
+                    self.io.close()
+
+                # XXX now also READ IO is done with the write IO
+                # this explicitly ignores the remote config
+                # that distinguishes READ from WRITE with different
+                # methods
+                self._io = self._push_io
+                if hasattr(self.io, 'close'):
+                    register(self.io.close)
+
+                self.storage_host = self.storage_host_push
+                self.store_base_path = self.store_base_path_push
+
+                # delete/update cached locations:
+                self._last_archive_path = None
+                self._last_keypath = (None, None)
+
+                store_base_path = (
+                    url_path2local_path(self.store_base_path)
+                    if self._local_io
+                    else self.store_base_path)
+
+                self.remote_git_dir, \
+                self.remote_archive_dir, \
+                self.remote_obj_dir = \
+                    self.get_layout_locations(store_base_path, self.archive_id)
 
-        def _finalize(data):
-            self._precommit()
-            stats = data.get('datalad_stats', None)
-            if self.repo.dirty:  # or self.tracker.dirty # for dry run
-                lgr.info("Repository found dirty -- adding and committing")
-                _call(self.repo.add, '.', options=self.options)  # so everything is committed
-
-                stats_str = ('\n\n' + stats.as_str(mode='full')) if stats else ''
-                _call(self._commit, "%s%s" % (', '.join(self._states), stats_str), options=["-a"])
-                if stats:
-                    _call(stats.reset)
             else:
-                lgr.info("Found branch non-dirty -- nothing was committed")
+                # no push-url: use existing IO
+                self._push_io = self._io
 
-            if aggregate:
-                from datalad.api import aggregate_metadata
-                aggregate_metadata(dataset=self.repo.path, guess_native_type=True)
-
-            if tag and stats:
-                # versions survive only in total_stats
-                total_stats = stats.get_total()
-                if total_stats.versions:
-                    last_version = total_stats.versions[-1]
-                    if isinstance(tag, string_types):
-                        tag_ = tag.format(stats=total_stats, data=data, last_version=last_version)
-                    else:
-                        tag_ = last_version
-                    # TODO: config.tag.sign
-                    stats_str = "\n\n" + total_stats.as_str(mode='full')
-                    tags = self.repo.get_tags(output='name')
-                    if tag_ in tags:
-                        # TODO: config.tag.allow_override
-                        if existing_tag == '+suffix':
-                            lgr.warning(
-                                "There is already a tag %s in the repository. Delete it first if you want it updated" % tag_)
-                            tag_ = None
-                        elif existing_tag is None:
-                            suf = 1
-                            while True:
-                                tag__ = '%s+%d' % (tag_, suf)
-                                if tag__ not in tags:
-                                    break
-                                suf += 1
-                            lgr.warning("There is already a tag %s in the repository. Tagging as %s" % (tag_, tag__))
-                            tag_ = tag__
-                        else:
-                            raise ValueError(existing_tag)
-                    self.repo.repo.create_tag(tag_, message="Automatically crawled and tagged by datalad %s.%s" % (
-                        __version__, stats_str))
-
-            if cleanup:
-                total_stats = stats.get_total()
-                if total_stats.add_git or total_stats.add_annex or total_stats.merges:
-                    if cfg.obtain('datalad.crawl.pipeline.housekeeping', default=True):
-                        lgr.info("House keeping: gc, repack and clean")
-                        # gc and repack
-                        self.repo.gc(allow_background=False)
-                        clean(dataset=self.repo.path)
-                    else:
-                        lgr.info("No git house-keeping performed as instructed by config")
-                else:
-                    lgr.info("No git house-keeping performed as no notable changes to git")
+        return self._push_io
 
-            self._states = set()
-            yield data
+    @handle_errors
+    def prepare(self):
 
-        return _finalize
+        gitdir = self.annex.getgitdir()
+        self._repo = AnnexRepo(gitdir)
+        self._verify_config()
 
-    def remove_obsolete(self):
-        """Remove obsolete files which were not referenced in queries to DB
+        self.get_store()
 
-        Note that it doesn't reset any state within statusdb upon call, so shouldn't be
-        called multiple times for the same state.
-        """
+        # report active special remote configuration/status
+        self.info = {
+            'store_base_path': str(self.store_base_path),
+            'storage_host': 'local'
+            if self._local_io() else self.storage_host,
+        }
 
-        # made as a class so could be reset
-        class _remove_obsolete(object):
-            def __call__(self_, data):
-                statusdb = self._statusdb
-                obsolete = statusdb.get_obsolete()
-                if obsolete:
-                    files_str = ": " + ', '.join(obsolete) if len(obsolete) < 10 else ""
-                    lgr.info('Removing %d obsolete files%s' % (len(obsolete), files_str))
-                    stats = data.get('datalad_stats', None)
-                    _call(self.repo.remove, obsolete)
-                    if stats:
-                        _call(stats.increment, 'removed', len(obsolete))
-                    for filepath in obsolete:
-                        statusdb.remove(filepath)
-                yield data
-
-            def reset(self_):
-                if self._statusdb:
-                    self._statusdb.reset()
-
-        return _remove_obsolete()
-
-    def remove(self, data, recursive=False):
-        """Removed passed along file name from git/annex"""
-        stats = data.get('datalad_stats', None)
-        self._states.add("Removed files")
-        filename = self._get_fpath(data, stats)
-        # TODO: not sure if we should may be check if exists, and skip/just complain if not
-        if stats:
-            _call(stats.increment, 'removed')
-        filepath = opj(self.repo.path, filename)
-        if lexists(filepath):
-            if os.path.isdir(filepath):
-                if recursive:
-                    _call(self.repo.remove, filename, recursive=True)
-                else:
-                    lgr.warning("Do not removing %s recursively, skipping", filepath)
-            else:
-                _call(self.repo.remove, filename)
-        else:
-            lgr.warning("Was asked to remove non-existing path %s", filename)
-        yield data
+        # TODO: following prob. needs hasattr instead:
+        if not isinstance(self.io, HTTPRemoteIO):
+            self.info['7z'] = ("not " if not self.io.get_7z() else "") + \
+                              "available"
+
+    @handle_errors
+    def transfer_store(self, key, filename):
+        self._ensure_writeable()
+
+        # we need a file-system compatible name for the key
+        key = _sanitize_key(key)
+
+        dsobj_dir, archive_path, key_path = self._get_obj_location(key)
+        key_path = dsobj_dir / key_path
+
+        if self.push_io.exists(key_path):
+            # if the key is here, we trust that the content is in sync
+            # with the key
+            return
+
+        self.push_io.mkdir(key_path.parent)
+
+        # We need to copy to a temp location to let checkpresent fail while the
+        # transfer is still in progress and furthermore not interfere with
+        # administrative tasks in annex/objects.
+        # In addition include uuid, to not interfere with parallel uploads from
+        # different clones.
+        transfer_dir = \
+            self.remote_git_dir / "ora-remote-{}".format(self._repo.uuid) / "transfer"
+        self.push_io.mkdir(transfer_dir)
+        tmp_path = transfer_dir / key
+
+        try:
+            self.push_io.put(filename, tmp_path, self.annex.progress)
+            # copy done, atomic rename to actual target
+            self.push_io.rename(tmp_path, key_path)
+        except Exception as e:
+            # whatever went wrong, we don't want to leave the transfer location
+            # blocked
+            self.push_io.remove(tmp_path)
+            raise e
+
+    @handle_errors
+    def transfer_retrieve(self, key, filename):
+        # we need a file-system compatible name for the key
+        key = _sanitize_key(key)
+
+        dsobj_dir, archive_path, key_path = self._get_obj_location(key)
+        abs_key_path = dsobj_dir / key_path
+        # sadly we have no idea what type of source gave checkpresent->true
+        # we can either repeat the checks, or just make two opportunistic
+        # attempts (at most)
+        try:
+            self.io.get(abs_key_path, filename, self.annex.progress)
+        except Exception as e1:
+            if isinstance(self.io, HTTPRemoteIO):
+                # no client-side archive access over HTTP
+                # Note: This is intentional, as it would mean one additional
+                # request per key. However, server response to the GET can
+                # consider archives on their end.
+                raise
+            # catch anything and keep it around for a potential re-raise
+            try:
+                self.io.get_from_archive(archive_path, key_path, filename,
+                                         self.annex.progress)
+            except Exception as e2:
+                # TODO properly report the causes
+                raise RIARemoteError('Failed to obtain key: {}'
+                                     ''.format([str(e1), str(e2)]))
+
+    @handle_errors
+    def checkpresent(self, key):
+        # we need a file-system compatible name for the key
+        key = _sanitize_key(key)
+
+        dsobj_dir, archive_path, key_path = self._get_obj_location(key)
+        abs_key_path = dsobj_dir / key_path
+        if self.io.exists(abs_key_path):
+            # we have an actual file for this key
+            return True
+        if isinstance(self.io, HTTPRemoteIO):
+            # no client-side archive access over HTTP
+            return False
+        # do not make a careful check whether an archive exists, because at
+        # present this requires an additional SSH call for remote operations
+        # which may be rather slow. Instead just try to run 7z on it and let
+        # it fail if no archive is around
+        # TODO honor future 'archive-mode' flag
+        return self.io.in_archive(archive_path, key_path)
+
+    @handle_errors
+    def remove(self, key):
+        # we need a file-system compatible name for the key
+        key = _sanitize_key(key)
+
+        self._ensure_writeable()
+
+        dsobj_dir, archive_path, key_path = self._get_obj_location(key)
+        key_path = dsobj_dir / key_path
+        if self.push_io.exists(key_path):
+            self.push_io.remove(key_path)
+        key_dir = key_path
+        # remove at most two levels of empty directories
+        for level in range(2):
+            key_dir = key_dir.parent
+            try:
+                self.push_io.remove_dir(key_dir)
+            except Exception:
+                break
 
-    def drop(self, all=False, force=False):
-        """Drop crawled file or all files if all is specified"""
-        def _drop(data):
-            if not all:
-                raise NotImplementedError("provide handling to drop specific file")
-            else:
-                lgr.debug("Dropping all files in %s", self.repo)
-                self.repo.drop([], options=['--all'] + ['--force'] if force else [])
-        return _drop
+    @handle_errors
+    def getcost(self):
+        # 100 is cheap, 200 is expensive (all relative to Config/Cost.hs)
+        # 100/200 are the defaults for local and remote operations in
+        # git-annex
+        # if we have the object tree locally, operations are cheap (100)
+        # otherwise expensive (200)
+        return '100' if self._local_io() else '200'
+
+    @handle_errors
+    def whereis(self, key):
+        # we need a file-system compatible name for the key
+        key = _sanitize_key(key)
+
+        dsobj_dir, archive_path, key_path = self._get_obj_location(key)
+        if isinstance(self.io, HTTPRemoteIO):
+            # display the URL for a request
+            # TODO: method of HTTPRemoteIO
+            # in case of a HTTP remote (unchecked for others), storage_host
+            # is not just a host, but a full URL without a path
+            return f'{self.storage_host}{dsobj_dir}/{key_path}'
+
+        return str(dsobj_dir / key_path) if self._local_io() \
+            else '{}: {}:{}'.format(
+                self.storage_host,
+                self.remote_git_dir,
+                sh_quote(str(key_path)),
+        )
 
-    def initiate_dataset(self, *args, **kwargs):
-        """Thin proxy to initiate_dataset node which initiates dataset as a subdataset to current annexificator
-        """
-        # now we can just refer to initiate_dataset which uses create
-        return initiate_dataset(*args, **kwargs)
+    @staticmethod
+    def get_layout_locations(base_path, dsid):
+        return get_layout_locations(1, base_path, dsid)
+
+    def _get_obj_location(self, key):
+        # Notes: - Changes to this method may require an update of
+        #          ORARemote._layout_version
+        #        - archive_path is always the same ATM. However, it might depend
+        #          on `key` in the future. Therefore build the actual filename
+        #          for the archive herein as opposed to `get_layout_locations`.
+
+        if not self._last_archive_path:
+            self._last_archive_path = self.remote_archive_dir / 'archive.7z'
+        if self._last_keypath[0] != key:
+            if self.remote_object_tree_version == '1':
+                key_dir = self.annex.dirhash_lower(key)
+
+            # If we didn't recognize the remote layout version, we set to
+            # read-only and promised to at least try and read according to our
+            # current version. So, treat that case as if remote version was our
+            # (client's) version.
+            else:
+                key_dir = self.annex.dirhash(key)
+            # double 'key' is not a mistake, but needed to achieve the exact
+            # same layout as the annex/objects tree
+            self._last_keypath = (key, Path(key_dir) / key / key)
+
+        return self.remote_obj_dir, self._last_archive_path, \
+            self._last_keypath[1]
+
+    # TODO: implement method 'error'
+
+
+def main():
+    """cmdline entry point"""
+    super_main(
+        cls=ORARemote,
+        remote_name='ora',
+        description=\
+        "transport file content to and from datasets hosted in RIA stores",
+    )
```

### Comparing `datalad-0.9.3/datalad/crawler/pipelines/nda.py` & `datalad-1.0.0/datalad/distribution/uninstall.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,171 +1,157 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""A pipeline for crawling NIMH data archive"""
+"""Thin shim around drop to preserve some backward-compatibility
 
+"""
 
-# Import necessary nodes
-from ..nodes.misc import assign
-from ..nodes.misc import switch
-from ..nodes.misc import continue_if
-from ..nodes.matches import a_href_match
-from ..nodes.s3 import crawl_s3
-from ..nodes.annex import Annexificator
-from ...consts import DATALAD_SPECIAL_REMOTE
+__docformat__ = 'restructuredtext'
 
-from datalad.support.nda_ import get_oracle_db
-from datalad.support.nda_ import image03_fields, image03_file_fields
-from datalad.support.nda_ import image03_Record
-from datalad.utils import auto_repr, updated
+import logging
 
-
-# Possibly instantiate a logger if you would like to log
-# during pipeline creation
-from logging import getLogger
-lgr = getLogger("datalad.crawler.pipelines.nda")
-
-DEFAULT_BUCKET = 'NDAR_Central'
-
-
-def collection_pipeline(bucket=DEFAULT_BUCKET, prefix=None):
-    """Pipeline to crawl/annex an entire openfmri bucket"""
-
-    lgr.info("Creating a pipeline for the openfmri bucket")
-    annex = Annexificator(
-        create=False,  # must be already initialized etc
-    )
-    sprefix = prefix + '/' if prefix else ''
-    return [
-        crawl_s3(bucket, prefix=prefix, recursive=False,
-                 strategy='commit-versions', repo=annex.repo,
-                 versioned=False),
-        switch('datalad_action',
-               {  # TODO: we should actually deal with subdirs primarily
-                   'commit': annex.finalize(tag=True),
-                   # should we bother removing anything? not sure
-                   # 'remove': annex.remove,
-                   'annex':  annex,
-                   'directory': [
-                       # for initiate_dataset we should replicate filename as handle_name, prefix
-                       assign({
-                           'prefix': sprefix + '%(filename)s/',
-                           'bucket': bucket,
-                           'handle_name': '%(filename)s'
-                       }, interpolate=True),
-                       annex.initiate_dataset(
-                           template='nda',
-                           data_fields=['bucket', 'prefix'],
-                       )
-                   ]
-               },
-               missing='skip',  # ok to not remove
-        ),
-    ]
-
-
-def bucket_pipeline(bucket=DEFAULT_BUCKET, prefix=None):
-    """Pipeline to crawl/annex NDA bucket"""
-
-    lgr.info("Creating a pipeline for the NDA bucket")
-    annex = Annexificator(
-        create=False,  # must be already initialized etc
-        special_remotes=[DATALAD_SPECIAL_REMOTE],
-        backend='MD5E'
-        # Primary purpose of this one is registration of all URLs with our
-        # upcoming "ultimate DB" so we don't get to git anything
-        # largefiles="exclude=CHANGES* and exclude=changelog.txt and exclude=dataset_description.json and exclude=README* and exclude=*.[mc]"
-    )
-
-    return [
-        crawl_s3(bucket,
-                 prefix=prefix, strategy='commit-versions',
-                 repo=annex.repo, versioned=False),
-        switch('datalad_action',
-               {
-                   'commit': annex.finalize(tag=True),
-                   'remove': annex.remove,
-                   'annex':  annex,
-                   'directory': None,
-               })
-    ]
-
-
-@auto_repr
-class crawl_mindar_images03(object):
-    """Crawl miNDAR DB for a given collection
-
-    TODO: generalize for other data structures other than image03, with their
-    own sets of "File" fields
-
-    Parameters
-    ----------
-    collection
-    """
-    def __init__(self, collection):
-        self.collection = collection
-
-    def __call__(self, data):
-
-        db = get_oracle_db()
-
-        query = "SELECT %s FROM IMAGE03 WHERE COLLECTION_ID=%s" \
-                % (','.join(image03_fields), self.collection)
-        c = db.cursor()
-        c.execute(query)
-        # query and wrap into named tuples to ease access
-        #import ipdb; ipdb.set_trace()
-        for rec in c.fetchall():  # TODO -- better access method?
-            rec = image03_Record(*rec)
-            for field in image03_file_fields:
-                url = getattr(rec, field)
-                if url:
-                    # generate a new
-                    yield updated(data, {
-                        'url': url,
-                    })
-        c.close()
-
-
-def pipeline(collection, archives=None):
-    """Pipeline to crawl/annex NDA
-
-    Parameters
-    ----------
-    archives:
-      Idea is to be able to control how archives treated -- extracted within
-      the same repository, or extracted into a submodule. TODO
-    """
-
-    assert archives is None, "nothing else is implemented"
-    lgr.info("Creating a pipeline for the NDA bucket")
-
-    annex = Annexificator(
-        create=False,  # must be already initialized etc
-        special_remotes=[DATALAD_SPECIAL_REMOTE],
-        backend='MD5E',
-        skip_problematic=True,  # TODO: make it cleaner for detection of s3 "directories"
-        # Primary purpose of this one is registration of all URLs with our
-        # upcoming "ultimate DB" so we don't get to git anything
-        # largefiles="exclude=CHANGES* and exclude=changelog.txt and exclude=dataset_description.json and exclude=README* and exclude=*.[mc]"
+from datalad.support.param import Parameter
+from datalad.support.constraints import (
+    EnsureStr,
+    EnsureNone,
+)
+from datalad.distribution.dataset import (
+    datasetmethod,
+    require_dataset,
+    Dataset,
+    EnsureDataset,
+)
+from datalad.interface.base import Interface
+from datalad.interface.common_opts import (
+    if_dirty_opt,
+    recursion_flag,
+)
+from datalad.interface.utils import handle_dirty_dataset
+from datalad.interface.results import get_status_dict
+from datalad.interface.base import (
+    build_doc,
+    eval_results,
+)
+from datalad.utils import (
+    ensure_list,
+)
+from datalad.core.local.status import get_paths_by_ds
+
+
+lgr = logging.getLogger('datalad.distribution.uninstall')
+
+dataset_argument = Parameter(
+    args=("-d", "--dataset"),
+    metavar="DATASET",
+    doc="""specify the dataset to perform the operation on.
+    If no dataset is given, an attempt is made to identify a dataset
+    based on the `path` given""",
+    constraints=EnsureDataset() | EnsureNone())
+
+
+check_argument = Parameter(
+    args=("--nocheck",),
+    doc="""whether to perform checks to assure the configured minimum
+    number (remote) source for data.[CMD:  Give this
+    option to skip checks CMD]""",
+    action="store_false",
+    dest='check')
+
+
+@build_doc
+class Uninstall(Interface):
+    """DEPRECATED: use the `drop` command"""
+    _action = 'uninstall'
+
+    _params_ = dict(
+        dataset=dataset_argument,
+        path=Parameter(
+            args=("path",),
+            metavar="PATH",
+            doc="path/name of the component to be uninstalled",
+            nargs="*",
+            constraints=EnsureStr() | EnsureNone()),
+        recursive=recursion_flag,
+        check=check_argument,
+        if_dirty=if_dirty_opt,
     )
 
-    return [
-        [
-            assign(
-                {'url': 'https://ndar.nih.gov/edit_collection.html?id=%s' % collection,
-                 'filename': 'collection.html'}
-            ),
-            annex,
-        ],
-        [
-            crawl_mindar_images03(collection),
-            continue_if({'url': "s3://(?P<bucket>[^/]*)/submission_(?P<url_submission_id>[0-9]*)/(?P<filename>.*[^/])$"}, re=True),
-            annex,
-            # TODO: add annex tags may be for dataset_id, submission_id, 
-        ],
-    ]
+    @staticmethod
+    @datasetmethod(name=_action)
+    @eval_results
+    def __call__(
+            path=None,
+            *,
+            dataset=None,
+            recursive=False,
+            check=True,
+            if_dirty='save-before'):
+        # all this command does is to map legacy call to their replacement
+        # with drop()
+        import warnings
+        warnings.warn(
+            "The `uninstall` command is deprecated and will be removed in "
+            "a future release. "
+            "Use the `drop` command for safer operation instead.",
+            DeprecationWarning)
+
+        reckless = None
+        if not check:
+            # the old uninstall/drop combo had no checks beyond git-annex
+            # key copy redundancy
+            reckless = 'kill'
+
+        paths_by_ds = None
+        if (reckless == 'kill' and not recursive) or if_dirty != 'ignore':
+            refds = require_dataset(dataset, check_installed=True,
+                                    purpose='uninstall')
+            # same path resolution that drop will do
+            paths_by_ds, errors = get_paths_by_ds(
+                refds, dataset, ensure_list(path),
+                subdsroot_mode='sub')
+
+        if reckless == 'kill' and not recursive:
+            # drop requires recursive with kill
+            # check check of the subdatasets to see if it is safe to enable it
+            if all(not len(Dataset(d).subdatasets(
+                    state='absent',
+                    result_xfm='paths',
+                    return_type='list',
+                    result_renderer='disabled'))
+                    for d in paths_by_ds.keys()):
+                # no dataset has any subdatasets, this is fine to set
+                recursive = True
+        # it has never made sense, but for "compatibility" reasons, and to keep
+        # the "old" implementation slower, even it uses the new implementation
+        if if_dirty != 'ignore':
+            for d in paths_by_ds.keys():
+                handle_dirty_dataset(Dataset(d), mode=if_dirty)
+
+        from datalad.api import drop
+        lgr.debug(
+            "Calling "
+            "drop(dataset=%r, path=%r, recursive=%r, what='all', reckless=%r)",
+            dataset, path, recursive, reckless)
+        for res in drop(
+            path=path,
+            dataset=dataset,
+            recursive=recursive,
+            what='all',
+            reckless=reckless,
+            return_type='generator',
+            result_renderer='disabled',
+            # we need to delegate the decision making to this uninstall shim
+            on_failure='ignore'):
+            if res['status'] == 'error':
+                msg, *rest = res["message"]
+                if isinstance(msg, str) and "--reckless availability" in msg:
+                    # Avoid confusing datalad-uninstall callers with the new
+                    # drop parametrization while uninstall still exists.
+                    msg = msg.replace("--reckless availability", "--nocheck")
+                    res["message"] = (msg, *rest)
+            yield res
+        return
```

### Comparing `datalad-0.9.3/datalad/consts.py` & `datalad-1.0.0/datalad/consts.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,65 +1,71 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 """constants for datalad
 """
 
 import os
 from os.path import join
-from os.path import expanduser
+import re
 
 # directory containing prepared metadata of a dataset repository:
-HANDLE_META_DIR = ".datalad"
-CRAWLER_META_DIR = join(HANDLE_META_DIR, 'crawl')
-CRAWLER_META_CONFIG_FILENAME = 'crawl.cfg'
-CRAWLER_META_CONFIG_PATH = join(CRAWLER_META_DIR, CRAWLER_META_CONFIG_FILENAME)
-CRAWLER_META_VERSIONS_DIR = join(CRAWLER_META_DIR, 'versions')
-# TODO: RENAME THIS UGLINESS?
-CRAWLER_META_STATUSES_DIR = join(CRAWLER_META_DIR, 'statuses')
-
-# Make use of those in datalad.metadata
-METADATA_DIR = join(HANDLE_META_DIR, 'meta')
-METADATA_FILENAME = 'meta.json'
+DATALAD_DOTDIR = ".datalad"
+
+DATASET_CONFIG_FILE = join(DATALAD_DOTDIR, 'config')
 
 ARCHIVES_SPECIAL_REMOTE = 'datalad-archives'
 DATALAD_SPECIAL_REMOTE = 'datalad'
 DATALAD_GIT_DIR = join('.git', 'datalad')
 
 # pregenerated using
 # python3 -c 'from datalad.customremotes.base import generate_uuids as guuid; print(guuid())'
 DATALAD_SPECIAL_REMOTES_UUIDS = {
     # should not be changed from now on!
     DATALAD_SPECIAL_REMOTE: 'cf13d535-b47c-5df6-8590-0793cb08a90a',
     ARCHIVES_SPECIAL_REMOTE: 'c04eb54b-4b4e-5755-8436-866b043170fa'
 }
+WEB_SPECIAL_REMOTE_UUID = '00000000-0000-0000-0000-000000000001'
 
 ARCHIVES_TEMP_DIR = join(DATALAD_GIT_DIR, 'tmp', 'archives')
 ANNEX_TEMP_DIR = join('.git', 'annex', 'tmp')
+ANNEX_TRANSFER_DIR = join('.git', 'annex', 'transfer')
+
+SEARCH_INDEX_DOTGITDIR = join('datalad', 'search_index')
 
 DATASETS_TOPURL = os.environ.get("DATALAD_DATASETS_TOPURL", None) \
-                  or "http://datasets.datalad.org/"
+                  or "https://datasets.datalad.org/"
 # safeguard
 if not DATASETS_TOPURL.endswith('/'):
     DATASETS_TOPURL += '/'
 
-# Centralized deployment
-LOCAL_CENTRAL_PATH = join(expanduser('~'), 'datalad')
-
 WEB_META_LOG = join(DATALAD_GIT_DIR, 'logs')
-WEB_META_DIR = join(DATALAD_GIT_DIR, 'metadata')
-WEB_HTML_DIR = join(DATALAD_GIT_DIR, 'web')
 
 # Format to use for time stamps
 TIMESTAMP_FMT = "%Y-%m-%dT%H:%M:%S%z"
 
-# We use custom ssh runner while interacting with git
-#GIT_SSH_COMMAND = "/tmp/sshrun"  # was a little shell script to help troubleshooting
-GIT_SSH_COMMAND = "datalad sshrun"
+# in order to avoid breakage, import runner-related const
+from datalad.runner.gitrunner import GIT_SSH_COMMAND
 
 # magic sha is from `git hash-object -t tree /dev/null`, i.e. from nothing
 PRE_INIT_COMMIT_SHA = '4b825dc642cb6eb9a060e54bf8d69288fbee4904'
+
+# git/datalad configuration item to provide a token for github
+CONFIG_HUB_TOKEN_FIELD = 'hub.oauthtoken'
+GITHUB_LOGIN_URL = 'https://github.com/login'
+GITHUB_TOKENS_URL = 'https://github.com/settings/tokens'
+
+# format of git-annex adjusted branch names
+ADJUSTED_BRANCH_EXPR = re.compile(r'^adjusted/(?P<name>[^(]+)\(.*\)$')
+
+# Reserved file names on Windows machines
+RESERVED_NAMES_WIN = {'CON', 'PRN', 'AUX', 'NUL', 'COM1', 'COM2', 'COM3',
+                      'COM4', 'COM5', 'COM6', 'COM7', 'COM8', 'COM9', 'LPT1',
+                      'LPT2', 'LPT3', 'LPT4', 'LPT5', 'LPT6', 'LPT7', 'LPT8',
+                      'LPT9'}
+# Characters that can't be a part of a file name on Windows
+ILLEGAL_CHARS_WIN = "[<>:/\\|?*\"]|[\0-\31]"
```

### Comparing `datalad-0.9.3/datalad/ui/base.py` & `datalad-1.0.0/datalad/support/third/loris_token_generator.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,40 +1,51 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
-# ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""Base classes for UI
-
-"""
-
-__docformat__ = 'restructuredtext'
-
-from abc import ABCMeta, abstractmethod
-from six import add_metaclass
+# ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### #
+import sys
+import json
+
+from urllib.request import (
+    Request,
+    urlopen,
+)
+from urllib.error import HTTPError
+
+from datalad.support.exceptions import (
+    AccessDeniedError,
+)
+from datalad.utils import ensure_unicode
+
+
+class LORISTokenGenerator(object):
+    """
+    Generate a LORIS API token by making a request to the
+    LORIS login API endpoint with the given username
+    and password.
+
+    url is the complete URL of the $LORIS/api/$VERSION/login
+    endpoint.
+    """
+    def __init__(self, url=None):
+        assert(url is not None)
+        self.url = url
+
+    def generate_token(self, user=None, password=None):
+        data = {'username': user, 'password' : password}
+        encoded_data = json.dumps(data).encode('utf-8')
+
+        request = Request(self.url, encoded_data)
+
+        try:
+            response = urlopen(request)
+        except HTTPError:
+            raise AccessDeniedError("Could not authenticate into LORIS")
+
+        str_response = ensure_unicode(response.read())
+        data = json.loads(str_response)
+        return data["token"]
 
-from ..utils import auto_repr
-
-
-@auto_repr
-@add_metaclass(ABCMeta)
-class InteractiveUI(object):
-    """Semi-abstract class for interfaces to implement interactive UI"""
-
-    @abstractmethod
-    def question(self, text,
-                 title=None, choices=None,
-                 default=None,
-                 hidden=False):
-        pass
-
-    def yesno(self, *args, **kwargs):
-        response = self.question(*args, choices=['yes', 'no'], **kwargs).rstrip('\n')
-        if response == 'yes':
-            return True
-        elif response == 'no':
-            return False
-        else:
-            raise RuntimeError("must not happen but did")
```

### Comparing `datalad-0.9.3/datalad/ui/tests/test_base.py` & `datalad-1.0.0/datalad/ui/tests/test_base.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,25 +1,34 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-wstrth: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 """tests for UI switcher"""
 
 __docformat__ = 'restructuredtext'
 
+from unittest.mock import patch
+
+from ...tests.utils_pytest import (
+    assert_equal,
+    assert_false,
+    assert_not_equal,
+    assert_raises,
+    with_testsui,
+)
 from .. import _UI_Switcher
-from ..dialog import DialogUI, ConsoleLog
-from ...tests.utils import assert_equal, assert_not_equal
-from ...tests.utils import assert_raises
-from ...tests.utils import assert_false
-from ...tests.utils import with_testsui
+from ..dialog import (
+    ConsoleLog,
+    DialogUI,
+    IPythonUI,
+)
 
 
 def test_ui_switcher():
     ui = _UI_Switcher('dialog')
     assert(isinstance(ui.ui, DialogUI))
     message_str = str(ui.message)
     assert_equal(message_str, str(ui._ui.message))
@@ -29,14 +38,24 @@
     assert_equal(str(ui.message), str(ui._ui.message))
     assert_not_equal(message_str, str(ui._ui.message))
     with assert_raises(AttributeError):
         ui.yesno
 
     ui.set_backend('annex')
 
+    # Let's pretend we are under IPython
+    class ZMQInteractiveShell(object):
+        pass
+
+    with patch('datalad.utils.get_ipython',
+               lambda: ZMQInteractiveShell(),
+               create=True):
+        ui = _UI_Switcher()
+        assert (isinstance(ui.ui, IPythonUI))
+
 
 def test_tests_ui():
     ui = _UI_Switcher('dialog')
     # Let's test our responses construct
     ui.set_backend('tests')
     with ui.add_responses('abc'):
         assert_equal(ui.question("text"), 'abc')
@@ -81,7 +100,10 @@
     assert_raises(AssertionError, nothing, 1, k=2)
 
     from datalad.ui import ui
 
     @with_testsui(responses='a')
     def ask():
         assert_equal(ui.question('what is a?'), 'a')
+
+
+    ask()
```

### Comparing `datalad-0.9.3/datalad/ui/dialog.py` & `datalad-1.0.0/datalad/ui/dialog.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 """Basic dialog-like interface for interactions in the terminal window
@@ -20,21 +20,22 @@
 import os
 import sys
 import time
 
 import getpass
 
 #!!! OPT adds >100ms to import time!!!
-# from mock import patch
+# from unittest.mock import patch
 from collections import deque
 from copy import copy
 
 from ..utils import auto_repr
 from ..utils import on_windows
 from .base import InteractiveUI
+from datalad.support.exceptions import CapturedException
 
 # Example APIs which might be useful to look for "inspiration"
 #  man debconf-devel
 #  man zenity
 #
 # "Fancy" output of progress etc in the terminal:
 # - docker has multiple simultaneous progressbars.  Apparently "navigation"
@@ -66,20 +67,34 @@
 
     progressbars = None
 
     def __init__(self, out=sys.stdout):
         self.out = out
 
     def message(self, msg, cr='\n'):
-        self.out.write(msg)
-        if cr:
-            self.out.write(cr)
+        from datalad.log import no_progress
+        with no_progress():
+            try:
+                self.out.write(msg)
+            except UnicodeEncodeError as e:
+                # all unicode magic has failed and the receiving end cannot handle
+                # a particular unicode char. rather than crashing, we replace the
+                # offending chars to be able to message at least something, and we
+                # log that we did that
+                encoding = self.out.encoding
+                lgr.debug(
+                    "Replacing unicode chars in message output for display: %s",
+                    e)
+                self.out.write(
+                    msg.encode(encoding, "replace").decode(encoding))
+            if cr:
+                self.out.write(cr)
 
     def error(self, error):
-        self.out.write("ERROR: %s\n" % error)
+        self.message("ERROR: %s" % error)
 
     def get_progressbar(self, *args, **kwargs):
         """Return a progressbar.  See e.g. `tqdmProgressBar` about the interface
 
         Additional parameter is backend to choose among available
         """
         backend = kwargs.pop('backend', None)
@@ -87,14 +102,19 @@
         if ConsoleLog.progressbars is None:
             from .progressbars import progressbars
             ConsoleLog.progressbars = progressbars
         else:
             progressbars = ConsoleLog.progressbars
 
         if backend is None:
+            # Resort to the configuration
+            from .. import cfg
+            backend = cfg.get('datalad.ui.progressbar', None)
+
+        if backend is None:
             try:
                 pbar = progressbars['tqdm']
             except KeyError:
                 pbar = progressbars.values()[0]  # any
         else:
             pbar = progressbars[backend]
         return pbar(*args, out=self.out, **kwargs)
@@ -106,80 +126,120 @@
 
 @auto_repr
 class SilentConsoleLog(ConsoleLog):
     """A ConsoleLog with a SilentProgressbar"""
 
     def get_progressbar(self, *args, **kwargs):
         from .progressbars import SilentProgressBar
-        return SilentProgressBar(*args, out=self.out, **kwargs)
+        return SilentProgressBar(*args, **kwargs)
+
+    def question(self, text, title=None, **kwargs):
+        msg = "A non-interactive silent UI was asked for a response to a question: %s." % text
+        if title is not None:
+            msg += ' Title: %s.' % title
+        if not kwargs.get('hidden'):
+            kwargs_str = ', '.join(
+                ('%s=%r' % (k, v)
+                for k, v in kwargs.items()
+                if v is not None))
+            if kwargs_str:
+                msg += " Additional arguments: %s" % kwargs_str
+        else:
+            msg += " Additional arguments are not shown because 'hidden' is set."
+        raise RuntimeError(msg)
+
+
+@auto_repr
+class QuietConsoleLog(ConsoleLog):
+    """A ConsoleLog with a LogProgressbar"""
+
+    def get_progressbar(self, *args, **kwargs):
+        from .progressbars import LogProgressBar
+        return LogProgressBar(*args, **kwargs)
 
 
 def getpass_echo(prompt='Password', stream=None):
     """Q&D workaround until we have proper 'centralized' UI -- just use getpass BUT enable echo
     """
-    prompt = '{}: '.format(prompt)
     if on_windows:
         # Can't do anything fancy yet, so just ask the one without echo
         return getpass.getpass(prompt=prompt, stream=stream)
     else:
         # We can mock patch termios so that ECHO is not turned OFF.
         # Side-effect -- additional empty line is printed
 
         # def _no_emptyline_write(out):
         #     # Additional mock to prevent not needed empty line print since we do have echo
         #     # doesn't work since we don't know the stream here really
         #     if out == '\n':
         #         return
         #     stream.write(out)
-        from mock import patch
+        from unittest.mock import patch
         with patch('termios.ECHO', 255 ** 2):
             #patch.object(stream, 'write', _no_emptyline_write(stream)):
             return getpass.getpass(prompt=prompt, stream=stream)
 
 
+def _get_value(value, hidden):
+    return "<hidden>" if hidden else value
+
+
 @auto_repr
 class DialogUI(ConsoleLog, InteractiveUI):
 
     def __init__(self, *args, **kwargs):
         super(DialogUI, self).__init__(*args, **kwargs)
         # ATM doesn't make sense to print the same title for subsequent questions
         # so we will store previous one and not show it if was the previous one shown
         # within 5 seconds from prev question
         self._prev_title = None
         self._prev_title_time = 0
 
+    def input(self, prompt, hidden=False):
+        """Request user input
+
+        Parameters
+        ----------
+        prompt: str
+          Prompt for the entry
+        """
+        # if not hidden:
+        #     self.out.write(msg + ": ")
+        #     self.out.flush()  # not effective for stderr for some reason under annex
+        #
+        #     # TODO: raw_input works only if stdin was not controlled by
+        #     # (e.g. if coming from annex).  So we might need to do the
+        #     # same trick as get_pass() does while directly dealing with /dev/pty
+        #     # and provide per-OS handling with stdin being override
+        #     response = (raw_input if PY2 else input)()
+        # else:
+        return (getpass.getpass if hidden else getpass_echo)(prompt)
+
     def question(self, text,
                  title=None, choices=None,
                  default=None,
-                 hidden=False):
+                 hidden=False,
+                 repeat=None):
         # Do initial checks first
         if default and choices and default not in choices:
             raise ValueError("default value %r is not among choices: %s"
-                             % (default, choices))
+                             % (_get_value(default, hidden), choices))
 
         msg = ''
         if title and not (title == self._prev_title and time.time() - self._prev_title_time < 5):
             # might not actually get displayed if all in/out redirected
             # self.out.write(title + "\n")
             # so merge into msg for getpass
             msg += title + os.linesep
 
         def mark_default(x):
             return "[%s]" % x \
                 if default is not None and x == default \
                 else x
 
-        def ask_repetition_match(msg):
-            response = getpass.getpass('{}: '.format(msg))
-            response_r = getpass.getpass('{} (repeat): '.format(msg))
-            if response != response_r:
-                return None
-            else:
-                return response
-
         if choices is not None:
             msg += "%s (choices: %s)" % (text, ', '.join(map(mark_default, choices)))
         elif default is not None:
             msg += '{} [{}]'.format(text, default)
         else:
             msg += text
         # Like this:
@@ -188,62 +248,109 @@
         #Question? [choice1|choice2]
         #[default] >>> yes
         attempt = 0
         while True:
             attempt += 1
             if attempt >= 100:
                 raise RuntimeError("This is 100th attempt. Something really went wrong")
-            # if not hidden:
-            #     self.out.write(msg + ": ")
-            #     self.out.flush()  # not effective for stderr for some reason under annex
-            #
-            #     # TODO: raw_input works only if stdin was not controlled by
-            #     # (e.g. if coming from annex).  So we might need to do the
-            #     # same trick as get_pass() does while directly dealing with /dev/pty
-            #     # and provide per-OS handling with stdin being override
-            #     response = (raw_input if PY2 else input)()
-            # else:
-            response = (ask_repetition_match if hidden else getpass_echo)(msg)
-            if response is None:
-                self.error("input mismatch, please start over")
-                continue
-            elif '\x03' in response:
+
+            response = self.input("{}: ".format(msg), hidden=hidden)
+            # TODO: dedicated option?  got annoyed by this one
+            # multiple times already, typically we are not defining
+            # new credentials where repetition would be needed.
+            if hidden and repeat is None:
+                repeat = hidden and choices is None
+
+            if repeat:
+                response_r = self.input('{} (repeat): '.format(msg), hidden=hidden)
+                if response != response_r:
+                    self.error("input mismatch, please start over")
+                    continue
+
+            if response and '\x03' in response:
                 # Ctrl-C is part of the response -> clearly we should not pretend it's all good
                 raise KeyboardInterrupt
 
             if not response and default:
                 response = default
                 break
 
             if choices and response not in choices:
                 self.error("%r is not among choices: %s. Repeat your answer"
-                           % (response, choices))
+                           % (_get_value(response, hidden), choices))
                 continue
             break
 
         self._prev_title = title
         self._prev_title_time = time.time()
 
         return response
 
 
+class IPythonUI(DialogUI):
+    """Custom to IPython frontend UI implementation
+
+    There is no way to discriminate between web notebook or qt console,
+    so we have just a single class for all.
+
+    TODO: investigate how to provide 'proper' displays for
+    IPython of progress bars so backend could choose the
+    appropriate one
+
+    """
+
+    _tqdm_frontend = "unknown"
+
+    def input(self, prompt, hidden=False):
+        # We cannot and probably do not need to "abuse" termios
+        if not hidden:
+            self.out.write(prompt)
+            self.out.flush()
+            return input()
+        else:
+            return getpass.getpass(prompt=prompt)
+
+    def get_progressbar(self, *args, **kwargs):
+        """Return a progressbar.  See e.g. `tqdmProgressBar` about the
+        interface
+
+        Additional parameter is backend to choose among available
+        """
+        backend = kwargs.pop('backend', None)
+        if self._tqdm_frontend == "unknown":
+            try:
+                from tqdm import tqdm_notebook  # check if available etc
+                self.__class__._tqdm_frontend = 'ipython'
+            except Exception as exc:
+                lgr.warning(
+                    "Regular progressbar will be used -- cannot import tqdm_notebook: %s",
+                    CapturedException(exc)
+                )
+                self.__class__._tqdm_frontend = None
+        if self._tqdm_frontend:
+            kwargs.update()
+        return super(IPythonUI, self).get_progressbar(
+                *args, frontend=self._tqdm_frontend, **kwargs)
+
+
 # poor man thingie for now
 @auto_repr
 class UnderAnnexUI(DialogUI):
     def __init__(self, specialremote=None, **kwargs):
         if 'out' not in kwargs:
             # to avoid buffering
             # http://stackoverflow.com/a/181654/1265472
             #kwargs['out'] = os.fdopen(sys.stderr.fileno(), 'w', 0)
             # but wasn't effective! sp kist straogjt for now
             kwargs['out'] = sys.stderr
         super(UnderAnnexUI, self).__init__(**kwargs)
         self.specialremote = specialremote
 
     def set_specialremote(self, specialremote):
+        lgr.debug("Setting specialremote of UI %s to %s", self, specialremote)
         self.specialremote = specialremote
 
     def get_progressbar(self, *args, **kwargs):
         if self.specialremote:
             kwargs = kwargs.copy()
             kwargs['backend'] = 'annex-remote'
             kwargs['remote'] = self.specialremote
```

### Comparing `datalad-0.9.3/datalad/downloaders/base.py` & `datalad-1.0.0/datalad/downloaders/base.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,37 +1,56 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 """Provide access to stuff (html, data files) via HTTP and HTTPS
 
 """
 
 __docformat__ = 'restructuredtext'
 
 import msgpack
 import os
+import sys
 import time
 
 from abc import ABCMeta, abstractmethod
+import os.path as op
 from os.path import exists, join as opj, isdir
-from six import PY2
-from six import binary_type, PY3
-from six import add_metaclass
 
 
 from .. import cfg
 from ..ui import ui
-from ..utils import auto_repr
-from ..dochelpers import exc_str
-from .credentials import CREDENTIAL_TYPES
+from ..utils import (
+    auto_repr,
+    ensure_unicode,
+    unlink,
+)
+from .credentials import (
+    CompositeCredential,
+)
+from datalad.downloaders import CREDENTIAL_TYPES
+from ..support.exceptions import (
+    AccessDeniedError,
+    AccessPermissionExpiredError,
+    AnonymousAccessDeniedError,
+    CapturedException,
+    DownloadError,
+    IncompleteDownloadError,
+    UnaccountedDownloadError,
+)
+from ..support.locking import (
+    InterProcessLock,
+    try_lock,
+    try_lock_informatively,
+)
 
 from logging import getLogger
 lgr = getLogger('datalad.downloaders')
 
 
 # TODO: remove headers, HTTP specific
 @auto_repr
@@ -52,16 +71,15 @@
     def download(self, f=None, pbar=None, size=None):
         raise NotImplementedError("must be implemented in subclases")
 
         # TODO: get_status ?
 
 
 @auto_repr
-@add_metaclass(ABCMeta)
-class BaseDownloader(object):
+class BaseDownloader(object, metaclass=ABCMeta):
     """Base class for the downloaders"""
 
     _DEFAULT_AUTHENTICATOR = None
     _DOWNLOAD_SIZE_TO_VERIFY_AUTH = 10000
 
     def __init__(self, credential=None, authenticator=None):
         """
@@ -72,16 +90,16 @@
           Provides necessary credential fields to be used by authenticator
         authenticator: Authenticator, optional
           Authenticator to use for authentication.
         """
         if not authenticator and self._DEFAULT_AUTHENTICATOR:
             authenticator = self._DEFAULT_AUTHENTICATOR()
 
-        if authenticator:
-            if not credential:
+        if authenticator and authenticator.requires_authentication:
+            if not credential and not authenticator.allows_anonymous:
                 msg = "Both authenticator and credentials must be provided." \
                       " Got only authenticator %s" % repr(authenticator)
                 if ui.yesno(
                     title=msg,
                     text="Do you want to enter %s credentials to be used?" % authenticator.DEFAULT_CREDENTIAL_TYPE
                 ):
                     credential = CREDENTIAL_TYPES[authenticator.DEFAULT_CREDENTIAL_TYPE](
@@ -110,100 +128,243 @@
 
         Returns
         -------
         None or bytes
         """
         # TODO: possibly wrap this logic outside within a decorator, which
         # would just call the corresponding method
-
         authenticator = self.authenticator
-        needs_authentication = authenticator and authenticator.requires_authentication
+        if authenticator:
+            needs_authentication = authenticator.requires_authentication
+        else:
+            needs_authentication = self.credential
+
+        # TODO: not sure yet, where is/are the right spot(s) to pass the URL:
+        if hasattr(self.credential, 'set_context'):
+            lgr.debug("set credential context as %s", url)
+            self.credential.set_context(auth_url=url)
 
         attempt, incomplete_attempt = 0, 0
+        result = None
+        credential_was_refreshed = False
         while True:
             attempt += 1
             if attempt > 20:
                 # are we stuck in a loop somehow? I think logic doesn't allow this atm
                 raise RuntimeError("Got to the %d'th iteration while trying to download %s" % (attempt, url))
+            exc_info = None
+            msg_types = ''
+            supported_auth_types = []
+            used_old_session = False
+            # Lock must be instantiated here, within each thread to avoid problems
+            # when used in our parallel.ProducerConsumer
+            # see https://github.com/datalad/datalad/issues/6483
+            interp_lock = InterProcessLock(
+                op.join(cfg.obtain('datalad.locations.locks'),
+                        'downloader-auth.lck')
+            )
 
             try:
-                used_old_session = False
-                access_denied = False
-                used_old_session = self._establish_session(url, allow_old=allow_old_session)
+                # Try to lock since it might desire to ask for credentials, but still allow to time out at 5 minutes
+                # while providing informative message on what other process might be holding it.
+                with try_lock_informatively(interp_lock, purpose="establish download session", proceed_unlocked=False):
+                    used_old_session = self._establish_session(url, allow_old=allow_old_session)
                 if not allow_old_session:
                     assert(not used_old_session)
-                lgr.log(5, "Calling out into %s for %s" % (method, url))
+                lgr.log(5, "Calling out into %s for %s", method, url)
                 result = method(url, **kwargs)
                 # assume success if no puke etc
                 break
             except AccessDeniedError as e:
-                lgr.debug("Access was denied: %s", exc_str(e))
-                access_denied = True
+                ce = CapturedException(e)
+                if hasattr(e, 'status') and e.status == 429:
+                    # Too many requests.
+                    # We can retry by continuing the loop.
+                    time.sleep(0.5*(attempt**1.2))
+                    continue
+
+                if isinstance(e, AnonymousAccessDeniedError):
+                    access_denied = "Anonymous access"
+                else:
+                    access_denied = "Access"
+                lgr.debug("%s was denied: %s", access_denied, ce)
+                supported_auth_types = e.supported_types
+                exc_info = sys.exc_info()
+
+                if supported_auth_types:
+                    msg_types = \
+                        " The failure response indicated that following " \
+                        "authentication types should be used: %s" % (
+                            ', '.join(supported_auth_types))
+                # keep inside except https://github.com/datalad/datalad/issues/3621
+                # TODO: what if it was anonimous attempt without authentication,
+                #     so it is not "requires_authentication" but rather
+                #     "supports_authentication"?  We should not report below in
+                # _get_new_credential that authentication has failed then since there
+                # were no authentication.  We might need a custom exception to
+                # be caught above about that
+
+                allow_old_session = False  # we will either raise or auth
+                # in case of parallel downloaders, one would succeed to get the
+                # lock, ask user if necessary and other processes would just wait
+                # got it to return back
+                with try_lock(interp_lock) as got_lock:
+                    if got_lock:
+                        if isinstance(e, AccessPermissionExpiredError) \
+                                and not credential_was_refreshed \
+                                and self.credential \
+                                and isinstance(self.credential, CompositeCredential):
+                            lgr.debug("Requesting refresh of the credential (once)")
+                            self.credential.refresh()
+                            # to avoid a loop of refreshes without giving a chance to
+                            # enter a new one, we will allow only a single refresh
+                            credential_was_refreshed = True
+                        else:
+                            self._handle_authentication(url, needs_authentication, e, ce,
+                                                        access_denied, msg_types,
+                                                        supported_auth_types,
+                                                        used_old_session)
+                    else:
+                        lgr.debug("The lock for downloader authentication was not available.")
+                        # We will just wait for the lock to become available,
+                        # and redo connect/download attempt
+                continue
+
             except IncompleteDownloadError as e:
+                ce = CapturedException(e)
+                exc_info = sys.exc_info()
                 incomplete_attempt += 1
                 if incomplete_attempt > 5:
                     # give up
                     raise
-                lgr.debug("Failed to download fully, will try again: %s", exc_str(e))
-                # TODO: may be fail ealier than after 20 attempts in such a case?
+                lgr.debug("Failed to download fully, will try again: %s", ce)
+                # TODO: may be fail earlier than after 20 attempts in such a case?
             except DownloadError:
                 # TODO Handle some known ones, possibly allow for a few retries, otherwise just let it go!
                 raise
 
-            if access_denied:  # moved logic outside of except for clarity
-                if needs_authentication:
-                    # so we knew it needs authentication
-                    if used_old_session:
-                        # Let's try with fresh ones
-                        allow_old_session = False
-                        continue
-                    else:
-                        # we did use new cookies, we knew that authentication is needed
-                        # but still failed. So possible cases:
-                        #  1. authentication credentials changed/were revoked
-                        #     - allow user to re-enter credentials
-                        #  2. authentication mechanisms changed
-                        #     - we can't do anything here about that
-                        #  3. bug in out code which would render authentication/cookie handling
-                        #     ineffective
-                        #     - not sure what to do about it
-                        if not ui.is_interactive:
-                            # We cannot ask, so we just (re)blow
-                            lgr.error(
-                                "Interface is non interactive, so we are "
-                                "reraising: %s" % exc_str(e))
-                            raise
-                        if ui.yesno(
-                                title="Authentication to access {url} has failed".format(url=url),
-                                text="Do you want to enter other credentials in case they were updated?"):
-                            self.credential.enter_new()
-                            allow_old_session = False
-                            continue
-                        else:
-                            raise DownloadError("Failed to download from %s given available credentials" % url)
-                else:  # None or False
-                    if needs_authentication is False:
-                        # those urls must or should NOT require authentication but we got denied
-                        raise DownloadError("Failed to download from %s, which must be available without "
-                                            "authentication but access was denied" % url)
-                    else:
-                        assert(needs_authentication is None)
-                        # So we didn't know if authentication necessary, and it seems to be necessary, so
-                        # Let's ask the user to setup authentication mechanism for this website
-                        raise AccessDeniedError(
-                            "Access to %s was denied but we don't know about this data provider. "
-                            "You would need to configure data provider authentication using TODO " % url)
-
         return result
 
+    def _handle_authentication(self, url, needs_authentication, e, ce,
+                               access_denied, msg_types, supported_auth_types,
+                               used_old_session):
+        if needs_authentication:
+            # so we knew it needs authentication
+            if not used_old_session:
+                # we did use new cookies, we knew that authentication is needed
+                # but still failed. So possible cases:
+                #  1. authentication credentials changed/were revoked
+                #     - allow user to re-enter credentials
+                #  2. authentication mechanisms changed
+                #     - we can't do anything here about that
+                #  3. bug in out code which would render
+                #  authentication/cookie handling
+                #     ineffective
+                #     - not sure what to do about it
+                if not ui.is_interactive:
+                    lgr.error(
+                        "Interface is non interactive, so we are "
+                        "reraising: %s", ce)
+                    raise e
+                self._enter_credentials(
+                    url,
+                    denied_msg=access_denied,
+                    auth_types=supported_auth_types,
+                    new_provider=False)
+        else:  # None or False
+            if needs_authentication is False:
+                # those urls must or should NOT require authentication
+                # but we got denied
+                raise DownloadError(
+                    "Failed to download from %s, which must be available"
+                    "without authentication but access was denied. "
+                    "Adjust your configuration for the provider.%s"
+                    % (url, msg_types))
+            else:
+                # how could be None or any other non-False bool(False)
+                assert (needs_authentication is None)
+                # So we didn't know if authentication necessary, and it
+                # seems to be necessary, so Let's ask the user to setup
+                # authentication mechanism for this website
+                self._enter_credentials(
+                    url,
+                    denied_msg=access_denied,
+                    auth_types=supported_auth_types,
+                    new_provider=True)
+
+    def _setup_new_provider(self, title, url, auth_types=None):
+        # Full new provider (TODO move into Providers?)
+        from .providers import Providers
+        providers = Providers.from_config_files()
+        while True:
+            provider = providers.enter_new(url, auth_types=auth_types)
+            if not provider:
+                if ui.yesno(
+                    title="Re-enter provider?",
+                    text="You haven't entered or saved provider, would you like to retry?",
+                    default=True
+                ):
+                    continue
+            break
+        return provider
+
+    def _enter_credentials(
+            self, url, denied_msg,
+            auth_types=[], new_provider=True):
+        """Use when authentication fails to set new credentials for url
+
+        Raises
+        ------
+        DownloadError
+          If no known credentials type or user refuses to update
+        """
+        title = f"{denied_msg} to {url} has failed."
+
+        if new_provider:
+            # No credential was known, we need to create an
+            # appropriate one
+            if not ui.yesno(
+                    title=title,
+                    text="Would you like to setup a new provider configuration"
+                         " to access url?",
+                    default=True
+            ):
+                assert not self.authenticator, "bug: incorrect assumption"
+                raise DownloadError(
+                    title +
+                    " No authenticator is known, cannot set any credential")
+            else:
+                provider = self._setup_new_provider(
+                    title, url, auth_types=auth_types)
+                self.authenticator = provider.authenticator
+                self.credential = provider.credential
+                if not (self.credential and self.credential.is_known):
+                    # TODO: or should we ask to re-enter?
+                    self.credential.enter_new()
+        else:
+            action_msg = "enter other credentials in case they were updated?"
+
+            if self.credential and ui.yesno(
+                    title=title,
+                    text="Do you want to %s" % action_msg):
+                self.credential.enter_new()
+            else:
+                raise DownloadError(
+                    "Failed to download from %s given available credentials"
+                    % url)
+
+        lgr.debug("set credential context as %s", url)
+        self.credential.set_context(auth_url=url)
+
     @staticmethod
     def _get_temp_download_filename(filepath):
         """Given a filepath, return the one to use as temp file during download
         """
-        # TODO: might better reside somewhere under .datalad/tmp or .git/datalad/tmp
+        # TODO: might better reside somewhere under .datalad/tmp or
+        # .git/datalad/tmp
         return filepath + ".datalad-download-temp"
 
     @abstractmethod
     def get_downloader_session(self, url):
         """
 
         Parameters
@@ -231,15 +392,17 @@
                 and hasattr(self.authenticator, 'failure_re') \
                 and self.authenticator.failure_re:
             assert hasattr(self.authenticator, 'check_for_auth_failure'), \
                 "%s has failure_re defined but no check_for_auth_failure" \
                 % self.authenticator
 
             if file_:
-                with open(file_) as fp:
+                # just read bytes and pass to check_for_auth_failure which
+                # will then encode regex into bytes (assuming utf-8 though)
+                with open(file_, 'rb') as fp:
                     content = fp.read(self._DOWNLOAD_SIZE_TO_VERIFY_AUTH)
             else:
                 assert(content is not None)
 
             self.authenticator.check_for_auth_failure(
                 content, "Download of the url %s has failed: " % url)
 
@@ -272,36 +435,42 @@
 
         target_size = downloader_session.size
         if size is not None:
             target_size = min(target_size, size)
 
         #### Specific to download
         if path:
+            download_dir = op.dirname(path)
+            if download_dir:
+                os.makedirs(download_dir, exist_ok=True)
             if isdir(path):
                 # provided path is a directory under which to save
                 filename = downloader_session.filename
+                if not filename:
+                    raise DownloadError(
+                        "File name could not be determined from {}".format(url))
                 filepath = opj(path, filename)
             else:
                 filepath = path
         else:
             filepath = downloader_session.filename
 
-        existed = exists(filepath)
+        existed = op.lexists(filepath)
         if existed and not overwrite:
-            raise DownloadError("File %s already exists" % filepath)
+            raise DownloadError("Path %s already exists" % filepath)
 
         # FETCH CONTENT
         # TODO: pbar = ui.get_progressbar(size=response.headers['size'])
         try:
             temp_filepath = self._get_temp_download_filename(filepath)
             if exists(temp_filepath):
                 # eventually we might want to continue the download
                 lgr.warning(
                     "Temporary file %s from the previous download was found. "
-                    "It will be overriden" % temp_filepath)
+                    "It will be overridden" % temp_filepath)
                 # TODO.  also logic below would clean it up atm
 
             with open(temp_filepath, 'wb') as fp:
                 # TODO: url might be a bit too long for the beast.
                 # Consider to improve to make it animated as well, or shorten here
                 pbar = ui.get_progressbar(label=url, fill_text=filepath, total=target_size)
                 t0 = time.time()
@@ -316,34 +485,32 @@
 
             # adjust atime/mtime according to headers/status
             if status.mtime:
                 lgr.log(5, "Setting mtime for %s to be %s", temp_filepath, status.mtime)
                 os.utime(temp_filepath, (time.time(), status.mtime))
 
             # place successfully downloaded over the filepath
-            os.rename(temp_filepath, filepath)
+            os.replace(temp_filepath, filepath)
 
             if stats:
                 stats.downloaded += 1
                 stats.overwritten += int(existed)
                 stats.downloaded_size += downloaded_size
                 stats.downloaded_time += downloaded_time
         except (AccessDeniedError, IncompleteDownloadError) as e:
             raise
         except Exception as e:
-            e_str = exc_str(e, limit=5)
-            lgr.error("Failed to download {url} into {filepath}: {e_str}".format(
-                **locals()
-            ))
-            raise DownloadError(exc_str(e))  # for now
+            ce = CapturedException(e)
+            lgr.error("Failed to download %s into %s: %s", url, filepath, ce)
+            raise DownloadError(ce) from e # for now
         finally:
             if exists(temp_filepath):
                 # clean up
                 lgr.debug("Removing a temporary download %s", temp_filepath)
-                os.unlink(temp_filepath)
+                unlink(temp_filepath)
 
         return filepath
 
     def download(self, url, path=None, **kwargs):
         """Fetch content as pointed by the URL optionally into a file
 
         Parameters
@@ -366,43 +533,40 @@
         return self.access(self._download, url, path=path, **kwargs)
 
     @property
     def cache(self):
         if self._cache is None:
             # TODO: move this all logic outside into a dedicated caching beast
             lgr.info("Initializing cache for fetches")
-            if PY2:
-                import anydbm as dbm
-            else:
-                import dbm
+            import dbm
             # Initiate cache.
             # Very rudimentary caching for now, might fail many ways
             cache_dir = cfg.obtain('datalad.locations.cache')
             if not exists(cache_dir):
                 os.makedirs(cache_dir)
             cache_path = opj(cache_dir, 'crawl_cache.dbm')
             self._cache = dbm.open(cache_path, 'c')
             import atexit
             atexit.register(self._cache.close)
         return self._cache
 
-    def _fetch(self, url, cache=None, size=None, allow_redirects=True):
+    def _fetch(self, url, cache=None, size=None, allow_redirects=True, decode=True):
         """Fetch content from a url into a file.
 
         Very similar to _download but lacks any "file" management and decodes
         content
 
         Parameters
         ----------
         url: str
           URL to download
         cache: bool, optional
-          If None, config is consulted either results should be cached.
-          Cache is operating based on url, so no verification of any kind
-          is carried out
+          If None, config is consulted to determine whether results should be
+          cached. Cache is operating based on url, so no verification of any
+          kind is carried out
 
         Returns
         -------
         bytes, dict
           content, headers
         """
         lgr.log(3, "_fetch(%r, cache=%r, size=%r, allow_redirects=%r)",
@@ -414,16 +578,17 @@
             cache_key = msgpack.dumps(url)
             lgr.debug("Loading content for url %s from cache", url)
             res = self.cache.get(cache_key)
             if res is not None:
                 try:
                     return msgpack.loads(res, encoding='utf-8')
                 except Exception as exc:
+                    ce = CapturedException(exc)
                     lgr.warning("Failed to unpack loaded from cache for %s: %s",
-                                url, exc_str(exc))
+                                url, ce)
 
         downloader_session = self.get_downloader_session(url, allow_redirects=allow_redirects)
 
         target_size = downloader_session.size
         if size is not None:
             if size == 0:
                 # no download of the content was requested -- just return headers and be done
@@ -435,26 +600,26 @@
             # Consider to improve to make it animated as well, or shorten here
             #pbar = ui.get_progressbar(label=url, fill_text=filepath, total=target_size)
             content = downloader_session.download(size=size)
             #pbar.finish()
             downloaded_size = len(content)
 
             # now that we know size based on encoded content, let's decode into string type
-            if PY3 and isinstance(content, binary_type):
-                content = content.decode()
+            if isinstance(content, bytes) and decode:
+                content = ensure_unicode(content)
             # downloaded_size = os.stat(temp_filepath).st_size
 
             self._verify_download(url, downloaded_size, target_size, None, content=content)
 
         except (AccessDeniedError, IncompleteDownloadError) as e:
             raise
         except Exception as e:
-            e_str = exc_str(e, limit=5)
-            lgr.error("Failed to fetch {url}: {e_str}".format(**locals()))
-            raise DownloadError(exc_str(e, limit=8))  # for now
+            ce = CapturedException(e)
+            lgr.error("Failed to fetch %s: %s", url, ce)
+            raise DownloadError(ce) from e  # for now
 
         if cache:
             # apparently requests' CaseInsensitiveDict is not serialazable
             # TODO:  may be we should reuse that type everywhere, to avoid
             # out own handling for case-handling
             self.cache[cache_key] = msgpack.dumps((content, dict(downloader_session.headers)))
 
@@ -469,18 +634,17 @@
           URL to access
 
         Returns
         -------
         bytes
           content
         """
-        lgr.info("Fetching %r", url)
+        lgr.debug("Fetching %r", url)
         # Do not return headers, just content
         out = self.access(self._fetch, url, **kwargs)
-        # import pdb; pdb.set_trace()
         return out[0]
 
     def get_status(self, url, old_status=None, **kwargs):
         """Return status of the url as a dict, None if N/A
 
         Parameters
         ----------
@@ -510,15 +674,15 @@
         # in those cases, and not just check the headers
         download_size = self._DOWNLOAD_SIZE_TO_VERIFY_AUTH \
             if self.authenticator \
             and hasattr(self.authenticator, 'failure_re') \
             and self.authenticator.failure_re \
             else 0
 
-        _, headers = self._fetch(url, cache=False, size=download_size)
+        _, headers = self._fetch(url, cache=False, size=download_size, decode=False)
 
         # extract from headers information to depict the status of the url
         status = self.get_status_from_headers(headers)
 
         if old_status is not None:
             raise NotImplementedError("Do not know yet how to deal with old_status. TODO")
 
@@ -543,30 +707,26 @@
         """
         return self.access(self._get_target_url, url)
 
     def _get_target_url(self, url):
         return self.get_downloader_session(url).url
 
 
-# Exceptions.  might migrate elsewhere
-# MIH: Completely non-obvious why this is here
-from ..support.exceptions import *
-
-
 #
 # Authenticators    XXX might go into authenticators.py
 #
 
 class Authenticator(object):
     """Abstract common class for different types of authentication
 
     Derived classes should get parameterized with options from the config files
     from "provider:" sections
     """
     requires_authentication = True
+    allows_anonymous = False
     # TODO: figure out interface
 
     DEFAULT_CREDENTIAL_TYPE = 'user_password'
 
     def authenticate(self, *args, **kwargs):
         """Derived classes will provide specific implementation
         """
```

### Comparing `datalad-0.9.3/datalad/downloaders/tests/utils.py` & `datalad-1.0.0/datalad/downloaders/tests/utils.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 """Downloader tests helper utils"""
```

### Comparing `datalad-0.9.3/datalad/downloaders/configs/nda.cfg` & `datalad-1.0.0/datalad/downloaders/configs/nda.cfg`

 * *Files identical despite different names*

### Comparing `datalad-0.9.3/datalad/downloaders/configs/nitrc.cfg` & `datalad-1.0.0/datalad/downloaders/configs/nitrc.cfg`

 * *Files 2% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 # TODO: we can't list all links which require authentication so we must
 # allow for some to be attempted to be downloaded WITHOUT authentication
 # and then only if fail -- should attempt to authenticate.  For now, to
 # move progress forward, just listed a few of those creatures since
 # they aren't usable yet since folks need to agree over and over again
 # (upon each login) to download files -- so we need to figure out how
 # to accomplish the mission here
-url_re = https?://fcon_1000\.projects\.nitrc\.org/indi/adhd200/index.html
+url_re = https?://fcon_1000\.projects\.nitrc\.org/indi/adhd200/index\.html
          https?://www\.nitrc\.org/frs/downloadlink\.php/(7058|3075|3479|9108)
 credential = nitrc
 authentication_type = html_form
 html_form_url = https://www.nitrc.org/account/login.php
 html_form_fields = form_loginname={user}
                    form_pw={password}
                    myloginButton=Log in with SSL
@@ -20,15 +20,15 @@
 html_form_failure_re = Invalid Password Or User Name
 #authentication_type = http_auth
 #http_auth_url = https://www.nitrc.org/account/login.php
 #http_auth_failure_re = Invalid Password Or User Name
 
 # requires JSESSIONID= cookie for the session... ???
 [provider:nitrc-ir]
-url_re = https?://www.nitrc.org/ir/.*
+url_re = https?://www\.nitrc\.org/ir/.*
 credential = nitrc
 authentication_type = http_auth
 http_auth_url = https://www.nitrc.org/ir/
 # if login pointer present -- we failed to login
 http_auth_failure_re = (Member login|Please use the Login Name and Password)
 
 [credential:nitrc]
```

### Comparing `datalad-0.9.3/datalad/downloaders/configs/crcns.cfg` & `datalad-1.0.0/datalad/downloaders/configs/crcns.cfg`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 [provider:crcns]
-url_re = https?://crcns.org/.*
-         ftp://crcns.org/.*
+url_re = https?://crcns\.org/.*
+         ftp://crcns\.org/.*
 # certificates = ??? ; (uses https)
 credential = crcns
 # for HTTP: could be html_form (see below), user_password,
 authentication_type = html_form
 html_form_url = https://crcns.org/login_form
 html_form_tagid = login_form
 html_form_fields = __ac_name={user}
@@ -20,15 +20,15 @@
 # similar to credential. Would results in closer to code arrangement
 # BENEFITS:  we might then specify multiple pairs, e.g. for :
 # THINK:  what if I have few different credentials for the same portal, which
 #         I use for different subsets of data (collaborations etc).
 #         With separateing authenticator out we could easier reuse pairs
 #         of credential_authenticator pairs!
 [provider:crcns-nersc]
-url_re = https://portal.nersc.gov/project/crcns/download/.*
+url_re = https://portal\.nersc\.gov/project/crcns/download/.*
 credential = crcns
 authentication_type = html_form
 html_form_url = https://portal.nersc.gov/project/crcns/download/index.php
 # Just the first/only form
 # html_form_tagid = login_form
 html_form_fields = username={user}
                    password={password}
```

### Comparing `datalad-0.9.3/datalad/downloaders/configs/kaggle.cfg` & `datalad-1.0.0/datalad/downloaders/configs/kaggle.cfg`

 * *Files 19% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 [provider:kaggle]
-url_re = https?://(.*\.|)kaggle.com/.*
+url_re = https?://(.*\.|)kaggle\.com/.*
 credential = kaggle
 authentication_type = html_form
 html_form_url = https://www.kaggle.com/account/login
 html_form_tagid = login-account
 html_form_fields = UserName={user}
                    Password={password}
                    get-started=Login
```

### Comparing `datalad-0.9.3/datalad/downloaders/configs/hcp.cfg` & `datalad-1.0.0/datalad/downloaders/configs/hcp.cfg`

 * *Files 10% similar despite different names*

```diff
@@ -1,24 +1,24 @@
 # lots of js and some redirects, not yet clear what's needed to work properly
 # the hcp-http below works!
 [provider:hcp-web]
-url_re = __https://db.humanconnectome.org/.*__
+url_re = __https://db\.humanconnectome\.org/.*__
 credential = hcp-db
 authentication_type = html_form
 html_form_url = https://db.humanconnectome.org/app/template/Login.vm
 html_form_tagid = login_form
 html_form_fields = login_method=LDAP
                    j_username={user}
                    j_password={password}
                    XNAT_CSRF=
 html_form_failure_re = HTTP Status 404
 html_form_success_re = Public Connectome Data
 
 [provider:hcp-http]
-url_re = https://db.humanconnectome.org/data/.*
+url_re = https://db\.humanconnectome\.org/data/.*
 credential = hcp-db
 authentication_type = http_auth
 http_auth_url = https://db.humanconnectome.org/data/JSESSION
 
 [provider:hcp-xnat]
 url_re = xnat://somewhereIdon'tknowyet
 credential = hcp-db
@@ -26,15 +26,15 @@
 
 [provider:hcp-s3]
 url_re = s3://hcp-openaccess.*
 credential = hcp-s3
 authentication_type = aws-s3
 
 [provider:balsa]
-url_re = https?://balsa.wustl.edu/.*
+url_re = https?://balsa\.wustl\.edu/.*
 # or could be a different custom one!  so we might allow for a choice
 # https://balsa.wustl.edu/register/register
 credential = hcp-db
 authentication_type = html_form
 # https://balsa.wustl.edu/login/auth
 html_form_url = https://balsa.wustl.edu/j_spring_security_check
 html_form_tagid = loginForm
```

### Comparing `datalad-0.9.3/datalad/downloaders/__init__.py` & `datalad-1.0.0/datalad/distribution/remove.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,20 +1,13 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""Sub-module to provide access (as to download/query etc) to the remote sites
-
+"""Obsolete module: moved to `local.remove`
 """
 
-__docformat__ = 'restructuredtext'
-
-from logging import getLogger
-lgr = getLogger('datalad.providers')
-
-# TODO: we might not need to instantiate it right here
-# lgr.debug("Initializing data providers credentials interface")
-# providers = Providers().from_config_files()
+# Import command class to ease 3rd-party transitions
+from datalad.local.remove import Remove
```

### Comparing `datalad-0.9.3/datalad/metadata/parsers/tests/test_rfc822.py` & `datalad-1.0.0/datalad/distribution/tests/test_create_test_dataset.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,91 +1,89 @@
-# emacs: -*- mode: python-mode; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""Test BIDS meta data parser """
+"""Test create testdataset helpers
 
+"""
+
+from glob import glob
 from os.path import join as opj
-from simplejson import dumps
-from datalad.distribution.dataset import Dataset
-from datalad.metadata.parsers.datalad_rfc822 import MetadataParser
-from nose.tools import assert_true, assert_false, assert_equal
-from datalad.tests.utils import with_tree, with_tempfile
+
+from datalad.core.local.repo import repo_from_path
+from datalad.distribution.create_test_dataset import _parse_spec
+from datalad.tests.utils_pytest import (
+    assert_raises,
+    assert_repo_status,
+    eq_,
+    ok_,
+    with_tempfile,
+)
+from datalad.utils import (
+    chpwd,
+    swallow_logs,
+    swallow_outputs,
+)
 
 
-@with_tree(tree={'.datalad': {'meta.rfc822': ''}})
-def test_has_metadata(path):
-    ds = Dataset(path)
-    p = MetadataParser(ds)
-    assert_true(p.has_metadata())
-    assert_equal(p.get_core_metadata_filenames(),
-                 [opj(path, '.datalad', 'meta.rfc822')])
+@with_tempfile(mkdir=True)
+def test_create(outdir=None):
+    from datalad.api import create
+    assert_raises(ValueError, create, outdir, description='Precious data', annex=False)
+
+
+def test_parse_spec():
+    eq_(_parse_spec('0/3/-1'), [(0, 0), (3, 3), (0, 1)])
+    eq_(_parse_spec('4-10'), [(4, 10)])
+    eq_(_parse_spec(''), [])
+
+
+def test_create_test_dataset():
+    # rudimentary smoke test
+    from datalad.api import create_test_dataset
+    with swallow_logs(), swallow_outputs():
+        dss = create_test_dataset(spec='2/1-2')
+    ok_(5 <= len(dss) <= 7)  # at least five - 1 top, two on top level, 1 in each
+    for ds in dss:
+        assert_repo_status(ds, annex=None)  # some of them are annex but we just don't check
+        ok_(len(glob(opj(ds, 'file*'))))
+
+
+def test_create_1test_dataset():
+    # and just a single dataset
+    from datalad.api import create_test_dataset
+    with swallow_outputs():
+        dss = create_test_dataset()
+    eq_(len(dss), 1)
+    assert_repo_status(dss[0], annex=False)
 
 
 @with_tempfile(mkdir=True)
-def test_has_no_metadata(path):
-    ds = Dataset(path)
-    p = MetadataParser(ds)
-    assert_false(p.has_metadata())
-    assert_equal(p.get_core_metadata_filenames(), [])
-
-
-@with_tree(tree={'.datalad': {'meta.rfc822': """\
-Name: studyforrest_phase2
-Version: 1.0.0-rc3
-Description: Basic summary
- A text with arbitrary length and content that can span multiple
- .
- paragraphs (this is a new one)
-License: CC0
- The person who associated a work with this deed has dedicated the work to the
- public domain by waiving all of his or her rights to the work worldwide under
- copyright law, including all related and neighboring rights, to the extent
- allowed by law.
- .
- You can copy, modify, distribute and perform the work, even for commercial
- purposes, all without asking permission.
-Maintainer: Mike One <mike@example.com>,
-            Anna Two <anna@example.com>,
-Homepage: http://studyforrest.org
-Funding: BMBFGQ1411, NSF 1429999
-Issue-Tracker: https://github.com/psychoinformatics-de/studyforrest-data-phase2/issues
-Cite-As: Cool (2016)
-DOI: 10.5281/zenodo.48421
-
-"""}})
-def test_get_metadata(path):
-
-    ds = Dataset(path)
-    meta = MetadataParser(ds).get_metadata('ID')
-    assert_equal(
-        dumps(meta, sort_keys=True, indent=2),
-        """\
-{
-  "@context": {
-    "@vocab": "http://schema.org/",
-    "doap": "http://usefulinc.com/ns/doap#"
-  },
-  "@id": "ID",
-  "bug-database": "https://github.com/psychoinformatics-de/studyforrest-data-phase2/issues",
-  "citation": "Cool (2016)",
-  "dcterms:conformsTo": "http://docs.datalad.org/metadata.html#v0-1",
-  "description": "A text with arbitrary length and content that can span multiple\\nparagraphs (this is a new one)",
-  "doap:Version": "1.0.0-rc3",
-  "doap:homepage": "http://studyforrest.org",
-  "doap:maintainer": [
-    "Mike One <mike@example.com>",
-    "Anna Two <anna@example.com>"
-  ],
-  "doap:shortdesc": "Basic summary",
-  "foaf:fundedBy": "BMBFGQ1411, NSF 1429999",
-  "license": [
-    "CC0",
-    "The person who associated a work with this deed has dedicated the work to the public domain by waiving all of his or her rights to the work worldwide under copyright law, including all related and neighboring rights, to the extent allowed by law.\\nYou can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission."
-  ],
-  "name": "studyforrest_phase2",
-  "sameAs": "http://dx.doi.org/10.5281/zenodo.48421"
-}""")
+def test_new_relpath(topdir=None):
+    from datalad.api import create_test_dataset
+    with swallow_logs(), chpwd(topdir), swallow_outputs():
+        dss = create_test_dataset('testds', spec='1')
+    eq_(dss[0], opj(topdir, 'testds'))
+    eq_(len(dss), 2)  # 1 top + 1 sub-dataset as demanded
+    for ds in dss:
+        assert_repo_status(ds, annex=False)
+
+
+@with_tempfile()
+def test_hierarchy(topdir=None):
+    # GH 1178
+    from datalad.api import create_test_dataset
+    with swallow_logs(), swallow_outputs():
+        dss = create_test_dataset(topdir, spec='1/1')
+
+    eq_(len(dss), 3)
+    eq_(dss[0], topdir)
+    for ids, ds in enumerate(dss):
+        assert_repo_status(ds, annex=False)
+        # each one should have 2 commits (but the last one)-- one for file and
+        # another one for sub-dataset
+        repo = repo_from_path(ds)
+        if not hasattr(repo, 'is_managed_branch') or not repo.is_managed_branch():
+            eq_(len(list(repo.get_branch_commits_())), 1 + int(ids < 2))
```

### Comparing `datalad-0.9.3/datalad/metadata/parsers/tests/test_aggregate.py` & `datalad-1.0.0/datalad/interface/tests/test_results.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,36 +1,52 @@
-# emacs: -*- mode: python-mode; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""Test aggregate meta data parser """
+"""Result utility tests
+
+"""
 
-import os
-from os.path import join as opj
 from datalad.distribution.dataset import Dataset
-from datalad.api import create
-from datalad.metadata.parsers.aggregate import MetadataParser
-from nose.tools import assert_true, assert_false, assert_equal
-from datalad.tests.utils import with_tempfile
-
-
-@with_tempfile(mkdir=True)
-def test_basic(path):
-    ds = Dataset(path).create()
-    p = MetadataParser(ds)
-    assert_false(p.has_metadata())
-    mpath = opj(ds.path, '.datalad', 'meta', 'something', 'deep')
-    os.makedirs(mpath)
-    assert_false(p.has_metadata())
-    with open(opj(mpath, 'meta.json'), 'w') as fp:
-        fp.write('{"name": "testmonkey", "dcterms:isPartOf": "%s", "@id": "unique"}' % ds.id)
-    assert_true(p.has_metadata())
-    #from datalad.metadata import get_metadata
-    #from json import dumps
-    #print(dumps(get_metadata(ds), indent=1))
-    #assert_equal(
-    #    p.get_metadata(),
-    #    [{'dcterms:hasPart': {'location': 'something/deep'}}])
+from datalad.interface.results import (
+    annexjson2result,
+    get_status_dict,
+)
+from datalad.runner import CommandError
+from datalad.tests.utils_pytest import (
+    eq_,
+    with_tempfile,
+)
+
+
+@ with_tempfile
+def test_annexjson2result(dspath=None):
+    # no explicit success means 'error'
+    eq_(annexjson2result(dict(), None),
+        dict(status='error'))
+    # unrecognized -> error
+    eq_(annexjson2result(dict(success='random'), None),
+        dict(status='error'))
+    # success is possible ;-)
+    eq_(annexjson2result(dict(success=True), None),
+        dict(status='ok'))
+
+    # path handling
+    # needs a dataset
+    ds = Dataset(dspath)
+    eq_(annexjson2result(dict(file='file1'), ds),
+        dict(status='error',
+             path=str(ds.pathobj / 'file1')))
+    # on all platforms, paths are reported in platform conventions
+    # although git-annex reports in posix
+    eq_(annexjson2result(dict(file='dir1/file1'), ds),
+        dict(status='error',
+             path=str(ds.pathobj / 'dir1' / 'file1')))
+
+
+def tests_status_dict_exit_code():
+    d = get_status_dict(exception=CommandError(code=105))
+    eq_(d['exit_code'], 105)
```

### Comparing `datalad-0.9.3/datalad/metadata/parsers/aggregate.py` & `datalad-1.0.0/datalad/runner/exception.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,92 +1,118 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""Parser for datalad's own aggregated metadata"""
-
-from os.path import join as opj
-
-from datalad.utils import find_files
-from datalad.support.json_py import load as jsonload
-from datalad.metadata import _simplify_meta_data_structure
-from datalad.metadata import is_implicit_metadata
-from datalad.metadata.parsers.base import BaseMetadataParser
-from datalad.metadata import _get_base_dataset_metadata
-
-
-# XXX could be moved to aggregate parser...
-def _adjust_subdataset_location(meta, subds_relpath):
-    # find implicit meta data for all contained subdatasets
-    for m in meta:
-        # skip non-implicit
-        if not is_implicit_metadata(m):
+"""Exception raise on a failed runner command execution
+"""
+from __future__ import annotations
+
+import logging
+import os
+from collections import Counter
+from typing import (
+    Any,
+    Optional,
+)
+
+lgr = logging.getLogger('datalad.runner.exception')
+
+
+class CommandError(RuntimeError):
+    """Thrown if a command call fails.
+
+    Note: Subclasses should override `to_str` rather than `__str__` because
+    `to_str` is called directly in datalad.cli.main.
+    """
+
+    def __init__(
+        self,
+        cmd: str | list[str] = "",
+        msg: str = "",
+        code: Optional[int] = None,
+        stdout: str | bytes = "",
+        stderr: str | bytes = "",
+        cwd: str | os.PathLike | None = None,
+        **kwargs: Any,
+    ) -> None:
+        RuntimeError.__init__(self, msg)
+        self.cmd = cmd
+        self.msg = msg
+        self.code = code
+        self.stdout = stdout
+        self.stderr = stderr
+        self.cwd = cwd
+        self.kwargs = kwargs
+
+    def to_str(self, include_output: bool = True) -> str:
+        from datalad.utils import (
+            ensure_unicode,
+            join_cmdline,
+        )
+        to_str = "{}: ".format(self.__class__.__name__)
+        cmd = self.cmd
+        if cmd:
+            to_str += "'{}'".format(
+                # go for a compact, normal looking, properly quoted
+                # command rendering if the command is in list form
+                join_cmdline(cmd) if isinstance(cmd, list) else cmd
+            )
+        if self.code:
+            to_str += " failed with exitcode {}".format(self.code)
+        if self.cwd:
+            # only if not under standard PWD
+            to_str += " under {}".format(self.cwd)
+        if self.msg:
+            # typically a command error has no specific idea
+            to_str += " [{}]".format(ensure_unicode(self.msg))
+
+        if self.kwargs:
+            to_str += " [info keys: {}]".format(
+                ', '.join(self.kwargs.keys()))
+
+            if 'stdout_json' in self.kwargs:
+                to_str += _format_json_error_messages(
+                    self.kwargs['stdout_json'])
+
+        if not include_output:
+            return to_str
+
+        if self.stdout:
+            to_str += " [out: '{}']".format(ensure_unicode(self.stdout).strip())
+        if self.stderr:
+            to_str += " [err: '{}']".format(ensure_unicode(self.stderr).strip())
+
+        return to_str
+
+    def __str__(self) -> str:
+        return self.to_str()
+
+
+def _format_json_error_messages(recs: list[dict]) -> str:
+    # there could be many, condense
+    msgs: Counter[str] = Counter()
+    for r in recs:
+        if r.get('success'):
             continue
-        # prefix all subdataset location information with the relpath of this
-        # subdataset
-        if 'dcterms:hasPart' in m:
-            parts = m['dcterms:hasPart']
-            if not isinstance(parts, list):
-                parts = [parts]
-            for p in parts:
-                if 'location' not in p:
-                    continue
-                loc = p.get('location', subds_relpath)
-                if loc != subds_relpath:
-                    p['location'] = opj(subds_relpath, loc)
-
-
-class MetadataParser(BaseMetadataParser):
-    def get_core_metadata_filenames(self):
-        return list(find_files(
-            'meta\.json',
-            topdir=opj(self.ds.path, '.datalad', 'meta'),
-            exclude=None,
-            exclude_vcs=False,
-            exclude_datalad=False,
-            dirs=False))
-
-    def get_metadata(self, dsid=None, full=False):
-        base_meta = _get_base_dataset_metadata(dsid if dsid else self.ds.id)
-        meta = [base_meta]
-        basepath = opj(self.ds.path, '.datalad', 'meta')
-        parts = []
-        for subds_meta_fname in self.get_core_metadata_filenames():
-            # get the part between the 'meta' dir and the filename
-            # which is the subdataset mountpoint
-            subds_path = subds_meta_fname[len(basepath) + 1:-10]
-            if not subds_path:
-                # this is a potentially existing cache of the native meta data
-                # of the superdataset, not for us...
-                continue
-            submeta_info = {
-                'location': subds_path}
-            # load aggregated meta data
-            subds_meta = jsonload(subds_meta_fname)
-            # we cannot simply append, or we get weired nested graphs
-            # proper way would be to expand the JSON-LD, extend the list and
-            # compact/flatten at the end. However assuming a single context
-            # we can cheat.
-            subds_meta = _simplify_meta_data_structure(subds_meta)
-            _adjust_subdataset_location(subds_meta, subds_path)
-            # sift through all meta data sets look for a meta data set that
-            # knows about being part of this dataset, so we record its @id as
-            # part
-            for md in subds_meta:
-                cand_id = md.get('dcterms:isPartOf', None)
-                if cand_id == dsid and '@id' in md:
-                    submeta_info['@id'] = md['@id']
-                    break
-
-            if subds_meta:
-                meta.extend(subds_meta)
-            parts.append(submeta_info)
-        if len(parts):
-            if len(parts) == 1:
-                parts = parts[0]
-            base_meta['dcterms:hasPart'] = parts
-
-        return meta
+        msg = '{}{}'.format(
+            ' {}\n'.format(r['note']) if r.get('note') else '',
+            '\n'.join(r.get('error-messages', [])),
+        )
+        if 'file' in r or 'key' in r:
+            msgs[msg] += 1
+
+    if not msgs:
+        return ''
+
+    return '\n>{}'.format(
+        '\n> '.join(
+            '{}{}'.format(
+                m,
+                ' [{} times]'.format(n) if n > 1 else '',
+            )
+            for m, n in msgs.items()
+        )
+    )
```

### Comparing `datalad-0.9.3/datalad/metadata/metadata.py` & `datalad-1.0.0/datalad/local/wtf.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,502 +1,623 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""Set and query metadata of datasets and their components"""
+"""provide information about this DataLad installation"""
 
 __docformat__ = 'restructuredtext'
 
-
 import logging
-import re
-import json
-from os import makedirs
-from os.path import dirname
-from os.path import relpath
-from os.path import exists
-from os.path import join as opj
-
-from datalad.interface.annotate_paths import AnnotatePaths
-from datalad.interface.annotate_paths import annotated2content_by_ds
-from datalad.interface.base import Interface
-from datalad.interface.save import Save
-from datalad.interface.results import get_status_dict
-from datalad.interface.utils import eval_results
-from datalad.interface.base import build_doc
-from datalad.support.constraints import EnsureNone
-from datalad.support.constraints import EnsureStr
-from datalad.support.gitrepo import GitRepo
-from datalad.support.annexrepo import AnnexRepo
-from datalad.support.param import Parameter
-from datalad.interface.common_opts import recursion_flag
-from datalad.interface.common_opts import recursion_limit
-from datalad.distribution.dataset import Dataset
-from datalad.distribution.dataset import EnsureDataset
-from datalad.distribution.dataset import datasetmethod
-from datalad.utils import unique
-
-lgr = logging.getLogger('datalad.metadata.metadata')
-
-valid_key = re.compile(r'^[0-9a-z._-]+$')
-
-
-def _parse_argspec(args):
-    """Little helper to get cmdline and python args into a uniform
-    shape
-
-    Returns
-    -------
-    tags, mapping
-      A list of tags, and a dict with a mapping of given metadatakeys
-      and their associates metadata values
-    """
-    tags = []
-    mapping = {}
-    if not args:
-        return tags, mapping
-    if not isinstance(args, (dict, list, tuple)):
-        raise ValueError(
-            'invalid metadata specification, must be a dict or sequence')
-
-    asdict = isinstance(args, dict)
-    for k in args.items() if isinstance(args, dict) else args:
-        v = None
-        if asdict:
-            # simple, came in from a dict
-            k, v = k
-            if v:
-                mapping[_get_key(k)] = v
-            else:
-                tags.append(k)
-        elif isinstance(k, list):
-            # list of lists, came from cmdline
-            if len(k) == 1:
-                tags.append(k[0])
-            elif len(k) > 1:
-                mapping[_get_key(k[0])] = k[1:]
-            else:
-                raise ValueError(
-                    'invalid metadata specification, something weird')
-        else:
-            tags.append(k)
-    return tags, mapping
-
-
-def _get_key(k):
-    # annex has caseinsensitive, good enough
-    k = k.lower()
-    # validate keys against annex constraints
-    if not valid_key.match(k):
-        raise ValueError(
-            'invalid metadata key "{}", must match pattern {}'.format(
-                k, valid_key.pattern))
-    return k
-
-
-@build_doc
-class Metadata(Interface):
-    """Metadata manipulation for files and whole datasets
-
-    Two types of metadata are supported:
-
-    1. metadata describing a dataset as a whole (dataset-global), and
-
-    2. metadata for individual files in a dataset.
-
-    Both types can be accessed and modified with this command.
-    Note, however, that this only refers to DataLad's native metadata,
-    and not to any other metadata that is possibly stored in files of a
-    dataset.
-
-    DataLad's native metadata capability is primarily targeting data
-    description via arbitrary tags and other (brief) key-value attributes
-    (with possibly multiple values for a single key).
-
-    Metadata key names are limited to alphanumerics (and [_-.]). Moreover,
-    all key names are converted to lower case.
-
-
-    *Dataset (global) metadata*
-
-    Metadata describing a dataset as a whole is stored in JSON format
-    in the dataset at .datalad/metadata/dataset.json. The amount of
-    metadata that can be stored is not limited by DataLad. However,
-    it should be kept brief as this information is stored in the Git
-    history of the dataset, and access or modification requires to
-    read the entire file.
-
-    Arbitrary metadata keys can be used. However, DataLad reserves the
-    keys 'tag' and 'definition' for its own use. The can still be
-    manipulated without any restrictions like any other metadata items,
-    but doing so can impact DataLad's metadata-related functionality,
-    handle with care.
-
-    The 'tag' key is used to store a list of (unique) tags.
-
-    The 'definition' key is used to store key-value mappings that define
-    metadata keys used elsewhere in the metadata. Using the feature is
-    optional (see --define-key). It can be useful in the context of
-    data discovery needs, where metadata keys can be precisely defined
-    by linking them to specific ontology terms.
-
-
-    *File metadata*
-
-    Metadata storage for individual files is provided by git-annex, and
-    generally the same rules as for dataset-global metadata apply.
-    However, there is just one reserved key name: 'tag'.
-
-    Again, the amount of metadata is not limited, but metadata is stored
-    in git-annex' internal data structures in the Git repository of a
-    dataset. Large amounts of metadata can slow its performance.
+import os
+import os.path as op
+import sys
+import tempfile
+import textwrap
+
+from collections import defaultdict
+from functools import partial
+
+from datalad.dochelpers import get_docstring_split
+from datalad.interface.base import (
+    Interface,
+    build_doc,
+)
+from datalad.utils import (
+    ensure_unicode,
+    getpwd,
+    unlink,
+    Path,
+)
+from datalad.support.external_versions import external_versions
+from datalad.support.exceptions import (
+    CapturedException,
+    CommandError,
+    InvalidGitRepositoryError,
+)
+from datalad import __version__
+
+lgr = logging.getLogger('datalad.local.wtf')
+
+
+# wording to use for items which were considered sensitive and thus not shown
+_HIDDEN = "<SENSITIVE, report disabled by configuration>"
+
+
+# formatting helper
+def _t2s(t):
+    res = []
+    for e in t:
+        if isinstance(e, tuple):
+            es = _t2s(e)
+            if es != '':
+                res += ['(%s)' % es]
+        elif e != '':
+            res += [e]
+    return '/'.join(res)
 
 
-    || CMDLINE >>
-    *Output rendering*
+def get_max_path_length(top_path=None, maxl=1000):
+    """Deduce the maximal length of the filename in a given path
+    """
+    if not top_path:
+        top_path = getpwd()
+    import random
+    from datalad import lgr
+    from datalad.support import path
+    prefix = path.join(top_path, "dl%d" % random.randint(1 ,100000))
+    # some smart folks could implement binary search for this
+    max_path_length = None
+    for i in range(maxl-len(prefix)):
+        filename = prefix + '_' * i
+        path_length = len(filename)
+        try:
+            with open(filename, 'w') as f:
+                max_path_length = path_length
+        except Exception as exc:
+            ce = CapturedException(exc)
+            lgr.debug(
+                "Failed to create sample file for length %d. Last succeeded was %s. Exception: %s",
+                path_length, max_path_length, ce)
+            break
+        unlink(filename)
+    return max_path_length
+
+
+def _describe_datalad():
+
+    return {
+        'version': ensure_unicode(__version__),
+    }
+
+
+def _describe_annex():
+    from datalad.cmd import (
+        GitWitlessRunner,
+        StdOutErrCapture,
+    )
+
+    runner = GitWitlessRunner()
+    try:
+        out = runner.run(
+            ['git', 'annex', 'version'], protocol=StdOutErrCapture)
+    except CommandError as e:
+        ce = CapturedException(e)
+        return dict(
+            version='not available',
+            message=ce.format_short(),
+        )
+    info = {}
+    for line in out['stdout'].split(os.linesep):
+        key = line.split(':')[0]
+        if not key:
+            continue
+        value = line[len(key) + 2:].strip()
+        key = key.replace('git-annex ', '')
+        if key.endswith('s'):
+            value = value.split()
+        info[key] = value
+    return info
+
+
+def _describe_system():
+    import platform as pl
+    from datalad import get_encoding_info
+    from datalad.utils import get_linux_distribution
+    try:
+        dist = get_linux_distribution()
+    except Exception as exc:
+        ce = CapturedException(exc)
+        lgr.warning("Failed to get distribution information: %s", ce)
+        dist = tuple()
+
+    return {
+        'type': os.name,
+        'name': pl.system(),
+        'release': pl.release(),
+        'version': pl.version(),
+        'distribution': ' '.join([_t2s(dist),
+                                  _t2s(pl.mac_ver()),
+                                  _t2s(pl.win32_ver())]).rstrip(),
+        'max_path_length': get_max_path_length(getpwd()),
+        'encoding': get_encoding_info(),
+        'filesystem': {l: _get_fs_type(l, p) for l, p in
+                       [('CWD', Path.cwd()),
+                        ('TMP', Path(tempfile.gettempdir())),
+                        ('HOME', Path.home())]}
+    }
+
+
+def _get_fs_type(loc, path, _was_warned=[]):
+    """Return file system info for given Paths. Provide pathlib path as input"""
+    res = {'path': path}
+    try:
+        from psutil import disk_partitions
+        parts = {Path(p.mountpoint): p  for p in disk_partitions()}
+        match = None
+        for mp in parts:
+            # if the mountpoint is the test path or its parent
+            # take it, whenever there is no match, or a longer match
+            if (mp == path or mp in path.parents) and (
+                    match is None or len(mp.parents) > len(match.parents)):
+                match = mp
+        match = parts[match]
+        for sattr, tattr in (('fstype', 'type'),
+                             ('maxpath', 'max_pathlength'),
+                             ('opts', 'mount_opts')):
+            if hasattr(match, sattr):
+                res[tattr] = getattr(match, sattr)
+    except Exception as exc:
+        ce = CapturedException(exc)
+        # if an exception occurs, leave out the fs type. The result renderer can
+        # display a hint based on its lack in the report
+        if not _was_warned:
+            (lgr.debug if isinstance(exc, ImportError) else lgr.warning) (
+                "Could not determine filesystem types due to %s", ce)
+            # Rely on side-effect of [] as default arg
+            _was_warned.append("warned")
+    return res
+
+
+def _describe_environment():
+    from datalad import get_envvars_info
+    return get_envvars_info()
+
+
+def _describe_python():
+    import platform
+    return {
+        'version': platform.python_version(),
+        'implementation': platform.python_implementation(),
+    }
+
+
+def _describe_configuration(cfg, sensitive):
+    if not cfg:
+        return _HIDDEN
+
+    # make it into a dict to be able to reassign
+    cfg = dict(cfg.items())
+
+    if sensitive != 'all':
+        # filter out some of the entries which known to be highly sensitive
+        for k in cfg.keys():
+            if 'user' in k or 'token' in k or 'passwd' in k:
+                cfg[k] = _HIDDEN
+
+    return cfg
+
+
+def _describe_extensions():
+    infos = {}
+    from datalad.support.entrypoints import iter_entrypoints
+    from importlib import import_module
+
+    for ename, emod, eload in iter_entrypoints('datalad.extensions'):
+        info = {}
+        infos[ename] = info
+        try:
+            ext = eload()
+            info['description'] = ext[0]
+            info['module'] = emod
+            mod = import_module(emod, package='datalad')
+            info['version'] = getattr(mod, '__version__', None)
+        except Exception as e:
+            ce = CapturedException(e)
+            info['load_error'] = ce.format_short()
+            continue
+        info['entrypoints'] = entry_points = {}
+        for ep in ext[1]:
+            ep_info = {
+                'module': ep[0],
+                'class': ep[1],
+                'names': ep[2:],
+            }
+            entry_points['{}.{}'.format(*ep[:2])] = ep_info
+            try:
+                import_module(ep[0], package='datalad')
+            except Exception as e:
+                ce = CapturedException(e)
+                ep_info['load_error'] = ce.format_short()
+                continue
+    return infos
 
-    By default, a short summary of the metadata for each dataset
-    (component) is rendered::
 
-      <path> (<type>): -|<keys> [<tags>]
+def _describe_metadata_elements(group):
+    infos = {}
+    from datalad.support.entrypoints import iter_entrypoints
+    from importlib import import_module
+    if sys.version_info < (3, 10):
+        # 3.10 is when it was no longer provisional
+        from importlib_metadata import distribution
+    else:
+        from importlib.metadata import distribution
+
+
+    for ename, emod, eload in iter_entrypoints(group):
+        info = {}
+        infos[f'{ename}'] = info
+        try:
+            info['module'] = emod
+            dist = distribution(emod.split('.', maxsplit=1)[0])
+            info['distribution'] = f'{dist.name} {dist.version}'
+            mod = import_module(emod, package='datalad')
+            version = getattr(mod, '__version__', None)
+            if version:
+                # no not clutter the report with no version
+                info['version'] = version
+            elem = eload()
+
+            doc, *_ = get_docstring_split(elem)
+            if doc:
+                info['doc'] = textwrap.fill(doc, 1000).strip()  # fill into a long line
+
+            generation = getattr(elem, '__generation__', None)
+            if generation:
+                info['generation'] = generation
+
+            try:
+                info['version'] = elem.get_version(elem)
+            except (AttributeError, NotImplementedError):
+                pass
+
+            # TODO?: levels - file, dataset.
+            # Too specific to each generation and might not be deducable
+        except Exception as e:
+            ce = CapturedException(e)
+            info['load_error'] = ce.format_short()
+            continue
+    return infos
+
+
+def _describe_dependencies():
+    return {
+        k: str(external_versions[k]) for k in external_versions.keys(query=True)
+    }
+
+
+def _describe_dataset(ds, sensitive):
+    from datalad.interface.results import success_status_map
+
+    try:
+        infos = {
+            'path': ds.path,
+            'repo': ds.repo.__class__.__name__ if ds.repo else None,
+            'id': ds.id,
+        }
+        # describe available branches and their states
+        branches = [
+            '%s@%s' % (b, next(ds.repo.get_branch_commits_(branch=b))[:7])
+            for b in ds.repo.get_branches()]
+        infos['branches'] = branches
+        return infos
+    except InvalidGitRepositoryError as e:
+        ce = CapturedException(e)
+        return {"invalid": ce.message}
+
+
+def _describe_location(res):
+    return {
+        'path': res['path'],
+        'type': res['type'],
+    }
+
+
+def _describe_credentials():
+    import keyring
+    from keyring.util import platform_
+
+    def describe_keyring_backend(be):
+        be_repr = repr(be)
+        return be.name if 'object at 0' in be_repr else be_repr.strip('<>')
+
+    # might later add information on non-keyring credentials gh-4981
+    props = {}
+
+    active_keyring = keyring.get_keyring()
+    krp = {
+        'config_file': Path(platform_.config_root(), 'keyringrc.cfg'),
+        'data_root': platform_.data_root(),
+        'active_backends': [
+            describe_keyring_backend(be)
+            for be in getattr(active_keyring, 'backends', [active_keyring])
+        ],
+    }
+    props.update(
+        keyring=krp,
+    )
+    return props
+
+
+# Actual callables for WTF. If None -- should be bound later since depend on
+# the context
+SECTION_CALLABLES = {
+    'datalad': _describe_datalad,
+    'python': _describe_python,
+    'git-annex': _describe_annex,
+    'system': _describe_system,
+    'environment': _describe_environment,
+    'configuration': None,
+    'location': None,
+    'extensions': _describe_extensions,
+    'metadata.extractors': lambda: _describe_metadata_elements('datalad.metadata.extractors'),
+    'metadata.indexers': lambda: _describe_metadata_elements('datalad.metadata.indexers'),
+    'metadata.filters': lambda: _describe_metadata_elements('datalad.metadata.filters'),
+    'dependencies': _describe_dependencies,
+    'dataset': None,
+    'credentials': _describe_credentials,
+}
+
+# look for top level sections and replace with all SECTION_CALLABLES_GROUPPED
+# Helper to look up SECTION_CALLABLES_GROUPPED when needed
+SECTION_CALLABLES_GROUPPED = defaultdict(list)
+for s in SECTION_CALLABLES:
+    section = s.split('.', 1)[0]
+    SECTION_CALLABLES_GROUPPED[section].append(s)
+    if section != s:
+        # add also subsection by itself
+        SECTION_CALLABLES_GROUPPED[s].append(s)
+# un-defaultdict it so we gain KeyError
+SECTION_CALLABLES_GROUPPED = dict(SECTION_CALLABLES_GROUPPED.items())
 
-    where <path> is the path of the respective component, <type> a label
-    for the type of dataset components metadata is presented for. Non-existant
-    metadata is indicated by a dash, otherwise a comma-separated list of
-    metadata keys (except for 'tag'), is followed by a list of tags, if there
-    are any.
 
+@build_doc
+class WTF(Interface):
+    """Generate a report about the DataLad installation and configuration
 
-    << CMDLINE ||
+    IMPORTANT: Sharing this report with untrusted parties (e.g. on the web)
+    should be done with care, as it may include identifying information, and/or
+    credentials or access tokens.
     """
-    # make the custom renderer the default, path reporting isn't the top
-    # priority here
     result_renderer = 'tailored'
 
+    from datalad.support.param import Parameter
+    from datalad.distribution.dataset import datasetmethod
+    from datalad.interface.base import eval_results
+    from datalad.distribution.dataset import EnsureDataset
+    from datalad.support.constraints import EnsureNone, EnsureChoice
+
     _params_ = dict(
         dataset=Parameter(
             args=("-d", "--dataset"),
-            doc="""""",
+            doc=""""specify the dataset to report on.
+            no dataset is given, an attempt is made to identify the dataset
+            based on the current working directory.""",
             constraints=EnsureDataset() | EnsureNone()),
-        path=Parameter(
-            args=("path",),
-            metavar="PATH",
-            doc="path(s) to set/get metadata",
-            nargs="*",
-            constraints=EnsureStr() | EnsureNone()),
-        add=Parameter(
-            args=('-a', '--add',),
-            nargs='+',
-            action='append',
-            metavar=('KEY', 'VAL'),
-            doc="""metadata items to add. If only a key is given, a
-            corresponding tag is added. If a key-value mapping (multiple
-            values at once are supported) is given, the values are
-            added to the metadata item of that key.""",
-            constraints=EnsureStr() | EnsureNone()),
-        init=Parameter(
-            args=('-i', '--init',),
-            nargs='+',
-            action='append',
-            metavar=('KEY', 'VAL'),
-            doc="""like --add, but tags are only added if no tag was present
-            before. Likewise, values are only added to a metadata key, if that
-            key did not exist before.""",
-            constraints=EnsureStr() | EnsureNone()),
-        remove=Parameter(
-            args=('--remove',),
-            nargs='+',
-            action='append',
-            metavar=('KEY', 'VAL'),
-            doc="""metadata values to remove. If only a key is given, a
-            corresponding tag is removed. If a key-value mapping (multiple
-            values at once are supported) is given, only those values are
-            removed from the metadata item of that key. If no values are left
-            after the removal, the entire item of that key is removed.""",
-            constraints=EnsureStr() | EnsureNone()),
-        reset=Parameter(
-            args=('--reset',),
-            nargs='+',
+        sensitive=Parameter(
+            args=("-s", "--sensitive",),
+            constraints=EnsureChoice(None, 'some', 'all'),
+            doc="""if set to 'some' or 'all', it will display sections such as 
+            config and metadata which could potentially contain sensitive 
+            information (credentials, names, etc.).  If 'some', the fields
+            which are known to be sensitive will still be masked out"""),
+        sections=Parameter(
+            args=("-S", "--section"),
             action='append',
-            metavar=('KEY', 'VAL'),
-            doc="""metadata items to remove. If only a key is given, a
-            corresponding metadata key with all its values is removed.
-            If a key-value mapping (multiple values at once are supported)
-            is given, any existing values for this key are replaced by the
-            given ones.""",
-            constraints=EnsureStr() | EnsureNone()),
-        define_key=Parameter(
-            args=('--define-key',),
-            nargs=2,
-            action='append',
-            metavar=('KEY', 'DEFINITION'),
-            doc="""convenience option to add an item in the dataset's
-            global metadata ('definition' key). This can be used to
-            define (custom) keys used in the datasets's metadata, for
-            example by providing a URL to an ontology term for a given
-            key label. This option does not need --dataset-global to
-            be set to be in effect.""",
-            constraints=EnsureStr() | EnsureNone()),
-        dataset_global=Parameter(
-            args=('-g', '--dataset-global'),
-            action='store_true',
-            doc="""Whether to perform metadata query or modification
-            on the global dataset metadata, or on individual dataset
-            components. For example, without this switch setting
-            metadata using the root path of a dataset, will set the
-            given metadata for all files in a dataset, whereas with
-            this flag only the metadata record of the dataset itself
-            will be altered."""),
-        recursive=recursion_flag,
-        recursion_limit=recursion_limit)
+            dest='sections',
+            metavar="SECTION",
+            constraints=EnsureChoice(None, *sorted(SECTION_CALLABLES_GROUPPED) + ['*']),
+            doc="""section to include.  If not set - depends on flavor.
+            '*' could be used to force all sections. If there are subsections 
+            like section.subsection available, then specifying just 'section'
+            would select all subsections for that section.
+            [CMD: This option can be given multiple times. CMD]"""),
+        flavor=Parameter(
+            args=("--flavor",),
+            constraints=EnsureChoice('full', 'short'),
+            doc="""Flavor of WTF. 'full' would produce markdown with exhaustive list of sections.
+            'short' will provide a condensed summary only of datalad and dependencies by default.
+            Use [CMD: --section CMD][PY: `section` PY] to list other sections"""),
+        decor=Parameter(
+            args=("-D", "--decor"),
+            constraints=EnsureChoice('html_details', None),
+            doc="""decoration around the rendering to facilitate embedding into
+            issues etc, e.g. use 'html_details' for posting collapsible entry
+            to GitHub issues."""),
+        clipboard=Parameter(
+            args=("-c", "--clipboard",),
+            action="store_true",
+            doc="""if set, do not print but copy to clipboard (requires pyperclip
+            module)"""),
+    )
 
     @staticmethod
-    @datasetmethod(name='metadata')
+    @datasetmethod(name='wtf')
     @eval_results
-    def __call__(
-            path=None,
-            dataset=None,
-            add=None,
-            init=None,
-            remove=None,
-            reset=None,
-            define_key=None,
-            dataset_global=False,
-            recursive=False,
-            recursion_limit=None):
-        # bring metadataset setter args in shape first
-        untag, remove = _parse_argspec(remove)
-        purge, reset = _parse_argspec(reset)
-        tag_add, add = _parse_argspec(add)
-        tag_init, init = _parse_argspec(init)
-        define_key = dict(define_key) if define_key else None
-        # merge all potential sources of tag specifications
-        all_untag = remove.get('tag', []) + untag
-        if all_untag:
-            remove['tag'] = all_untag
-        all_addtag = add.get('tag', []) + tag_add
-        if all_addtag:
-            add['tag'] = all_addtag
-        all_inittag = init.get('tag', []) + tag_init
-        if all_inittag:
-            init['tag'] = all_inittag
-
-        lgr.debug("Will 'init' metadata items: %s", init)
-        lgr.debug("Will 'add' metadata items: %s", add)
-        lgr.debug("Will 'remove' metadata items: %s", remove)
-        lgr.debug("Will 'reset' metadata items: %s", reset)
-        lgr.debug("Will 'purge' metadata items: %s", purge)
-
-        refds_path = Interface.get_refds_path(dataset)
-        res_kwargs = dict(action='metadata', logger=lgr, refds=refds_path)
-
-        to_process = []
-        for ap in AnnotatePaths.__call__(
-                dataset=refds_path,
-                path=path,
-                recursive=recursive,
-                recursion_limit=recursion_limit,
-                action='metadata',
-                unavailable_path_status='error',
-                nondataset_path_status='error',
-                force_subds_discovery=False,
-                return_type='generator',
-                on_failure='ignore'):
-            if ap.get('status', None):
-                # this is done
-                yield ap
-                continue
-            if ap.get('type', None) == 'dataset':
-                if ap.get('state', None) == 'absent':
-                    # just discovered via recursion, but not relevant here
-                    continue
-                if GitRepo.is_valid_repo(ap['path']):
-                    ap['process_content'] = True
-            to_process.append(ap)
-
-        content_by_ds, ds_props, completed, nondataset_paths = \
-            annotated2content_by_ds(
-                to_process,
-                refds_path=refds_path)
-        assert(not completed)
-
-        # iterate over all datasets, order doesn't matter
-        to_save = []
-        for ds_path in content_by_ds:
-            # ignore submodule entries
-            content = [ap for ap in content_by_ds[ds_path]
-                       if ap.get('type', None) != 'dataset' or ap['path'] == ds_path]
-            if not content:
-                # nothing other than subdatasets were given or discovered in
-                # this dataset, ignore
-                continue
-            ds = Dataset(ds_path)
-            if dataset_global or define_key:
-                db_path = opj(ds.path, '.datalad', 'metadata', 'dataset.json')
-                db = {}
-                if exists(db_path):
-                    db_fp = open(db_path)
-                    # need to read manually, load() would puke on an empty file
-                    db_content = db_fp.read()
-                    # minimize time for collision
-                    db_fp.close()
-                    if db_content:
-                        db = json.loads(db_content)
-                # TODO make manipulation order identical to what git-annex does
-                for k, v in init.items() if init else []:
-                    if k not in db:
-                        db[k] = v
-                for k in purge:
-                    if k in db:
-                        del db[k]
-                for k, v in reset.items():
-                    db[k] = v
-                for k, v in add.items():
-                    db[k] = sorted(unique(
-                        db.get(k, []) + v))
-                for k, v in remove.items():
-                    existing_data = db.get(k, [])
-                    if isinstance(existing_data, dict):
-                        db[k] = {dk: existing_data[dk]
-                                 for dk in set(existing_data).difference(v)}
-                    else:
-                        db[k] = list(set(existing_data).difference(v))
-                    # wipe out if empty
-                    if not db[k]:
-                        del db[k]
-
-                added_def = False
-                if define_key:
-                    defs = db.get('definition', {})
-                    for k, v in define_key.items():
-                        if k in defs:
-                            if not defs[k] == v:
-                                yield get_status_dict(
-                                    status='error',
-                                    ds=ds,
-                                    message=(
-                                        "conflicting definition for key '%s': '%s' != '%s'",
-                                        k, v, defs[k]),
-                                    **res_kwargs)
-                                continue
-                        else:
-                            defs[k] = v
-                            added_def = True
-                    db['definition'] = defs
-                # store, if there is anything
-                if db:
-                    if not exists(dirname(db_path)):
-                        makedirs(dirname(db_path))
-                    db_fp = open(db_path, 'w')
-                    # produce relatively compact, but also diff-friendly format
-                    json.dump(
-                        db,
-                        db_fp,
-                        indent=0,
-                        separators=(',', ':\n'),
-                        sort_keys=True)
-                    # minimize time for collision
-                    db_fp.close()
-                    # use add not save to also cover case of a fresh file
-                    ds.add(db_path, save=False)
-                    to_save.append(dict(
-                        path=db_path,
-                        parentds=ds.path,
-                        type='file'))
-                elif exists(db_path):
-                    # no metadata left, kill file
-                    ds.remove(db_path)
-                    to_save.append(dict(
-                        path=ds.path,
-                        type='dataset'))
-                if added_def or init or add or remove or reset or purge:
-                    # if anything happended or could have happended
-                    yield get_status_dict(
-                        status='ok',
-                        ds=ds,
-                        metadata=db,
-                        **res_kwargs)
-            elif not isinstance(ds.repo, AnnexRepo):
-                # report on all explicitly requested paths only
-                for ap in [c for c in content if ap.get('raw_input', False)]:
-                    yield dict(
-                        ap,
-                        status='impossible',
-                        message=(
-                            'non-annex dataset %s has no file metadata support', ds),
-                        **res_kwargs)
-                continue
-            ds_paths = [p['path'] for p in content]
-            if not dataset_global:
-                if reset or purge or add or init or remove:
-                    # file metadata manipulation
-                    mod_paths = []
-                    for mp in ds.repo.set_metadata(
-                            ds_paths,
-                            reset=reset,
-                            add=add,
-                            init=init,
-                            remove=remove,
-                            purge=purge,
-                            # we always go recursive
-                            # TODO is that a good thing? But how to otherwise distinuish
-                            # this kind of recursive from the one across datasets in
-                            # the API?
-                            recursive=True):
-                        if mp.get('success', False):
-                            mod_paths.append(mp['file'])
-                        else:
-                            yield get_status_dict(
-                                status='error',
-                                message='setting metadata failed',
-                                path=opj(ds.path, mp[0]),
-                                type='file',
-                                **res_kwargs)
-                    # query the actually modified paths only
-                    ds_paths = mod_paths
-
-                # and lastly, query -- even if we set before -- there could
-                # be side-effect from multiple set paths on an individual
-                # path, hence we need to query to get the final result
-                for file, meta in ds.repo.get_metadata(ds_paths):
-                    r = get_status_dict(
-                        status='ok',
-                        path=opj(ds.path, file),
-                        type='file',
-                        metadata=meta,
-                        **res_kwargs)
-                    yield r
-        # save potential modifications to dataset global metadata
-        if not to_save:
-            return
-        for res in Save.__call__(
-                path=to_save,
-                dataset=refds_path,
-                message='[DATALAD] dataset metadata update',
-                return_type='generator',
-                result_xfm=None,
-                result_filter=None,
-                on_failure='ignore'):
-            yield res
+    def __call__(*, dataset=None, sensitive=None, sections=None, flavor="full", decor=None, clipboard=None):
+        from datalad.distribution.dataset import require_dataset
+        from datalad.support.exceptions import NoDatasetFound
+        from datalad.interface.results import get_status_dict
+
+        ds = None
+        try:
+            ds = require_dataset(dataset, check_installed=False, purpose='report')
+        except NoDatasetFound:
+            # failure is already logged
+            pass
+        if ds and not ds.is_installed():
+            # warn that the dataset is bogus
+            yield dict(
+                action='wtf',
+                path=ds.path,
+                status='impossible',
+                message=(
+                    'No dataset found at %s. Reporting on the dataset is '
+                    'not attempted.', ds.path),
+                logger=lgr
+            )
+            # we don't deal with absent datasets
+            ds = None
+        if sensitive:
+            if ds is None:
+                from datalad import cfg
+            else:
+                cfg = ds.config
+        else:
+            cfg = None
+
+        from datalad.ui import ui
+        from datalad.support.external_versions import external_versions
+
+        infos = dict()
+        res = get_status_dict(
+            action='wtf',
+            path=ds.path if ds else ensure_unicode(op.abspath(op.curdir)),
+            type='dataset' if ds else 'directory',
+            status='ok',
+            logger=lgr,
+            decor=decor,
+            infos=infos,
+            flavor=flavor,
+        )
+
+        # Define section callables which require variables.
+        # so there is no side-effect on module level original
+        section_callables = SECTION_CALLABLES.copy()
+        section_callables['location'] = partial(_describe_location, res)
+        section_callables['configuration'] = \
+            partial(_describe_configuration, cfg, sensitive)
+        if ds:
+            section_callables['dataset'] = \
+                partial(_describe_dataset, ds, sensitive)
+        else:
+            section_callables.pop('dataset')
+        assert all(section_callables.values())  # check if none was missed
+
+        asked_for_all_sections = sections is not None and any(s == '*' for s in sections)
+        if sections is None or asked_for_all_sections:
+            if flavor == 'full' or asked_for_all_sections:
+                sections = sorted(list(section_callables))
+            elif flavor == 'short':
+                sections = ['datalad', 'dependencies']
+            else:
+                raise ValueError(flavor)
+
+        # flatten and handle (skip with a record non-matching ones) section groups
+        for sg in sections:
+            try:
+                for s in SECTION_CALLABLES_GROUPPED[sg]:
+                    infos[s] = section_callables[s]()
+            except KeyError as exc:
+                yield get_status_dict(
+                    action='wtf',
+                    path=Path.cwd(),
+                    status='impossible',
+                    message=(
+                      'Requested section <%s> was not found among the '
+                      'available infos. Skipping report.', exc.args[0]),
+                    logger=lgr
+                )
+
+        if clipboard:
+            external_versions.check(
+                'pyperclip', msg="It is needed to be able to use clipboard")
+            import pyperclip
+            report = _render_report(res)
+            pyperclip.copy(report)
+            ui.message("WTF information of length %s copied to clipboard"
+                       % len(report))
+        yield res
+        return
 
     @staticmethod
     def custom_result_renderer(res, **kwargs):
         from datalad.ui import ui
-        if res['status'] != 'ok' or not res.get('action', None) == 'metadata':
-            # logging complained about this already
-            return
-        # list the path, available metadata keys, and tags
-        path = relpath(res['path'],
-                       res['refds']) if res.get('refds', None) else res['path']
-        meta = res.get('metadata', {})
-        ui.message('{path}{type}:{spacer}{meta}{tags}'.format(
-            path=path,
-            type=' ({})'.format(res['type']) if 'type' in res else '',
-            spacer=' ' if len([m for m in meta if m != 'tag']) else '',
-            meta=','.join(k for k in sorted(meta.keys()) if not k == 'tag')
-                 if meta else ' -',
-            tags='' if 'tag' not in meta else ' [{}]'.format(
-                 ','.join(meta['tag']))))
+        out = _render_report(res)
+        if out:
+            ui.message(out)
+        # add any necessary hints afterwards
+        maybe_show_hints(res)
+
+
+def maybe_show_hints(res):
+    """Helper to add hints to custom result rendering"""
+    # check for missing file system info
+    # if the system record lacks file system information, hint at a psutil
+    # problem. Query for TMP should be safe, is included in system info
+    from datalad.ui.utils import show_hint
+
+    if 'system' in res.get('infos', {}) and \
+        'type' not in res['infos']['system']['filesystem']['TMP']:
+        try:
+            import psutil
+        except ImportError:
+            show_hint('Hint: install psutil to get filesystem information')
+
+
+def _render_report(res):
+    report = u'# WTF'
+    if not (res.get('status') == 'ok' and 'infos' in res):
+        # some other record not intended to be rendered
+        return
+
+    def _unwind(text, val, top):
+        if isinstance(val, dict):
+            for k in sorted(val):
+                text += u'\n{}{} {}{} '.format(
+                    '##' if not top else top,
+                    '-' if top else '',
+                    k,
+                    ':' if top else '')
+                text = _unwind(text, val[k], u'{}  '.format(top))
+        elif isinstance(val, (list, tuple)):
+            for i, v in enumerate(val):
+                text += u'\n{}{} '.format(top, '-')
+                text = _unwind(text, v, u'{}  '.format(top))
+        else:
+            text += u'{}'.format(val)
+        return text
+
+    def _unwind_short(text, val, top):
+        if isinstance(val, dict):
+            if not top:
+                text += '\n'
+                for k, v in val.items():
+                    text += "- " + _unwind_short(k, v, top + ' ') + '\n'
+            else:
+                text += ": " + ' '.join('%s=%s' % i for i in val.items())
+        elif isinstance(val, (list, tuple)):
+            text += (' ' if not top else '\n').join(map(str, val))
+        else:
+            text += u'{}'.format(val)
+        return text
+
+    unwinder = {'full': _unwind, 'short': _unwind_short}[res.get('flavor', 'full')]
+    report = unwinder(report, res.get('infos', {}), '')
+    decor = res.get('decor', None)
+
+    if not decor:
+        return report
+
+    if decor == 'html_details':
+        report = """\
+<details><summary>DataLad %s WTF (%s)</summary>
+
+%s
+</details>
+        """ % (__version__, ', '.join(res.get('infos', {})), report)
+    else:
+        raise ValueError("Unknown value of decor=%s" % decor)
+    return report
```

### Comparing `datalad-0.9.3/datalad/metadata/search.py` & `datalad-1.0.0/datalad/interface/utils.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,426 +1,469 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""Interface for managing metadata
+"""Interface utility functions
+
 """
 
 __docformat__ = 'restructuredtext'
 
-import os
-import re
+import logging
 import sys
-
-from operator import itemgetter
-from os.path import join as opj, exists
-from six import string_types
-from six import text_type
-from six import iteritems
-from six import reraise
-from six import PY3
-from datalad.interface.base import Interface
-from datalad.interface.base import build_doc
-from datalad.distribution.dataset import Dataset
-from datalad.distribution.dataset import datasetmethod, EnsureDataset, \
-    require_dataset
-from datalad.distribution.utils import get_git_dir
-from ..support.param import Parameter
-from ..support.constraints import EnsureNone
-from ..support.constraints import EnsureChoice
-from ..log import lgr
-from . import get_metadata, flatten_metadata_graph, pickle
-
-from datalad.consts import LOCAL_CENTRAL_PATH
-from datalad.utils import assure_list
-from datalad.utils import decode_input
-from datalad.utils import get_path_prefix
-from datalad.support.exceptions import NoDatasetArgumentFound
-from datalad.support import ansi_colors
+from time import time
+from os import listdir
+from os.path import join as opj
+from os.path import isdir
+from os.path import relpath
+from os.path import sep
+
+import json
+
+from typing import (
+    TypeVar
+)
+# avoid import from API to not get into circular imports
+from datalad.utils import with_pathsep as _with_sep  # TODO: RF whenever merge conflict is not upon us
+from datalad.utils import (
+    path_startswith,
+    path_is_subpath,
+    ensure_unicode,
+    getargspec,
+)
+from datalad.support.gitrepo import GitRepo
+from datalad.support.exceptions import (
+    CapturedException,
+)
+from datalad import cfg as dlcfg
+from datalad.dochelpers import single_or_plural
 from datalad.ui import ui
+import datalad.support.ansi_colors as ac
 
+anInterface = TypeVar('anInterface', bound='Interface')
 
-@build_doc
-class Search(Interface):
-    """Search within available in datasets' meta data
-    """
-    # XXX prevent common args from being added to the docstring
-    _no_eval_results = True
-
-    _params_ = dict(
-        dataset=Parameter(
-            args=("-d", "--dataset"),
-            doc="""specify the dataset to perform the query operation on. If
-            no dataset is given, an attempt is made to identify the dataset
-            based on the current working directory and/or the `path` given""",
-            constraints=EnsureDataset() | EnsureNone()),
-        match=Parameter(
-            args=("match",),
-            metavar='STRING',
-            nargs="+",
-            # by default would use sys.getdefaultencoding() (yoh thinks) which
-            # might be ascii, so let's use the one of stdin
-            # Seems to be needed for PY2 and works in PY3 without this
-            type=decode_input,
-            doc="a string (or a regular expression if "
-                "[PY: `regex=True` PY][CMD: --regex CMD]) to search for "
-                "in all meta data values. If multiple provided, all must have "
-                "a match among some fields of a dataset"),
-        #match=Parameter(
-        #    args=('-m', '--match',),
-        #    metavar='REGEX',
-        #    action='append',
-        #    nargs=2,
-        #    doc="""Pair of two regular expressions to match a property and its
-        #    value.[CMD:  This option can be given multiple times CMD]"""),
-        search=Parameter(
-            args=('-s', '--search'),
-            metavar='PROPERTY',
-            action='append',
-            # could also be regex
-            doc="""name of the property to search for any match.[CMD:  This
-            option can be given multiple times. CMD] By default, all properties
-            are searched."""),
-        report=Parameter(
-            args=('-r', '--report'),
-            metavar='PROPERTY',
-            action='append',
-            # could also be regex
-            doc="""name of the property to report for any match.[CMD:  This
-            option can be given multiple times. CMD] If '*' is given, all
-            properties are reported."""),
-        report_matched=Parameter(
-            args=('-R', '--report-matched',),
-            action="store_true",
-            doc="""flag to report those fields which have matches. If `report`
-             option values are provided, union of matched and those in `report`
-             will be output"""),
-        # Theoretically they should be CMDLINE specific I guess?
-        format=Parameter(
-            args=('-f', '--format'),
-            constraints=EnsureChoice('custom', 'json', 'yaml'),
-            doc="""format for output."""
-        ),
-        regex=Parameter(
-            args=("--regex",),
-            action="store_true",
-            doc="flag for STRING to be used as a (Python) regular expression "
-                "which should match the value"),
-    )
-
-    @staticmethod
-    @datasetmethod(name='search')
-    def __call__(match,
-                 dataset=None,
-                 search=None,
-                 report=None,
-                 report_matched=False,
-                 format='custom',
-                 regex=False):
-        """
-        Yields
-        ------
-        location : str
-            (relative) path to the dataset
-        report : dict
-            fields which were requested by `report` option
-        """
-        lgr.debug("Initiating search for match=%r and dataset %r",
-                  match, dataset)
-        try:
-            ds = require_dataset(dataset, check_installed=True, purpose='dataset search')
-            if ds.id is None:
-                raise NoDatasetArgumentFound(
-                    "This does not seem to be a dataset (no DataLad dataset ID "
-                    "found). 'datalad create --force %s' can initialize "
-                    "this repository as a DataLad dataset" % ds.path)
-        except NoDatasetArgumentFound:
-            exc_info = sys.exc_info()
-            if dataset is None:
-                if not ui.is_interactive:
-                    raise NoDatasetArgumentFound(
-                        "No DataLad dataset found. Specify a dataset to be "
-                        "searched, or run interactively to get assistance "
-                        "installing a queriable superdataset."
-                    )
-                # none was provided so we could ask user either he possibly wants
-                # to install our beautiful mega-duper-super-dataset?
-                # TODO: following logic could possibly benefit other actions.
-                if os.path.exists(LOCAL_CENTRAL_PATH):
-                    central_ds = Dataset(LOCAL_CENTRAL_PATH)
-                    if central_ds.is_installed():
-                        if ui.yesno(
-                            title="No DataLad dataset found at current location",
-                            text="Would you like to search the DataLad "
-                                 "superdataset at %r?"
-                                  % LOCAL_CENTRAL_PATH):
-                            pass
-                        else:
-                            reraise(*exc_info)
-                    else:
-                        raise NoDatasetArgumentFound(
-                            "No DataLad dataset found at current location. "
-                            "The DataLad superdataset location %r exists, "
-                            "but does not contain an dataset."
-                            % LOCAL_CENTRAL_PATH)
-                elif ui.yesno(
-                        title="No DataLad dataset found at current location",
-                        text="Would you like to install the DataLad "
-                             "superdataset at %r?"
-                             % LOCAL_CENTRAL_PATH):
-                    from datalad.api import install
-                    central_ds = install(LOCAL_CENTRAL_PATH, source='///')
-                    ui.message(
-                        "From now on you can refer to this dataset using the "
-                        "label '///'"
-                    )
-                else:
-                    reraise(*exc_info)
-
-                lgr.info(
-                    "Performing search using DataLad superdataset %r",
-                    central_ds.path
-                )
-                for res in central_ds.search(
-                        match,
-                        search=search, report=report,
-                        report_matched=report_matched,
-                        format=format, regex=regex):
-                    yield res
-                return
-            else:
-                raise
-
-        cache_dir = opj(opj(ds.path, get_git_dir(ds.path)), 'datalad', 'cache')
-        mcache_fname = opj(cache_dir, 'metadata.p%d' % pickle.HIGHEST_PROTOCOL)
+lgr = logging.getLogger('datalad.interface.utils')
 
-        meta = None
-        if os.path.exists(mcache_fname):
-            lgr.debug("use cached metadata of '{}' from {}".format(ds, mcache_fname))
-            meta, checksum = pickle.load(open(mcache_fname, 'rb'))
-            # TODO add more sophisticated tests to decide when the cache is no longer valid
-            if checksum != ds.repo.get_hexsha():
-                # errrr, try again below
-                meta = None
-
-        # don't put in 'else', as yet to be written tests above might fail and require
-        # regenerating meta data
-        if meta is None:
-            lgr.info("Loading and caching local meta-data... might take a few seconds")
-            if not exists(cache_dir):
-                os.makedirs(cache_dir)
-
-            meta = get_metadata(ds, guess_type=False, ignore_subdatasets=False,
-                                ignore_cache=False)
-            # merge all info on datasets into a single dict per dataset
-            meta = flatten_metadata_graph(meta)
-            # extract graph, if any
-            meta = meta.get('@graph', meta)
-            # build simple queriable representation
-            if not isinstance(meta, list):
-                meta = [meta]
-
-            # sort entries by location (if present)
-            sort_keys = ('location', 'description', 'id')
-            # note with str() instead of '%' getting encoding issues...
-            meta = sorted(meta, key=lambda m: tuple("%s" % (m.get(x, ""),) for x in sort_keys))
-
-            # use pickle to store the optimized graph in the cache
-            pickle.dump(
-                # graph plus checksum from what it was built
-                (meta, ds.repo.get_hexsha()),
-                open(mcache_fname, 'wb'))
-            lgr.debug("cached meta data graph of '{}' in {}".format(ds, mcache_fname))
-
-        if report in ('', ['']):
-            report = []
-        elif report and not isinstance(report, list):
-            report = [report]
-
-        match = assure_list(match)
-        search = assure_list(search)
-        # convert all to lower case for case insensitive matching
-        search = {x.lower() for x in search}
-
-        def get_in_matcher(m):
-            """Function generator to provide closure for a specific value of m"""
-            mlower = m.lower()
-
-            def matcher(s):
-                return mlower in s.lower()
-            return matcher
-
-        matchers = [
-            re.compile(match_).search
-            if regex
-            else get_in_matcher(match_)
-            for match_ in match
-        ]
-
-        # location should be reported relative to current location
-        # We will assume that noone chpwd while we are yielding
-        ds_path_prefix = get_path_prefix(ds.path)
-
-        # So we could provide a useful message whenever there were not a single
-        # dataset with specified `--search` properties
-        observed_properties = set()
-
-        # for every meta data set
-        for mds in meta:
-            hit = False
-            hits = [False] * len(matchers)
-            matched_fields = set()
-            if not mds.get('type', mds.get('schema:type', None)) == 'Dataset':
-                # we are presently only dealing with datasets
-                continue
-            # TODO consider the possibility of nested and context/graph dicts
-            # but so far we were trying to build simple lists of dicts, as much
-            # as possible
-            if not isinstance(mds, dict):
-                raise NotImplementedError("nested meta data is not yet supported")
-
-            # manual loop for now
-            for k, v in iteritems(mds):
-                if search:
-                    k_lower = k.lower()
-                    if k_lower not in search:
-                        if observed_properties is not None:
-                            # record for providing a hint later
-                            observed_properties.add(k_lower)
-                        continue
-                    # so we have a hit, no need to track
-                    observed_properties = None
-                if isinstance(v, (dict, list, tuple)):
-                    v_values = v.values() if isinstance(v, dict) else v
-                    v = ' '.join(map(text_type, v_values))
-                for imatcher, matcher in enumerate(matchers):
-                    if matcher(v):
-                        hits[imatcher] = True
-                        matched_fields.add(k)
-                if all(hits):
-                    hit = True
-                    # no need to do it longer than necessary
-                    if not report_matched:
-                        break
-
-            if hit:
-                location = mds.get('location', '.')
-                report_ = matched_fields.union(report if report else {}) \
-                    if report_matched else report
-                if report_ == ['*']:
-                    report_dict = mds
-                elif report_:
-                    report_dict = {k: mds[k] for k in report_ if k in mds}
-                    if report_ and not report_dict:
-                        lgr.debug(
-                            'meta data match for %s, but no to-be-reported '
-                            'properties (%s) found. Present properties: %s',
-                            location, ", ".join(report_), ", ".join(sorted(mds))
-                        )
-                else:
-                    report_dict = {}  # it was empty but not None -- asked to
-                    # not report any specific field
-                if isinstance(location, (list, tuple)):
-                    # could be that the same dataset installed into multiple
-                    # locations. For now report them separately
-                    for l in location:
-                        yield opj(ds_path_prefix, l), report_dict
-                else:
-                    yield opj(ds_path_prefix, location), report_dict
-
-        if search and observed_properties is not None:
-            import difflib
-            suggestions = {
-                s: difflib.get_close_matches(s, observed_properties)
-                for s in search
-            }
-            suggestions_str = "\n ".join(
-                "%s for %s" % (", ".join(choices), s)
-                for s, choices in iteritems(suggestions) if choices
-            )
-            lgr.warning(
-                "Found no properties which matched one of the one you "
-                "specified (%s).  May be you meant one among: %s.\n"
-                "Suggestions:\n"
-                " %s",
-                ", ".join(search),
-                ", ".join(observed_properties),
-                suggestions_str if suggestions_str.strip() else "none"
-            )
-
-    @staticmethod
-    def result_renderer_cmdline(res, cmdlineargs):
-        from datalad.ui import ui
-        if res is None:
-            res = []
-
-        format = cmdlineargs.format or 'custom'
-        if format == 'custom':
-
-            if cmdlineargs.report in ('*', ['*']) \
-                    or cmdlineargs.report_matched \
-                    or (cmdlineargs.report is not None
-                        and len(cmdlineargs.report) > 1):
-                # multiline if multiple were requested and we need to disambiguate
-                ichr = jchr = '\n'
-                fmt = ' {k}: {v}'
-            else:
-                jchr = ', '
-                ichr = ' '
-                fmt = '{v}'
-
-            anything = False
-            for location, r in res:
-                # XXX Yarik thinks that Match should be replaced with actual path to the dataset
-                ui.message('{}{}{}{}'.format(
-                    ansi_colors.color_word(location, ansi_colors.DATASET),
-                    ':' if r else '',
-                    ichr,
-                    jchr.join(
-                        [
-                            fmt.format(
-                                k=ansi_colors.color_word(k, ansi_colors.FIELD),
-                                v=pretty_bytes(r[k]))
-                            for k in sorted(r)
-                        ])))
-                anything = True
-            if not anything:
-                ui.message("Nothing to report")
-        elif format == 'json':
-            import json
-            ui.message(json.dumps(list(map(itemgetter(1), res)), indent=2))
-        elif format == 'yaml':
-            import yaml
-            lgr.warning("yaml output support is not yet polished")
-            ui.message(yaml.safe_dump(list(map(itemgetter(1), res)),
-                                      allow_unicode=True))
 
+# TODO remove
+# only `drop` and `uninstall` are still using this
+def handle_dirty_dataset(ds, mode, msg=None):
+    """Detect and treat unsaved changes as instructed by `mode`
+
+    Parameters
+    ----------
+    ds : Dataset or None
+      Dataset to be inspected. Does nothing if `None`.
+    mode : {'fail', 'ignore', 'save-before'}
+      How to act upon discovering unsaved changes.
+    msg : str or None
+      Custom message to use for a potential commit.
+
+    Returns
+    -------
+    None
+    """
+    if ds is None:
+        # nothing to be handled
+        return
+    if msg is None:
+        msg = '[DATALAD] auto-saved changes'
+
+    # make sure that all pending changes (batched annex operations, etc.)
+    # are actually reflected in Git
+    if ds.repo:
+        ds.repo.precommit()
+
+    if mode == 'ignore':
+        return
+    elif mode == 'fail':
+        if not ds.repo or ds.repo.dirty:
+            raise RuntimeError('dataset {} has unsaved changes'.format(ds))
+    elif mode == 'save-before':
+        if not ds.is_installed():
+            raise RuntimeError('dataset {} is not yet installed'.format(ds))
+        from datalad.core.local.save import Save
+        Save.__call__(dataset=ds, message=msg, updated=True)
+    else:
+        raise ValueError("unknown if-dirty mode '{}'".format(mode))
 
-_lines_regex = re.compile('[\n\r]')
 
+def get_tree_roots(paths):
+    """Return common root paths for a set of paths
 
-def pretty_bytes(s):
-    """Helper to provide sensible rendering for lists, dicts, and unicode
+    This function determines the smallest set of common root
+    paths and sorts all given paths under the respective
+    root.
 
-    encoded into byte-stream (why really???)
+    Returns
+    -------
+    dict
+      paths by root
     """
-    if isinstance(s, list):
-        return ", ".join(map(pretty_bytes, s))
-    elif isinstance(s, dict):
-        return pretty_bytes(["%s=%s" % (pretty_bytes(k), pretty_bytes(v))
-                             for k, v in s.items()])
-    elif isinstance(s, text_type):
-        s_ = (os.linesep + "  ").join(_lines_regex.split(s))
-        try:
-            if PY3:
-                return s_
-            return s_.encode('utf-8')
-        except UnicodeEncodeError:
-            lgr.warning("Failed to encode value correctly. Ignoring errors in encoding")
-            # TODO: get current encoding
-            return s_.encode('utf-8', 'ignore') if isinstance(s_, string_types) else "ERROR"
+    paths_ws = [_with_sep(p) for p in paths]
+    # sort all paths under their potential roots
+    roots = {}
+    # start from the top to get all paths down the line
+    # and collate them into as few roots as possible
+    for s in sorted(paths_ws):
+        if any([s.startswith(r) for r in roots]):
+            # this path is already covered by a known root
+            continue
+        # find all sub paths
+        subs = [p for p in paths if p.startswith(s)]
+        roots[s.rstrip(sep)] = subs
+    return roots
+
+
+# TODO(OPT)? YOH: from a cursory review seems like possibly an expensive function
+# whenever many paths were provided (e.g. via shell glob).
+# Might be worth testing on some usecase and py-spy'ing if notable portion
+# of time is spent.
+def discover_dataset_trace_to_targets(basepath, targetpaths, current_trace,
+                                      spec, includeds=None):
+    """Discover the edges and nodes in a dataset tree to given target paths
+
+    Parameters
+    ----------
+    basepath : path
+      Path to a start or top-level dataset. Really has to be a path to a
+      dataset!
+    targetpaths : list(path)
+      Any non-zero number of paths that are termination points for the
+      search algorithm. Can be paths to datasets, directories, or files
+      (and any combination thereof).
+    current_trace : list
+      For a top-level call this should probably always be `[]`
+    spec : dict
+      `content_by_ds`-style dictionary that will receive information about the
+      discovered datasets. Specifically, for each discovered dataset there
+      will be an item with its path under the key (path) of the respective
+      superdataset.
+    includeds : sequence, optional
+      Any paths given are treated as existing subdatasets, regardless of
+      whether they can be found in the filesystem. Such subdatasets will appear
+      under the key of the closest existing dataset in the `spec`.
+
+    Returns
+    -------
+    None
+      Function calls itself recursively and populates `spec` dict in-place.
+      Keys are dataset paths, values are sets of subdataset paths
+    """
+    # convert to set for faster lookup
+    includeds = includeds if isinstance(includeds, set) else \
+        set() if includeds is None else set(includeds)
+    # this beast walks the directory tree from a given `basepath` until
+    # it discovers any of the given `targetpaths`
+    # if it finds one, it commits any accumulated trace of visited
+    # datasets on this edge to the spec
+    valid_repo = GitRepo.is_valid_repo(basepath)
+    if valid_repo:
+        # we are passing into a new dataset, extend the dataset trace
+        current_trace = current_trace + [basepath]
+    # this edge is not done, we need to try to reach any downstream
+    # dataset
+    undiscovered_ds = set(t for t in targetpaths)  # if t != basepath)
+    # whether anything in this directory matched a targetpath
+    filematch = False
+    if isdir(basepath):
+        for p in listdir(basepath):
+            p = ensure_unicode(opj(basepath, p))
+            if not isdir(p):
+                if p in targetpaths:
+                    filematch = True
+                # we cannot have anything below this one
+                continue
+            # OPT listdir might be large and we could have only few items
+            # in `targetpaths` -- so traverse only those in spec which have
+            # leading dir basepath
+            # filter targets matching this downward path
+            downward_targets = set(
+                t for t in targetpaths if path_startswith(t, p))
+            if not downward_targets:
+                continue
+            # remove the matching ones from the "todo" list
+            undiscovered_ds.difference_update(downward_targets)
+            # go one deeper
+            discover_dataset_trace_to_targets(
+                p, downward_targets, current_trace, spec,
+                includeds=includeds if not includeds else includeds.intersection(
+                    downward_targets))
+    undiscovered_ds = [t for t in undiscovered_ds
+                       if includeds and
+                       path_is_subpath(t, current_trace[-1]) and
+                       t in includeds]
+    if filematch or basepath in targetpaths or undiscovered_ds:
+        for i, p in enumerate(current_trace[:-1]):
+            # TODO RF prepare proper annotated path dicts
+            subds = spec.get(p, set())
+            subds.add(current_trace[i + 1])
+            spec[p] = subds
+        if undiscovered_ds:
+            spec[current_trace[-1]] = spec.get(current_trace[-1], set()).union(
+                undiscovered_ds)
+
+
+def get_result_filter(fx):
+    """Wrap a filter into a helper to be able to accept additional
+    arguments, if the filter doesn't support it already"""
+    _fx = fx
+    if fx and not getargspec(fx).keywords:
+        def _fx(res, **kwargs):
+            return fx(res)
+    return _fx
+
+
+def eval_results(wrapped):
+    import warnings
+    from datalad.interface.base import eval_results as eval_results_moved
+    warnings.warn("datalad.interface.utils.eval_results is obsolete. "
+                  "Use datalad.interface.base.eval_results instead",
+                  DeprecationWarning)
+    return eval_results_moved(wrapped)
+
+
+def generic_result_renderer(res):
+    if res.get('status', None) != 'notneeded':
+        path = res.get('path', None)
+        if path and res.get('refds'):
+            try:
+                path = relpath(path, res['refds'])
+            except ValueError:
+                # can happen, e.g., on windows with paths from different
+                # drives. just go with the original path in this case
+                pass
+        ui.message('{action}({status}):{path}{type}{msg}{err}'.format(
+            action=ac.color_word(
+                res.get('action', '<action-unspecified>'),
+                ac.BOLD),
+            status=ac.color_status(res.get('status', '<status-unspecified>')),
+            path=' {}'.format(path) if path else '',
+            type=' ({})'.format(
+                ac.color_word(res['type'], ac.MAGENTA)
+            ) if 'type' in res else '',
+            msg=' [{}]'.format(
+                res['message'][0] % res['message'][1:]
+                if isinstance(res['message'], tuple) else res[
+                    'message'])
+            if res.get('message', None) else '',
+            err=ac.color_word(' [{}]'.format(
+                res['error_message'][0] % res['error_message'][1:]
+                if isinstance(res['error_message'], tuple) else res[
+                    'error_message']), ac.RED)
+            if res.get('error_message', None) and res.get('status', None) != 'ok' else ''))
+
+
+# keep for legacy compatibility
+default_result_renderer = generic_result_renderer
+
+
+def render_action_summary(action_summary):
+    ui.message("action summary:\n  {}".format(
+        '\n  '.join('{} ({})'.format(
+            act,
+            ', '.join('{}: {}'.format(status, action_summary[act][status])
+                      for status in sorted(action_summary[act])))
+                    for act in sorted(action_summary))))
+
+
+def _display_suppressed_message(nsimilar, ndisplayed, last_ts, final=False):
+    # +1 because there was the original result + nsimilar displayed.
+    n_suppressed = nsimilar - ndisplayed + 1
+    if n_suppressed > 0:
+        ts = time()
+        # rate-limit update of suppression message, with a large number
+        # of fast-paced results updating for each one can result in more
+        # CPU load than the actual processing
+        # arbitrarily go for a 2Hz update frequency -- it "feels" good
+        if last_ts is None or final or (ts - last_ts > 0.5):
+            ui.message('  [{} similar {} been suppressed; disable with datalad.ui.suppress-similar-results=off]'
+                       .format(n_suppressed,
+                               single_or_plural("message has",
+                                                "messages have",
+                                                n_suppressed, False)),
+                       cr="\n" if final else "\r")
+            return ts
+    return last_ts
+
+
+def _process_results(
+        results,
+        cmd_class,
+        on_failure,
+        action_summary,
+        incomplete_results,
+        result_renderer,
+        result_log_level,
+        allkwargs):
+    # private helper pf @eval_results
+    # loop over results generated from some source and handle each
+    # of them according to the requested behavior (logging, rendering, ...)
+
+    # used to track repeated messages in the generic renderer
+    last_result = None
+    # the timestamp of the last renderer result
+    last_result_ts = None
+    # counter for detected repetitions
+    last_result_reps = 0
+    # how many repetitions to show, before suppression kicks in
+    render_n_repetitions = \
+        dlcfg.obtain('datalad.ui.suppress-similar-results-threshold') \
+            if sys.stdout.isatty() \
+               and dlcfg.obtain('datalad.ui.suppress-similar-results') \
+            else float("inf")
+
+    for res in results:
+        if not res or 'action' not in res:
+            # XXX Yarik has to no clue on how to track the origin of the
+            # record to figure out WTF, so he just skips it
+            # but MIH thinks leaving a trace of that would be good
+            lgr.debug('Drop result record without "action": %s', res)
+            continue
+
+        actsum = action_summary.get(res['action'], {})
+        if res['status']:
+            actsum[res['status']] = actsum.get(res['status'], 0) + 1
+            action_summary[res['action']] = actsum
+        ## log message, if there is one and a logger was given
+        msg = res.get('message', None)
+        # remove logger instance from results, as it is no longer useful
+        # after logging was done, it isn't serializable, and generally
+        # pollutes the output
+        res_lgr = res.pop('logger', None)
+        if msg and res_lgr:
+            if isinstance(res_lgr, logging.Logger):
+                # didn't get a particular log function, go with default
+                res_lgr = getattr(
+                    res_lgr,
+                    default_logchannels[res['status']]
+                    if result_log_level == 'match-status'
+                    else result_log_level)
+            msg = res['message']
+            msgargs = None
+            if isinstance(msg, tuple):
+                msgargs = msg[1:]
+                msg = msg[0]
+            if 'path' in res:
+                # result path could be a path instance
+                path = str(res['path'])
+                if msgargs:
+                    # we will pass the msg for %-polation, so % should be doubled
+                    path = path.replace('%', '%%')
+                msg = '{} [{}({})]'.format(
+                    msg, res['action'], path)
+            if msgargs:
+                # support string expansion of logging to avoid runtime cost
+                try:
+                    res_lgr(msg, *msgargs)
+                except TypeError as exc:
+                    raise TypeError(
+                        "Failed to render %r with %r from %r: %s"
+                        % (msg, msgargs, res, str(exc))
+                    ) from exc
+            else:
+                res_lgr(msg)
+
+        ## output rendering
+        if result_renderer is None or result_renderer == 'disabled':
+            pass
+        elif result_renderer == 'generic':
+            last_result_reps, last_result, last_result_ts = \
+                _render_result_generic(
+                    res, render_n_repetitions,
+                    last_result_reps, last_result, last_result_ts)
+        elif result_renderer in ('json', 'json_pp'):
+            _render_result_json(res, result_renderer.endswith('_pp'))
+        elif result_renderer == 'tailored':
+            cmd_class.custom_result_renderer(res, **allkwargs)
+        elif hasattr(result_renderer, '__call__'):
+            _render_result_customcall(res, result_renderer, allkwargs)
+        else:
+            raise ValueError(f'unknown result renderer "{result_renderer}"')
+
+        ## error handling
+        # looks for error status, and report at the end via
+        # an exception
+        if on_failure in ('continue', 'stop') \
+                and res['status'] in ('impossible', 'error'):
+            incomplete_results.append(res)
+            if on_failure == 'stop':
+                # first fail -> that's it
+                # raise will happen after the loop
+                break
+        yield res
+    # make sure to report on any issues that we had suppressed
+    _display_suppressed_message(
+        last_result_reps, render_n_repetitions, last_result_ts, final=True)
+
+
+def _render_result_generic(
+        res, render_n_repetitions,
+        # status vars
+        last_result_reps, last_result, last_result_ts):
+    # which result dict keys to inspect for changes to discover repetitions
+    # of similar messages
+    repetition_keys = set(('action', 'status', 'type', 'refds'))
+
+    trimmed_result = {k: v for k, v in res.items() if k in repetition_keys}
+    if res.get('status', None) != 'notneeded' \
+            and trimmed_result == last_result:
+        # this is a similar report, suppress if too many, but count it
+        last_result_reps += 1
+        if last_result_reps < render_n_repetitions:
+            generic_result_renderer(res)
+        else:
+            last_result_ts = _display_suppressed_message(
+                last_result_reps, render_n_repetitions, last_result_ts)
     else:
-        return str(s).encode()
+        # this one is new, first report on any prev. suppressed results
+        # by number, and then render this fresh one
+        last_result_ts = _display_suppressed_message(
+            last_result_reps, render_n_repetitions, last_result_ts,
+            final=True)
+        generic_result_renderer(res)
+        last_result_reps = 0
+    return last_result_reps, trimmed_result, last_result_ts
+
+
+def _render_result_json(res, prettyprint):
+    ui.message(json.dumps(
+        {k: v for k, v in res.items()
+         if k not in ('logger')},
+        sort_keys=True,
+        indent=2 if prettyprint else None,
+        default=str))
+
+
+def _render_result_customcall(res, result_renderer, allkwargs):
+    try:
+        result_renderer(res, **allkwargs)
+    except Exception as e:
+        lgr.warning('Result rendering failed for: %s [%s]',
+                    res, CapturedException(e))
+
+
+def keep_result(res, rfilter, **kwargs):
+    if not rfilter:
+        return True
+    try:
+        if not rfilter(res, **kwargs):
+            # give the slightest indication which filter was employed
+            raise ValueError(
+                'excluded by filter {} with arguments {}'.format(rfilter, kwargs))
+    except ValueError as e:
+        # make sure to report the excluded result to massively improve
+        # debugging experience
+        lgr.debug('Not reporting result (%s): %s', CapturedException(e), res)
+        return False
+    return True
+
+
+def xfm_result(res, xfm):
+    if not xfm:
+        return res
+
+    return xfm(res)
```

### Comparing `datalad-0.9.3/datalad/utils.py` & `datalad-1.0.0/datalad/config.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,1347 +1,1167 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
+"""
+"""
 
-import collections
-import hashlib
-import re
-import six.moves.builtins as __builtin__
-import time
-
+import json
 import logging
-import shutil
 import os
-import sys
-import tempfile
-import platform
-import gc
-import glob
-import wrapt
-
-from contextlib import contextmanager
-from functools import wraps
-from time import sleep
-from inspect import getargspec
-
-from os.path import sep as dirsep
-from os.path import commonprefix
-from os.path import curdir, basename, exists, realpath, islink, join as opj
-from os.path import isabs, normpath, expandvars, expanduser, abspath, sep
-from os.path import isdir
-from os.path import relpath
-from os.path import stat
-from os.path import dirname
-from os.path import split as psplit
-import posixpath
-
-
-from six import text_type, binary_type, string_types
-
-# from datalad.dochelpers import get_docstring_split
-from datalad.consts import TIMESTAMP_FMT
+import re
+import threading
+import warnings
+from collections import namedtuple
+from fasteners import InterProcessLock
+from functools import (
+    lru_cache,
+    wraps,
+)
+from pathlib import Path
+
+
+import datalad
+from datalad.consts import DATASET_CONFIG_FILE
+from datalad.runner import (
+    CommandError,
+    GitRunner,
+    KillOutput,
+    StdOutErrCapture,
+)
+from datalad.utils import (
+    getpwd,
+    on_windows,
+)
+
+lgr = logging.getLogger('datalad.config')
+
+# git-config key syntax with a section and a subsection
+# see git-config(1) for syntax details
+cfg_k_regex = re.compile(r'([a-zA-Z0-9-.]+\.[^\0\n]+)$', flags=re.MULTILINE)
+# identical to the key regex, but with an additional group for a
+# value in a null-delimited git-config dump
+cfg_kv_regex = re.compile(
+    r'([a-zA-Z0-9-.]+\.[^\0\n]+)\n(.*)$',
+    flags=re.MULTILINE | re.DOTALL
+)
+cfg_section_regex = re.compile(r'(.*)\.[^.]+')
+cfg_sectionoption_regex = re.compile(r'(.*)\.([^.]+)')
+
+
+_scope_reload_doc = """
+        scope : {'branch', 'local', 'global', 'override'}, optional
+          Indicator which configuration file to modify. 'branch' indicates the
+          persistent configuration in .datalad/config of a dataset; 'local'
+          the configuration of a dataset's Git repository in .git/config;
+          'global' refers to the general configuration that is not specific to
+          a single repository (usually in $USER/.gitconfig); 'override'
+          limits the modification to the ConfigManager instance, and the
+          assigned value overrides any setting from any other source.
+          Note: 'dataset' is being DEPRECATED in favor of 'branch'.
+        where: {'branch', 'local', 'global', 'override'}, optional
+          DEPRECATED, use 'scope'.
+        reload : bool
+          Flag whether to reload the configuration from file(s) after
+          modification. This can be disable to make multiple sequential
+          modifications slightly more efficient.""".lstrip()
+
+# Selection of os.stat_result fields we care to collect/compare to judge
+# on either file has changed to warrant reload of configuration.
+_stat_result = namedtuple('_stat_result', 'st_ino st_size st_ctime st_mtime')
+
+
+# we cannot import external_versions here, as the cfg comes before anything
+# and we would have circular imports
+@lru_cache()
+def get_git_version(runner=None):
+    """Return version of available git"""
+    runner = runner or GitRunner()
+    return runner.run('git version'.split(),
+                      protocol=StdOutErrCapture)['stdout'].split()[2]
+
+
+def _scope_reload(obj):
+    """Helper decorator to simplify providing repetitive docstring"""
+    obj.__doc__ = obj.__doc__ % _scope_reload_doc
+    return obj
 
-lgr = logging.getLogger("datalad.utils")
 
-lgr.log(5, "Importing datalad.utils")
 #
-# Some useful variables
+# TODO: remove when deprecated 'where' is removed. Some time >= 0.17
 #
-platform_system = platform.system().lower()
-on_windows = platform_system == 'windows'
-on_osx = platform_system == 'darwin'
-on_linux = platform_system == 'linux'
-try:
-    linux_distribution_name, linux_distribution_release \
-        = platform.linux_distribution()[:2]
-    on_debian_wheezy = on_linux \
-                       and linux_distribution_name == 'debian' \
-                       and linux_distribution_release.startswith('7.')
-except:  # pragma: no cover
-    # MIH: IndexError?
-    on_debian_wheezy = False
-    linux_distribution_name = linux_distribution_release = None
-
-# Maximal length of cmdline string
-# Did not find anything in Python which could tell at run time and
-# probably   getconf ARG_MAX   might not be available
-# The last one would be the most conservative/Windows
-CMD_MAX_ARG = 2097152 if on_linux else 262144 if on_osx else 32767
+def _where_to_scope(func):
+    @wraps(func)
+    def wrapper(*args, **kwargs):
+        if 'where' in kwargs:
+            if 'scope' in kwargs:
+                raise ValueError("Do not specify both 'scope' and DEPRECATED 'where'")
+            kwargs = kwargs.copy()
+            where = kwargs.pop('where')
+            if where == 'dataset':
+                warnings.warn("'where=\"dataset\"' is deprecated, use 'scope=\"branch\"' instead",
+                              DeprecationWarning)
+                where = 'branch'
+            else:
+                warnings.warn("'where' is deprecated, use 'scope' instead",
+                              DeprecationWarning)
+            kwargs['scope'] = where
+        return func(*args, **kwargs)
+    return wrapper
 
-#
-# Little helpers
-#
 
+def parse_gitconfig_dump(dump, cwd=None, multi_value=True):
+    """Parse a dump-string from `git config -z --list`
+
+    This parser has limited support for discarding unrelated output
+    that may contaminate the given dump. It does so performing a
+    relatively strict matching of configuration key syntax, and discarding
+    lines in the output that are not valid git-config keys.
 
-def get_func_kwargs_doc(func):
-    """ Provides args for a function
+    There is also built-in support for parsing outputs generated
+    with --show-origin (see return value).
 
     Parameters
     ----------
-    func: str
-      name of the function from which args are being requested
-
-    Returns
-    -------
-    list
-      of the args that a function takes in
-    """
-    return getargspec(func)[0]
-
-    # TODO: format error message with descriptions of args
-    # return [repr(dict(get_docstring_split(func)[1]).get(x)) for x in getargspec(func)[0]]
-
+    dump : str
+      Null-byte separated output
+    cwd : path-like, optional
+      Use this absolute path to convert relative paths for origin reports
+      into absolute paths. By default, the process working directory
+      PWD is used.
+    multi_value : bool, optional
+      If True, report values from multiple specifications of the
+      same key as a tuple of values assigned to this key. Otherwise,
+      the last configuration is reported.
 
-def any_re_search(regexes, value):
-    """Return if any of regexes (list or str) searches succesfully for value"""
-    for regex in assure_tuple_or_list(regexes):
-        if re.search(regex, value):
-            return True
-    return False
-
-
-def not_supported_on_windows(msg=None):
-    """A little helper to be invoked to consistently fail whenever functionality is
-    not supported (yet) on Windows
-    """
-    if on_windows:
-        raise NotImplementedError("This functionality is not yet implemented for Windows OS"
-                                  + (": %s" % msg if msg else ""))
-
-
-def shortened_repr(value, l=30):
-    try:
-        if hasattr(value, '__repr__') and (value.__repr__ is not object.__repr__):
-            value_repr = repr(value)
-            if not value_repr.startswith('<') and len(value_repr) > l:
-                value_repr = "<<%s...>>" % (value_repr[:l - 8])
-            elif value_repr.startswith('<') and value_repr.endswith('>') and ' object at 0x':
-                raise ValueError("I hate those useless long reprs")
-        else:
-            raise ValueError("gimme class")
-    except Exception as e:
-        value_repr = "<%s>" % value.__class__.__name__.split('.')[-1]
-    return value_repr
-
-
-def __auto_repr__(obj):
-    attr_names = tuple()
-    if hasattr(obj, '__dict__'):
-        attr_names += tuple(obj.__dict__.keys())
-    if hasattr(obj, '__slots__'):
-        attr_names += tuple(obj.__slots__)
-
-    items = []
-    for attr in sorted(set(attr_names)):
-        if attr.startswith('_'):
+    Returns:
+    --------
+    dict, set
+      Configuration items are returned as key/value pairs in a dictionary.
+      The second tuple-item will be a set of identifiers comprising all
+      source files/blobs, if origin information was included
+      in the dump (--show-origin). An empty set is returned otherwise.
+      For actual files a Path object is included in the set, for a git-blob
+      a Git blob ID prefixed with 'blob:' is reported.
+    """
+    cwd = Path(getpwd() if cwd is None else cwd)
+    dct = {}
+    fileset = set()
+    for line in dump.split('\0'):
+        # line is a null-delimited chunk
+        k = None
+        # in anticipation of output contamination, process within a loop
+        # where we can reject non syntax compliant pieces
+        while line:
+            if line.startswith('file:') or line.startswith('blob:'):
+                fileset.add(line)
+                break
+            if line.startswith('command line:'):
+                # no origin that we could as a pathobj
+                break
+            # try getting key/value pair from the present chunk
+            k, v = _gitcfg_rec_to_keyvalue(line)
+            if k is not None:
+                # we are done with this chunk when there is a good key
+                break
+            # discard the first line and start over
+            ignore, line = line.split('\n', maxsplit=1)
+            lgr.debug('Non-standard git-config output, ignoring: %s', ignore)
+        if not k:
+            # nothing else to log, all ignored dump was reported before
             continue
-        value = getattr(obj, attr)
-        # TODO:  should we add this feature to minimize some talktative reprs
-        # such as of URL?
-        #if value is None:
-        #    continue
-        items.append("%s=%s" % (attr, shortened_repr(value)))
-
-    return "%s(%s)" % (obj.__class__.__name__, ', '.join(items))
-
-
-def auto_repr(cls):
-    """Decorator for a class to assign it an automagic quick and dirty __repr__
-
-    It uses public class attributes to prepare repr of a class
-
-    Original idea: http://stackoverflow.com/a/27799004/1265472
-    """
-
-    cls.__repr__ = __auto_repr__
-    return cls
-
-
-def is_interactive():
-    """Return True if all in/outs are tty"""
-    # TODO: check on windows if hasattr check would work correctly and add value:
-    #
-    return sys.stdin.isatty() and sys.stdout.isatty() and sys.stderr.isatty()
-
-
-def md5sum(filename):
-    with open(filename, 'rb') as f:
-        return hashlib.md5(f.read()).hexdigest()
-
+        # multi-value reporting
+        present_v = dct.get(k, None)
+        if present_v is None or not multi_value:
+            dct[k] = v
+        else:
+            if isinstance(present_v, tuple):
+                dct[k] = present_v + (v,)
+            else:
+                dct[k] = (present_v, v)
+    # take blobs with verbatim markup
+    origin_blobs = set(f for f in fileset if f.startswith('blob:'))
+    # convert file specifications to Path objects with absolute paths
+    origin_paths = set(Path(f[5:]) for f in fileset if f.startswith('file:'))
+    origin_paths = set(f if f.is_absolute() else cwd / f for f in origin_paths)
+    return dct, origin_paths.union(origin_blobs)
 
-def sorted_files(dout):
-    """Return a (sorted) list of files under dout
-    """
-    return sorted(sum([[opj(r, f)[len(dout) + 1:] for f in files]
-                       for r, d, files in os.walk(dout)
-                       if not '.git' in r], []))
 
-_VCS_REGEX = '%s\.(?:git|gitattributes|svn|bzr|hg)(?:%s|$)' % (dirsep, dirsep)
-_DATALAD_REGEX = '%s\.(?:datalad)(?:%s|$)' % (dirsep, dirsep)
+# keep alias with previous name for now
+_parse_gitconfig_dump = parse_gitconfig_dump
 
 
-def find_files(regex, topdir=curdir, exclude=None, exclude_vcs=True, exclude_datalad=False, dirs=False):
-    """Generator to find files matching regex
+def _gitcfg_rec_to_keyvalue(rec):
+    """Helper for parse_gitconfig_dump()
 
     Parameters
     ----------
-    regex: basestring
-    exclude: basestring, optional
-      Matches to exclude
-    exclude_vcs:
-      If True, excludes commonly known VCS subdirectories.  If string, used
-      as regex to exclude those files (regex: `%r`)
-    exclude_datalad:
-      If True, excludes files known to be datalad meta-data files (e.g. under
-      .datalad/ subdirectory) (regex: `%r`)
-    topdir: basestring, optional
-      Directory where to search
-    dirs: bool, optional
-      Either to match directories as well as files
-    """
-
-    for dirpath, dirnames, filenames in os.walk(topdir):
-        names = (dirnames + filenames) if dirs else filenames
-        # TODO: might want to uniformize on windows to use '/'
-        paths = (opj(dirpath, name) for name in names)
-        for path in filter(re.compile(regex).search, paths):
-            path = path.rstrip(dirsep)
-            if exclude and re.search(exclude, path):
-                continue
-            if exclude_vcs and re.search(_VCS_REGEX, path):
-                continue
-            if exclude_datalad and re.search(_DATALAD_REGEX, path):
-                continue
-            yield path
-find_files.__doc__ %= (_VCS_REGEX, _DATALAD_REGEX)
+    rec: str
+      Key/value specification string
 
+    Returns
+    -------
+    str, str
+      Parsed key and value. Key and/or value could be None
+      if not syntax-compliant (former) or absent (latter).
+    """
+    kv_match = cfg_kv_regex.match(rec)
+    if kv_match:
+        k, v = kv_match.groups()
+    elif cfg_k_regex.match(rec):
+        # could be just a key without = value, which git treats as True
+        # if asked for a bool
+        k, v = rec, None
+    else:
+        # no value, no good key
+        k = v = None
+    return k, v
 
-def expandpath(path, force_absolute=True):
-    """Expand all variables and user handles in a path.
-
-    By default return an absolute path
-    """
-    path = expandvars(expanduser(path))
-    if force_absolute:
-        path = abspath(path)
-    return path
-
-
-def posix_relpath(path, start=None):
-    """Behave like os.path.relpath, but always return POSIX paths...
-
-    on any platform."""
-    # join POSIX style
-    return posixpath.join(
-        # split and relpath native style
-        # python2.7 ntpath implementation of relpath cannot handle start=None
-        *psplit(
-            relpath(path, start=start if start is not None else '')))
-
-
-def is_explicit_path(path):
-    """Return whether a path explicitly points to a location
-
-    Any absolute path, or relative path starting with either '../' or
-    './' is assumed to indicate a location on the filesystem. Any other
-    path format is not considered explicit."""
-    path = expandpath(path, force_absolute=False)
-    return isabs(path) \
-        or path.startswith(os.curdir + os.sep) \
-        or path.startswith(os.pardir + os.sep)
 
+def _update_from_env(store):
+    overrides = {}
+    dct = {}
+    for k in os.environ:
+        if k == "DATALAD_CONFIG_OVERRIDES_JSON":
+            try:
+                overrides = json.loads(os.environ[k])
+            except json.decoder.JSONDecodeError as exc:
+                lgr.warning("Failed to load DATALAD_CONFIG_OVERRIDES_JSON: %s",
+                            exc)
+        elif k.startswith('DATALAD_'):
+            dct[k.replace('__', '-').replace('_', '.').lower()] = os.environ[k]
+    if overrides:
+        store.update(overrides)
+    store.update(dct)
 
-def rotree(path, ro=True, chmod_files=True):
-    """To make tree read-only or writable
 
-    Parameters
-    ----------
-    path : string
-      Path to the tree/directory to chmod
-    ro : bool, optional
-      Either to make it R/O (default) or RW
-    chmod_files : bool, optional
-      Either to operate also on files (not just directories)
-    """
-    if ro:
-        chmod = lambda f: os.chmod(f, os.stat(f).st_mode & ~stat.S_IWRITE)
+def anything2bool(val):
+    if hasattr(val, 'lower'):
+        val = val.lower()
+    if val in {"off", "no", "false", "0"} or not bool(val):
+        return False
+    elif val in {"on", "yes", "true", True} \
+            or (hasattr(val, 'isdigit') and val.isdigit() and int(val)) \
+            or isinstance(val, int) and val:
+        return True
     else:
-        chmod = lambda f: os.chmod(f, os.stat(f).st_mode | stat.S_IWRITE | stat.S_IREAD)
-
-    for root, dirs, files in os.walk(path, followlinks=False):
-        if chmod_files:
-            for f in files:
-                fullf = opj(root, f)
-                # might be the "broken" symlink which would fail to stat etc
-                if exists(fullf):
-                    chmod(fullf)
-        chmod(root)
-
-
-def rmtree(path, chmod_files='auto', *args, **kwargs):
-    """To remove git-annex .git it is needed to make all files and directories writable again first
+        raise TypeError(
+            "Got value %s which could not be interpreted as a boolean"
+            % repr(val))
+
+
+class ConfigManager(object):
+    """Thin wrapper around `git-config` with support for a dataset configuration.
+
+    The general idea is to have an object that is primarily used to read/query
+    configuration option.  Upon creation, current configuration is read via one
+    (or max two, in the case of the presence of dataset-specific configuration)
+    calls to `git config`.  If this class is initialized with a Dataset
+    instance, it supports reading and writing configuration from
+    ``.datalad/config`` inside a dataset too. This file is committed to Git and
+    hence useful to ship certain configuration items with a dataset.
+
+    The API aims to provide the most significant read-access API of a
+    dictionary, the Python ConfigParser, and GitPython's config parser
+    implementations.
+
+    This class is presently not capable of efficiently writing multiple
+    configurations items at once.  Instead, each modification results in a
+    dedicated call to `git config`. This author thinks this is OK, as he
+    cannot think of a situation where a large number of items need to be
+    written during normal operation.
+
+    Each instance carries a public `overrides` attribute. This dictionary
+    contains variables that override any setting read from a file. The overrides
+    are persistent across reloads.
+
+    Any DATALAD_* environment variable is also presented as a configuration
+    item. Settings read from environment variables are not stored in any of the
+    configuration files, but are read dynamically from the environment at each
+    `reload()` call. Their values take precedence over any specification in
+    configuration files, and even overrides.
 
     Parameters
     ----------
-    chmod_files : string or bool, optional
-       Either to make files writable also before removal.  Usually it is just
-       a matter of directories to have write permissions.
-       If 'auto' it would chmod files on windows by default
-    `*args` :
-    `**kwargs` :
-       Passed into shutil.rmtree call
-    """
-    # Give W permissions back only to directories, no need to bother with files
-    if chmod_files == 'auto':
-        chmod_files = on_windows
-
-    if not (os.path.islink(path) or not os.path.isdir(path)):
-        rotree(path, ro=False, chmod_files=chmod_files)
-        shutil.rmtree(path, *args, **kwargs)
-    else:
-        # just remove the symlink
-        os.unlink(path)
-
-
-def rmtemp(f, *args, **kwargs):
-    """Wrapper to centralize removing of temp files so we could keep them around
+    dataset : Dataset, optional
+      If provided, all `git config` calls are executed in this dataset's
+      directory. Moreover, any modifications are, by default, directed to
+      this dataset's configuration file (which will be created on demand)
+    overrides : dict, optional
+      Variable overrides, see general class documentation for details.
+    source : {'any', 'local', 'branch', 'branch-local'}, optional
+      Which sources of configuration setting to consider. If 'branch',
+      configuration items are only read from a dataset's persistent
+      configuration file in current branch, if any is present
+      (the one in ``.datalad/config``, not
+      ``.git/config``); if 'local', any non-committed source is considered
+      (local and global configuration in Git config's terminology);
+      if 'branch-local', persistent configuration in current dataset branch
+      and local, but not global or system configuration are considered; if 'any'
+      all possible sources of configuration are considered.
+      Note: 'dataset' and 'dataset-local' are deprecated in favor of 'branch'
+      and 'branch-local'.
+    """
+    # Lock for running changing operation across multiple threads.
+    # Since config itself to the same path could
+    # potentially be created independently in multiple threads, and we might be
+    # modifying global config as well, making lock static should not allow more than
+    # one thread to  write at a time, even if to different repositories.
+    _run_lock = threading.Lock()
+
+    def __init__(self, dataset=None, overrides=None, source='any'):
+        # TODO: remove along with the removal of deprecated 'where'
+        if source in ('dataset', 'dataset-local'):
+            source_new = source.replace('dataset', 'branch')
+            warnings.warn("'source=\"%s\"' is deprecated, use 'source=\"%s\"' instead"
+                          % (source, source_new),
+                          DeprecationWarning)
+            source = source_new
+
+        if source not in ('any', 'local', 'branch', 'branch-local'):
+            raise ValueError(
+                'Unknown ConfigManager(source=) setting: {}'.format(source))
+        store = dict(
+            # store in a simple dict
+            # no subclassing, because we want to be largely read-only, and implement
+            # config writing separately
+            cfg={},
+            # track the files that jointly make up the config in this store
+            files=set(),
+            # and their modification times to be able to avoid needless unforced reloads
+            stats=None,
+        )
+        self._stores = dict(
+            # populated with info from git
+            git=store,
+            # only populated with info from committed dataset config
+            branch=store.copy(),
+        )
+        # merged representation (the only one that existed pre datalad 0.14)
+        # will be built on initial reload
+        self._merged_store = {}
+
+        self._repo_dot_git = None
+        self._repo_pathobj = None
+        if dataset:
+            if hasattr(dataset, 'dot_git'):
+                # `dataset` is actually a Repo instance
+                self._repo_dot_git = dataset.dot_git
+                self._repo_pathobj = dataset.pathobj
+            elif dataset.repo:
+                self._repo_dot_git = dataset.repo.dot_git
+                self._repo_pathobj = dataset.repo.pathobj
+
+        self._config_cmd = ['git', 'config']
+        # public dict to store variables that always override any setting
+        # read from a file
+        # `hasattr()` is needed because `datalad.cfg` is generated upon first module
+        # import, hence when this code runs first, there cannot be any config manager
+        # to inherit from
+        self.overrides = datalad.cfg.overrides.copy() if hasattr(datalad, 'cfg') else {}
+        if overrides is not None:
+            self.overrides.update(overrides)
+        if dataset is None:
+            if source in ('branch', 'branch-local'):
+                raise ValueError(
+                    'ConfigManager configured to read from a branch of a dataset only, '
+                    'but no dataset given')
+            # The caller didn't specify a repository. Unfortunately there is
+            # no known way to tell git to ignore possible local git repository,
+            # and unsetting of --git-dir could cause other problems.
+            # See https://lore.kernel.org/git/YscCKuoDPBbs4iPX@lena.dartmouth.edu/T/ .
+            # Setting the git directory to /dev/null or on Windows analogous nul file
+            # (could be anywhere, see https://stackoverflow.com/a/27773642/1265472)
+            # see allow to achieve the goal to prevent a repository in the current
+            # working directory from leaking configuration into the output.
+            nul = 'b:\\nul' if on_windows else '/dev/null'
+            self._config_cmd = ['git', f'--git-dir={nul}', 'config']
+
+        self._src_mode = source
+        run_kwargs = dict()
+        self._runner = None
+        if dataset is not None:
+            if hasattr(dataset, '_git_runner'):
+                # `dataset` is actually a Repo instance
+                self._runner = dataset._git_runner
+            elif dataset.repo:
+                self._runner = dataset.repo._git_runner
+            else:
+                # make sure we run the git config calls in the dataset
+                # to pick up the right config files
+                run_kwargs['cwd'] = dataset.path
+        if self._runner is None:
+            self._runner = GitRunner(**run_kwargs)
+
+        self.reload(force=True)
+
+    def reload(self, force=False):
+        """Reload all configuration items from the configured sources
+
+        If `force` is False, all files configuration was previously read from
+        are checked for differences in the modification times. If no difference
+        is found for any file no reload is performed. This mechanism will not
+        detect newly created global configuration files, use `force` in this case.
+        """
+        run_args = ['-z', '-l', '--show-origin']
 
-    It will not remove the temporary file/directory if DATALAD_TESTS_TEMP_KEEP
-    environment variable is defined
-    """
-    if not os.environ.get('DATALAD_TESTS_TEMP_KEEP'):
-        if not os.path.lexists(f):
-            lgr.debug("Path %s does not exist, so can't be removed" % f)
-            return
-        lgr.log(5, "Removing temp file: %s" % f)
-        # Can also be a directory
-        if os.path.isdir(f):
-            rmtree(f, *args, **kwargs)
-        else:
-            # on windows boxes there is evidence for a latency of
-            # more than a second until a file is considered no
-            # longer "in-use"
-            # WindowsError is not known on Linux, and if IOError
-            # or any other exception is thrown then if except
-            # statement has WindowsError in it -- NameError
-            exceptions = (OSError, WindowsError) if on_windows else OSError
-            for i in range(50):
+        # update from desired config sources only
+        # 2-step strategy:
+        #   - load datalad dataset config from dataset
+        #   - load git config from all supported by git sources
+        # in doing so we always stay compatible with where Git gets its
+        # config from, but also allow to override persistent information
+        # from dataset locally or globally
+
+        # figure out what needs to be reloaded at all
+        to_run = {}
+        # committed branch config
+        # well, actually not necessarily committed
+
+        if self._src_mode != 'local' and self._repo_pathobj:
+            # we have to read the branch config from this existing repo
+            if self._repo_dot_git == self._repo_pathobj:
+                # this is a bare repo, we go with the default HEAD,
+                # if it has a config
                 try:
-                    os.unlink(f)
-                except exceptions:
-                    if i < 49:
-                        sleep(0.1)
-                        continue
-                    else:
-                        raise
-                break
-    else:
-        lgr.info("Keeping temp file: %s" % f)
-
-
-def file_basename(name, return_ext=False):
-    """
-    Strips up to 2 extensions of length up to 4 characters and starting with alpha
-    not a digit, so we could get rid of .tar.gz etc
-    """
-    bname = basename(name)
-    fbname = re.sub('(\.[a-zA-Z_]\S{1,4}){0,2}$', '', bname)
-    if return_ext:
-        return fbname, bname[len(fbname) + 1:]
-    else:
-        return fbname
-
-
-def escape_filename(filename):
-    """Surround filename in "" and escape " in the filename
-    """
-    filename = filename.replace('"', r'\"').replace('`', r'\`')
-    filename = '"%s"' % filename
-    return filename
-
-
-def encode_filename(filename):
-    """Encode unicode filename
-    """
-    if isinstance(filename, text_type):
-        return filename.encode(sys.getfilesystemencoding())
-    else:
-        return filename
-
-
-def decode_input(s):
-    """Given input string/bytes, decode according to stdin codepage (or UTF-8)
-    if not defined
-
-    If fails -- issue warning and decode allowing for errors
-    being replaced
-    """
-    if isinstance(s, text_type):
-        return s
-    else:
-        encoding = sys.stdin.encoding or 'UTF-8'
-        try:
-            return s.decode(encoding)
-        except UnicodeDecodeError as exc:
-            lgr.warning(
-                "Failed to decode input string using %s encoding. "
-                "Decoding allowing for errors", encoding)
-            return s.decode(encoding, errors='replace')
+                    # will blow if absent
+                    self._runner.run([
+                        'git', 'cat-file', '-e', 'HEAD:.datalad/config'],
+                        protocol=KillOutput)
+                    to_run['branch'] = run_args + [
+                        '--blob', 'HEAD:.datalad/config']
+                except CommandError:
+                    # all good, just no branch config
+                    pass
+            else:
+                # non-bare repo
+                # we could use the same strategy as for bare repos, and rely
+                # on the committed config, however, for now we must pay off
+                # the sins of the past and work with the file at hand
+                dataset_cfgfile = self._repo_pathobj / DATASET_CONFIG_FILE
+                if dataset_cfgfile.exists() and (
+                        force or self._need_reload(self._stores['branch'])):
+                    # we have the file and are forced or encourages to (re)load
+                    to_run['branch'] = run_args + [
+                        '--file', str(dataset_cfgfile)]
+
+        if self._src_mode != 'branch' and (
+                force or self._need_reload(self._stores['git'])):
+            to_run['git'] = run_args + ['--local'] \
+                if self._src_mode == 'branch-local' \
+                else run_args
+
+        # reload everything that was found todo
+        while to_run:
+            store_id, runargs = to_run.popitem()
+            self._stores[store_id] = self._reload(runargs)
+
+        # always update the merged representation, even if we did not reload
+        # anything from a file. ENV or overrides could change independently
+        # start with the commit dataset config
+        merged = self._stores['branch']['cfg'].copy()
+        # local config always takes precedence
+        merged.update(self._stores['git']['cfg'])
+        # superimpose overrides
+        merged.update(self.overrides)
+        # override with environment variables, unless we only want to read the
+        # dataset's commit config
+        if self._src_mode != 'branch':
+            _update_from_env(merged)
+        self._merged_store = merged
+
+    def _need_reload(self, store):
+        storestats = store['stats']
+        if not storestats:
+            return True
 
+        # we have read files before
+        # check if any file we read from has changed
+        curstats = self._get_stats(store)
+        return any(curstats[f] != storestats[f] for f in store['files'])
+
+    def _reload(self, run_args):
+        # query git-config
+        stdout, stderr = self._run(
+            run_args,
+            protocol=StdOutErrCapture,
+            # always expect git-config to output utf-8
+            encoding='utf-8',
+        )
+        store = {}
+        store['cfg'], store['files'] = parse_gitconfig_dump(
+            stdout, cwd=self._runner.cwd)
+
+        # update stats of config files, they have just been discovered
+        # and should still exist
+        store['stats'] = self._get_stats(store)
+        return store
+
+    def _get_stats(self, store):
+        stats = {}
+        for f in store['files']:
+            if isinstance(f, Path):
+                if f.exists():
+                    stat = f.stat()
+                    stats[f] = _stat_result(
+                        stat.st_ino,
+                        stat.st_size,
+                        stat.st_ctime,
+                        stat.st_mtime)
+                else:
+                    stats[f] = None
+            elif f.startswith('blob:'):
+                # we record the specific shasum of the blob
+                stats[f] = self._runner.run(
+                    ['git', 'rev-parse', f[5:]],
+                    protocol=StdOutErrCapture)['stdout'].strip()
+            else:
+                stats[f] = None
+        return stats
 
-if on_windows:
-    def lmtime(filepath, mtime):
-        """Set mtime for files.  On Windows a merely adapter to os.utime
+    @_scope_reload
+    @_where_to_scope
+    def obtain(self, var, default=None, dialog_type=None, valtype=None,
+               store=False, scope=None, reload=True, **kwargs):
         """
-        os.utime(filepath, (time.time(), mtime))
-else:
-    def lmtime(filepath, mtime):
-        """Set mtime for files, while not de-referencing symlinks.
+        Convenience method to obtain settings interactively, if needed
 
-        To overcome absence of os.lutime
-
-        Works only on linux and OSX ATM
+        A UI will be used to ask for user input in interactive sessions.
+        Questions to ask, and additional explanations can be passed directly
+        as arguments, or retrieved from a list of pre-configured items.
+
+        Additionally, this method allows for type conversion and storage
+        of obtained settings. Both aspects can also be pre-configured.
+
+        Parameters
+        ----------
+        var : str
+          Variable name including any section like `git config` expects them,
+          e.g. 'core.editor'
+        default : any type
+          In interactive sessions and if `store` is True, this default value
+          will be presented to the user for confirmation (or modification).
+          In all other cases, this value will be silently assigned unless
+          there is an existing configuration setting.
+        dialog_type : {'question', 'yesno', None}
+          Which dialog type to use in interactive sessions. If `None`,
+          pre-configured UI options are used.
+        store : bool
+          Whether to store the obtained value (or default)
+        %s
+        `**kwargs`
+          Additional arguments for the UI function call, such as a question
+          `text`.
         """
-        from .cmd import Runner
-        # convert mtime to format touch understands [[CC]YY]MMDDhhmm[.SS]
-        smtime = time.strftime("%Y%m%d%H%M.%S", time.localtime(mtime))
-        lgr.log(3, "Setting mtime for %s to %s == %s", filepath, mtime, smtime)
-        Runner().run(['touch', '-h', '-t', '%s' % smtime, filepath])
-        rfilepath = realpath(filepath)
-        if islink(filepath) and exists(rfilepath):
-            # trust noone - adjust also of the target file
-            # since it seemed like downloading under OSX (was it using curl?)
-            # didn't bother with timestamps
-            lgr.log(3, "File is a symlink to %s Setting mtime for it to %s",
-                    rfilepath, mtime)
-            os.utime(rfilepath, (time.time(), mtime))
-        # doesn't work on OSX
-        # Runner().run(['touch', '-h', '-d', '@%s' % mtime, filepath])
-
-
-def assure_tuple_or_list(obj):
-    """Given an object, wrap into a tuple if not list or tuple
-    """
-    if isinstance(obj, (list, tuple)):
-        return obj
-    return (obj,)
-
-
-def assure_list(s, copy=False, iterate=True):
-    """Given not a list, would place it into a list. If None - empty list is returned
-
-    Parameters
-    ----------
-    s: list or anything
-    copy: bool, optional
-      If list is passed, it would generate a shallow copy of the list
-    iterate: bool, optional
-      If it is not a list, but something iterable (but not a text_type)
-      iterate over it.
-    """
-
-    if isinstance(s, list):
-        return s if not copy else s[:]
-    elif isinstance(s, text_type):
-        return [s]
-    elif iterate and hasattr(s, '__iter__'):
-        return list(s)
-    elif s is None:
-        return []
-    else:
-        return [s]
-
-
-def assure_list_from_str(s, sep='\n'):
-    """Given a multiline string convert it to a list of return None if empty
-
-    Parameters
-    ----------
-    s: str or list
-    """
-
-    if not s:
-        return None
-
-    if isinstance(s, list):
-        return s
-    return s.split(sep)
-
-
-def assure_dict_from_str(s, **kwargs):
-    """Given a multiline string with key=value items convert it to a dictionary
-
-    Parameters
-    ----------
-    s: str or dict
-
-    Returns None if input s is empty
-    """
-
-    if not s:
-        return None
-
-    if isinstance(s, dict):
-        return s
-
-    out = {}
-    for value_str in assure_list_from_str(s, **kwargs):
-        if '=' not in value_str:
-            raise ValueError("{} is not in key=value format".format(repr(value_str)))
-        k, v = value_str.split('=', 1)
-        if k in out:
-            err = "key {} was already defined in {}, but new value {} was provided".format(k, out, v)
-            raise ValueError(err)
-        out[k] = v
-    return out
-
-
-def assure_unicode(s, encoding='utf-8'):
-    """Convert/decode to unicode (PY2) or str (PY3) if of 'binary_type'"""
-    return s.decode(encoding) if isinstance(s, binary_type) else s
-
-def assure_bool(s):
-    """Convert value into boolean following convention for strings
-
-    to recognize on,True,yes as True, off,False,no as False
-    """
-    if isinstance(s, string_types):
-        if s.isdigit():
-            return bool(int(s))
-        sl = s.lower()
-        if sl in {'y', 'yes', 'true', 'on'}:
-            return True
-        elif sl in {'n', 'no', 'false', 'off'}:
-            return False
-        else:
-            raise ValueError("Do not know how to treat %r as a boolean" % s)
-    return bool(s)
-
-
-def unique(seq, key=None):
-    """Given a sequence return a list only with unique elements while maintaining order
-
-    This is the fastest solution.  See
-    https://www.peterbe.com/plog/uniqifiers-benchmark
-    and
-    http://stackoverflow.com/a/480227/1265472
-    for more information.
-    Enhancement -- added ability to compare for uniqueness using a key function
-
-    Parameters
-    ----------
-    seq:
-      Sequence to analyze
-    key: callable, optional
-      Function to call on each element so we could decide not on a full
-      element, but on its member etc
-    """
-    seen = set()
-    seen_add = seen.add
-    if not key:
-        return [x for x in seq if not (x in seen or seen_add(x))]
-    else:
-        # OPT: could be optimized, since key is called twice, but for our cases
-        # should be just as fine
-        return [x for x in seq if not (key(x) in seen or seen_add(key(x)))]
-
-
-def generate_chunks(container, size):
-    """Given a container, generate chunks from it with size up to `size`
-    """
-    # There could be a "smarter" solution but I think this would suffice
-    assert size > 0,  "Size should be non-0 positive"
-    while container:
-        yield container[:size]
-        container = container[size:]
-
-#
-# Generators helpers
-#
-
-def saved_generator(gen):
-    """Given a generator returns two generators, where 2nd one just replays
-
-    So the first one would be going through the generated items and 2nd one
-    would be yielding saved items
-    """
-    saved = []
-
-    def gen1():
-        for x in gen:  # iterating over original generator
-            saved.append(x)
-            yield x
-
-    def gen2():
-        for x in saved:  # yielding saved entries
-            yield x
-
-    return gen1(), gen2()
-
+        # do local import, as this module is import prominently and the
+        # could theoretically import all kind of weird things for type
+        # conversion
+        from datalad.interface.common_cfg import definitions as cfg_defs
+
+        # fetch what we know about this variable
+        cdef = cfg_defs.get(var, {})
+        # type conversion setup
+        if valtype is None and 'type' in cdef:
+            valtype = cdef['type']
+        if valtype is None:
+            valtype = lambda x: x
+
+        # any default?
+        if default is None and 'default' in cdef:
+            default = cdef['default']
+
+        _value = None
+        if var in self:
+            # nothing needs to be obtained, it is all here already
+            _value = self[var]
+        elif store is False and default is not None:
+            # nothing will be stored, and we have a default -> no user confirmation
+            # we cannot use logging, because we want to use the config to configure
+            # the logging
+            #lgr.debug('using default {} for config setting {}'.format(default, var))
+            _value = default
 
-#
-# Decorators
-#
-def better_wraps(to_be_wrapped):
-    """Decorator to replace `functools.wraps`
-
-    This is based on `wrapt` instead of `functools` and in opposition to `wraps`
-    preserves the correct signature of the decorated function.
-    It is written with the intention to replace the use of `wraps` without any
-    need to rewrite the actual decorators.
-    """
-
-    @wrapt.decorator(adapter=to_be_wrapped)
-    def intermediator(to_be_wrapper, instance, args, kwargs):
-        return to_be_wrapper(*args, **kwargs)
-
-    return intermediator
-
-
-# Borrowed from pandas
-# Copyright: 2011-2014, Lambda Foundry, Inc. and PyData Development Team
-# License: BSD-3
-def optional_args(decorator):
-    """allows a decorator to take optional positional and keyword arguments.
-        Assumes that taking a single, callable, positional argument means that
-        it is decorating a function, i.e. something like this::
-
-            @my_decorator
-            def function(): pass
-
-        Calls decorator with decorator(f, `*args`, `**kwargs`)"""
-
-    @better_wraps(decorator)
-    def wrapper(*args, **kwargs):
-        def dec(f):
-            return decorator(f, *args, **kwargs)
-
-        is_decorating = not kwargs and len(args) == 1 and isinstance(args[0], collections.Callable)
-        if is_decorating:
-            f = args[0]
-            args = []
-            return dec(f)
-        else:
-            return dec
-
-    return wrapper
-
-
-# TODO: just provide decorators for tempfile.mk* functions. This is ugly!
-def get_tempfile_kwargs(tkwargs=None, prefix="", wrapped=None):
-    """Updates kwargs to be passed to tempfile. calls depending on env vars
-    """
-    if tkwargs is None:
-        tkwargs_ = {}
-    else:
-        # operate on a copy of tkwargs to avoid any side-effects
-        tkwargs_ = tkwargs.copy()
-
-    # TODO: don't remember why I had this one originally
-    # if len(targs)<2 and \
-    if 'prefix' not in tkwargs_:
-        tkwargs_['prefix'] = '_'.join(
-            ['datalad_temp'] +
-            ([prefix] if prefix else []) +
-            ([''] if (on_windows or not wrapped) else [wrapped.__name__]))
-
-    directory = os.environ.get('DATALAD_TESTS_TEMP_DIR')
-    if directory and 'dir' not in tkwargs_:
-        tkwargs_['dir'] = directory
-
-    return tkwargs_
-
-
-@optional_args
-def line_profile(func):
-    """Q&D helper to line profile the function and spit out stats
-    """
-    import line_profiler
-    prof = line_profiler.LineProfiler()
+        if _value is not None:
+            # we got everything we need and can exit early
+            try:
+                return valtype(_value)
+            except Exception as e:
+                raise ValueError(
+                    "value '{}' of existing configuration for '{}' cannot be "
+                    "converted to the desired type '{}' ({})".format(
+                        _value, var, valtype, e)) from e
+
+        # now we need to try to obtain something from the user
+        from datalad.ui import ui
+
+        # configure UI
+        dialog_opts = kwargs
+        if dialog_type is None:  # no override
+            # check for common knowledge on how to obtain a value
+            if 'ui' in cdef:
+                dialog_type = cdef['ui'][0]
+                # pull standard dialog settings
+                dialog_opts = cdef['ui'][1]
+                # update with input
+                dialog_opts.update(kwargs)
+
+        if (not ui.is_interactive or dialog_type is None) and default is None:
+            raise RuntimeError(
+                "cannot obtain value for configuration item '{}', "
+                "not preconfigured, no default, no UI available".format(var))
+
+        if not hasattr(ui, dialog_type):
+            raise ValueError("UI '{}' does not support dialog type '{}'".format(
+                ui, dialog_type))
+
+        # configure storage destination, if needed
+        if store:
+            if scope is None and 'destination' in cdef:
+                scope = cdef['destination']
+            if scope is None:
+                raise ValueError(
+                    "request to store configuration item '{}', but no "
+                    "storage destination specified".format(var))
+
+        # obtain via UI
+        dialog = getattr(ui, dialog_type)
+        _value = dialog(default=default, **dialog_opts)
+
+        if _value is None:
+            # we got nothing
+            if default is None:
+                raise RuntimeError(
+                    "could not obtain value for configuration item '{}', "
+                    "not preconfigured, no default".format(var))
+            # XXX maybe we should return default here, even it was returned
+            # from the UI -- if that is even possible
 
-    @wraps(func)
-    def newfunc(*args, **kwargs):
+        # execute type conversion before storing to check that we got
+        # something that looks like what we want
         try:
-            pfunc = prof(func)
-            return pfunc(*args, **kwargs)
-        finally:
-            prof.print_stats()
-    return newfunc
+            value = valtype(_value)
+        except Exception as e:
+            raise ValueError(
+                "cannot convert user input `{}` to desired type ({})".format(
+                    _value, e)) from e
+            # XXX we could consider "looping" until we have a value of proper
+            # type in case of a user typo...
+
+        if store:
+            # store value as it was before any conversion, needs to be str
+            # anyway
+            # needs string conversion nevertheless, because default could come
+            # in as something else
+            self.add(var, '{}'.format(_value), scope=scope, reload=reload)
+        return value
+
+    def __repr__(self):
+        # give full list of all tracked config files, plus overrides
+        return "ConfigManager({}{})".format(
+            [str(p) for p in self._stores['branch']['files'].union(
+                self._stores['git']['files'])],
+            ', overrides={!r}'.format(self.overrides) if self.overrides else '',
+        )
+
+    def __str__(self):
+        # give path of dataset, if there is any, plus overrides
+        return "ConfigManager({}{})".format(
+            self._repo_pathobj if self._repo_pathobj else '',
+            'with overrides' if self.overrides else '',
+        )
 
-#
-# Context Managers
-#
+    #
+    # Compatibility with dict API
+    #
+    def __len__(self):
+        return len(self._merged_store)
 
+    def __getitem__(self, key):
+        return self._merged_store.__getitem__(key)
 
-@contextmanager
-def nothing_cm():
-    """Just a dummy cm to programmically switch context managers"""
-    yield
-
-
-@contextmanager
-def swallow_outputs():
-    """Context manager to help consuming both stdout and stderr, and print()
-
-    stdout is available as cm.out and stderr as cm.err whenever cm is the
-    yielded context manager.
-    Internally uses temporary files to guarantee absent side-effects of swallowing
-    into StringIO which lacks .fileno.
-
-    print mocking is necessary for some uses where sys.stdout was already bound
-    to original sys.stdout, thus mocking it later had no effect. Overriding
-    print function had desired effect
-    """
+    def __contains__(self, key):
+        return self._merged_store.__contains__(key)
 
-    class StringIOAdapter(object):
-        """Little adapter to help getting out/err values
+    def keys(self):
+        """Returns list of configuration item names"""
+        return self._merged_store.keys()
+
+    # XXX should this be *args?
+    def get(self, key, default=None, get_all=False):
+        """D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.
+
+        Parameters
+        ----------
+        default : optional
+          Value to return when key is not present. `None` by default.
+        get_all : bool, optional
+          If True, return all values of multiple identical configuration keys.
+          By default only the last specified value is returned.
         """
-        def __init__(self):
-            kw = get_tempfile_kwargs({}, prefix="outputs")
-
-            self._out = open(tempfile.mktemp(**kw), 'w')
-            self._err = open(tempfile.mktemp(**kw), 'w')
-
-        def _read(self, h):
-            with open(h.name) as f:
-                return f.read()
-
-        @property
-        def out(self):
-            if not self._out.closed:
-                self._out.flush()
-            return self._read(self._out)
-
-        @property
-        def err(self):
-            if not self._err.closed:
-                self._err.flush()
-            return self._read(self._err)
-
-        @property
-        def handles(self):
-            return self._out, self._err
-
-        def cleanup(self):
-            self._out.close()
-            self._err.close()
-            out_name = self._out.name
-            err_name = self._err.name
-            del self._out
-            del self._err
-            gc.collect()
-            rmtemp(out_name)
-            rmtemp(err_name)
-
-    def fake_print(*args, **kwargs):
-        sep = kwargs.pop('sep', ' ')
-        end = kwargs.pop('end', '\n')
-        file = kwargs.pop('file', sys.stdout)
-
-        if file in (oldout, olderr, sys.stdout, sys.stderr):
-            # we mock
-            sys.stdout.write(sep.join(args) + end)
+        try:
+            val = self[key]
+            if get_all or not isinstance(val, tuple):
+                return val
+            else:
+                return val[-1]
+        except KeyError:
+            # return as-is, default could be a tuple, hence do not subject to
+            # get_all processing
+            return default
+
+    def get_from_source(self, source, key, default=None):
+        """Like get(), but a source can be specific.
+
+        If `source` is 'branch', only the committed configuration is queried,
+        overrides are applied. In the case of 'local', the committed
+        configuration is ignored, but overrides and configuration from
+        environment variables are applied as usual.
+        """
+        # TODO: remove along with the removal of deprecated 'where'
+        if source == 'dataset':
+            warnings.warn("'source=\"%s\"' is deprecated, use 'source=\"%s\"' instead"
+                          % (source, 'branch'),
+                          DeprecationWarning)
+            source = 'branch'
+        if source not in ('branch', 'local'):
+            raise ValueError("source must be 'branch' or 'local'")
+        if source == 'branch':
+            return self.overrides.get(
+                key,
+                self._stores['branch']['cfg'].get(
+                    key,
+                    default))
         else:
-            # must be some other file one -- leave it alone
-            oldprint(*args, sep=sep, end=end, file=file)
-
-    from .ui import ui
-    # preserve -- they could have been mocked already
-    oldprint = getattr(__builtin__, 'print')
-    oldout, olderr = sys.stdout, sys.stderr
-    olduiout = ui.out
-    adapter = StringIOAdapter()
-
-    try:
-        sys.stdout, sys.stderr = adapter.handles
-        ui.out = adapter.handles[0]
-        setattr(__builtin__, 'print', fake_print)
-
-        yield adapter
-    finally:
-        sys.stdout, sys.stderr, ui.out = oldout, olderr, olduiout
-        setattr(__builtin__, 'print',  oldprint)
-        adapter.cleanup()
-
-
-@contextmanager
-def swallow_logs(new_level=None, file_=None, name='datalad'):
-    """Context manager to consume all logs.
+            if key not in self._stores['branch']['cfg']:
+                # the key is not in the committed config, hence we can
+                # just report based on the merged representation
+                return self.get(key, default)
+            else:
+                # expensive case, rebuild a config without the committed
+                # dataset config contributing
+                env = {}
+                _update_from_env(env)
+                return env.get(
+                    key,
+                    self.overrides.get(
+                        key,
+                        self._stores['git']['cfg'].get(
+                            key,
+                            default)))
 
-    """
-    lgr = logging.getLogger(name)
+    #
+    # Compatibility with ConfigParser API
+    #
+    def sections(self):
+        """Returns a list of the sections available"""
+        return list(set([cfg_section_regex.match(k).group(1) for k in self._merged_store]))
+
+    def options(self, section):
+        """Returns a list of options available in the specified section."""
+        opts = []
+        for k in self._merged_store:
+            sec, opt = cfg_sectionoption_regex.match(k).groups()
+            if sec == section:
+                opts.append(opt)
+        return opts
+
+    def has_section(self, section):
+        """Indicates whether a section is present in the configuration"""
+        for k in self._merged_store:
+            if k.startswith(section):
+                return True
+        return False
 
-    # Keep old settings
-    old_level = lgr.level
-    old_handlers = lgr.handlers
-
-    # Let's log everything into a string
-    # TODO: generalize with the one for swallow_outputs
-    class StringIOAdapter(object):
-        """Little adapter to help getting out values
+    def has_option(self, section, option):
+        """If the given section exists, and contains the given option"""
+        for k in self._merged_store:
+            sec, opt = cfg_sectionoption_regex.match(k).groups()
+            if sec == section and opt == option:
+                return True
+        return False
 
-        And to stay consistent with how swallow_outputs behaves
+    def _get_type(self, typefn, section, option):
+        key = '.'.join([section, option])
+        # Mimic the handling of get_value(..., default=None), while still going
+        # through get() in order to get its default tuple handling.
+        if key not in self:
+            raise KeyError(key)
+        return typefn(self.get(key))
+
+    def getint(self, section, option):
+        """A convenience method which coerces the option value to an integer"""
+        return self._get_type(int, section, option)
+
+    def getfloat(self, section, option):
+        """A convenience method which coerces the option value to a float"""
+        return self._get_type(float, section, option)
+
+    def getbool(self, section, option, default=None):
+        """A convenience method which coerces the option value to a bool
+
+        Values "on", "yes", "true" and any int!=0 are considered True
+        Values which evaluate to bool False, "off", "no", "false" are considered
+        False
+        TypeError is raised for other values.
         """
-        def __init__(self):
-            if file_ is None:
-                kw = get_tempfile_kwargs({}, prefix="logs")
-                out_file = tempfile.mktemp(**kw)
-            else:
-                out_file = file_
-            # PY3 requires clearly one or another.  race condition possible
-            self._out = open(out_file, 'a')
-            self._final_out = None
-
-        def _read(self, h):
-            with open(h.name) as f:
-                return f.read()
-
-        @property
-        def out(self):
-            if self._final_out is not None:
-                # we closed and cleaned up already
-                return self._final_out
-            else:
-                self._out.flush()
-                return self._read(self._out)
+        key = '.'.join([section, option])
+        # Mimic the handling of get_value(..., default=None), while still going
+        # through get() in order to get its default tuple handling.
+        if default is None and key not in self:
+            raise KeyError(key)
+        val = self.get(key, default=default)
+        if val is None:  # no value at all, git treats it as True
+            return True
+        return anything2bool(val)
 
-        @property
-        def lines(self):
-            return self.out.split('\n')
-
-        @property
-        def handle(self):
-            return self._out
-
-        def cleanup(self):
-            # store for access while object exists
-            self._final_out = self.out
-            self._out.close()
-            out_name = self._out.name
-            del self._out
-            gc.collect()
-            if not file_:
-                rmtemp(out_name)
-
-        def assert_logged(self, msg=None, level=None, regex=True, **kwargs):
-            """Provide assertion on either a msg was logged at a given level
-
-            If neither `msg` nor `level` provided, checks if anything was logged
-            at all.
-
-            Parameters
-            ----------
-            msg: str, optional
-              Message (as a regular expression, if `regex`) to be searched.
-              If no msg provided, checks if anything was logged at a given level.
-            level: str, optional
-              String representing the level to be logged
-            regex: bool, optional
-              If False, regular `assert_in` is used
-            **kwargs: str, optional
-              Passed to `assert_re_in` or `assert_in`
-            """
-            from datalad.tests.utils import assert_re_in
-            from datalad.tests.utils import assert_in
+    # this is a hybrid of ConfigParser and dict API
+    def items(self, section=None):
+        """Return a list of (name, value) pairs for each option
 
-            if regex:
-                match = '\[%s\] ' % level if level else "\[\S+\] "
-            else:
-                match = '[%s] ' % level if level else ''
+        Optionally limited to a given section.
+        """
+        if section is None:
+            return self._merged_store.items()
+        return [(k, v) for k, v in self._merged_store.items()
+                if cfg_section_regex.match(k).group(1) == section]
 
-            if msg:
-                match += msg
+    #
+    # Compatibility with GitPython's ConfigParser
+    #
+    def get_value(self, section, option, default=None):
+        """Like `get()`, but with an optional default value
 
-            if match:
-                (assert_re_in if regex else assert_in)(match, self.out, **kwargs)
+        If the default is not None, the given default value will be returned in
+        case the option did not exist. This behavior imitates GitPython's
+        config parser.
+        """
+        try:
+            return self['.'.join((section, option))]
+        except KeyError as e:
+            # this strange dance is needed because gitpython does it this way
+            if default is not None:
+                return default
             else:
-                assert not kwargs, "no kwargs to be passed anywhere"
-                assert self.out, "Nothing was logged!?"
-
-    adapter = StringIOAdapter()
-    # TODO: it does store messages but without any formatting, i.e. even without
-    # date/time prefix etc.  IMHO it should preserve formatting in case if file_ is
-    # set
-    swallow_handler = logging.StreamHandler(adapter.handle)
-    # we want to log levelname so we could test against it
-    swallow_handler.setFormatter(
-        logging.Formatter('[%(levelname)s] %(message)s'))
-    # Inherit filters
-    swallow_handler.filters = sum([h.filters for h in old_handlers], [])
-    lgr.handlers = [swallow_handler]
-    if old_level < logging.DEBUG:  # so if HEAVYDEBUG etc -- show them!
-        lgr.handlers += old_handlers
-
-    if isinstance(new_level, str):
-        new_level = getattr(logging, new_level)
-
-    if new_level is not None:
-        lgr.setLevel(new_level)
-
-    try:
-        yield adapter
-        # TODO: if file_ and there was an exception -- most probably worth logging it?
-        # although ideally it should be the next log outside added to that file_ ... oh well
-    finally:
-        lgr.handlers, lgr.level = old_handlers, old_level
-        adapter.cleanup()
-
-
-# TODO: May be melt in with swallow_logs at some point:
-@contextmanager
-def disable_logger(logger=None):
-    """context manager to temporarily disable logging
+                raise e
 
-    This is to provide one of swallow_logs' purposes without unnecessarily
-    creating temp files (see gh-1865)
-
-    Parameters
-    ----------
-    logger: Logger
-        Logger whose handlers will be ordered to not log anything.
-        Default: datalad's topmost Logger ('datalad')
-    """
-
-    class NullFilter(logging.Filter):
-        """Filter class to reject all records
+    #
+    # Modify configuration (proxy respective git-config call)
+    #
+    @_scope_reload
+    def _run(self, args, scope=None, reload=False, **kwargs):
+        """Centralized helper to run "git config" calls
+
+        Parameters
+        ----------
+        args : list
+          Arguments to pass for git config
+        %s
+        **kwargs
+          Keywords arguments for Runner's call
         """
-        def filter(self, record):
-            return 0
-
-    if logger is None:
-        # default: all of datalad's logging:
-        logger = logging.getLogger('datalad')
-
-    filter_ = NullFilter(logger.name)
-    [h.addFilter(filter_) for h in logger.handlers]
-
-    try:
-        yield logger
-    finally:
-        [h.removeFilter(filter_) for h in logger.handlers]
-
-
-#
-# Additional handlers
-#
-_sys_excepthook = sys.excepthook  # Just in case we ever need original one
-
-
-def setup_exceptionhook(ipython=False):
-    """Overloads default sys.excepthook with our exceptionhook handler.
-
-       If interactive, our exceptionhook handler will invoke
-       pdb.post_mortem; if not interactive, then invokes default handler.
-    """
-
-    def _datalad_pdb_excepthook(type, value, tb):
-        import traceback
-        traceback.print_exception(type, value, tb)
-        print()
-        if is_interactive():
-            import pdb
-            pdb.post_mortem(tb)
-
-    if ipython:
-        from IPython.core import ultratb
-        sys.excepthook = ultratb.FormattedTB(mode='Verbose',
-                                             # color_scheme='Linux',
-                                             call_pdb=is_interactive())
-    else:
-        sys.excepthook = _datalad_pdb_excepthook
-
-
-def assure_dir(*args):
-    """Make sure directory exists.
-
-    Joins the list of arguments to an os-specific path to the desired
-    directory and creates it, if it not exists yet.
-    """
-    dirname = opj(*args)
-    if not exists(dirname):
-        os.makedirs(dirname)
-    return dirname
-
-
-def updated(d, update):
-    """Return a copy of the input with the 'update'
-
-    Primarily for updating dictionaries
-    """
-    d = d.copy()
-    d.update(update)
-    return d
-
-
-def getpwd():
-    """Try to return a CWD without dereferencing possible symlinks
-
-    If no PWD found in the env, output of getcwd() is returned
-    """
-    try:
-        return os.environ['PWD']
-    except KeyError:
-        return os.getcwd()
-
-
-class chpwd(object):
-    """Wrapper around os.chdir which also adjusts environ['PWD']
+        if scope:
+            args = self._get_location_args(scope) + args
+        if '-l' in args:
+            # we are just reading, no need to reload, no need to lock
+            out = self._runner.run(self._config_cmd + args, **kwargs)
+            return out['stdout'], out['stderr']
+
+        # all other calls are modifications
+        if '--file' in args:
+            # all paths we are passing are absolute
+            custom_file = Path(args[args.index('--file') + 1])
+            custom_file.parent.mkdir(exist_ok=True)
+        lockfile = None
+        if self._repo_dot_git and ('--local' in args or '--file' in args):
+            # modification of config in a dataset
+            lockfile = self._repo_dot_git / 'config.dataladlock'
+        else:
+            # follow pattern in downloaders for lockfile location
+            lockfile = Path(self.obtain('datalad.locations.locks')) \
+                       / 'gitconfig.lck'
+
+        with ConfigManager._run_lock, InterProcessLock(lockfile, logger=lgr):
+            out = self._runner.run(self._config_cmd + args, **kwargs)
+
+        if reload:
+            self.reload()
+            # this function is only used to modify config. If any manager
+            # has modified the global scope, and is not itself the global
+            # manager, we reload that one too in order to avoid stale
+            # configuration reports
+            if scope == 'global' and self is not datalad.cfg:
+                datalad.cfg.reload()
+        return out['stdout'], out['stderr']
 
-    The reason is that otherwise PWD is simply inherited from the shell
-    and we have no ability to assess directory path without dereferencing
-    symlinks.
+    def _get_location_args(self, scope, args=None):
+        if args is None:
+            args = []
+        cfg_labels = ('branch', 'local', 'global', 'override')
+        if scope not in cfg_labels:
+            raise ValueError(
+                "unknown configuration label '{}' (not in {})".format(
+                    scope, cfg_labels))
+        if scope == 'branch':
+            if not self._repo_pathobj:
+                raise ValueError(
+                    'ConfigManager cannot store configuration to dataset, '
+                    'none specified')
+            dataset_cfgfile = self._repo_pathobj / DATASET_CONFIG_FILE
+            args.extend(['--file', str(dataset_cfgfile)])
+        elif scope == 'global':
+            args.append('--global')
+        elif scope == 'local':
+            args.append('--local')
+        return args
+
+    @_scope_reload
+    @_where_to_scope
+    def add(self, var, value, scope='branch', reload=True):
+        """Add a configuration variable and value
+
+        Parameters
+        ----------
+        var : str
+          Variable name including any section like `git config` expects them, e.g.
+          'core.editor'
+        value : str
+          Variable value
+        %s"""
+        if scope == 'override':
+            from datalad.utils import ensure_list
+            val = ensure_list(self.overrides.pop(var, None))
+            val.append(value)
+            self.overrides[var] = val[0] if len(val) == 1 else val
+            if reload:
+                self.reload(force=True)
+            return
 
-    If used as a context manager it allows to temporarily change directory
-    to the given path
-    """
-    def __init__(self, path, mkdir=False, logsuffix=''):
+        self._run(['--add', var, value], scope=scope, reload=reload,
+                  protocol=StdOutErrCapture)
 
-        if path:
-            pwd = getpwd()
-            self._prev_pwd = pwd
-        else:
-            self._prev_pwd = None
+    @_scope_reload
+    @_where_to_scope
+    def set(self, var, value, scope='branch', reload=True, force=False):
+        """Set a variable to a value.
+
+        In opposition to `add`, this replaces the value of `var` if there is
+        one already.
+
+        Parameters
+        ----------
+        var : str
+          Variable name including any section like `git config` expects them, e.g.
+          'core.editor'
+        value : str
+          Variable value
+        force: bool
+          if set, replaces all occurrences of `var` by a single one with the
+          given `value`. Otherwise raise if multiple entries for `var` exist
+          already
+        %s"""
+        if scope == 'override':
+            self.overrides[var] = value
+            if reload:
+                self.reload(force=True)
             return
 
-        if not isabs(path):
-            path = normpath(opj(pwd, path))
-        if not os.path.exists(path) and mkdir:
-            self._mkdir = True
-            os.mkdir(path)
-        else:
-            self._mkdir = False
-        lgr.debug("chdir %r -> %r %s", self._prev_pwd, path, logsuffix)
-        os.chdir(path)  # for grep people -- ok, to chdir here!
-        os.environ['PWD'] = path
-
-    def __enter__(self):
-        # nothing more to do really, chdir was in the constructor
-        pass
+        from datalad.support.gitrepo import to_options
 
-    def __exit__(self, exc_type, exc_val, exc_tb):
-        if self._prev_pwd:
-            # Need to use self.__class__ so this instance, if the entire
-            # thing mocked during the test, still would use correct chpwd
-            self.__class__(self._prev_pwd, logsuffix="(coming back)")
+        self._run(to_options(replace_all=force) + [var, value],
+                  scope=scope, reload=reload, protocol=StdOutErrCapture)
 
+    @_scope_reload
+    @_where_to_scope
+    def rename_section(self, old, new, scope='branch', reload=True):
+        """Rename a configuration section
+
+        Parameters
+        ----------
+        old : str
+          Name of the section to rename.
+        new : str
+          Name of the section to rename to.
+        %s"""
+        if scope == 'override':
+            self.overrides = {
+                (new + k[len(old):]) if k.startswith(old + '.') else k: v
+                for k, v in self.overrides.items()
+            }
+            if reload:
+                self.reload(force=True)
+            return
 
-def with_pathsep(path):
-    """Little helper to guarantee that path ends with /"""
-    return path + sep if not path.endswith(sep) else path
+        self._run(['--rename-section', old, new], scope=scope, reload=reload)
 
+    @_scope_reload
+    @_where_to_scope
+    def remove_section(self, sec, scope='branch', reload=True):
+        """Rename a configuration section
+
+        Parameters
+        ----------
+        sec : str
+          Name of the section to remove.
+        %s"""
+        if scope == 'override':
+            self.overrides = {
+                k: v
+                for k, v in self.overrides.items()
+                if not k.startswith(sec + '.')
+            }
+            if reload:
+                self.reload(force=True)
+            return
 
-def get_path_prefix(path, pwd=None):
-    """Get path prefix (for current directory)
+        self._run(['--remove-section', sec], scope=scope, reload=reload)
 
-    Returns relative path to the topdir, if we are under topdir, and if not
-    absolute path to topdir.  If `pwd` is not specified - current directory
-    assumed
-    """
-    pwd = pwd or getpwd()
-    if not isabs(path):
-        # if not absolute -- relative to pwd
-        path = opj(getpwd(), path)
-    path_ = with_pathsep(path)
-    pwd_ = with_pathsep(pwd)
-    common = commonprefix((path_, pwd_))
-    if common.endswith(sep) and common in {path_, pwd_}:
-        # we are in subdir or above the path = use relative path
-        location_prefix = relpath(path, pwd)
-        # if benign "here" - cut off
-        if location_prefix in (curdir, curdir + sep):
-            location_prefix = ''
-        return location_prefix
-    else:
-        # just return absolute path
-        return path
+    @_scope_reload
+    @_where_to_scope
+    def unset(self, var, scope='branch', reload=True):
+        """Remove all occurrences of a variable
+
+        Parameters
+        ----------
+        var : str
+          Name of the variable to remove
+        %s"""
+        if scope == 'override':
+            self.overrides.pop(var, None)
+            if reload:
+                self.reload(force=True)
+            return
 
+        # use unset all as it is simpler for now
+        self._run(['--unset-all', var], scope=scope, reload=reload)
 
-def _get_normalized_paths(path, prefix):
-    if isabs(path) != isabs(prefix):
-        raise ValueError("Bot paths must either be absolute or relative. "
-                         "Got %r and %r" % (path, prefix))
-    path = with_pathsep(path)
-    prefix = with_pathsep(prefix)
-    return path, prefix
 
+def rewrite_url(cfg, url):
+    """Any matching 'url.<base>.insteadOf' configuration is applied
 
-def path_startswith(path, prefix):
-    """Return True if path starts with prefix path
+    Any URL that starts with such a configuration will be rewritten
+    to start, instead, with <base>. When more than one insteadOf
+    strings match a given URL, the longest match is used.
 
     Parameters
     ----------
-    path: str
-    prefix: str
-    """
-    path, prefix = _get_normalized_paths(path, prefix)
-    return path.startswith(prefix)
-
+    cfg : ConfigManager or dict
+      dict-like with configuration variable name/value-pairs.
+    url : str
+      URL to be rewritten, if matching configuration is found.
 
-def path_is_subpath(path, prefix):
-    """Return True if path is a subpath of prefix
-
-    It will return False if path == prefix.
-
-    Parameters
-    ----------
-    path: str
-    prefix: str
+    Returns
+    -------
+    str
+      Rewritten or unmodified URL.
     """
-    path, prefix = _get_normalized_paths(path, prefix)
-    return (len(prefix) < len(path)) and path.startswith(prefix)
-
-
-def knows_annex(path):
-    """Returns whether at a given path there is information about an annex
+    insteadof = {
+        # only leave the base url
+        k[4:-10]: v
+        for k, v in cfg.items()
+        if k.startswith('url.') and k.endswith('.insteadof')
+    }
+
+    # all config that applies
+    matches = {
+        key: v
+        for key, val in insteadof.items()
+        for v in (val if isinstance(val, tuple) else (val,))
+        if url.startswith(v)
+    }
+    # find longest match, like Git does
+    if matches:
+        rewrite_base, match = sorted(
+            matches.items(),
+            key=lambda x: len(x[1]),
+            reverse=True,
+        )[0]
+        if sum(match == v for v in matches.values()) > 1:
+            lgr.warning(
+                "Ignoring URL rewrite configuration for '%s', "
+                "multiple conflicting definitions exists: %s",
+                match,
+                ['url.{}.insteadof'.format(k)
+                 for k, v in matches.items()
+                 if v == match]
+            )
+        else:
+            url = '{}{}'.format(rewrite_base, url[len(match):])
+    return url
 
-    It is just a thin wrapper around GitRepo.is_with_annex() classmethod
-    which also checks for `path` to exist first.
 
-    This includes actually present annexes, but also uninitialized ones, or
-    even the presence of a remote annex branch.
-    """
-    from os.path import exists
-    if not exists(path):
-        lgr.debug("No annex: test path {0} doesn't exist".format(path))
-        return False
-    from datalad.support.gitrepo import GitRepo
-    return GitRepo(path, init=False, create=False).is_with_annex()
+# for convenience, bind to class too
+ConfigManager.rewrite_url = rewrite_url
 
+#
+# Helpers for bypassing git-config when _writing_ config items,
+# mostly useful when a large number of changes needs to be made
+# and directly file manipulation without a safety net is worth
+# the risk for performance reasons.
+#
 
-@contextmanager
-def make_tempfile(content=None, wrapped=None, **tkwargs):
-    """Helper class to provide a temporary file name and remove it at the end (context manager)
+def quote_config(v):
+    """Helper to perform minimal quoting of config keys/value parts
 
     Parameters
     ----------
-    mkdir : bool, optional (default: False)
-        If True, temporary directory created using tempfile.mkdtemp()
-    content : str or bytes, optional
-        Content to be stored in the file created
-    wrapped : function, optional
-        If set, function name used to prefix temporary file name
-    `**tkwargs`:
-        All other arguments are passed into the call to tempfile.mk{,d}temp(),
-        and resultant temporary filename is passed as the first argument into
-        the function t.  If no 'prefix' argument is provided, it will be
-        constructed using module and function names ('.' replaced with
-        '_').
-
-    To change the used directory without providing keyword argument 'dir' set
-    DATALAD_TESTS_TEMP_DIR.
-
-    Examples
-    --------
-        >>> from os.path import exists
-        >>> from datalad.utils import make_tempfile
-        >>> with make_tempfile() as fname:
-        ...    k = open(fname, 'w').write('silly test')
-        >>> assert not exists(fname)  # was removed
-
-        >>> with make_tempfile(content="blah") as fname:
-        ...    assert open(fname).read() == "blah"
+    v : str
+      To-be-quoted string
     """
-
-    if tkwargs.get('mkdir', None) and content is not None:
-        raise ValueError("mkdir=True while providing content makes no sense")
-
-    tkwargs_ = get_tempfile_kwargs(tkwargs, wrapped=wrapped)
-
-    # if DATALAD_TESTS_TEMP_DIR is set, use that as directory,
-    # let mktemp handle it otherwise. However, an explicitly provided
-    # dir=... will override this.
-    mkdir = tkwargs_.pop('mkdir', False)
-
-    filename = {False: tempfile.mktemp,
-                True: tempfile.mkdtemp}[mkdir](**tkwargs_)
-    filename = realpath(filename)
-
-    if content:
-        with open(filename, 'w' + ('b' if isinstance(content, binary_type) else '')) as f:
-            f.write(content)
-
-    if __debug__:
-        # TODO mkdir
-        lgr.debug('Created temporary thing named %s"' % filename)
-    try:
-        yield filename
-    finally:
-        # glob here for all files with the same name (-suffix)
-        # would be useful whenever we requested .img filename,
-        # and function creates .hdr as well
-        lsuffix = len(tkwargs_.get('suffix', ''))
-        filename_ = lsuffix and filename[:-lsuffix] or filename
-        filenames = glob.glob(filename_ + '*')
-        if len(filename_) < 3 or len(filenames) > 5:
-            # For paranoid yoh who stepped into this already ones ;-)
-            lgr.warning("It is unlikely that it was intended to remove all"
-                        " files matching %r. Skipping" % filename_)
-            return
-        for f in filenames:
-            try:
-                rmtemp(f)
-            except OSError:  # pragma: no cover
-                pass
+    white = (' ', '\t')
+    comment = ('#', ';')
+    # backslashes need to be quoted in any case
+    v = v.replace('\\', '\\\\')
+    # must not have additional unquoted quotes
+    v = v.replace('"', '\\"')
+    if v[0] in white or v[-1] in white or any(c in v for c in comment):
+        # quoting the value due to leading/trailing whitespace
+        # or occurrence of comment char
+        v = '"{}"'.format(v)
+    return v
 
 
-def _path_(*p):
-    """Given a path in POSIX" notation, regenerate one in native to the env one"""
-    if on_windows:
-        return opj(*map(lambda x: x.split('/'), p))
-    else:
-        # Assume that all others as POSIX compliant so nothing to be done
-        return opj(*p)
-
-
-def get_timestamp_suffix(time_=None, prefix='-'):
-    """Return a time stamp (full date and time up to second)
-
-    primarily to be used for generation of log files names
-    """
-    args = []
-    if time_ is not None:
-        if isinstance(time_, int):
-            time_ = time.gmtime(time_)
-        args.append(time_)
-    return time.strftime(prefix + TIMESTAMP_FMT, *args)
-
-
-def get_logfilename(dspath, cmd='datalad'):
-    """Return a filename to use for logging under a dataset/repository
-
-    directory would be created if doesn't exist, but dspath must exist
-    and be a directory
-    """
-    assert(exists(dspath))
-    assert(isdir(dspath))
-    ds_logdir = assure_dir(dspath, '.git', 'datalad', 'logs')  # TODO: use WEB_META_LOG whenever #789 merged
-    return opj(ds_logdir, 'crawl-%s.log' % get_timestamp_suffix())
-
-
-def get_trace(edges, start, end, trace=None):
-    """Return the trace/path to reach a node in a tree.
+def write_config_section(fobj, suite, name, props):
+    """Write a config section with (multiple) settings.
 
     Parameters
     ----------
-    edges : sequence(2-tuple)
-      The tree given by a sequence of edges (parent, child) tuples. The
-      nodes can be identified by any value and data type that supports
-      the '==' operation.
-    start :
-      Identifier of the start node. Must be present as a value in the parent
-      location of an edge tuple in order to be found.
-    end :
-      Identifier of the target/end node. Must be present as a value in the child
-      location of an edge tuple in order to be found.
-    trace : list
-      Mostly useful for recursive calls, and used internally.
-
-    Returns
-    -------
-    None or list
-      Returns a list with the trace to the target (the starts and the target
-      are not included in the trace, hence if start and end are directly connected
-      an empty list is returned), or None when no trace to the target can be found,
-      or start and end are identical.
-    """
-    # the term trace is used to avoid confusion with a path in the sense
-    # of a filesystem path, but the analogy fits and nodes can be paths
-    if trace is None:
-        trace = []
-    if not edges:
-        raise ValueError("no edges given")
-    for cand in edges:
-        cand_super, cand_sub = cand
-        if cand_sub in trace:
-            # only DAGs, skip any cyclic traces
-            continue
-        if trace and cand_super != trace[-1]:
-            # only consider edges that lead off the end of the trace
-            continue
-        if not trace and cand_super != start:
-            # we got nothing yet, and this edges is not matching the start
-            continue
-        if cand_sub == end:
-            return trace
-        # dive into potential subnodes
-        cand_trace = get_trace(
-            edges,
-            start,
-            end,
-            trace + [cand_sub])
-        if cand_trace:
-            return cand_trace
-    return None
-
-
-def get_dataset_root(path):
-    """Return the root of an existent dataset containing a given path
-
-    The root path is returned in the same absolute or relative form
-    as the input argument. If no associated dataset exists, or the
-    input path doesn't exist, None is returned.
-    """
-    suffix = '.git'
-    if not isdir(path):
-        path = dirname(path)
-    apath = abspath(path)
-    # while we can still go up
-    while psplit(apath)[1]:
-        if exists(opj(path, suffix)):
-            return path
-        # new test path in the format we got it
-        path = normpath(opj(path, os.pardir))
-        # no luck, next round
-        apath = abspath(path)
-    return None
-
-
-def try_multiple(ntrials, exception, base, f, *args, **kwargs):
-    """Call f multiple times making exponentially growing delay between the calls"""
-    from .dochelpers import exc_str
-    for trial in range(1, ntrials+1):
-        try:
-            return f(*args, **kwargs)
-        except exception as exc:
-            if trial == ntrials:
-                raise  # just reraise on the last trial
-            t = base ** trial
-            lgr.warning("Caught %s on trial #%d. Sleeping %f and retrying",
-                        exc_str(exc), trial, t)
-            sleep(t)
-
-
-def slash_join(base, extension):
-    """Join two strings with a '/', avoiding duplicate slashes
-
-    If any of the strings is None the other is returned as is.
-    """
-    if extension is None:
-        return base
-    if base is None:
-        return extension
-    return '/'.join(
-        (base.rstrip('/'),
-         extension.lstrip('/')))
-
-
-def safe_print(s):
-    """Print with protection against UTF-8 encoding errors"""
-    # A little bit of dance to be able to test this code
-    print_f = getattr(__builtin__, "print")
-    try:
-        print_f(s)
-    except UnicodeEncodeError:
-        # failed to encode so let's do encoding while ignoring errors
-        # to print at least something
-        # explicit `or ascii` since somehow on buildbot it seemed to return None
-        s = s.encode(getattr(sys.stdout, 'encoding', 'ascii') or 'ascii', errors='ignore') \
-            if hasattr(s, 'encode') else s
-        print_f(s.decode())
-
-lgr.log(5, "Done importing datalad.utils")
-
+    fobj : File
+       Opened target file
+    suite : str
+       First item of the section name, e.g. 'submodule', or
+       'datalad'
+    name : str
+       Remainder of the section name
+    props : dict
+       Keys are configuration setting names within the section
+       context (i.e. not duplicating `suite` and/or `name`, values
+       are configuration setting values.
+    """
+    fmt = '[{_suite_} {_q_}{_name_}{_q_}]\n'
+    for p in props:
+        fmt += '\t{p} = {{{p}}}\n'.format(p=p)
+    quoted_name = quote_config(name)
+    fobj.write(
+        fmt.format(
+            _suite_=suite,
+            _q_='' if quoted_name.startswith('"') else '"',
+            _name_=quoted_name,
+            **{k: quote_config(v) for k, v in props.items()}))
+
+
+def warn_on_undefined_git_identity(cfg: ConfigManager):
+    """Check whether a Git identity is defined, and warn if not"""
+    for cfgkey, envs in (
+            ('user.name', ('GIT_AUTHOR_NAME', 'GIT_COMMITTER_NAME')),
+            ('user.email', ('GIT_AUTHOR_EMAIL', 'GIT_COMMITTER_EMAIL'))):
+        if cfgkey not in cfg \
+                and not any(e in os.environ for e in envs):
+            lgr.warning(
+                "It is highly recommended to configure Git before using "
+                "DataLad. Set both 'user.name' and 'user.email' "
+                "configuration variables."
+            )
+            break  # one warning enough
```

### Comparing `datalad-0.9.3/datalad/distribution/tests/test_utils.py` & `datalad-1.0.0/datalad/distribution/tests/test_utils.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,31 +1,36 @@
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 """Test distribution utils
 
 """
 
 import os
 from os.path import join as opj
 
 from datalad.distribution.utils import _get_flexible_source_candidates
-from datalad.distribution.utils import get_git_dir
-
-from datalad.tests.utils import with_tempfile
-from datalad.tests.utils import eq_
-from datalad.tests.utils import assert_raises
-
-from datalad.utils import on_windows
+from datalad.support.gitrepo import GitRepo
+from datalad.tests.utils_pytest import (
+    assert_raises,
+    eq_,
+    known_failure_windows,
+    with_tempfile,
+)
+from datalad.utils import (
+    on_windows,
+    unlink,
+)
 
 
+@known_failure_windows
 def test_get_flexible_source_candidates():
     f = _get_flexible_source_candidates
     # for http and https (dummy transport) we should get /.git source added
     eq_(f('http://e.c'), ['http://e.c', 'http://e.c/.git'])
     eq_(f('http://e.c/s/p'), ['http://e.c/s/p', 'http://e.c/s/p/.git'])
     # for those candidates should be just the original address, since git
     # understands those just fine
@@ -52,28 +57,7 @@
 
     # incorrect ones will stay incorrect
     eq_(f('../s1/s2', 'http://e.c/.git'), ['http://e.c/../s1/s2/.git'])
 
     # when source is not relative, but base_url is specified as just the destination path,
     # not really a "base url" as name was suggesting, then it should be ignored
     eq_(f('http://e.c/p', '/path'), ['http://e.c/p', 'http://e.c/p/.git'])
-
-
-@with_tempfile
-def test_get_git_dir(path):
-    # minimal, only missing coverage
-    assert_raises(RuntimeError, get_git_dir, path)
-
-    srcpath = opj(path, 'src')
-    targetpath = opj(path, 'target')
-    targetgitpath = opj(targetpath, '.git')
-    os.makedirs(srcpath)
-    os.makedirs(targetpath)
-    if not on_windows:
-        # with PY3 would also work with Windows 6+
-        os.symlink(srcpath, targetgitpath)
-        eq_(srcpath, get_git_dir(targetpath))
-        # cleanup for following test
-        os.unlink(targetgitpath)
-    with open(targetgitpath, 'w') as f:
-        f.write('gitdir: {}'.format(srcpath))
-    eq_(srcpath, get_git_dir(targetpath))
```

### Comparing `datalad-0.9.3/datalad/distribution/tests/test_dataset_binding.py` & `datalad-1.0.0/datalad/distribution/tests/test_dataset_binding.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,21 +1,29 @@
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 """Test binding of functions to Dataset class
 
 """
 
 from os.path import join as opj
-from ..dataset import Dataset, datasetmethod
-from datalad.tests.utils import assert_raises, eq_
+
+from datalad.tests.utils_pytest import (
+    assert_raises,
+    eq_,
+)
+
+from ..dataset import (
+    Dataset,
+    datasetmethod,
+)
 
 
 def test_decorator():
 
     @datasetmethod
     def func(a, b, dataset=None, some_more=True):
 
@@ -50,8 +58,43 @@
     assert_raises(TypeError, ds.func, 1, 2, dataset='whatever')
 
     # test name parameter:
     @datasetmethod(name="new_name")
     def another(some, dataset=None):
         return some
 
-    eq_(ds.new_name('whatever'), 'whatever')
+    eq_(ds.new_name('whatever'), 'whatever')
+
+
+def test_decorator_star():
+    @datasetmethod
+    def func(a, b, *, dataset=None, some_more=True):
+
+        return {'a': a, 'b': b, 'dataset': dataset, 'some_more': some_more}
+
+    ds = Dataset(opj('some', 'where'))
+
+    orig = func(1, 2, dataset=ds, some_more=False)
+    eq_(orig['a'], 1)
+    eq_(orig['b'], 2)
+    eq_(orig['dataset'], ds)
+    eq_(orig['some_more'], False)
+
+    # general call
+    bound = ds.func(1, 2, some_more=False)
+    eq_(orig, bound)
+
+    # use default value
+    bound = ds.func(1, 2)
+    orig['some_more'] = True
+    eq_(orig, bound)
+
+    # too few arguments:
+    assert_raises(TypeError, ds.func, 1)
+
+    # too much arguments, by using original call with bound function,
+    # raises proper TypeError:
+    assert_raises(TypeError, ds.func, 1, 2, ds)
+
+    # keyword argument 'dataset' is invalidated in Dataset-bound function:
+    # raises proper TypeError:
+    assert_raises(TypeError, ds.func, 1, 2, dataset='whatever')
```

### Comparing `datalad-0.9.3/datalad/distribution/tests/test_install.py` & `datalad-1.0.0/datalad/distribution/tests/test_install.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,197 +1,162 @@
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 """Test install action
 
 """
 
-from datalad.tests.utils import known_failure_v6
-from datalad.tests.utils import known_failure_direct_mode
-
-
 import logging
 import os
-
+from os.path import (
+    basename,
+    dirname,
+    exists,
+    isdir,
+)
 from os.path import join as opj
-from os.path import isdir
-from os.path import exists
-from os.path import realpath
-from os.path import basename
-from os.path import dirname
-
-from mock import patch
-
-from datalad.utils import getpwd
-
-from datalad.api import create
-from datalad.api import install
-from datalad.api import get
-from datalad import consts
-from datalad.utils import chpwd
+from unittest.mock import patch
+
+import pytest
+
+from datalad import cfg as dl_cfg
+from datalad.api import (
+    create,
+    get,
+    install,
+)
+from datalad.cmd import WitlessRunner as Runner
 from datalad.interface.results import YieldDatasets
-from datalad.interface.results import YieldRelativePaths
-from datalad.support.exceptions import InsufficientArgumentsError
-from datalad.support.exceptions import InstallFailedError
-from datalad.support.exceptions import IncompleteResultsError
-from datalad.support.gitrepo import GitRepo
-from datalad.support.gitrepo import GitCommandError
+from datalad.support import path as op
 from datalad.support.annexrepo import AnnexRepo
-from datalad.cmd import Runner
-from datalad.tests.utils import create_tree
-from datalad.tests.utils import with_tempfile
-from datalad.tests.utils import assert_in
-from datalad.tests.utils import with_tree
-from datalad.tests.utils import with_testrepos
-from datalad.tests.utils import eq_
-from datalad.tests.utils import ok_
-from datalad.tests.utils import assert_false
-from datalad.tests.utils import ok_file_has_content
-from datalad.tests.utils import assert_not_in
-from datalad.tests.utils import assert_raises
-from datalad.tests.utils import assert_is_instance
-from datalad.tests.utils import assert_result_count
-from datalad.tests.utils import assert_status
-from datalad.tests.utils import assert_in_results
-from datalad.tests.utils import assert_not_in_results
-from datalad.tests.utils import ok_startswith
-from datalad.tests.utils import ok_clean_git
-from datalad.tests.utils import serve_path_via_http
-from datalad.tests.utils import swallow_logs
-from datalad.tests.utils import use_cassette
-from datalad.tests.utils import skip_if_no_network
-from datalad.tests.utils import put_file_under_git
-from datalad.tests.utils import integration
-from datalad.tests.utils import slow
-from datalad.tests.utils import usecase
-from datalad.tests.utils import get_datasets_topdir
-from datalad.utils import _path_
-from datalad.utils import rmtree
+from datalad.support.exceptions import (
+    IncompleteResultsError,
+    InsufficientArgumentsError,
+)
+from datalad.support.gitrepo import GitRepo
+from datalad.support.network import get_local_file_url
+from datalad.tests.utils_testdatasets import (
+    _make_dataset_hierarchy,
+    _mk_submodule_annex,
+)
+from datalad.tests.utils_pytest import (
+    DEFAULT_BRANCH,
+    DEFAULT_REMOTE,
+    assert_false,
+    assert_in,
+    assert_in_results,
+    assert_is_instance,
+    assert_not_in,
+    assert_raises,
+    assert_repo_status,
+    assert_result_count,
+    assert_status,
+    create_tree,
+    eq_,
+    get_datasets_topdir,
+    integration,
+    known_failure_githubci_win,
+    known_failure_windows,
+    ok_,
+    ok_file_has_content,
+    ok_startswith,
+    put_file_under_git,
+    serve_path_via_http,
+    skip_if_no_network,
+    skip_if_on_windows,
+    skip_ssh,
+    slow,
+    swallow_logs,
+    use_cassette,
+    usecase,
+    with_tempfile,
+    with_tree,
+)
+from datalad.utils import (
+    Path,
+    _path_,
+    chpwd,
+    getpwd,
+    on_windows,
+    rmtree,
+)
 
 from ..dataset import Dataset
-from ..utils import _get_installationpath_from_url
-from ..utils import _get_git_url_from_source
 
 ###############
 # Test helpers:
 ###############
 
-
-def test_installationpath_from_url():
-    for p in ('lastbit',
-              'lastbit/',
-              '/lastbit',
-              'lastbit.git',
-              'lastbit.git/',
-              'http://example.com/lastbit',
-              'http://example.com/lastbit.git',
-              'http://lastbit:8000'
-              ):
-        eq_(_get_installationpath_from_url(p), 'lastbit')
-    # we need to deal with quoted urls
-    for url in (
-        # although some docs say that space could've been replaced with +
-        'http://localhost:8000/+last%20bit',
-        'http://localhost:8000/%2Blast%20bit',
-        '///%2Blast%20bit',
-        '///d1/%2Blast%20bit',
-        '///d1/+last bit',
-    ):
-        eq_(_get_installationpath_from_url(url), '+last bit')
-    # and the hostname alone
-    eq_(_get_installationpath_from_url("http://hostname"), 'hostname')
-    eq_(_get_installationpath_from_url("http://hostname/"), 'hostname')
-
-
-def test_get_git_url_from_source():
-
-    # resolves datalad RIs:
-    eq_(_get_git_url_from_source('///subds'), consts.DATASETS_TOPURL + 'subds')
-    assert_raises(NotImplementedError, _get_git_url_from_source,
-                  '//custom/subds')
-
-    # doesn't harm others:
-    eq_(_get_git_url_from_source('http://example.com'), 'http://example.com')
-    eq_(_get_git_url_from_source('/absolute/path'), '/absolute/path')
-    eq_(_get_git_url_from_source('file://localhost/some'),
-        'file://localhost/some')
-    eq_(_get_git_url_from_source('localhost/another/path'),
-        'localhost/another/path')
-    eq_(_get_git_url_from_source('user@someho.st/mydir'),
-        'user@someho.st/mydir')
-    eq_(_get_git_url_from_source('ssh://somewhe.re/else'),
-        'ssh://somewhe.re/else')
-    eq_(_get_git_url_from_source('git://github.com/datalad/testrepo--basic--r1'),
-        'git://github.com/datalad/testrepo--basic--r1')
-
-
+@pytest.mark.parametrize("annex", [False, True])
 @with_tree(tree={'file.txt': '123'})
 @serve_path_via_http
 @with_tempfile
-def _test_guess_dot_git(annex, path, url, tdir):
+def test_guess_dot_git(path=None, url=None, tdir=None, *, annex):
     repo = (AnnexRepo if annex else GitRepo)(path, create=True)
-    repo.add('file.txt', commit=True, git=not annex)
+    repo.add('file.txt', git=not annex)
+    repo.commit()
 
     # we need to prepare to be served via http, otherwise it must fail
     with swallow_logs() as cml:
         assert_raises(IncompleteResultsError, install, path=tdir, source=url)
     ok_(not exists(tdir))
 
-    Runner(cwd=path)(['git', 'update-server-info'])
+    Runner(cwd=path).run(['git', 'update-server-info'])
 
     with swallow_logs() as cml:
         installed = install(tdir, source=url)
         assert_not_in("Failed to get annex.uuid", cml.out)
-    eq_(realpath(installed.path), realpath(tdir))
+    eq_(installed.pathobj.resolve(), Path(tdir).resolve())
     ok_(exists(tdir))
-    ok_clean_git(tdir, annex=annex)
-
-
-def test_guess_dot_git():
-    for annex in False, True:
-        yield _test_guess_dot_git, annex
+    assert_repo_status(tdir, annex=annex)
 
 
 ######################
 # Test actual Install:
 ######################
 
 def test_insufficient_args():
     assert_raises(InsufficientArgumentsError, install)
     assert_raises(InsufficientArgumentsError, install, description="some")
     assert_raises(InsufficientArgumentsError, install, None)
     assert_raises(InsufficientArgumentsError, install, None, description="some")
 
-
+# ValueError: path is on mount 'D:', start on mount 'C:
+@known_failure_githubci_win
 @with_tempfile(mkdir=True)
-def test_invalid_args(path):
+def test_invalid_args(path=None):
     assert_raises(IncompleteResultsError, install, 'Zoidberg', source='Zoidberg')
     # install to an invalid URL
     assert_raises(ValueError, install, 'ssh://mars:Zoidberg', source='Zoidberg')
     # install to a remote location
     assert_raises(ValueError, install, 'ssh://mars/Zoidberg', source='Zoidberg')
     # make fake dataset
     ds = create(path)
-    assert_raises(IncompleteResultsError, install, '/higherup.', 'Zoidberg', dataset=ds)
+    # explicit 'source' as a kwarg
+    assert_raises(IncompleteResultsError, install, '/higherup.', source='Zoidberg', dataset=ds)
+    # or obscure form for multiple installation "things"
+    assert_raises(IncompleteResultsError, install, ['/higherup.', 'Zoidberg'], dataset=ds)
+    # and if just given without keyword arg for source -- standard Python exception
+    assert_raises(TypeError, install, '/higherup.', 'Zoidberg', dataset=ds)
 
 
 # This test caused a mysterious segvault in gh-1350. I reimplementation of
 # the same test functionality in test_clone.py:test_clone_crcns that uses
 # `clone` instead of `install` passes without showing this behavior
 # This test is disabled until some insight into the cause of the issue
 # materializes.
 #@skip_if_no_network
 #@use_cassette('test_install_crcns')
 #@with_tempfile(mkdir=True)
 #@with_tempfile(mkdir=True)
-#def test_install_crcns(tdir, ds_path):
+#def test_install_crcns(tdir=None, ds_path=None):
 #    with chpwd(tdir):
 #        with swallow_logs(new_level=logging.INFO) as cml:
 #            install("all-nonrecursive", source='///')
 #            # since we didn't log decorations such as log level atm while
 #            # swallowing so lets check if exit code is returned or not
 #            # I will test both
 #            assert_not_in('ERROR', cml.out)
@@ -214,17 +179,16 @@
 #    crcns = ds.install("///crcns")
 #    ok_(crcns.is_installed())
 #    eq_(crcns.path, opj(ds_path, "crcns"))
 #    assert_in(crcns.path, ds.get_subdatasets(absolute=True))
 
 
 @skip_if_no_network
-@use_cassette('test_install_crcns')
 @with_tree(tree={'sub': {}})
-def test_install_datasets_root(tdir):
+def test_install_datasets_root(tdir=None):
     with chpwd(tdir):
         ds = install("///")
         ok_(ds.is_installed())
         eq_(ds.path, opj(tdir, get_datasets_topdir()))
 
         # do it a second time:
         result = install("///", result_xfm=None, return_type='list')
@@ -233,320 +197,394 @@
 
         # and a third time into an existing something, that is not a dataset:
         with open(opj(tdir, 'sub', 'a_file.txt'), 'w') as f:
             f.write("something")
 
         with assert_raises(IncompleteResultsError) as cme:
             install("sub", source='///')
-        assert_in("already exists and not empty", str(cme.exception))
+        assert_in("already exists and not empty", str(cme.value))
 
 
-@known_failure_v6  #FIXME
-@with_testrepos('.*basic.*', flavors=['local-url', 'network', 'local'])
+@pytest.mark.parametrize("type_", ["git", "annex"])
+@with_tree(tree={'test.dat': "doesn't matter",
+                 'INFO.txt': "some info",
+                 'test-annex.dat': "irrelevant"})
 @with_tempfile(mkdir=True)
-def test_install_simple_local(src, path):
-    origin = Dataset(path)
+def test_install_simple_local(src_repo=None, path=None, *, type_):
 
-    # now install it somewhere else
-    ds = install(path, source=src, description='mydummy')
-    eq_(ds.path, path)
-    ok_(ds.is_installed())
-    if not isinstance(origin.repo, AnnexRepo):
-        # this means it is a GitRepo
-        ok_(isinstance(origin.repo, GitRepo))
-        # stays plain Git repo
-        ok_(isinstance(ds.repo, GitRepo))
-        ok_(not isinstance(ds.repo, AnnexRepo))
-        ok_(GitRepo.is_valid_repo(ds.path))
-        eq_(set(ds.repo.get_indexed_files()),
-            {'test.dat', 'INFO.txt'})
-        ok_clean_git(path, annex=False)
+    src_ds = Dataset(src_repo).create(result_renderer='disabled', force=True,
+                                      annex=(type_ == "annex"))
+    src_ds.save(['INFO.txt', 'test.dat'], to_git=True)
+    if type_ == 'annex':
+        src_ds.save('test-annex.dat', to_git=False)
+    elif type_ == 'git':
+        pass
     else:
-        # must be an annex
-        ok_(isinstance(ds.repo, AnnexRepo))
-        ok_(AnnexRepo.is_valid_repo(ds.path, allow_noninitialized=False))
-        eq_(set(ds.repo.get_indexed_files()),
-            {'test.dat', 'INFO.txt', 'test-annex.dat'})
-        ok_clean_git(path, annex=True)
-        # no content was installed:
-        ok_(not ds.repo.file_has_content('test-annex.dat'))
-        uuid_before = ds.repo.uuid
-        eq_(ds.repo.get_description(), 'mydummy')
-
-    # installing it again, shouldn't matter:
-    res = install(path, source=src, result_xfm=None, return_type='list')
-    assert_status('notneeded', res)
-    ok_(ds.is_installed())
-    if isinstance(origin.repo, AnnexRepo):
-        eq_(uuid_before, ds.repo.uuid)
+        raise ValueError("'type' must be 'git' or 'annex'")
+    # equivalent repo on github:
+    url = "https://github.com/datalad/testrepo--basic--r1.git"
+    sources = [src_ds.path,
+               get_local_file_url(src_ds.path, compatibility='git')]
+    if not dl_cfg.get('datalad.tests.nonetwork'):
+        sources.append(url)
+
+    for src in sources:
+        origin = Dataset(path)
+
+        # now install it somewhere else
+        ds = install(path, source=src, description='mydummy')
+        eq_(ds.path, path)
+        ok_(ds.is_installed())
+        if not isinstance(origin.repo, AnnexRepo):
+            # this means it is a GitRepo
+            ok_(isinstance(origin.repo, GitRepo))
+            # stays plain Git repo
+            ok_(isinstance(ds.repo, GitRepo))
+            ok_(not isinstance(ds.repo, AnnexRepo))
+            ok_(GitRepo.is_valid_repo(ds.path))
+            files = ds.repo.get_indexed_files()
+            assert_in('test.dat', files)
+            assert_in('INFO.txt', files)
+            assert_repo_status(path, annex=False)
+        else:
+            # must be an annex
+            ok_(isinstance(ds.repo, AnnexRepo))
+            ok_(AnnexRepo.is_valid_repo(ds.path, allow_noninitialized=False))
+            files = ds.repo.get_indexed_files()
+            assert_in('test.dat', files)
+            assert_in('INFO.txt', files)
+            assert_in('test-annex.dat', files)
+            assert_repo_status(path, annex=True)
+            # no content was installed:
+            ok_(not ds.repo.file_has_content('test-annex.dat'))
+            uuid_before = ds.repo.uuid
+            ok_(uuid_before)  # we actually have an uuid
+            eq_(ds.repo.get_description(), 'mydummy')
+
+        # installing it again, shouldn't matter:
+        res = install(path, source=src, result_xfm=None, return_type='list')
+        assert_status('notneeded', res)
+        ok_(ds.is_installed())
+        if isinstance(origin.repo, AnnexRepo):
+            eq_(uuid_before, ds.repo.uuid)
 
+        # cleanup before next iteration
+        rmtree(path)
 
-@known_failure_v6  #FIXME
-@with_testrepos(flavors=['local-url', 'network', 'local'])
+
+@known_failure_githubci_win
+@with_tree(tree={'test.dat': "doesn't matter",
+                 'INFO.txt': "some info",
+                 'test-annex.dat': "irrelevant"})
 @with_tempfile
-def test_install_dataset_from_just_source(url, path):
-    with chpwd(path, mkdir=True):
-        ds = install(source=url)
+def test_install_dataset_from_just_source(src_repo=None, path=None):
 
-    ok_startswith(ds.path, path)
-    ok_(ds.is_installed())
-    ok_(GitRepo.is_valid_repo(ds.path))
-    ok_clean_git(ds.path, annex=None)
-    assert_in('INFO.txt', ds.repo.get_indexed_files())
+    src_ds = Dataset(src_repo).create(result_renderer='disabled', force=True)
+    src_ds.save(['INFO.txt', 'test.dat'], to_git=True)
+    src_ds.save('test-annex.dat', to_git=False)
+    # equivalent repo on github:
+    src_url = "https://github.com/datalad/testrepo--basic--r1.git"
+    sources = [src_ds.path,
+               get_local_file_url(src_ds.path, compatibility='git')]
+    if not dl_cfg.get('datalad.tests.nonetwork'):
+        sources.append(src_url)
+
+    for url in sources:
+
+        with chpwd(path, mkdir=True):
+            ds = install(source=url)
+
+        ok_startswith(ds.path, path)
+        ok_(ds.is_installed())
+        ok_(GitRepo.is_valid_repo(ds.path))
+        assert_repo_status(ds.path, annex=None)
+        assert_in('INFO.txt', ds.repo.get_indexed_files())
+
+        # cleanup before next iteration
+        rmtree(path)
 
 
-@known_failure_v6  #FIXME
-@with_testrepos(flavors=['local'])
+@with_tree(tree={'test.dat': "doesn't matter",
+                 'INFO.txt': "some info",
+                 'test-annex.dat': "irrelevant"})
 @with_tempfile(mkdir=True)
-def test_install_dataset_from_instance(src, dst):
-    origin = Dataset(src)
+def test_install_dataset_from_instance(src=None, dst=None):
+    origin = Dataset(src).create(result_renderer='disabled', force=True)
+    origin.save(['INFO.txt', 'test.dat'], to_git=True)
+    origin.save('test-annex.dat', to_git=False)
+
     clone = install(source=origin, path=dst)
 
     assert_is_instance(clone, Dataset)
     ok_startswith(clone.path, dst)
     ok_(clone.is_installed())
     ok_(GitRepo.is_valid_repo(clone.path))
-    ok_clean_git(clone.path, annex=None)
+    assert_repo_status(clone.path, annex=None)
     assert_in('INFO.txt', clone.repo.get_indexed_files())
 
 
-@with_testrepos(flavors=['network'])
+@known_failure_githubci_win
+@skip_if_no_network
 @with_tempfile
-@known_failure_v6  #FIXME
-def test_install_dataset_from_just_source_via_path(url, path):
+def test_install_dataset_from_just_source_via_path(path=None):
     # for remote urls only, the source could be given to `path`
     # to allows for simplistic cmdline calls
-    # Q (ben): remote urls only? Sure? => TODO
+
+    url = "https://github.com/datalad/testrepo--basic--r1.git"
 
     with chpwd(path, mkdir=True):
         ds = install(url)
 
     ok_startswith(ds.path, path)
     ok_(ds.is_installed())
     ok_(GitRepo.is_valid_repo(ds.path))
-    ok_clean_git(ds.path, annex=None)
+    assert_repo_status(ds.path, annex=None)
     assert_in('INFO.txt', ds.repo.get_indexed_files())
 
 
 @with_tree(tree={
     'ds': {'test.txt': 'some'},
     })
 @serve_path_via_http
 @with_tempfile(mkdir=True)
-def test_install_dataladri(src, topurl, path):
+def test_install_dataladri(src=None, topurl=None, path=None):
     # make plain git repo
     ds_path = opj(src, 'ds')
     gr = GitRepo(ds_path, create=True)
     gr.add('test.txt')
     gr.commit('demo')
-    Runner(cwd=gr.path)(['git', 'update-server-info'])
+    Runner(cwd=gr.path).run(['git', 'update-server-info'])
     # now install it somewhere else
     with patch('datalad.consts.DATASETS_TOPURL', topurl), \
             swallow_logs():
         ds = install(path, source='///ds')
     eq_(ds.path, path)
-    ok_clean_git(path, annex=False)
+    assert_repo_status(path, annex=False)
     ok_file_has_content(opj(path, 'test.txt'), 'some')
 
 
-@with_testrepos('submodule_annex', flavors=['local', 'local-url', 'network'])
 @with_tempfile(mkdir=True)
 @with_tempfile(mkdir=True)
-def test_install_recursive(src, path_nr, path_r):
+@with_tempfile(mkdir=True)
+def test_install_recursive(src=None, path_nr=None, path_r=None):
+
+    _make_dataset_hierarchy(src)
+
     # first install non-recursive:
     ds = install(path_nr, source=src, recursive=False)
     ok_(ds.is_installed())
     for sub in ds.subdatasets(recursive=True, result_xfm='datasets'):
         ok_(not sub.is_installed(),
             "Unintentionally installed: %s" % (sub,))
-    # this also means, subdatasets to be listed as not fulfilled:
-    eq_(set(ds.subdatasets(recursive=True, fulfilled=False, result_xfm='relpaths')),
-        {'subm 1', '2'})
+    # this also means, subdatasets to be listed as absent:
+    eq_(set(ds.subdatasets(recursive=True, state='absent', result_xfm='relpaths')),
+        {'sub1'})
 
     # now recursively:
     # don't filter implicit results so we can inspect them
-    ds_list = install(path_r, source=src, recursive=True, result_filter=None)
-    # installed a dataset and two subdatasets
-    eq_(len(ds_list), 3)
-    eq_(sum([isinstance(i, Dataset) for i in ds_list]), 3)
+    res = install(path_r, source=src, recursive=True,
+                  result_xfm=None, result_filter=None)
+    # installed a dataset and four subdatasets
+    assert_result_count(res, 5, action='install', type='dataset')
     # we recurse top down during installation, so toplevel should appear at
     # first position in returned list
-    eq_(ds_list[0].path, path_r)
-    top_ds = ds_list[0]
+    eq_(res[0]['path'], path_r)
+    top_ds = Dataset(res[0]['path'])
     ok_(top_ds.is_installed())
 
     # the subdatasets are contained in returned list:
     # (Note: Until we provide proper (singleton) instances for Datasets,
     # need to check for their paths)
-    assert_in(opj(top_ds.path, 'subm 1'), [i.path for i in ds_list])
-    assert_in(opj(top_ds.path, '2'), [i.path for i in ds_list])
+    assert_in_results(res, path=opj(top_ds.path, 'sub1'), type='dataset')
+    assert_in_results(res, path=opj(top_ds.path, 'sub1', 'sub2'),
+                      type='dataset')
+    assert_in_results(res, path=opj(top_ds.path, 'sub1', 'sub2', 'sub3'),
+                      type='dataset')
+    assert_in_results(res, path=opj(top_ds.path, 'sub1', 'sub2', 'sub3',
+                                    'sub4'),
+                      type='dataset')
 
-    eq_(len(top_ds.subdatasets(recursive=True)), 2)
+    eq_(len(top_ds.subdatasets(recursive=True)), 4)
 
     for subds in top_ds.subdatasets(recursive=True, result_xfm='datasets'):
         ok_(subds.is_installed(),
             "Not installed: %s" % (subds,))
         # no content was installed:
-        ok_(not any(subds.repo.file_has_content(
-            subds.repo.get_annexed_files())))
-    # no unfulfilled subdatasets:
-    ok_(top_ds.subdatasets(recursive=True, fulfilled=False) == [])
+        ainfo = subds.repo.get_content_annexinfo(init=None,
+                                                 eval_availability=True)
+        assert_false(any(st["has_content"] for st in ainfo.values()))
+    # no absent subdatasets:
+    ok_(top_ds.subdatasets(recursive=True, state='absent') == [])
+
+    # check if we can install recursively into a dataset
+    # https://github.com/datalad/datalad/issues/2982
+    subds = ds.install('recursive-in-ds', source=src, recursive=True)
+    ok_(subds.is_installed())
+    for subsub in subds.subdatasets(recursive=True, result_xfm='datasets'):
+        ok_(subsub.is_installed())
 
+    # check that we get subdataset instances manufactured from notneeded results
+    # to install existing subdatasets again
+    eq_(subds, ds.install('recursive-in-ds'))
 
-@with_testrepos('submodule_annex', flavors=['local'])
+
+@with_tempfile(mkdir=True)
 @with_tempfile(mkdir=True)
-def test_install_recursive_with_data(src, path):
+def test_install_recursive_with_data(src=None, path=None):
+
+    _make_dataset_hierarchy(src)
 
     # now again; with data:
     res = install(path, source=src, recursive=True, get_data=True,
                   result_filter=None, result_xfm=None)
     assert_status('ok', res)
     # installed a dataset and two subdatasets, and one file with content in
-    # each, plus the report that we got all content in each dataset's root dir
-    eq_(len(res), 9)
-    assert_result_count(res, 3, type='dataset')
+    # each
+    assert_result_count(res, 5, type='dataset', action='install')
+    assert_result_count(res, 2, type='file', action='get')
     # we recurse top down during installation, so toplevel should appear at
     # first position in returned list
     eq_(res[0]['path'], path)
     top_ds = YieldDatasets()(res[0])
     ok_(top_ds.is_installed())
+
+    def all_have_content(repo):
+        ainfo = repo.get_content_annexinfo(init=None, eval_availability=True)
+        return all(st["has_content"] for st in ainfo.values())
+
     if isinstance(top_ds.repo, AnnexRepo):
-        ok_(all(top_ds.repo.file_has_content(top_ds.repo.get_annexed_files())))
+        ok_(all_have_content(top_ds.repo))
+
     for subds in top_ds.subdatasets(recursive=True, result_xfm='datasets'):
         ok_(subds.is_installed(), "Not installed: %s" % (subds,))
         if isinstance(subds.repo, AnnexRepo):
-            ok_(all(subds.repo.file_has_content(subds.repo.get_annexed_files())))
-
+            ok_(all_have_content(subds.repo))
 
-# @known_failure_direct_mode  #FIXME:
-# If we use all testrepos, we get a mixed hierarchy. Therefore ok_clean_git
-# fails if we are in direct mode and run into a plain git beneath an annex, due
-# to currently impossible recursion of `AnnexRepo._submodules_dirty_direct_mode`
 
-@slow  # 88.0869s  because of going through multiple test repos, ~8sec each time
-@with_testrepos('.*annex.*', flavors=['local'])
+@with_tree(tree={'test.dat': "doesn't matter",
+                 'INFO.txt': "some info",
+                 'test-annex.dat': "irrelevant"})
 # 'local-url', 'network'
 # TODO: Somehow annex gets confused while initializing installed ds, whose
 # .git/config show a submodule url "file:///aaa/bbb%20b/..."
 # this is delivered by with_testrepos as the url to clone
 @with_tempfile
-def test_install_into_dataset(source, top_path):
+def test_install_into_dataset(source=None, top_path=None):
+    src_ds = Dataset(source).create(result_renderer='disabled', force=True)
+    src_ds.save(['INFO.txt', 'test.dat'], to_git=True)
+    src_ds.save('test-annex.dat', to_git=False)
 
     ds = create(top_path)
-    ok_clean_git(ds.path)
+    assert_repo_status(ds.path)
 
-    subds = ds.install("sub", source=source, save=False)
-    if isinstance(subds.repo, AnnexRepo) and subds.repo.is_direct_mode():
-        ok_(exists(opj(subds.path, '.git')))
-    else:
-        ok_(isdir(opj(subds.path, '.git')))
+    subds = ds.install("sub", source=source)
+    ok_(isdir(opj(subds.path, '.git')))
     ok_(subds.is_installed())
     assert_in('sub', ds.subdatasets(result_xfm='relpaths'))
     # sub is clean:
-    ok_clean_git(subds.path, annex=None)
+    assert_repo_status(subds.path, annex=None)
     # top is too:
-    ok_clean_git(ds.path, annex=None)
-    ds.save('addsub')
+    assert_repo_status(ds.path, annex=None)
+    ds.save(message='addsub')
     # now it is:
-    ok_clean_git(ds.path, annex=None)
+    assert_repo_status(ds.path, annex=None)
 
     # but we could also save while installing and there should be no side-effect
     # of saving any other changes if we state to not auto-save changes
     # Create a dummy change
     create_tree(ds.path, {'dummy.txt': 'buga'})
-    ok_clean_git(ds.path, untracked=['dummy.txt'])
+    assert_repo_status(ds.path, untracked=['dummy.txt'])
     subds_ = ds.install("sub2", source=source)
     eq_(subds_.path, opj(ds.path, "sub2"))  # for paranoid yoh ;)
-    ok_clean_git(ds.path, untracked=['dummy.txt'])
+    assert_repo_status(ds.path, untracked=['dummy.txt'])
 
     # and we should achieve the same behavior if we create a dataset
     # and then decide to add it
     create(_path_(top_path, 'sub3'))
-    ok_clean_git(ds.path, untracked=['dummy.txt', 'sub3/'])
-    ds.add('sub3')
-    ok_clean_git(ds.path, untracked=['dummy.txt'])
+    assert_repo_status(ds.path, untracked=['dummy.txt', 'sub3/'])
+    ds.save('sub3')
+    assert_repo_status(ds.path, untracked=['dummy.txt'])
 
 
+@slow   # 15sec on Yarik's laptop
 @usecase  # 39.3074s
 @skip_if_no_network
-@use_cassette('test_install_crcns')
 @with_tempfile
-@known_failure_direct_mode  #FIXME
-def test_failed_install_multiple(top_path):
+def test_failed_install_multiple(top_path=None):
     ds = create(top_path)
 
     create(_path_(top_path, 'ds1'))
     create(_path_(top_path, 'ds3'))
-    ok_clean_git(ds.path, annex=None, untracked=['ds1/', 'ds3/'])
+    assert_repo_status(ds.path, annex=None, untracked=['ds1/', 'ds3/'])
 
     # specify install with multiple paths and one non-existing
     with assert_raises(IncompleteResultsError) as cme:
         ds.install(['ds1', 'ds2', '///crcns', '///nonexisting', 'ds3'],
                    on_failure='continue')
 
     # install doesn't add existing submodules -- add does that
-    ok_clean_git(ds.path, annex=None, untracked=['ds1/', 'ds3/'])
-    ds.add(['ds1', 'ds3'])
-    ok_clean_git(ds.path, annex=None)
+    assert_repo_status(ds.path, annex=None, untracked=['ds1/', 'ds3/'])
+    ds.save(['ds1', 'ds3'])
+    assert_repo_status(ds.path, annex=None)
     # those which succeeded should be saved now
     eq_(ds.subdatasets(result_xfm='relpaths'), ['crcns', 'ds1', 'ds3'])
     # and those which didn't -- listed
-    eq_(set(r.get('source_url', r['path']) for r in cme.exception.failed),
+    eq_(set(r.get('source_url', r['path']) for r in cme.value.failed),
         {'///nonexisting', _path_(top_path, 'ds2')})
 
 
-@with_testrepos('submodule_annex', flavors=['local', 'local-url', 'network'])
 @with_tempfile(mkdir=True)
-def test_install_known_subdataset(src, path):
+@with_tempfile(mkdir=True)
+def test_install_known_subdataset(src=None, path=None):
+
+    _mk_submodule_annex(src, fname="test-annex.dat", fcontent="whatever")
 
     # get the superdataset:
     ds = install(path, source=src)
     # subdataset not installed:
     subds = Dataset(opj(path, 'subm 1'))
     assert_false(subds.is_installed())
-    assert_in('subm 1', ds.subdatasets(fulfilled=False, result_xfm='relpaths'))
-    assert_not_in('subm 1', ds.subdatasets(fulfilled=True, result_xfm='relpaths'))
+    assert_in('subm 1', ds.subdatasets(state='absent', result_xfm='relpaths'))
+    assert_not_in('subm 1', ds.subdatasets(state='present', result_xfm='relpaths'))
     # install it:
     ds.install('subm 1')
     ok_(subds.is_installed())
     ok_(AnnexRepo.is_valid_repo(subds.path, allow_noninitialized=False))
     # Verify that it is the correct submodule installed and not
     # new repository initiated
-    eq_(set(subds.repo.get_indexed_files()),
-        {'test.dat', 'INFO.txt', 'test-annex.dat'})
-    assert_not_in('subm 1', ds.subdatasets(fulfilled=False, result_xfm='relpaths'))
-    assert_in('subm 1', ds.subdatasets(fulfilled=True, result_xfm='relpaths'))
+    assert_in("test-annex.dat", subds.repo.get_indexed_files()),
+    assert_not_in('subm 1', ds.subdatasets(state='absent', result_xfm='relpaths'))
+    assert_in('subm 1', ds.subdatasets(state='present', result_xfm='relpaths'))
 
     # now, get the data by reinstalling with -g:
     ok_(subds.repo.file_has_content('test-annex.dat') is False)
     with chpwd(ds.path):
         result = get(path='subm 1', dataset=os.curdir)
         assert_in_results(result, path=opj(subds.path, 'test-annex.dat'))
         ok_(subds.repo.file_has_content('test-annex.dat') is True)
         ok_(subds.is_installed())
 
 
 @slow  # 46.3650s
 @with_tempfile(mkdir=True)
 @with_tempfile(mkdir=True)
-@known_failure_direct_mode  #FIXME
-def test_implicit_install(src, dst):
+def test_implicit_install(src=None, dst=None):
 
     origin_top = create(src)
     origin_sub = origin_top.create("sub")
     origin_subsub = origin_sub.create("subsub")
     with open(opj(origin_top.path, "file1.txt"), "w") as f:
         f.write("content1")
-    origin_top.add("file1.txt")
+    origin_top.save("file1.txt")
     with open(opj(origin_sub.path, "file2.txt"), "w") as f:
         f.write("content2")
-    origin_sub.add("file2.txt")
+    origin_sub.save("file2.txt")
     with open(opj(origin_subsub.path, "file3.txt"), "w") as f:
         f.write("content3")
-    origin_subsub.add("file3.txt")
+    origin_subsub.save("file3.txt")
     origin_top.save(recursive=True)
 
     # first, install toplevel:
     ds = install(dst, source=src)
     ok_(ds.is_installed())
 
     sub = Dataset(opj(ds.path, "sub"))
@@ -587,25 +625,27 @@
         result = get(path=opj("sub", "subsub"), get_data=False, result_xfm='datasets')
         ok_(sub.is_installed())
         ok_(subsub.is_installed())
         eq_(result, [sub, subsub])
 
 
 @with_tempfile(mkdir=True)
-def test_failed_install(dspath):
+def test_failed_install(dspath=None):
     ds = create(dspath)
     assert_raises(IncompleteResultsError,
                   ds.install,
                   "sub",
                   source="http://nonexistingreallyanything.datalad.org/bla")
 
 
-@with_testrepos('submodule_annex', flavors=['local'])
 @with_tempfile(mkdir=True)
-def test_install_list(path, top_path):
+@with_tempfile(mkdir=True)
+def test_install_list(path=None, top_path=None):
+
+    _mk_submodule_annex(path, fname="test-annex.dat", fcontent="whatever")
 
     # we want to be able to install several things, if these are known
     # (no 'source' allowed). Therefore first toplevel:
     ds = install(top_path, source=path, recursive=False)
     assert_not_in('annex.hardlink', ds.config)
     ok_(ds.is_installed())
     sub1 = Dataset(opj(top_path, 'subm 1'))
@@ -624,52 +664,53 @@
     ok_(sub2.is_installed())
     eq_(set(result), {sub1.path, sub2.path})
     # and if we request it again via get, result should be empty
     get_result = ds.get(path=['subm 1', '2'], get_data=False)
     assert_status('notneeded', get_result)
 
 
-@with_testrepos('submodule_annex', flavors=['local'])
 @with_tempfile(mkdir=True)
-def test_reckless(path, top_path):
+@with_tempfile(mkdir=True)
+def test_reckless(path=None, top_path=None):
+    _mk_submodule_annex(path, fname="test-annex.dat", fcontent="whatever")
+
     ds = install(top_path, source=path, reckless=True)
     eq_(ds.config.get('annex.hardlink', None), 'true')
     eq_(ds.repo.repo_info()['untrusted repositories'][0]['here'], True)
 
 
 @with_tree(tree={'top_file.txt': 'some',
                  'sub 1': {'sub1file.txt': 'something else',
                            'subsub': {'subsubfile.txt': 'completely different',
                                       }
                            },
                  'sub 2': {'sub2file.txt': 'meaningless',
                            }
                  })
 @with_tempfile(mkdir=True)
-@known_failure_direct_mode  #FIXME
-def test_install_recursive_repeat(src, path):
-    subsub_src = Dataset(opj(src, 'sub 1', 'subsub')).create(force=True)
-    sub1_src = Dataset(opj(src, 'sub 1')).create(force=True)
-    sub2_src = Dataset(opj(src, 'sub 2')).create(force=True)
+def test_install_recursive_repeat(src=None, path=None):
     top_src = Dataset(src).create(force=True)
-    top_src.add('.', recursive=True)
-    ok_clean_git(top_src.path)
+    sub1_src = top_src.create('sub 1', force=True)
+    sub2_src = top_src.create('sub 2', force=True)
+    subsub_src = sub1_src.create('subsub', force=True)
+    top_src.save(recursive=True)
+    assert_repo_status(top_src.path)
 
     # install top level:
     top_ds = install(path, source=src)
     ok_(top_ds.is_installed() is True)
     sub1 = Dataset(opj(path, 'sub 1'))
     ok_(sub1.is_installed() is False)
     sub2 = Dataset(opj(path, 'sub 2'))
     ok_(sub2.is_installed() is False)
     subsub = Dataset(opj(path, 'sub 1', 'subsub'))
     ok_(subsub.is_installed() is False)
 
     # install again, now with data and recursive, but recursion_limit 1:
-    result = get(os.curdir, dataset=path, recursive=True, recursion_limit=1,
+    result = get(path, dataset=path, recursive=True, recursion_limit=1,
                  result_xfm='datasets')
     # top-level dataset was not reobtained
     assert_not_in(top_ds, result)
     assert_in(sub1, result)
     assert_in(sub2, result)
     assert_not_in(subsub, result)
     ok_(top_ds.repo.file_has_content('top_file.txt') is True)
@@ -678,35 +719,40 @@
 
     # install sub1 again, recursively and with data
     top_ds.install('sub 1', recursive=True, get_data=True)
     ok_(subsub.is_installed())
     ok_(subsub.repo.file_has_content('subsubfile.txt'))
 
 
-@with_testrepos('submodule_annex', flavors=['local'])
+@with_tempfile(mkdir=True)
 @with_tempfile(mkdir=True)
 @with_tempfile
-def test_install_skip_list_arguments(src, path, path_outside):
+def test_install_skip_list_arguments(src=None, path=None, path_outside=None):
+    _mk_submodule_annex(src, fname="test-annex.dat", fcontent="whatever")
+
     ds = install(path, source=src)
     ok_(ds.is_installed())
 
     # install a list with valid and invalid items:
     result = ds.install(
         path=['subm 1', 'not_existing', path_outside, '2'],
         get_data=False,
         on_failure='ignore', result_xfm=None, return_type='list')
     # good and bad results together
     ok_(isinstance(result, list))
     eq_(len(result), 4)
-    # check that we have an 'impossible' status for both invalid args
+    # check that we have an 'impossible/error' status for both invalid args
     # but all the other tasks have been accomplished
-    for skipped, msg in [(opj(ds.path, 'not_existing'), "path does not exist"),
-                         (path_outside, "path not associated with any dataset")]:
-        assert_result_count(
-            result, 1, status='impossible', message=msg, path=skipped)
+    assert_result_count(
+        result, 1, status='impossible', message="path does not exist",
+        path=opj(ds.path, 'not_existing'))
+    assert_result_count(
+        result, 1, status='error',
+        message=("path not associated with dataset %s", ds),
+        path=path_outside)
     for sub in [Dataset(opj(path, 'subm 1')), Dataset(opj(path, '2'))]:
         assert_result_count(
             result, 1, status='ok',
             message=('Installed subdataset in order to get %s', sub.path))
         ok_(sub.is_installed())
 
     # return of get is always a list, by default, even if just one thing was gotten
@@ -714,17 +760,18 @@
     # content of the subdataset
     with assert_raises(IncompleteResultsError) as cme:
         ds.install(path=['subm 1', 'not_existing'])
     with assert_raises(IncompleteResultsError) as cme:
         ds.get(path=['subm 1', 'not_existing'])
 
 
-@with_testrepos('submodule_annex', flavors=['local'])
 @with_tempfile(mkdir=True)
-def test_install_skip_failed_recursive(src, path):
+@with_tempfile(mkdir=True)
+def test_install_skip_failed_recursive(src=None, path=None):
+    _mk_submodule_annex(src, fname="test-annex.dat", fcontent="whatever")
 
     # install top level:
     ds = install(path, source=src)
     sub1 = Dataset(opj(path, 'subm 1'))
     sub2 = Dataset(opj(path, '2'))
     # sabotage recursive installation of 'subm 1' by polluting the target:
     with open(opj(path, 'subm 1', 'blocking.txt'), "w") as f:
@@ -734,96 +781,92 @@
         result = ds.get(
             os.curdir, recursive=True,
             on_failure='ignore', result_xfm=None)
         # toplevel dataset was in the house already
         assert_result_count(
             result, 0, path=ds.path, type='dataset')
         # subm 1 should fail to install. [1] since comes after '2' submodule
-        assert_in_results(result, status='error', path=sub1.path)
+        assert_in_results(
+            result, status='error', path=sub1.path, type='dataset',
+            message='target path already exists and not empty, refuse to '
+                    'clone into target path')
         assert_in_results(result, status='ok', path=sub2.path)
 
-        cml.assert_logged(
-            msg="target path already exists and not empty".format(sub1.path),
-            regex=False, level='ERROR')
-    # this is not in effect that this message is not propagated up
-    # assert_in(
-    #     "destination path '{}' already exists and is not an empty directory".format(
-    #         sub1.path),
-    #     result[0]['message'][2])
-
 
 @with_tree(tree={'top_file.txt': 'some',
                  'sub 1': {'sub1file.txt': 'something else',
                            'subsub': {'subsubfile.txt': 'completely different',
                                       }
                            },
                  'sub 2': {'sub2file.txt': 'meaningless',
                            }
                  })
 @with_tempfile(mkdir=True)
-def test_install_noautoget_data(src, path):
+def test_install_noautoget_data(src=None, path=None):
     subsub_src = Dataset(opj(src, 'sub 1', 'subsub')).create(force=True)
     sub1_src = Dataset(opj(src, 'sub 1')).create(force=True)
     sub2_src = Dataset(opj(src, 'sub 2')).create(force=True)
     top_src = Dataset(src).create(force=True)
-    top_src.add('.', recursive=True)
+    top_src.save(recursive=True)
 
     # install top level:
     # don't filter implicitly installed subdataset to check them for content
     cdss = install(path, source=src, recursive=True, result_filter=None)
     # there should only be datasets in the list of installed items,
     # and none of those should have any data for their annexed files yet
     for ds in cdss:
-        assert_false(any(ds.repo.file_has_content(ds.repo.get_annexed_files())))
+        ainfo = ds.repo.get_content_annexinfo(init=None,
+                                              eval_availability=True)
+        assert_false(any(st["has_content"] for st in ainfo.values()))
 
 
 @with_tempfile
 @with_tempfile
-def test_install_source_relpath(src, dest):
+def test_install_source_relpath(src=None, dest=None):
     ds1 = create(src)
     src_ = basename(src)
     with chpwd(dirname(src)):
         ds2 = install(dest, source=src_)
 
 
+@known_failure_windows  #FIXME
 @integration  # 41.2043s
 @with_tempfile
 @with_tempfile
 @with_tempfile
 @with_tempfile
-@known_failure_direct_mode  #FIXME
-def test_install_consistent_state(src, dest, dest2, dest3):
+def test_install_consistent_state(src=None, dest=None, dest2=None, dest3=None):
     # if we install a dataset, where sub-dataset "went ahead" in that branch,
     # while super-dataset was not yet updated (e.g. we installed super before)
     # then it is desired to get that default installed branch to get to the
     # position where previous location was pointing to.
     # It is indeed a mere heuristic which might not hold the assumption in some
     # cases, but it would work for most simple and thus mostly used ones
     ds1 = create(src)
     sub1 = ds1.create('sub1')
 
     def check_consistent_installation(ds):
         datasets = [ds] + list(
-            map(Dataset, ds.subdatasets(recursive=True, fulfilled=True,
+            map(Dataset, ds.subdatasets(recursive=True, state='present',
                                         result_xfm='paths')))
         assert len(datasets) == 2  # in this test
         for ds in datasets:
-            # all of them should be in master branch
-            eq_(ds.repo.get_active_branch(), "master")
+            # all of them should be in the default branch
+            eq_(ds.repo.get_active_branch(), DEFAULT_BRANCH)
             # all of them should be clean, so sub should be installed in a "version"
             # as pointed by the super
             ok_(not ds.repo.dirty)
 
     dest_ds = install(dest, source=src)
     # now we progress sub1 by adding sub2
     subsub2 = sub1.create('sub2')
 
     # and progress subsub2 forward to stay really thorough
     put_file_under_git(subsub2.path, 'file.dat', content="data")
-    subsub2.save("added a file")  # above function does not commit
+    subsub2.save(message="added a file")  # above function does not commit
 
     # just installing a submodule -- apparently different code/logic
     # but also the same story should hold - we should install the version pointed
     # by the super, and stay all clean
     dest_sub1 = dest_ds.install('sub1')
     check_consistent_installation(dest_ds)
 
@@ -843,62 +886,174 @@
     # to it based on dest3_ds
     dest3_ds.install('sub1', recursive=True)
     check_consistent_installation(dest3_ds)
 
     # TODO: makes a nice use-case for an update operation
 
 
-from datalad.tests.utils import skip_ssh
-
 @skip_ssh
 @with_tempfile
 @with_tempfile
-def test_install_subds_with_space(opath, tpath):
+def test_install_subds_with_space(opath=None, tpath=None):
     ds = create(opath)
     ds.create('sub ds')
     # works even now, boring
     # install(tpath, source=opath, recursive=True)
-    # do via ssh!
-    install(tpath, source="localhost:" + opath, recursive=True)
+    if on_windows:
+        # on windows we cannot simply prepend datalad-test: to a path
+        # and get a working sshurl...
+        install(tpath, source=opath, recursive=True)
+    else:
+        # do via ssh!
+        install(tpath, source="datalad-test:" + opath, recursive=True)
     assert Dataset(opj(tpath, 'sub ds')).is_installed()
 
 
 # https://github.com/datalad/datalad/issues/2232
 @with_tempfile
 @with_tempfile
-def test_install_from_tilda(opath, tpath):
+def test_install_from_tilda(opath=None, tpath=None):
     ds = create(opath)
     ds.create('sub ds')
     orelpath = os.path.join(
         '~',
         os.path.relpath(opath, os.path.expanduser('~'))
     )
     assert orelpath.startswith('~')  # just to make sure no normalization
     install(tpath, source=orelpath, recursive=True)
     assert Dataset(opj(tpath, 'sub ds')).is_installed()
 
 
+@skip_if_on_windows  # create_sibling incompatible with win servers
 @skip_ssh
 @usecase
 @with_tempfile(mkdir=True)
-def test_install_subds_from_another_remote(topdir):
+def test_install_subds_from_another_remote(topdir=None):
     # https://github.com/datalad/datalad/issues/1905
+    from datalad.support.network import PathRI
     with chpwd(topdir):
         origin_ = 'origin'
         clone1_ = 'clone1'
         clone2_ = 'clone2'
 
-        origin = create(origin_, no_annex=True)
+        origin = create(origin_, annex=False)
         clone1 = install(source=origin, path=clone1_)
         # print("Initial clone")
-        clone1.create_sibling('ssh://localhost%s/%s' % (getpwd(), clone2_), name=clone2_)
+        clone1.create_sibling('ssh://datalad-test%s/%s' % (PathRI(getpwd()).posixpath, clone2_), name=clone2_)
 
         # print("Creating clone2")
-        clone1.publish(to=clone2_)
+        clone1.push(to=clone2_)
         clone2 = Dataset(clone2_)
         # print("Initiating subdataset")
         clone2.create('subds1')
 
         # print("Updating")
         clone1.update(merge=True, sibling=clone2_)
         # print("Installing within updated dataset -- should be able to install from clone2")
-        clone1.install('subds1')
+        clone1.install('subds1')
+
+
+# Takes > 2 sec
+# Do not use cassette
+@pytest.mark.parametrize("suffix", ["", "/.git"])
+@skip_if_no_network
+@with_tempfile
+def test_datasets_datalad_org(tdir=None, *, suffix):
+    # Test that git annex / datalad install, get work correctly on our datasets.datalad.org
+    # Apparently things can break, especially with introduction of the
+    # smart HTTP backend for apache2 etc
+    ds = install(tdir, source='///dicoms/dartmouth-phantoms/bids_test6-PD+T2w' + suffix)
+    eq_(ds.config.get(f'remote.{DEFAULT_REMOTE}.annex-ignore', None),
+        None)
+    # assert_result_count and not just assert_status since for some reason on
+    # Windows we get two records due to a duplicate attempt (as res[1]) to get it
+    # again, which is reported as "notneeded".  For the purpose of this test
+    # it doesn't make a difference.
+    assert_result_count(
+        ds.get(op.join('001-anat-scout_ses-{date}', '000001.dcm')),
+        1,
+        status='ok')
+    assert_status('ok', ds.drop(what='all', reckless='kill', recursive=True))
+
+
+# https://github.com/datalad/datalad/issues/3469
+@with_tempfile(mkdir=True)
+def test_relpath_semantics(path=None):
+    with chpwd(path):
+        super = create('super')
+        create('subsrc')
+        sub = install(
+            dataset='super', source='subsrc', path=op.join('super', 'sub'))
+        eq_(sub.path, op.join(super.path, 'sub'))
+
+
+@with_tempfile(mkdir=True)
+@serve_path_via_http
+def test_install_branch(path=None, url=None):
+    path = Path(path)
+    ds_a = create(path / "ds_a")
+    ds_a.create("sub")
+
+    repo_a = ds_a.repo
+    repo_a.commit(msg="c1", options=["--allow-empty"])
+    repo_a.checkout(DEFAULT_BRANCH + "-other", ["-b"])
+    repo_a.commit(msg="c2", options=["--allow-empty"])
+    repo_a.checkout(DEFAULT_BRANCH)
+
+    # Clone from URL with custom branch specified should work
+    assert ds_a.repo.call_git_success(['update-server-info'])
+    tmp_path = path / "tmp"
+    os.mkdir(tmp_path)
+    with chpwd(tmp_path):
+        ds_b = install(url + "ds_a/.git", branch=DEFAULT_BRANCH + "-other")
+    repo_b = ds_b.repo
+    eq_(repo_b.get_corresponding_branch() or repo_b.get_active_branch(),
+        DEFAULT_BRANCH + "-other")
+
+    ds_b = install(source=ds_a.path, path=str(path / "ds_b"),
+                   branch=DEFAULT_BRANCH + "-other", recursive=True)
+
+    repo_b = ds_b.repo
+    eq_(repo_b.get_corresponding_branch() or repo_b.get_active_branch(),
+        DEFAULT_BRANCH + "-other")
+
+    repo_sub = Dataset(ds_b.pathobj / "sub").repo
+    eq_(repo_sub.get_corresponding_branch() or repo_sub.get_active_branch(),
+        DEFAULT_BRANCH)
+
+
+def _create_test_install_recursive_github(path):  # pragma: no cover
+    # to be ran once to populate a hierarchy of test datasets on github
+    # Making it a full round-trip would require github credentials on CI etc
+    ds = create(opj(path, "testrepo  gh"))
+    # making them with spaces and - to ensure that we consistently use the mapping
+    # for create and for get/clone/install
+    ds.create("sub _1")
+    ds.create("sub _1/d/sub_-  1")
+    import datalad.distribution.create_sibling_github  # to bind API
+    ds.create_sibling_github(
+        "testrepo  gh",
+        github_organization='datalad',
+        recursive=True,
+        # yarik forgot to push first, "replace" is not working in non-interactive IIRC
+        # existing='reconfigure'
+    )
+    return ds.push(recursive=True, to='github')
+
+
+@skip_if_no_network
+@with_tempfile(mkdir=True)
+def test_install_recursive_github(path=None):
+    # test recursive installation of a hierarchy of datasets created on github
+    # using datalad create-sibling-github.  Following invocation was used to poplate it
+    #
+    # out = _create_test_install_recursive_github(path)
+
+    # "testrepo  gh" was mapped by our sanitization in create_sibling_github to testrepo_gh, thus
+    for i, url in enumerate([
+        'https://github.com/datalad/testrepo_gh',
+        # optionally made available to please paranoids, but with all takes too long (22sec)
+        #'https://github.com/datalad/testrepo_gh.git',
+        #'git@github.com:datalad/testrepo_gh.git',
+    ]):
+        ds = install(source=url, path=opj(path, "clone%i" % i), recursive=True)
+        eq_(len(ds.subdatasets(recursive=True, state='present')), 2)
```

### Comparing `datalad-0.9.3/datalad/distribution/tests/test_get.py` & `datalad-1.0.0/datalad/core/local/tests/test_diff.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,516 +1,544 @@
-# ex: set sts=4 ts=4 sw=4 noet:
+# emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil; coding: utf-8 -*-
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""Test get action
+"""test dataset diff
 
 """
 
+__docformat__ = 'restructuredtext'
 
-from datalad.tests.utils import known_failure_v6
-from datalad.tests.utils import known_failure_direct_mode
+import os
+import os.path as op
+from unittest.mock import patch
+
+import datalad.utils as ut
+from datalad.api import (
+    create,
+    diff,
+    save,
+)
+from datalad.cmd import (
+    GitWitlessRunner,
+    StdOutCapture,
+)
+from datalad.consts import PRE_INIT_COMMIT_SHA
+from datalad.distribution.dataset import Dataset
+from datalad.support.exceptions import NoDatasetFound
+from datalad.tests.utils_pytest import (
+    DEFAULT_BRANCH,
+    OBSCURE_FILENAME,
+    SkipTest,
+    assert_in,
+    assert_raises,
+    assert_repo_status,
+    assert_result_count,
+    assert_status,
+    chpwd,
+    create_tree,
+    eq_,
+    get_deeply_nested_structure,
+    has_symlink_capability,
+    known_failure_githubci_win,
+    neq_,
+    ok_,
+    with_tempfile,
+)
+from datalad.utils import Path
+
+
+def test_magic_number():
+    # we hard code the magic SHA1 that represents the state of a Git repo
+    # prior to the first commit -- used to diff from scratch to a specific
+    # commit
+    # given the level of dark magic, we better test whether this stays
+    # constant across Git versions (it should!)
+    out = GitWitlessRunner().run(
+        'cd ./ | git hash-object --stdin -t tree',
+        protocol=StdOutCapture)
+    eq_(out['stdout'].strip(), PRE_INIT_COMMIT_SHA)
+
+
+@with_tempfile(mkdir=True)
+@with_tempfile(mkdir=True)
+def test_repo_diff(path=None, norepo=None):
+    ds = Dataset(path).create()
+    assert_repo_status(ds.path)
+    assert_raises(ValueError, ds.repo.diff, fr='WTF', to='MIKE')
+
+    if ds.repo.is_managed_branch():
+        fr_base = DEFAULT_BRANCH
+        to = DEFAULT_BRANCH
+    else:
+        fr_base = "HEAD"
+        to = None
+
+    # no diff
+    eq_(ds.repo.diff(fr_base, to), {})
+    # bogus path makes no difference
+    eq_(ds.repo.diff(fr_base, to, paths=['THIS']), {})
+    # let's introduce a known change
+    create_tree(ds.path, {'new': 'empty'})
+    ds.save(to_git=True)
+    assert_repo_status(ds.path)
+    eq_(ds.repo.diff(fr=fr_base + '~1', to=fr_base),
+        {ut.Path(ds.repo.pathobj / 'new'): {
+            'state': 'added',
+            'type': 'file',
+            'bytesize': 5,
+            'gitshasum': '7b4d68d70fcae134d5348f5e118f5e9c9d3f05f6'}})
+    # modify known file
+    create_tree(ds.path, {'new': 'notempty'})
+    eq_(ds.repo.diff(fr='HEAD', to=None),
+        {ut.Path(ds.repo.pathobj / 'new'): {
+            'state': 'modified',
+            'type': 'file',
+            # the beast is modified, but no change in shasum -> not staged
+            'gitshasum': '7b4d68d70fcae134d5348f5e118f5e9c9d3f05f6',
+            'prev_gitshasum': '7b4d68d70fcae134d5348f5e118f5e9c9d3f05f6'}})
+    # per path query gives the same result
+    eq_(ds.repo.diff(fr=fr_base, to=to),
+        ds.repo.diff(fr=fr_base, to=to, paths=['new']))
+    # also given a directory as a constraint does the same
+    eq_(ds.repo.diff(fr=fr_base, to=to),
+        ds.repo.diff(fr=fr_base, to=to, paths=['.']))
+    # but if we give another path, it doesn't show up
+    eq_(ds.repo.diff(fr=fr_base, to=to, paths=['other']), {})
+
+    # make clean
+    ds.save()
+    assert_repo_status(ds.path)
+
+    # untracked stuff
+    create_tree(ds.path, {'deep': {'down': 'untracked', 'down2': 'tobeadded'}})
+    # default is to report all files
+    eq_(ds.repo.diff(fr='HEAD', to=None),
+        {
+            ut.Path(ds.repo.pathobj / 'deep' / 'down'): {
+                'state': 'untracked',
+                'type': 'file'},
+            ut.Path(ds.repo.pathobj / 'deep' / 'down2'): {
+                'state': 'untracked',
+                'type': 'file'}})
+    # but can be made more compact
+    eq_(ds.repo.diff(fr='HEAD', to=None, untracked='normal'),
+        {
+            ut.Path(ds.repo.pathobj / 'deep'): {
+                'state': 'untracked',
+                'type': 'directory'}})
+
+    # again a unmatching path constrained will give an empty report
+    eq_(ds.repo.diff(fr='HEAD', to=None, paths=['other']), {})
+    # perfect match and anything underneath will do
+    eq_(ds.repo.diff(fr='HEAD', to=None, paths=['deep']),
+        {
+            ut.Path(ds.repo.pathobj / 'deep' / 'down'): {
+                'state': 'untracked',
+                'type': 'file'},
+            ut.Path(ds.repo.pathobj / 'deep' / 'down2'): {
+                'state': 'untracked',
+                'type': 'file'}})
+
+
+def _dirty_results(res):
+    return [r for r in res if r.get('state', None) != 'clean']
+
+
+# this is an extended variant of `test_repo_diff()` above
+# that focuses on the high-level command API
+@with_tempfile(mkdir=True)
+@with_tempfile(mkdir=True)
+def test_diff(path=None, norepo=None):
+    with chpwd(norepo):
+        assert_raises(NoDatasetFound, diff)
+    ds = Dataset(path).create()
+    assert_repo_status(ds.path)
+    # reports stupid revision input
+    assert_result_count(
+        ds.diff(fr='WTF', on_failure='ignore', result_renderer='disabled'),
+        1,
+        status='impossible',
+        message="Git reference 'WTF' invalid")
+    # no diff
+    assert_result_count(_dirty_results(ds.diff(result_renderer='disabled')), 0)
+    assert_result_count(
+        _dirty_results(ds.diff(fr='HEAD', result_renderer='disabled')), 0)
+    # bogus path makes no difference
+    assert_result_count(
+        _dirty_results(ds.diff(path='THIS', fr='HEAD', result_renderer='disabled')),
+        0)
+    # let's introduce a known change
+    create_tree(ds.path, {'new': 'empty'})
+    ds.save(to_git=True)
+    assert_repo_status(ds.path)
+
+    if ds.repo.is_managed_branch():
+        fr_base = DEFAULT_BRANCH
+        to = DEFAULT_BRANCH
+    else:
+        fr_base = "HEAD"
+        to = None
+
+    res = _dirty_results(ds.diff(fr=fr_base + '~1', to=to, result_renderer='disabled'))
+    assert_result_count(res, 1)
+    assert_result_count(
+        res, 1, action='diff', path=op.join(ds.path, 'new'), state='added')
+    # we can also find the diff without going through the dataset explicitly
+    with chpwd(ds.path):
+        assert_result_count(
+            _dirty_results(diff(fr=fr_base + '~1', to=to,
+                                result_renderer='disabled')),
+            1,
+            action='diff', path=op.join(ds.path, 'new'), state='added')
+    # no diff against HEAD
+    assert_result_count(_dirty_results(ds.diff(result_renderer='disabled')), 0)
+    # modify known file
+    create_tree(ds.path, {'new': 'notempty'})
+    res = _dirty_results(ds.diff(result_renderer='disabled'))
+    assert_result_count(res, 1)
+    assert_result_count(
+        res, 1, action='diff', path=op.join(ds.path, 'new'),
+        state='modified')
+    # but if we give another path, it doesn't show up
+    assert_result_count(ds.diff(path='otherpath', result_renderer='disabled'), 0)
+    # giving the right path must work though
+    assert_result_count(
+        ds.diff(path='new', result_renderer='disabled'), 1,
+        action='diff', path=op.join(ds.path, 'new'), state='modified')
+    # stage changes
+    ds.repo.add('.', git=True)
+    # no change in diff, staged is not committed
+    assert_result_count(_dirty_results(ds.diff(result_renderer='disabled')), 1)
+    ds.save()
+    assert_repo_status(ds.path)
+    assert_result_count(_dirty_results(ds.diff(result_renderer='disabled')), 0)
+
+    # untracked stuff
+    create_tree(ds.path, {'deep': {'down': 'untracked', 'down2': 'tobeadded'}})
+    # a plain diff should report the untracked file
+    # but not directly, because the parent dir is already unknown
+    res = _dirty_results(ds.diff(result_renderer='disabled'))
+    assert_result_count(res, 1)
+    assert_result_count(
+        res, 1, state='untracked', type='directory',
+        path=op.join(ds.path, 'deep'))
+    # report of individual files is also possible
+    assert_result_count(
+        ds.diff(untracked='all', result_renderer='disabled'), 2, state='untracked',
+        type='file')
+    # an unmatching path will hide this result
+    assert_result_count(ds.diff(path='somewhere', result_renderer='disabled'), 0)
+    # perfect match and anything underneath will do
+    assert_result_count(
+        ds.diff(path='deep', result_renderer='disabled'), 1, state='untracked',
+        path=op.join(ds.path, 'deep'),
+        type='directory')
+    assert_result_count(
+        ds.diff(path='deep', result_renderer='disabled'), 1,
+        state='untracked', path=op.join(ds.path, 'deep'))
+    ds.repo.add(op.join('deep', 'down2'), git=True)
+    # now the remaining file is the only untracked one
+    assert_result_count(
+        ds.diff(result_renderer='disabled'), 1, state='untracked',
+        path=op.join(ds.path, 'deep', 'down'),
+        type='file')
 
-from os import curdir
-from os.path import join as opj, basename
-from glob import glob
-
-from datalad.api import create
-from datalad.api import get
-from datalad.api import install
-from datalad.interface.results import only_matching_paths
-from datalad.distribution.get import _get_flexible_source_candidates_for_submodule
-from datalad.support.annexrepo import AnnexRepo
-from datalad.support.exceptions import InsufficientArgumentsError
-from datalad.support.exceptions import RemoteNotAvailableError
-from datalad.tests.utils import ok_
-from datalad.tests.utils import ok_clean_git
-from datalad.tests.utils import eq_
-from datalad.tests.utils import with_tempfile
-from datalad.tests.utils import with_testrepos
-from datalad.tests.utils import with_tree
-from datalad.tests.utils import create_tree
-from datalad.tests.utils import assert_raises
-from datalad.tests.utils import assert_in
-from datalad.tests.utils import assert_status
-from datalad.tests.utils import assert_in_results
-from datalad.tests.utils import assert_not_in_results
-from datalad.tests.utils import assert_result_count
-from datalad.tests.utils import assert_message
-from datalad.tests.utils import serve_path_via_http
-from datalad.tests.utils import slow
-from datalad.utils import with_pathsep
-from datalad.utils import chpwd
-from datalad.utils import assure_list
-from datalad.utils import rmtree
-
-from ..dataset import Dataset
-
-
-def _make_dataset_hierarchy(path):
-    origin = Dataset(path).create()
-    origin_sub1 = origin.create('sub1')
-    origin_sub2 = origin_sub1.create('sub2')
-    with open(opj(origin_sub2.path, 'file_in_annex.txt'), "w") as f:
-        f.write('content2')
-    origin_sub3 = origin_sub2.create('sub3')
-    with open(opj(origin_sub3.path, 'file_in_annex.txt'), "w") as f:
-        f.write('content3')
-    origin_sub4 = origin_sub3.create('sub4')
-    origin.add('.', recursive=True)
-    return origin, origin_sub1, origin_sub2, origin_sub3, origin_sub4
-
-
-@with_tempfile
-@with_tempfile
-def test_get_flexible_source_candidates_for_submodule(t, t2):
-    f = _get_flexible_source_candidates_for_submodule
-    # for now without mocking -- let's just really build a dataset
-    ds = create(t)
-    clone = install(
-        t2, source=t,
-        result_xfm='datasets', return_type='item-or-list')
-
-    # first one could just know about itself or explicit url provided
-    sshurl = 'ssh://e.c'
-    httpurl = 'http://e.c'
-    # Expansion with '/.git' no longer done in this helper
-    #sm_httpurls = [httpurl, httpurl + '/.git']
-    sm_httpurls = [httpurl]
-    eq_(f(ds, 'sub'), [])
-    eq_(f(ds, 'sub', sshurl), [sshurl])
-    eq_(f(ds, 'sub', httpurl), sm_httpurls)
-    eq_(f(ds, 'sub', None), [])  # otherwise really we have no clue were to get from
-
-    # but if we work on dsclone then it should also add urls deduced from its
-    # own location default remote for current branch
-    eq_(f(clone, 'sub'), [t + '/sub'])
-    eq_(f(clone, 'sub', sshurl), [t + '/sub', sshurl])
-    eq_(f(clone, 'sub', httpurl), [t + '/sub'] + sm_httpurls)
-    eq_(f(clone, 'sub'), [t + '/sub'])  # otherwise really we have no clue were to get from
-    # TODO: check that http:// urls for the dataset itself get resolved
-
-    # TODO: many more!!
-
-
-@with_tempfile(mkdir=True)
-@with_tempfile(content="doesntmatter")
-@known_failure_direct_mode  #FIXME
-def test_get_invalid_call(path, file_outside):
-
-    # no argument at all:
-    assert_raises(InsufficientArgumentsError, get, None)
-    assert_raises(InsufficientArgumentsError, get, [])
-    # invalid dataset:
-    assert_status('impossible', get(None, dataset=path, on_failure='ignore'))
 
-    # have a plain git:
-    ds = Dataset(path)
-    ds.create(no_annex=True)
-    with open(opj(path, "some.txt"), "w") as f:
-        f.write("whatever")
-    ds.add("some.txt", to_git=True)
-    ds.save("Initial commit.")
-
-    # make it an annex:
-    AnnexRepo(path, init=True, create=True)
-    # call get again on a file in git:
-    result = ds.get("some.txt")
-    assert_status('notneeded', result)
-
-    # invalid source:
-    # yoh:  but now we would need to add it to annex since clever code first
-    # checks what needs to be fetched at all
-    create_tree(path, {'annexed.dat': 'some'})
-    ds.add("annexed.dat")
-    ds.repo.drop("annexed.dat", options=['--force'])
-    with assert_raises(RemoteNotAvailableError) as ce:
-        ds.get("annexed.dat", source='MysteriousRemote')
-    eq_("MysteriousRemote", ce.exception.remote)
-
-    res = ds.get("NotExistingFile.txt", on_failure='ignore')
-    assert_status('impossible', res)
-    assert_message("path does not exist", res)
-
-    # path outside repo errors as with most other commands:
-    res = ds.get(file_outside, on_failure='ignore')
-    assert_in_results(
-        res, status='impossible', message='path not associated with any dataset')
+@with_tempfile(mkdir=True)
+def test_diff_recursive(path=None):
+    ds = Dataset(path).create()
+    sub = ds.create('sub')
+    # look at the last change, and confirm a dataset was added
+    res = ds.diff(fr=DEFAULT_BRANCH + '~1', to=DEFAULT_BRANCH,
+                  result_renderer='disabled')
+    assert_result_count(
+        res, 1, action='diff', state='added', path=sub.path, type='dataset')
+    # now recursive
+    res = ds.diff(recursive=True, fr=DEFAULT_BRANCH + '~1', to=DEFAULT_BRANCH,
+                  result_renderer='disabled')
+    # we also get the entire diff of the subdataset from scratch
+    assert_status('ok', res)
+    ok_(len(res) > 3)
+    # one specific test
+    assert_result_count(
+        res, 1, action='diff', state='added',
+        path=op.join(sub.path, '.datalad', 'config'))
+
+    # now we add a file to just the parent
+    create_tree(
+        ds.path,
+        {'onefile': 'tobeadded', 'sub': {'twofile': 'tobeadded'}})
+    res = ds.diff(recursive=True, untracked='all', result_renderer='disabled')
+    assert_result_count(_dirty_results(res), 3)
+    assert_result_count(
+        res, 1,
+        action='diff', state='untracked', path=op.join(ds.path, 'onefile'),
+        type='file')
+    assert_result_count(
+        res, 1,
+        action='diff', state='modified', path=sub.path, type='dataset')
+    assert_result_count(
+        res, 1,
+        action='diff', state='untracked', path=op.join(sub.path, 'twofile'),
+        type='file')
+    # intentional save in two steps to make check below easier
+    ds.save('sub', recursive=True)
+    ds.save()
+    assert_repo_status(ds.path)
+
+    head_ref = DEFAULT_BRANCH if ds.repo.is_managed_branch() else 'HEAD'
+
+    # look at the last change, only one file was added
+    res = ds.diff(fr=head_ref + '~1', to=head_ref, annex='basic',
+                  result_renderer='disabled')
+    assert_result_count(_dirty_results(res), 1)
+    assert_result_count(
+        res, 1,
+        action='diff', state='added', path=op.join(ds.path, 'onefile'),
+        type='file')
+
+    # now the exact same thing with recursion, must not be different from the
+    # call above
+    res = ds.diff(recursive=True, fr=head_ref + '~1', to=head_ref,
+                  annex='basic', result_renderer='disabled')
+    assert_result_count(_dirty_results(res), 1)
+    # last change in parent
+    assert_result_count(
+        res, 1, action='diff', state='added', path=op.join(ds.path, 'onefile'),
+        type='file')
+
+    if ds.repo.is_managed_branch():
+        raise SkipTest(
+            "Test assumption broken: https://github.com/datalad/datalad/issues/3818")
+    # one further back brings in the modified subdataset, and the added file
+    # within it
+    res = ds.diff(recursive=True, fr=head_ref + '~2', to=head_ref,
+                  annex='basic', result_renderer='disabled')
+    assert_result_count(_dirty_results(res), 3)
+    assert_result_count(
+        res, 1,
+        action='diff', state='added', path=op.join(ds.path, 'onefile'),
+        type='file')
+    assert_result_count(
+        res, 1,
+        action='diff', state='added', path=op.join(sub.path, 'twofile'),
+        type='file')
+    assert_result_count(
+        res, 1,
+        action='diff', state='modified', path=sub.path, type='dataset')
 
 
-@with_testrepos('basic_annex', flavors='clone')
-def test_get_single_file(path):
+@with_tempfile(mkdir=True)
+@with_tempfile()
+def test_path_diff(_path=None, linkpath=None):
+    # do the setup on the real path, not the symlink, to have its
+    # bugs not affect this test of status()
+    ds = get_deeply_nested_structure(str(_path))
+    if has_symlink_capability():
+        # make it more complicated by default
+        ut.Path(linkpath).symlink_to(_path, target_is_directory=True)
+        path = linkpath
+    else:
+        path = _path
 
     ds = Dataset(path)
-    ok_(ds.is_installed())
-    ok_(ds.repo.file_has_content('test-annex.dat') is False)
-    result = ds.get("test-annex.dat")
-    assert_result_count(result, 1)
-    assert_status('ok', result)
-    eq_(result[0]['path'], opj(ds.path, 'test-annex.dat'))
-    eq_(result[0]['annexkey'], ds.repo.get_file_key('test-annex.dat'))
-    ok_(ds.repo.file_has_content('test-annex.dat') is True)
-
-
-@with_tree(tree={'file1.txt': 'whatever 1',
-                 'file2.txt': 'whatever 2',
-                 'file3.txt': 'whatever 3',
-                 'file4.txt': 'whatever 4'})
-@serve_path_via_http
-@with_tempfile(mkdir=True)
-@known_failure_direct_mode  #FIXME
-def test_get_multiple_files(path, url, ds_dir):
-    from os import listdir
-    from datalad.support.network import RI
-
-    file_list = [f for f in listdir(path) if not f.startswith('.')]
-
-    # prepare urls:
-    [RI(url + f) for f in file_list]
-
-    # prepare origin
-    origin = Dataset(path).create(force=True)
-    origin.add(file_list)
-    origin.save("initial")
-
-    ds = install(
-        ds_dir, source=path,
-        result_xfm='datasets', return_type='item-or-list')
-
-    # no content present:
-    ok_(not any(ds.repo.file_has_content(file_list)))
-
-    # get two plus an invalid one:
-    result = ds.get(['file1.txt', 'file2.txt', 'not_existing.txt'],
-                    on_failure='ignore')
-    assert_status('impossible', [result[0]])
-    assert_status(['ok', 'notneeded'], result[1:])
-    # explicitly given not existing file was skipped:
-    # (see test_get_invalid_call)
-    eq_(set([basename(item.get('path')) for item in result[1:]]),
-        {'file1.txt', 'file2.txt'})
-    ok_(all(ds.repo.file_has_content(['file1.txt', 'file2.txt'])))
-
-    # get all of them:
-    result = ds.get(curdir)
-    # there were two files left to get:
-    eq_(set([basename(item.get('path')) for item in result if item['type'] == 'file']),
-        {'file3.txt', 'file4.txt'})
-    ok_(all(ds.repo.file_has_content(file_list)))
-
-
-@with_tree(tree={'file1.txt': 'something',
-                 'subdir': {'file2.txt': 'something else',
-                            'subsubdir': {
-                                'file3.txt': 'something completely different',
-                                'file4.txt': 'something'
-                            }}})
-@with_tempfile(mkdir=True)
-@known_failure_direct_mode  #FIXME
-def test_get_recurse_dirs(o_path, c_path):
-
-    # prepare source:
-    origin = Dataset(o_path).create(force=True)
-    origin.add('.')
-
-    ds = install(
-        c_path, source=o_path,
-        result_xfm='datasets', return_type='item-or-list')
-
-    file_list = ['file1.txt',
-                 opj('subdir', 'file2.txt'),
-                 opj('subdir', 'subsubdir', 'file3.txt'),
-                 opj('subdir', 'subsubdir', 'file4.txt')]
-    files_in_sub = [f for f in file_list if f.startswith(with_pathsep('subdir'))]
-
-    # no content present:
-    ok_(not any(ds.repo.file_has_content(file_list)))
-
-    result = ds.get('subdir')
-
-    # check result:
-    assert_status('ok', result)
-    eq_(set([item.get('path')[len(ds.path) + 1:] for item in result
-             if item['type'] == 'file']),
-        set(files_in_sub))
-    # we also get one report on the subdir
-    eq_(len(result) - 1, len(files_in_sub))
-
-    # got all files beneath subdir:
-    ok_(all(ds.repo.file_has_content(files_in_sub)))
-
-    # additionally got file1.txt silently, since it has the same content as
-    # subdir/subsubdir/file4.txt:
-    ok_(ds.repo.file_has_content('file1.txt') is True)
-
-
-@slow  # 15.1496s
-@with_testrepos('submodule_annex', flavors='local')
-@with_tempfile(mkdir=True)
-@known_failure_v6  #FIXME
-def test_get_recurse_subdatasets(src, path):
-
-    ds = install(
-        path, source=src,
-        result_xfm='datasets', return_type='item-or-list')
-
-    # ask for the two subdatasets specifically. This will obtain them,
-    # but not any content of any files in them
-    subds1, subds2 = ds.get(['subm 1', '2'], get_data=False,
-                            description="youcouldnotmakethisup",
-                            result_xfm='datasets')
-    for d in (subds1, subds2):
-        eq_(d.repo.get_description(), 'youcouldnotmakethisup')
-
-    # there are 3 files to get: test-annex.dat within each dataset:
-    rel_path_sub1 = opj(basename(subds1.path), 'test-annex.dat')
-    rel_path_sub2 = opj(basename(subds2.path), 'test-annex.dat')
-    annexed_files = {'test-annex.dat',
-                     rel_path_sub1,
-                     rel_path_sub2}
-
-    # None of them is currently present:
-    ok_(ds.repo.file_has_content('test-annex.dat') is False)
-    ok_(subds1.repo.file_has_content('test-annex.dat') is False)
-    ok_(subds2.repo.file_has_content('test-annex.dat') is False)
-
-    ok_clean_git(subds1.path)
-    # explicitly given path in subdataset => implicit recursion:
-    # MIH: Nope, we fulfill the dataset handle, but that doesn't
-    #      imply fulfilling all file handles
-    result = ds.get(rel_path_sub1, recursive=True)
-    # all good actions
-    assert_status('ok', result)
-
-    assert_in_results(result, path=opj(ds.path, rel_path_sub1), status='ok')
-    ok_(subds1.repo.file_has_content('test-annex.dat') is True)
-
-    # drop it:
-    subds1.repo.drop('test-annex.dat')
-    ok_(subds1.repo.file_has_content('test-annex.dat') is False)
-
-    # now, with a path not explicitly pointing within a
-    # subdataset, but recursive option:
-    # get everything:
-    result = ds.get(recursive=True, result_filter=lambda x: x.get('type') != 'dataset')
-    assert_status('ok', result)
-
-    eq_(set([item.get('path')[len(ds.path) + 1:] for item in result
-             if item['type'] == 'file']),
-        annexed_files)
-    ok_(ds.repo.file_has_content('test-annex.dat') is True)
-    ok_(subds1.repo.file_has_content('test-annex.dat') is True)
-    ok_(subds2.repo.file_has_content('test-annex.dat') is True)
-
-    # drop them:
-    ds.repo.drop('test-annex.dat')
-    subds1.repo.drop('test-annex.dat')
-    subds2.repo.drop('test-annex.dat')
-    ok_(ds.repo.file_has_content('test-annex.dat') is False)
-    ok_(subds1.repo.file_has_content('test-annex.dat') is False)
-    ok_(subds2.repo.file_has_content('test-annex.dat') is False)
-
-    # now, the very same call, but without recursive:
-    result = ds.get('.', recursive=False)
-    assert_status('ok', result)
-    # one report is on the requested dir
-    eq_(len(result) - 1, 1)
-    assert_result_count(
-        result, 1, path=opj(ds.path, 'test-annex.dat'), status='ok')
-    ok_(ds.repo.file_has_content('test-annex.dat') is True)
-    ok_(subds1.repo.file_has_content('test-annex.dat') is False)
-    ok_(subds2.repo.file_has_content('test-annex.dat') is False)
-
-
-@with_testrepos('submodule_annex', flavors='local')
-@with_tempfile(mkdir=True)
-def test_get_greedy_recurse_subdatasets(src, path):
-
-    ds = install(
-        path, source=src,
-        result_xfm='datasets', return_type='item-or-list')
-
-    # GIMME EVERYTHING
-    ds.get(['subm 1', '2'])
-
-    # We got all content in the subdatasets
-    subds1, subds2 = ds.subdatasets(result_xfm='datasets')
-    ok_(ds.repo.file_has_content('test-annex.dat') is False)
-    ok_(subds1.repo.file_has_content('test-annex.dat') is True)
-    ok_(subds2.repo.file_has_content('test-annex.dat') is True)
-
-
-@with_testrepos('submodule_annex', flavors='local')
-@with_tempfile(mkdir=True)
-def test_get_install_missing_subdataset(src, path):
-
-    ds = install(
-        path=path, source=src,
-        result_xfm='datasets', return_type='item-or-list')
-    ds.create(force=True)  # force, to cause dataset initialization
-    subs = ds.subdatasets(result_xfm='datasets')
-    ok_(all([not sub.is_installed() for sub in subs]))
-
-    # we don't install anything, if no explicitly given path points into a
-    # not yet installed subdataset:
-    ds.get(curdir)
-    ok_(all([not sub.is_installed() for sub in subs]))
-
-    # but we do, whenever a given path is contained in such a subdataset:
-    file_ = opj(subs[0].path, 'test-annex.dat')
-    ds.get(file_)
-    ok_(subs[0].is_installed())
-    ok_(subs[0].repo.file_has_content('test-annex.dat') is True)
-
-    # but we fulfill any handles, and dataset handles too
-    ds.get(curdir, recursive=True)
-    ok_(all([sub.is_installed() for sub in subs]))
-
-
-@slow  # 13.4610s
-# @with_tree(tree={'file_in_git.txt': 'no idea',
-#                  'subds': {'file_in_annex.txt': 'content'}})
-@with_tempfile(mkdir=True)
-@with_tempfile(mkdir=True)
-@known_failure_direct_mode  #FIXME
-def test_get_mixed_hierarchy(src, path):
-
-    origin = Dataset(src).create(no_annex=True)
-    origin_sub = origin.create('subds')
-    with open(opj(origin.path, 'file_in_git.txt'), "w") as f:
-        f.write('no idea')
-    with open(opj(origin_sub.path, 'file_in_annex.txt'), "w") as f:
-        f.write('content')
-    origin.add('file_in_git.txt', to_git=True)
-    origin_sub.add('file_in_annex.txt')
-    origin.save()
-
-    # now, install that thing:
-    ds, subds = install(
-        path, source=src, recursive=True,
-        result_xfm='datasets', return_type='item-or-list', result_filter=None)
-    ok_(subds.repo.file_has_content("file_in_annex.txt") is False)
-
-    # and get:
-    result = ds.get(curdir, recursive=True)
-    # git repo and subds
-    assert_status(['ok', 'notneeded'], result)
-    assert_result_count(
-        result, 1, path=opj(subds.path, "file_in_annex.txt"), status='ok')
-    ok_(subds.repo.file_has_content("file_in_annex.txt") is True)
-
-
-@with_testrepos('submodule_annex', flavors='local')
-@with_tempfile(mkdir=True)
-def test_autoresolve_multiple_datasets(src, path):
-    with chpwd(path):
-        ds1 = install(
-            'ds1', source=src,
-            result_xfm='datasets', return_type='item-or-list')
-        ds2 = install(
-            'ds2', source=src,
-            result_xfm='datasets', return_type='item-or-list')
-        results = get([opj('ds1', 'test-annex.dat')] + glob(opj('ds2', '*.dat')))
-        # each ds has one file
-        assert_result_count(results, 2, type='file', action='get', status='ok')
-        ok_(ds1.repo.file_has_content('test-annex.dat') is True)
-        ok_(ds2.repo.file_has_content('test-annex.dat') is True)
-
-
-@slow  # 20 sec
-@with_tempfile(mkdir=True)
-@with_tempfile(mkdir=True)
-@known_failure_direct_mode  #FIXME
-def test_get_autoresolve_recurse_subdatasets(src, path):
-
-    origin = Dataset(src).create()
-    origin_sub = origin.create('sub')
-    origin_subsub = origin_sub.create('subsub')
-    with open(opj(origin_subsub.path, 'file_in_annex.txt'), "w") as f:
-        f.write('content')
-    origin.add('.', recursive=True)
-
-    ds = install(
-        path, source=src,
-        result_xfm='datasets', return_type='item-or-list')
-    eq_(len(ds.subdatasets(fulfilled=True)), 0)
-
-    results = get(opj(ds.path, 'sub'), recursive=True, result_xfm='datasets')
-    eq_(len(ds.subdatasets(fulfilled=True, recursive=True)), 2)
-    subsub = Dataset(opj(ds.path, 'sub', 'subsub'))
-    ok_(subsub.is_installed())
-    assert_in(subsub, results)
-    # all file handles are fulfilled by default
-    ok_(Dataset(opj(ds.path, 'sub', 'subsub')).repo.file_has_content(
-        "file_in_annex.txt") is True)
-
-
-@slow  # 92sec
-@with_tempfile(mkdir=True)
-@with_tempfile(mkdir=True)
-@known_failure_direct_mode  #FIXME
-def test_recurse_existing(src, path):
-    origin_ds = _make_dataset_hierarchy(src)
-
-    # make sure recursion_limit works as expected across a range of depths
-    for depth in range(len(origin_ds)):
-        res = install(
-            path, source=src, recursive=True, recursion_limit=depth,
-            result_xfm=None, return_type='list', result_filter=None)
-        # we expect one dataset per level
+    if has_symlink_capability():
+        assert ds.pathobj != ds.repo.pathobj
+
+    plain_recursive = ds.diff(recursive=True, annex='all', result_renderer='disabled')
+    # check integrity of individual reports with a focus on how symlinks
+    # are reported
+    for res in plain_recursive:
+        # anything that is an "intended" symlink should be reported
+        # as such. In contrast, anything that is a symlink for mere
+        # technical reasons (annex using it for something in some mode)
+        # should be reported as the thing it is representing (i.e.
+        # a file)
+        if 'link2' in str(res['path']):
+            assert res['type'] == 'symlink', res
+        else:
+            assert res['type'] != 'symlink', res
+        # every item must report its parent dataset
+        assert_in('parentds', res)
+
+    # bunch of smoke tests
+    # query of '.' is same as no path
+    eq_(plain_recursive, ds.diff(path='.', recursive=True, annex='all',
+                                 result_renderer='disabled'))
+    # duplicate paths do not change things
+    eq_(plain_recursive, ds.diff(path=['.', '.'], recursive=True, annex='all',
+                                 result_renderer='disabled'))
+    # neither do nested paths
+    eq_(plain_recursive,
+        ds.diff(path=['.', 'subds_modified'], recursive=True, annex='all',
+                result_renderer='disabled'))
+    # when invoked in a subdir of a dataset it still reports on the full thing
+    # just like `git status`, as long as there are no paths specified
+    with chpwd(op.join(path, 'directory_untracked')):
+        plain_recursive = diff(recursive=True, annex='all',
+                               result_renderer='disabled')
+    # should be able to take absolute paths and yield the same
+    # output
+    eq_(plain_recursive, ds.diff(path=ds.path, recursive=True, annex='all',
+                                 result_renderer='disabled'))
+
+    # query for a deeply nested path from the top, should just work with a
+    # variety of approaches
+    rpath = op.join('subds_modified', 'subds_lvl1_modified',
+                    u'{}_directory_untracked'.format(OBSCURE_FILENAME))
+    apathobj = ds.pathobj / rpath
+    apath = str(apathobj)
+    for p in (rpath, apath, None):
+        if p is None:
+            # change into the realpath of the dataset and
+            # query with an explicit path
+            with chpwd(ds.path):
+                res = ds.diff(
+                    path=op.join('.', rpath),
+                    recursive=True,
+                    annex='all', result_renderer='disabled')
+        else:
+            res = ds.diff(
+                path=p,
+                recursive=True,
+                annex='all', result_renderer='disabled')
         assert_result_count(
-            res, depth + 1, type='dataset', status='ok')
-        rmtree(path)
+            res,
+            1,
+            state='untracked',
+            type='directory',
+            refds=ds.path,
+            # path always comes out a full path inside the queried dataset
+            path=apath,
+        )
+
+    assert_result_count(
+        ds.diff(
+            recursive=True, result_renderer='disabled'),
+        1,
+        path=apath)
+    # limiting recursion will exclude this particular path
+    assert_result_count(
+        ds.diff(
+            recursive=True,
+            recursion_limit=1, result_renderer='disabled'),
+        0,
+        path=apath)
+    # negative limit is unlimited limit
+    eq_(
+        ds.diff(recursive=True, recursion_limit=-1, result_renderer='disabled'),
+        ds.diff(recursive=True, result_renderer='disabled')
+    )
+
+
+@with_tempfile(mkdir=True)
+@with_tempfile(mkdir=True)
+def test_diff_nods(path=None, otherpath=None):
+    ds = Dataset(path).create()
+    assert_result_count(
+        ds.diff(path=otherpath, on_failure='ignore', result_renderer='disabled'),
+        1,
+        status='error',
+        message='path not underneath this dataset')
+    otherds = Dataset(otherpath).create()
+    assert_result_count(
+        ds.diff(path=otherpath, on_failure='ignore', result_renderer='disabled'),
+        1,
+        path=otherds.path,
+        status='error',
+        message=(
+            'dataset containing given paths is not underneath the '
+            'reference dataset %s: %s', ds, otherds.path)
+    )
+
+
+@with_tempfile(mkdir=True)
+def test_diff_rsync_syntax(path=None):
+    # three nested datasets
+    ds = Dataset(path).create()
+    subds = ds.create('sub')
+    subsubds = subds.create(Path('subdir', 'deep'))
+    justtop = ds.diff(fr=PRE_INIT_COMMIT_SHA, path='sub', result_renderer='disabled')
+    # we only get a single result, the subdataset in question
+    assert_result_count(justtop, 1)
+    assert_result_count(justtop, 1, type='dataset', path=subds.path)
+    # now with "peak inside the dataset" syntax
+    inside = ds.diff(fr=PRE_INIT_COMMIT_SHA, path='sub' + os.sep,
+                     result_renderer='disabled')
+    # we get both subdatasets, but nothing else inside the nested one
+    assert_result_count(inside, 2, type='dataset')
+    assert_result_count(inside, 1, type='dataset', path=subds.path)
+    assert_result_count(inside, 1, type='dataset', path=subsubds.path)
+    assert_result_count(inside, 0, type='file', parentds=subsubds.path)
+    # if we point to the subdir in 'sub' the reporting wrt the subsubds
+    # doesn't change. It is merely a path constraint within the queried
+    # subds, but because the subsubds is still underneath it, nothing changes
+    inside_subdir = ds.diff(
+        fr=PRE_INIT_COMMIT_SHA, path=op.join('sub', 'subdir'),
+        result_renderer='disabled')
+    assert_result_count(inside_subdir, 2, type='dataset')
+    assert_result_count(inside_subdir, 1, type='dataset', path=subds.path)
+    assert_result_count(inside_subdir, 1, type='dataset', path=subsubds.path)
+    assert_result_count(inside_subdir, 0, type='file', parentds=subsubds.path)
+    # but the rest is different (e.g. all the stuff in .datalad is gone)
+    neq_(inside, inside_subdir)
+    # just for completeness, we get more when going full recursive
+    rec = ds.diff(fr=PRE_INIT_COMMIT_SHA, recursive=True, path='sub' + os.sep,
+                  result_renderer='disabled')
+    assert(len(inside) < len(rec))
+
 
-    # now install all but the last two levels, no data
-    root, sub1, sub2 = install(
-        path, source=src, recursive=True, recursion_limit=2,
-        result_xfm='datasets', result_filter=None)
-    ok_(sub2.repo.file_has_content('file_in_annex.txt') is False)
-    sub3 = Dataset(opj(sub2.path, 'sub3'))
-    ok_(not sub3.is_installed())
-    # now get all content in all existing datasets, no new datasets installed
-    # in the process
-    files = root.get(curdir, recursive=True, recursion_limit='existing')
-    assert_not_in_results(files, type='dataset', status='ok')
-    assert_result_count(files, 1, type='file', status='ok')
-    ok_(sub2.repo.file_has_content('file_in_annex.txt') is True)
-    ok_(not sub3.is_installed())
-    # now pull down all remaining datasets, no data
-    sub3, sub4 = root.get(
-        curdir, recursive=True, get_data=False,
-        result_xfm='datasets', result_filter=lambda x: x['status'] == 'ok')
-    ok_(sub4.is_installed())
-    ok_(sub3.repo.file_has_content('file_in_annex.txt') is False)
-    # aaannd all data
-    files = root.get(curdir, recursive=True, result_filter=lambda x: x['status'] == 'ok' and x['type'] == 'file')
-    eq_(len(files), 1)
-    ok_(sub3.repo.file_has_content('file_in_annex.txt') is True)
-
-
-@slow  # 33sec
-@with_tempfile(mkdir=True)
-@with_tempfile(mkdir=True)
-@known_failure_direct_mode  #FIXME
-def test_get_in_unavailable_subdataset(src, path):
-    _make_dataset_hierarchy(src)
-    root = install(
-        path, source=src,
-        result_xfm='datasets', return_type='item-or-list')
-    targetpath = opj('sub1', 'sub2')
-    targetabspath = opj(root.path, targetpath)
-    res = get(targetabspath)
-    assert_result_count(res, 2, status='ok', action='install', type='dataset')
-    # dry-fit result filter that only returns the result that matched the requested
-    # path
-    filtered = [r for r in res if only_matching_paths(r, path=targetabspath)]
-    assert_result_count(
-        filtered, 1, status='ok', action='install', type='dataset',
-        path=targetabspath)
-    # we got the dataset, and its immediate content, but nothing below
-    sub2 = Dataset(targetabspath)
-    ok_(sub2.is_installed())
-    ok_(sub2.repo.file_has_content('file_in_annex.txt') is True)
-    ok_(not Dataset(opj(targetabspath, 'sub3')).is_installed())
+@with_tempfile(mkdir=True)
+def test_diff_nonexistent_ref_unicode(path=None):
+    ds = Dataset(path).create()
+    assert_result_count(
+        ds.diff(fr="HEAD", to=u"β", on_failure="ignore", result_renderer='disabled'),
+        1,
+        path=ds.path,
+        status="impossible")
+
+
+# https://github.com/datalad/datalad/issues/3997
+@with_tempfile(mkdir=True)
+def test_no_worktree_impact_false_deletions(path=None):
+    ds = Dataset(path).create()
+    # create a branch that has no new content
+    ds.repo.call_git(['checkout', '-b', 'test'])
+    # place two successive commits with file additions into the default branch
+    ds.repo.call_git(['checkout', DEFAULT_BRANCH])
+    (ds.pathobj / 'identical').write_text('should be')
+    ds.save()
+    (ds.pathobj / 'new').write_text('yes')
+    ds.save()
+    # now perform a diff for the last commit, there is one file that remained
+    # identifical
+    ds.repo.call_git(['checkout', 'test'])
+    res = ds.diff(fr=DEFAULT_BRANCH + '~1', to=DEFAULT_BRANCH,
+                  result_renderer='disabled')
+    # under no circumstances can there be any reports on deleted files
+    # because we never deleted anything
+    assert_result_count(res, 0, state='deleted')
+    # the identical file must be reported clean
+    assert_result_count(
+        res,
+        1,
+        state='clean',
+        path=str(ds.pathobj / 'identical'),
+    )
+
+
+@with_tempfile(mkdir=True)
+def test_diff_fr_none_one_get_content_annexinfo_call(path=None):
+    from datalad.support.annexrepo import AnnexRepo
+    ds = Dataset(path).create()
+    (ds.pathobj / "foo").write_text("foo")
+    ds.save()
+    # get_content_annexinfo() is expensive.  If fr=None, we should
+    # only need to call it once.
+    with patch.object(AnnexRepo, "get_content_annexinfo") as gca:
+        res = ds.diff(fr=None, to="HEAD", annex="all", result_renderer='disabled')
+        eq_(gca.call_count, 1)
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `datalad-0.9.3/datalad/distribution/tests/test_add.py` & `datalad-1.0.0/datalad/local/tests/test_run_procedure.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,365 +1,452 @@
-# ex: set sts=4 ts=4 sw=4 noet:
+# emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-; coding: utf-8 -*-
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""Test add action
+"""test command datalad run-procedure
 
 """
 
-from datalad.tests.utils import known_failure_direct_mode
+__docformat__ = 'restructuredtext'
 
-import logging
-from os.path import join as opj
+import json
+import os.path as op
+import sys
+from unittest.mock import patch
+
+from datalad.api import run_procedure
+from datalad.cmd import (
+    KillOutput,
+    WitlessRunner,
+)
+from datalad.distribution.dataset import Dataset
+from datalad.support.exceptions import (
+    CommandError,
+    InsufficientArgumentsError,
+)
+from datalad.tests.utils_pytest import (
+    OBSCURE_FILENAME,
+    assert_false,
+    assert_in_results,
+    assert_not_in_results,
+    assert_raises,
+    assert_repo_status,
+    assert_result_count,
+    assert_true,
+    eq_,
+    known_failure_windows,
+    ok_file_has_content,
+    patch_config,
+    with_tempfile,
+    with_tree,
+    xfail_buggy_annex_info,
+)
+from datalad.utils import (
+    chpwd,
+    quote_cmdlinearg,
+    swallow_outputs,
+)
+
+
+@with_tempfile(mkdir=True)
+def test_invalid_call(path=None):
+    with chpwd(path):
+        # ^ Change directory so that we don't fail with an
+        # InvalidGitRepositoryError if the test is executed from a git
+        # worktree.
+
+        # needs spec or discover
+        assert_raises(InsufficientArgumentsError, run_procedure)
+        # an unknown procedure should cause an error
+        assert_raises(ValueError, run_procedure, 'unknown')
 
-from datalad.api import create
-from datalad.api import add
-from datalad.api import install
-from datalad.support.exceptions import InsufficientArgumentsError
-from datalad.tests.utils import ok_
-from datalad.tests.utils import ok_clean_git
-from datalad.tests.utils import ok_file_under_git
-from datalad.tests.utils import eq_
-from datalad.tests.utils import with_tempfile
-from datalad.tests.utils import with_tree
-from datalad.tests.utils import assert_raises
-from datalad.tests.utils import assert_false
-from datalad.tests.utils import assert_in
-from datalad.tests.utils import assert_not_in
-from datalad.tests.utils import assert_status
-from datalad.tests.utils import assert_result_count
-from datalad.tests.utils import serve_path_via_http
-from datalad.tests.utils import SkipTest
-from datalad.tests.utils import create_tree
-from datalad.utils import chpwd
 
-from ..dataset import Dataset
+@with_tree(tree={'README.md': 'dirty'})
+def test_dirty(path=None):
+    ds = Dataset(path).create(force=True)
+    # must fail, because README.md is to be modified, but already dirty
+    assert_in_results(
+        ds.run_procedure('cfg_yoda',
+                         on_failure="ignore", result_renderer=None),
+        action="run", status="error")
+    # make sure that was the issue
+    # save to git explicitly to keep the test simple and avoid unlocking...
+    ds.save('README.md', to_git=True)
+    ds.run_procedure('cfg_yoda')
+    assert_repo_status(ds.path)
 
 
-@with_tempfile(mkdir=True)
-def test_add_insufficient_args(path):
-    # no argument:
-    assert_raises(InsufficientArgumentsError, add)
-    # no `path`, no `source`:
-    assert_raises(InsufficientArgumentsError, add, dataset=path)
+@with_tree(tree={
+    'code': {'datalad_test_proc.py': """\
+import sys
+import os.path as op
+from datalad.api import save, Dataset
+
+with open(op.join(sys.argv[1], 'fromproc.txt'), 'w') as f:
+    f.write('hello\\n')
+save(dataset=Dataset(sys.argv[1]), path='fromproc.txt')
+"""},
+    'cfg_yoda.sh': """\
+#!/bin/bash
+echo "I am never to be ran but could be discovered"
+exit 1
+"""})
+@with_tempfile
+def test_procedure_discovery(path=None, super_path=None):
     with chpwd(path):
-        res = add(path="some", on_failure='ignore')
-        assert_status('impossible', res)
-    ds = Dataset(opj(path, 'ds'))
-    ds.create()
-    # non-existing path outside
-    assert_status('impossible', ds.add(opj(path, 'outside'), on_failure='ignore'))
-    # existing path outside
-    with open(opj(path, 'outside'), 'w') as f:
-        f.write('doesnt matter')
-    assert_status('impossible', ds.add(opj(path, 'outside'), on_failure='ignore'))
-
-
-tree_arg = dict(tree={'test.txt': 'some',
-                      'test_annex.txt': 'some annex',
-                      'test1.dat': 'test file 1',
-                      'test2.dat': 'test file 2',
-                      'dir': {'testindir': 'someother',
-                              'testindir2': 'none'},
-                      'dir2': {'testindir3': 'someother3'}})
-
-
-@with_tree(**tree_arg)
-def test_add_files(path):
-    ds = Dataset(path)
-    ds.create(force=True)
-    ok_(ds.repo.dirty)
-
-    test_list_1 = ['test_annex.txt']
-    test_list_2 = ['test.txt']
-    test_list_3 = ['test1.dat', 'test2.dat']
-    test_list_4 = [opj('dir', 'testindir'), opj('dir', 'testindir2')]
-    all_files = test_list_1 + test_list_2 + test_list_3 + test_list_4
-    unstaged = set(all_files)
-    staged = set()
-
-    for arg in [(test_list_1[0], False),
-                (test_list_2[0], True),
-                (test_list_3, False),
-                (test_list_4, False)]:
-        # special case 4: give the dir:
-        if arg[0] == test_list_4:
-            result = ds.add('dir', to_git=arg[1], save=False)
-        else:
-            result = ds.add(arg[0], to_git=arg[1], save=False,
-                            result_xfm='relpaths',
-                            return_type='item-or-list')
-            # order depends on how annex processes it, so let's sort
-            eq_(sorted(result), sorted(arg[0]))
-        # added, but not committed:
-        ok_(ds.repo.dirty)
-
-        # get sets for comparison:
-        annexed = set(ds.repo.get_annexed_files())
-        indexed = set(ds.repo.get_indexed_files())
-        # ignore the initial config file in index:
-        indexed.remove(opj('.datalad', 'config'))
-        indexed.remove(opj('.datalad', '.gitattributes'))
-        indexed.remove('.gitattributes')
-        if isinstance(arg[0], list):
-            for x in arg[0]:
-                unstaged.remove(x)
-                staged.add(x)
-        else:
-            unstaged.remove(arg[0])
-            staged.add(arg[0])
-
-        # added, but nothing else was:
-        eq_(staged, indexed)
-        ok_(unstaged.isdisjoint(annexed))
-        ok_(unstaged.isdisjoint(indexed))
+        # ^ Change directory so that we don't fail with an
+        # InvalidGitRepositoryError if the test is executed from a git
+        # worktree.
+        ps = run_procedure(discover=True, result_renderer='disabled')
+        # there are a few procedures coming with datalad, needs to find them
+        assert_true(len(ps) > 2)
+        # we get essential properties
+        _check_procedure_properties(ps)
 
+    # set up dataset with registered procedure (c&p from test_basics):
+    ds = Dataset(path).create(force=True)
+    # extra check: must not pick up cfg_yoda.sh in top directory
+    ds.run_procedure('cfg_yoda')
+
+    # path to a procedure which is not under any "standard" location but
+    # present in the dataset
+    code_dir_procedure_path = op.join(ds.path, 'code', 'datalad_test_proc.py')
+    top_dir_procedure_path = op.join(ds.path, 'cfg_yoda.sh')
+
+    # run discovery on the dataset:
+    ps = ds.run_procedure(discover=True, result_renderer='disabled')
+    # it should not be found magically by default
+    assert_not_in_results(ps, path=code_dir_procedure_path)
+    assert_not_in_results(ps, path=top_dir_procedure_path)
+
+    with patch_config({'datalad.locations.extra-procedures': op.join(ds.path, 'code')}):
+        # run discovery on the dataset:
+        ps = ds.run_procedure(discover=True, result_renderer='disabled')
+        # still needs to find procedures coming with datalad
+        assert_true(len(ps) > 3)
+        # and procedure under the path we specified
+        assert_result_count(ps, 1, path=code_dir_procedure_path)
+        assert_not_in_results(ps, path=top_dir_procedure_path)
+
+    # multiple extra locations
+    with patch_config({'datalad.locations.extra-procedures': [op.join(ds.path, 'code'), ds.path]}):
+        # run discovery on the dataset:
+        ps = ds.run_procedure(discover=True, result_renderer='disabled')
+        # still needs to find procedures coming with datalad
+        assert_true(len(ps) > 4)
+        # and procedure under the path we specified
+        assert_result_count(ps, 1, path=code_dir_procedure_path)
+        assert_result_count(ps, 1, path=top_dir_procedure_path)
+
+    # configure dataset to look for procedures in its code folder
+    ds.config.add(
+        'datalad.locations.dataset-procedures',
+        'code',
+        scope='branch')
+    ds.save(op.join('.datalad', 'config'))
+
+    # run discovery on the dataset:
+    ps = ds.run_procedure(discover=True, result_renderer='disabled')
+
+    # still needs to find procedures coming with datalad
+    assert_true(len(ps) > 2)
+    # we get three essential properties
+    _check_procedure_properties(ps)
+    # dataset's procedure needs to be in the results
+    # and only a single one
+    assert_result_count(ps, 1, path=code_dir_procedure_path)
+    # a subdir shouldn't be considered a procedure just because it's "executable"
+    assert_not_in_results(ps, path=op.join(ds.path, 'code', 'testdir'))
+
+    # make it a subdataset and try again:
+    # first we need to save the beast to make install work
+    ds.save()
+    super = Dataset(super_path).create()
+    super.install('sub', source=ds.path)
+
+    ps = super.run_procedure(discover=True, result_renderer='disabled')
+    # still needs to find procedures coming with datalad
+    assert_true(len(ps) > 2)
+    _check_procedure_properties(ps)
+    # dataset's procedure needs to be in the results
+    assert_in_results(ps, path=op.join(super.path, 'sub', 'code',
+                                       'datalad_test_proc.py'))
+
+    if not ds.repo.is_managed_branch():  # no symlinks
+        import os
+
+        # create a procedure which is a broken symlink, but recognizable as a
+        # python script:
+        os.symlink(op.join(super.path, 'sub', 'not_existent'),
+                   op.join(super.path, 'sub', 'code', 'broken_link_proc.py'))
+        # broken symlink at procedure location, but we can't tell, whether it is
+        # an actual procedure without any guess on how to execute it:
+        os.symlink(op.join(super.path, 'sub', 'not_existent'),
+                   op.join(super.path, 'sub', 'code', 'unknwon_broken_link'))
+
+        ps = super.run_procedure(discover=True, result_renderer='disabled')
+        # still needs to find procedures coming with datalad and the dataset
+        # procedure registered before
+        assert_true(len(ps) > 3)
+        assert_in_results(ps, path=op.join(super.path, 'sub', 'code',
+                                           'broken_link_proc.py'),
+                          state='absent')
+        assert_in_results(
+            ps,
+            path=op.join(super.path, 'sub', 'code',
+                         'unknwon_broken_link'),
+            state='absent')
+
+
+def _check_procedure_properties(ps):
+    """a common check that we get three essential properties"""
+    eq_(
+        sum(['procedure_type' in p and
+             'procedure_callfmt' in p and
+             'path' in p
+             for p in ps]),
+        len(ps))
 
-@with_tempfile(mkdir=True)
-@known_failure_direct_mode  #FIXME
-def test_add_recursive(path):
-    # make simple hierarchy
-    parent = Dataset(path).create()
-    ok_clean_git(parent.path)
-    sub1 = parent.create(opj('down', 'sub1'))
-    ok_clean_git(parent.path)
-    sub2 = parent.create('sub2')
-    # next one make the parent dirty
-    subsub = sub2.create('subsub')
-    ok_clean_git(parent.path, index_modified=['sub2'])
-    res = parent.save()
-    ok_clean_git(parent.path)
-
-    # now add content deep in the hierarchy
-    create_tree(subsub.path, {'new': 'empty'})
-    ok_clean_git(parent.path, index_modified=['sub2'])
-
-    # recursive add should not even touch sub1, because
-    # it knows that it is clean
-    res = parent.add('.', recursive=True)
-    # the key action is done
-    assert_result_count(
-        res, 1, path=opj(subsub.path, 'new'), action='add', status='ok')
-    # sub1 is untouched, and not reported
-    assert_result_count(res, 0, path=sub1.path)
-    # saved all the way up
-    assert_result_count(res, 3, action='save', status='ok')
-    ok_clean_git(parent.path)
-
-
-@with_tree(**tree_arg)
-@known_failure_direct_mode  #FIXME
-def test_add_dirty_tree(path):
-    ds = Dataset(path)
-    ds.create(force=True, save=False)
-    subds = ds.create('dir', force=True)
-    ok_(subds.repo.dirty)
-
-    # no subds without recursive:
-    ds.add('.', recursive=False)
-    ok_(subds.repo.dirty)
-    # nosubds with recursion limit too low:
-    ds.add('.', recursive=True, recursion_limit=0)
-    ok_(subds.repo.dirty)
-
-    # add while also instructing annex to add in parallel 2 jobs (smoke testing
-    # for that effect ATM)
-    added1 = ds.add(opj('dir', 'testindir'), jobs=2)
-    # added to annex, so annex output record
-    assert_result_count(
-        added1, 1,
-        path=opj(ds.path, 'dir', 'testindir'), action='add',
-        annexkey='MD5E-s9--3f0f870d18d6ba60a79d9463ff3827ea',
-        status='ok')
-    assert_in('testindir', Dataset(opj(path, 'dir')).repo.get_annexed_files())
-    ok_(subds.repo.dirty)
-
-    # this tests wants to add the content to subdir before updating the
-    # parent, now we can finally say that explicitly
-    added2 = ds.add('dir/.', to_git=True)
-    # added to git, so parsed git output record
-    assert_result_count(
-        added2, 1,
-        path=opj(ds.path, 'dir', 'testindir2'), action='add',
-        message='non-large file; adding content to git repository',
-        status='ok')
-    assert_in('testindir2', Dataset(opj(path, 'dir')).repo.get_indexed_files())
-    ok_clean_git(ds.path)
-
-    # We used to fail to add to pure git repository, but now it should all be
-    # just fine
-    subds = ds.create('git-sub', no_annex=True)
-    with open(opj(subds.path, 'somefile.txt'), "w") as f:
-        f.write("bla bla")
-    result = ds.add(opj('git-sub', 'somefile.txt'), to_git=False)
-    # adds the file
-    assert_result_count(
-        result, 1,
-        action='add', path=opj(subds.path, 'somefile.txt'), status='ok')
-    # but also saves both datasets
-    assert_result_count(
-        result, 2,
-        action='save', status='ok', type='dataset')
 
+@with_tree(tree={
+    'code': {'datalad_test_proc.py': """\
+import sys
+import os.path as op
+from datalad.api import save, Dataset
+
+with open(op.join(sys.argv[1], 'fromproc.txt'), 'w') as f:
+    f.write('{}\\n'.format(sys.argv[2]))
+save(dataset=Dataset(sys.argv[1]), path='fromproc.txt')
+"""}})
+def test_configs(path=None):
 
-@with_tree(**tree_arg)
-def test_relpath_add(path):
+    # set up dataset with registered procedure (c&p from test_basics):
     ds = Dataset(path).create(force=True)
-    with chpwd(opj(path, 'dir')):
-        eq_(add('testindir')[0]['path'],
-            opj(ds.path, 'dir', 'testindir'))
-        # and now add all
-        add('..')
-    # auto-save enabled
-    assert_false(ds.repo.dirty)
-
-
-@with_tree(tree={'file1.txt': 'whatever 1',
-                 'file2.txt': 'whatever 2',
-                 'file3.txt': 'whatever 3',
-                 'file4.txt': 'whatever 4',
-                 'file5.txt': 'whatever 5',
-                 'file6.txt': 'whatever 6',
-                 'file7.txt': 'whatever 7'})
-@serve_path_via_http
-@with_tempfile(mkdir=True)
-def test_add_source(path, url, ds_dir):
-    raise SkipTest('functionality is not supported ATM')
-    from os import listdir
-    from datalad.support.network import RI
-
-    urls = [RI(url + f) for f in listdir(path)]
-    ds = Dataset(ds_dir).create()
-    eq_(len(ds.repo.get_annexed_files()), 0)
-
-    # add a remote source to git => fail:
-    assert_raises(NotImplementedError, ds.add, source=urls[0], to_git=True)
-    # annex add a remote source:
-    ds.add(source=urls[0])
-    eq_(len(ds.repo.get_annexed_files()), 1)
-
-    # add two remote source an give local names:
-    ds.add(path=['local1.dat', 'local2.dat'], source=urls[1:3])
-    annexed = ds.repo.get_annexed_files()
-    eq_(len(annexed), 3)
-    assert_in('local1.dat', annexed)
-    assert_in('local2.dat', annexed)
-
-    # add a second source for one of them
-    ds.add(path='local1.dat', source=urls[3])
-    eq_(len(annexed), 3)
-    whereis_dict = ds.repo.whereis('local1.dat', output='full')
-    reg_urls = [whereis_dict[uuid]['urls'] for uuid in whereis_dict
-                if not whereis_dict[uuid]['here']]
-    eq_(len(reg_urls), 1)  # one remote for 'local1.dat', that is not "here"
-    eq_({str(urls[1]), str(urls[3])},
-        set(reg_urls[0]))
-
-    # just to be sure compare to 'local2.dat':
-    whereis_dict = ds.repo.whereis('local2.dat', output='full')
-    reg_urls = [whereis_dict[uuid]['urls'] for uuid in whereis_dict
-                if not whereis_dict[uuid]['here']]
-    eq_(len(reg_urls), 1)  # one remote for 'local2.dat', that is not "here"
-    eq_([urls[2]], reg_urls[0])
-
-    # provide more paths than sources:
-    # report failure on non-existing 'local4.dat':
-    result = ds.add(path=['local3.dat', 'local4.dat'], source=urls[4])
-    ok_(all([r['success'] is False and r['note'] == 'not found'
-             for r in result if r['file'] == 'local4.dat']))
-
-    with open(opj(ds.path, 'local4.dat'), 'w') as f:
-        f.write('local4 content')
-
-    ds.add(path=['local3.dat', 'local4.dat'], source=urls[4])
-    annexed = ds.repo.get_annexed_files()
-    eq_(len(annexed), 5)
-    assert_in('local3.dat', annexed)
-    assert_in('local4.dat', annexed)
-
-    # 'local3.dat' has a remote source
-    whereis_dict = ds.repo.whereis('local3.dat', output='full')
-    reg_urls = [whereis_dict[uuid]['urls'] for uuid in whereis_dict
-                if not whereis_dict[uuid]['here']]
-    eq_(len(reg_urls), 1)  # one remote for 'local3.dat', that is not "here"
-    eq_([urls[4]], reg_urls[0])
-
-    # 'local4.dat' has no remote source
-    whereis_dict = ds.repo.whereis('local4.dat', output='full')
-    reg_urls = [whereis_dict[uuid]['urls'] for uuid in whereis_dict
-                if not whereis_dict[uuid]['here']]
-    eq_(len(reg_urls), 0)
-
-    # provide more sources than paths:
-    ds.add('local5.dat', source=urls[5:])
-    annexed = ds.repo.get_annexed_files()
-    assert_in('local5.dat', annexed)
-    eq_(len(annexed), 5 + len(urls[5:]))
-
-    # Note: local4.dat didn't come from an url,
-    # but 'local1.dat' consumes two urls
-    eq_(len(annexed), len(urls))
-    # all files annexed (-2 for '.git' and '.datalad'):
-    eq_(len(annexed), len(listdir(ds.path)) - 2)
+    ds.run_procedure('cfg_yoda')
+    # configure dataset to look for procedures in its code folder
+    ds.config.add(
+        'datalad.locations.dataset-procedures',
+        'code',
+        scope='branch')
+
+    # 1. run procedure based on execution guessing by run_procedure:
+    ds.run_procedure(spec=['datalad_test_proc', 'some_arg'])
+    # look for traces
+    ok_file_has_content(op.join(ds.path, 'fromproc.txt'), 'some_arg\n')
+
+    # 2. now configure specific call format including usage of substitution config
+    # for run:
+    ds.config.add(
+        'datalad.procedures.datalad_test_proc.call-format',
+        u'%s {script} {ds} {{mysub}} {args}' % quote_cmdlinearg(sys.executable),
+        scope='branch'
+    )
+    ds.config.add(
+        'datalad.run.substitutions.mysub',
+        'dataset-call-config',
+        scope='branch'
+    )
+    # TODO: Should we allow for --inputs/--outputs arguments for run_procedure
+    #       (to be passed into run)?
+    ds.unlock("fromproc.txt")
+    # run again:
+    ds.run_procedure(spec=['datalad_test_proc', 'some_arg'])
+    # look for traces
+    ok_file_has_content(op.join(ds.path, 'fromproc.txt'), 'dataset-call-config\n')
+
+    # 3. have a conflicting config at user-level, which should override the
+    # config on dataset level:
+    ds.config.add(
+        'datalad.procedures.datalad_test_proc.call-format',
+        u'%s {script} {ds} local {args}' % quote_cmdlinearg(sys.executable),
+        scope='local'
+    )
+    ds.unlock("fromproc.txt")
+    # run again:
+    ds.run_procedure(spec=['datalad_test_proc', 'some_arg'])
+    # look for traces
+    ok_file_has_content(op.join(ds.path, 'fromproc.txt'), 'local\n')
+
+    # 4. get configured help message:
+    r = ds.run_procedure('datalad_test_proc', help_proc=True,
+                         on_failure='ignore')
+    assert_true(len(r) == 1)
+    assert_in_results(r, status="impossible")
+
+    ds.config.add(
+        'datalad.procedures.datalad_test_proc.help',
+        "This is a help message",
+        scope='branch'
+    )
+
+    r = ds.run_procedure('datalad_test_proc', help_proc=True)
+    assert_true(len(r) == 1)
+    assert_in_results(r, message="This is a help message", status='ok')
 
 
-@with_tree(**tree_arg)
-@with_tempfile(mkdir=True)
-@known_failure_direct_mode  #FIXME
-def test_add_subdataset(path, other):
-    subds = create(opj(path, 'dir'), force=True)
-    ds = create(path, force=True)
-    ok_(subds.repo.dirty)
-    ok_(ds.repo.dirty)
-    assert_not_in('dir', ds.subdatasets(result_xfm='relpaths'))
-    # without a base dataset the next is interpreted as "add everything
-    # in subds to subds"
-    add(subds.path)
-    ok_clean_git(subds.path)
-    assert_not_in('dir', ds.subdatasets(result_xfm='relpaths'))
-    # but with a base directory we add the dataset subds as a subdataset
-    # to ds
-    ds.add(subds.path)
-    assert_in('dir', ds.subdatasets(result_xfm='relpaths'))
-    #  create another one
-    other = create(other)
-    # install into superdataset, but don't add
-    other_clone = install(source=other.path, path=opj(ds.path, 'other'))
-    ok_(other_clone.is_installed)
-    assert_not_in('other', ds.subdatasets(result_xfm='relpaths'))
-    # now add, it should pick up the source URL
-    ds.add('other')
-    # and that is why, we can reobtain it from origin
-    ds.uninstall('other')
-    ok_(other_clone.is_installed)
-    ds.get('other')
-    ok_(other_clone.is_installed)
+@with_tree(tree={
+    'code': {'datalad_test_proc.py': """\
+import sys
+import os.path as op
+from datalad.api import save, Dataset
+
+with open(op.join(sys.argv[1], sys.argv[2]), 'w') as f:
+    f.write('hello\\n')
+save(dataset=Dataset(sys.argv[1]), path=sys.argv[2])
+"""}})
+def test_spaces(path=None):
+    """
+    Test whether args with spaces are correctly parsed.
+    """
+    ds = Dataset(path).create(force=True)
+    ds.run_procedure('cfg_yoda')
+    # configure dataset to look for procedures in its code folder
+    ds.config.add(
+        'datalad.locations.dataset-procedures',
+        'code',
+        scope='branch')
+    # 1. run procedure based on execution guessing by run_procedure:
+    ds.run_procedure(spec=['datalad_test_proc', 'with spaces', 'unrelated'])
+    # check whether file has name with spaces
+    ok_file_has_content(op.join(ds.path, 'with spaces'), 'hello\n')
+
+
+@known_failure_windows
+@with_tree(tree={OBSCURE_FILENAME:
+                 {"code": {"just2args.py": """
+import sys
+print(sys.argv)
+# script, dataset, and two others
+assert len(sys.argv) == 4
+"""}}})
+def test_quoting(path=None):
+    ds = Dataset(op.join(path, OBSCURE_FILENAME)).create(force=True)
+    # Our custom procedure fails if it receives anything other than two
+    # procedure arguments (so the script itself receives 3). Check a few cases
+    # from the Python API and CLI.
+    ds.config.add("datalad.locations.dataset-procedures", "code",
+                  scope='branch')
+    with swallow_outputs():
+        ds.run_procedure(spec=["just2args", "with ' sing", 'with " doub'])
+        assert_in_results(
+            ds.run_procedure(spec=["just2args", "still-one arg"],
+                             on_failure="ignore", result_renderer=None),
+            action="run", status="error")
+
+        runner = WitlessRunner(cwd=ds.path)
+        runner.run(
+            "datalad run-procedure just2args \"with ' sing\" 'with \" doub'",
+            protocol=KillOutput)
+        with assert_raises(CommandError):
+            runner.run(
+                "datalad run-procedure just2args 'still-one arg'",
+                protocol=KillOutput)
 
 
+@xfail_buggy_annex_info
 @with_tree(tree={
-    'file.txt': 'some text',
-    'empty': '',
-    'file2.txt': 'some text to go to annex',
-    '.gitattributes': '* annex.largefiles=(not(mimetype=text/*))'}
-)
-@known_failure_direct_mode  #FIXME
-def test_add_mimetypes(path):
-    # XXX apparently there is symlinks dereferencing going on while deducing repo
-    #    type there!!!! so can't use following invocation  -- TODO separately
-    import os
-    path = os.path.realpath(path)  # yoh gives up for now
+    # "TEXT" ones
+    'empty': '',  # we have special rule to treat empty ones as text
+    # check various structured files - libmagic might change its decisions which
+    # can effect git-annex. https://github.com/datalad/datalad/issues/3361
+    'JSON': """\
+{
+    "name": "John Smith",
+    "age": 33
+}
+""",
+    'YAML': """\
+--- # The Smiths
+- {name: John Smith, age: 33}
+- name: Mary Smith
+  age: 27
+""",
+    'MARKDOWN': """\
+# Title
+
+## Section1
+
+When the earth was flat
+
+## Section2
+""",
+    # BINARY ones
+    '0blob': '\x00',
+    'emptyline': '\n',  # libmagic: "binary" "application/octet-stream"
+})
+def test_text2git(path=None):
+    # Test if files being correctly annexed in a ds configured with text2git.
+    TEXT_FILES = ('JSON', 'YAML', 'MARKDOWN', 'empty')
+    BINARY_FILES = ('0blob', 'emptyline')
+
     ds = Dataset(path).create(force=True)
-    ds.repo.add('.gitattributes')
-    ds.repo.commit('added attributes to git explicitly')
-    # now test that those files will go into git/annex correspondingly
-    __not_tested__ = ds.add(['file.txt', 'empty'])
-    ok_clean_git(path, untracked=['file2.txt'])
-    # Empty one considered to be  application/octet-stream  i.e. non-text
-    ok_file_under_git(path, 'empty', annexed=True)
-    ok_file_under_git(path, 'file.txt', annexed=False)
-
-    # But we should be able to force adding file to annex when desired
-    ds.add('file2.txt', to_git=False)
-    ok_file_under_git(path, 'file2.txt', annexed=True)
+    ds.run_procedure('cfg_text2git')
+    ds.save(path=TEXT_FILES + BINARY_FILES, message="added all files")
+    assert_repo_status(ds.path)
+
+    # check that text files are not annexed
+    for f in TEXT_FILES:
+        assert_false(ds.repo.is_under_annex(f))
+    # and trivial binaries - annexed
+    for f in BINARY_FILES:
+        assert_true(ds.repo.is_under_annex(f))
+
+
+@with_tree(tree={".datalad": {"procedures": {"print_args": """
+import sys
+print(sys.argv)
+"""}}})
+def test_name_with_underscore(path=None):
+    ds = Dataset(path).create(force=True)
+
+    # we are using the presence of a managed branch as a proxy indicator
+    # for a crippled FS, were we cannot trust the executable bit
+    # which is our only indicator in the absence of a file extension
+    if not ds.repo.is_managed_branch():
+        # Procedure name with underscore can't be reached directly with a DATALAD_
+        # environment variable.
+        with patch.dict("os.environ",
+                        {"DATALAD_PROCEDURES_PRINT_ARGS_CALL__FORMAT":
+                         '%s {script}' % sys.executable}):
+            with assert_raises(ValueError):
+                ds.run_procedure(spec=["print_args"])
+
+    # But it can be set via DATALAD_CONFIG_OVERRIDES_JSON.
+    with patch.dict("os.environ",
+                    {"DATALAD_CONFIG_OVERRIDES_JSON":
+                     json.dumps({
+                         "datalad.procedures.print_args.call-format":
+                         "%s {script}" % sys.executable
+                    })}):
+        ds.config.reload()
+        ds.run_procedure(spec=["print_args"])
+
+
+@with_tempfile
+def test_call_fmt_from_env_requires_reload(path=None):
+    ds = Dataset(path).create()
+    subds = ds.create("sub")
+    (subds.pathobj / ".datalad" / "procedures").mkdir(
+        parents=True, exist_ok=True)
+    script = subds.pathobj / ".datalad" / "procedures" / "p"
+    script.write_text("import sys; assert len(sys.argv) == 1")
+    with patch.dict("os.environ",
+                    {"DATALAD_PROCEDURES_P_CALL__FORMAT":
+                     f"{sys.executable} {{script}}"}):
+        # This will fail if the above environment variable isn't in effect.
+        ds.run_procedure("p")
+
+
+@with_tempfile
+def test_run_proc_with_dict(path=None):
+    # Test whether a result from run_procedure(discover=True) will be accepted
+    ds = Dataset(path).create()
+    g = ds.run_procedure(discover=True, result_renderer='disabled',
+                         return_type='generator')
+    for proc in g:
+        if proc['procedure_name'] == 'cfg_text2git':
+            break
+    ds.run_procedure(spec=proc)
```

### Comparing `datalad-0.9.3/datalad/distribution/publish.py` & `datalad-1.0.0/datalad/core/distributed/push.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,820 +1,940 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""High-level interface for dataset (component) publishing
+"""Interface for dataset (component) pushing
 
 """
 
+__docformat__ = 'restructuredtext'
+
+from itertools import chain
 import logging
 import re
-from collections import OrderedDict
-from os.path import join as opj
-
-from git.remote import PushInfo as PI
 
-from datalad import ssh_manager
-from datalad.interface.annotate_paths import AnnotatePaths
-from datalad.interface.annotate_paths import annotated2content_by_ds
-from datalad.interface.base import Interface
-from datalad.interface.base import build_doc
-from datalad.interface.utils import eval_results
-from datalad.interface.results import get_status_dict
-from datalad.interface.common_opts import annex_copy_opts, recursion_flag, \
-    recursion_limit, git_opts, annex_opts, jobs_opt
-from datalad.interface.common_opts import missing_sibling_opt
+from datalad.interface.base import (
+    Interface,
+    build_doc,
+    eval_results,
+)
+from datalad.interface.common_opts import (
+    jobs_opt,
+    recursion_limit,
+    recursion_flag,
+)
+from datalad.interface.utils import render_action_summary
+from datalad.interface.results import annexjson2result
+from datalad.log import log_progress
+from datalad.support.annexrepo import (
+    AnnexRepo,
+)
+from datalad.support.gitrepo import GitRepo
 from datalad.support.param import Parameter
-from datalad.support.constraints import EnsureStr
-from datalad.support.constraints import EnsureChoice
-from datalad.support.constraints import EnsureNone
-from datalad.support.annexrepo import AnnexRepo
-from datalad.support.sshconnector import sh_quote
-from datalad.support.exceptions import InsufficientArgumentsError
-from datalad.support.network import URL, RI, SSHRI, is_ssh
-
-
-from datalad.utils import assure_list
-from datalad.dochelpers import exc_str
-
-from .dataset import EnsureDataset
-from .dataset import Dataset
-from .dataset import datasetmethod
-from .dataset import require_dataset
-
-__docformat__ = 'restructuredtext'
+from datalad.support.constraints import (
+    EnsureStr,
+    EnsureNone,
+    EnsureChoice,
+)
+from datalad.support.exceptions import CommandError
+from datalad.utils import (
+    Path,
+    ensure_list,
+    todo_interface_for_extensions,
+)
+
+from datalad.distribution.dataset import (
+    Dataset,
+    EnsureDataset,
+    datasetmethod,
+    require_dataset,
+    resolve_path,
+)
+from datalad.core.local.diff import diff_dataset
 
-lgr = logging.getLogger('datalad.distribution.publish')
 
-
-def _push(ds, remote, things2push, force=False):
-    lgr.debug("Attempt to push '%s' to sibling '%s'", things2push, remote)
-    push_res = ds.repo.push(remote=remote, refspec=things2push, force=force)
-    if things2push and ds.config.get('remote.{}.push'.format(remote)):
-        # we aim to push both auto-detected and possibly configured once
-        # above we pushed the result of auto-detection, now push the
-        # configured ones
-        lgr.debug("Secondary push since custom push targets provided")
-        push_res.extend(
-            ds.repo.push(remote=remote, force=force))
-    if not push_res:
-        return 'notneeded', 'Git reported nothing was pushed'
-    errors = ['{} -> {} {}'.format(
-        pi.local_ref,
-        pi.remote_ref,
-        pi.summary.strip()) for pi in push_res if (pi.flags & PI.ERROR) == PI.ERROR]
-    successes = [pi.summary.strip() for pi in push_res if (pi.flags & PI.ERROR) != PI.ERROR]
-    if errors:
-        return 'error', \
-               ('failed to push to %s: %s;%s',
-                remote,
-                '; '.join(errors),
-                ' pushed: {}'.format(successes) if successes else '')
-    else:
-        return 'ok', ('pushed to %s: %s', remote, successes)
-
-
-def has_diff(ds, refspec, remote, paths):
-    """Return bool if a dataset was modified wrt to a given remote state"""
-    if refspec:
-        remote_branch_name = refspec[11:] \
-            if refspec.startswith('refs/heads/') \
-            else refspec
-    else:
-        # there was no tracking branch, check the push target
-        remote_branch_name = ds.repo.get_active_branch()
-
-    remote_ref = '/'.join((remote, remote_branch_name))
-    if remote_ref not in ds.repo.get_remote_branches():
-        lgr.debug("Remote '%s' has no branch matching %r. Will publish",
-                  remote, remote_branch_name)
-        # we don't have any remote state, need to push for sure
-        return True
-
-    lgr.debug("Testing for changes with respect to '%s' of remote '%s'",
-              remote_branch_name, remote)
-    current_commit = ds.repo.get_hexsha()
-    within_ds_paths = [p for p in paths if p['path'] != ds.path]
-    commit_differ = current_commit != ds.repo.get_hexsha(remote_ref)
-    # yoh: not sure what "logic" was intended here for comparing only
-    # some files.  By now we get a list of files, if any were changed,
-    # from the commit on remote, and somehow diff says below that they didn't differ...
-    # but if commit is different -- there must be differences and we
-    # should publish. otherwise now skips publishing root dataset
-    # although its master is behind by 1 commit.  Moreover there could
-    # be an empty commit -- shouldn't we publish then???
-    if not commit_differ and within_ds_paths:
-        # only if any paths is different from just the parentds root
-        # in which case we can do the same muuuch cheaper (see below)
-        # if there were custom paths, we will look at the diff
-        lgr.debug("Since paths provided, looking at diff")
-        return any(ds.diff(
-            path=within_ds_paths,
-            revision=remote_ref,
-            # only commited changes in this dataset
-            staged=False,
-            # consider only commited changes in subdataset
-            ignore_subdatasets='dirty')) > 0
-    else:
-        # if commits differ at all
-        lgr.debug("Since no paths provided, comparing commits")
-        return commit_differ
-
-
-def _publish_data(ds, remote, paths, annex_copy_options, force, transfer_data, **kwargs):
-    # paths are annotated paths for now, changes below
-    if not isinstance(ds.repo, AnnexRepo):
-        # impossible to publish annex'ed data
-        return
-
-    if ds.config.getbool('remote.{}'.format(remote), 'annex-ignore', False):
-        # configuration says: don't do it
-        return
-
-    if not ds.config.get('.'.join(('remote', remote, 'annex-uuid')), None):
-        # this remote either isn't an annex, or hasn't been properly initialized
-        for ap in paths:
-            # this is only a problem if this path
-            ap['status'] = 'impossible' \
-                           if transfer_data == 'all' or ap.get('raw_input', False) \
-                           else 'notneeded'
-            ap['message'] = \
-                ("annex for remote '%s' not available, or not properly configured",
-                 remote)
-            yield ap
-        return
-
-    # what data to transfer?
-    if transfer_data == 'all':
-        paths = ['.']
-    elif transfer_data == 'auto':
-        # keep only paths that were requested and are not the base path of the dataset
-        # if the resulting list is empty, the "auto" mode of _publish_data() will
-        # kick in and consult "wanted"
-        paths = [p['path'] for p in paths
-                 if p.get('raw_input', False) and
-                 not p['path'] == ds.path]
-    else:
-        raise ValueError(
-            "unknown label '{}' for `transfer_data` option".format(
-                transfer_data))
-
-    # TODO do we really have to call annex for that, or can we take it from
-    # the config instead?
-    remote_wanted = ds.repo.get_preferred_content('wanted', remote)
-    if not (paths or annex_copy_options or remote_wanted):
-        # nothing that we could tell git annex
-        return
-
-    # we should now know what needs doing
-    lgr.info("Publishing {0} data to {1}".format(ds, remote))
-    # overwrite URL with pushurl if any, reason:
-    # https://git-annex.branchable.com/bugs/annex_ignores_pushurl_and_uses_only_url_upon___34__copy_--to__34__/
-    # Note: This shouldn't happen anymore with newly added siblings.
-    #       But for now check for it, until we agree on how to fix existing
-    #       ones.
-    pushurl = ds.config.get('remote.{}.pushurl'.format(remote), None)
-    annexurl = ds.config.get('remote.{}.annexurl'.format(remote), None)
-    annex_copy_options_ = annex_copy_options or ''
-    if pushurl and not annexurl:
-        annex_copy_options_ += ' -c "remote.{}.annexurl={}"'.format(remote, pushurl)
-    if not paths and remote_wanted:
-        lgr.debug("Invoking copy --auto")
-        annex_copy_options_ += ' --auto'
-    # TODO:  we might need additional logic comparing the state of git-annex
-    # branch locally and on remote to see if information about the 'copy'
-    # was also reflected on the remote end
-    #git_annex_hexsha = ds.repo.get_hexsha('git-annex')
-    # TODO: must be the same if we merged/pushed before, if not -- skip
-    # special logic may be with a warning
-    if not force:
-        # if we force, we do not trust local knowledge and do the checks
-        annex_copy_options_ += ' --fast'
-    # TODO this things needs to return JSON
-    ncopied = 0
-    for r in ds.repo.copy_to(
-            files=[p for p in paths
-                   # TODO we may have to check for any file in Git, but this one can
-                   # easily happen with --since
-                   if not p == opj(ds.path, '.gitmodules')],
-            remote=remote,
-            options=annex_copy_options_):
-        ncopied += 1
-        # TODO RF to have copy_to() yield JSON and convert that one
-        # at present only the "good" results come out
-        yield get_status_dict(status='ok', path=opj(ds.path, r),
-                              type='file', parentds=ds.path, **kwargs)
-
-    if ncopied:
-        _check_and_update_remote_server_info(ds, remote)
-
-    # if ds.submodules:
-    #     # NOTE: we might need to init them on the remote, but needs to
-    #     #  be done only if remote is sshurl and it is not bare there
-    #     #  (which I think we do not even support ATM)...
-    #     #  or we could do that in the hook, as it is done for now
-    #     #  (see create_sibling.py)
-    #     #
-    #     pass
-
-    # TODO unclear why this was commented out
-    # if ds.repo.get_hexsha('git-annex') != git_annex_hexsha:
-    #     # there were changes which should be pushed
-    #     lgr.debug(
-    #         "We have progressed git-annex branch should fetch/merge/push it to %s again",
-    #         remote)
-    #     ds.repo.fetch(remote=remote, refspec='git-annex')
-    #     ds.repo.merge_annex(remote)
-    #     _log_push_info(ds.repo.push(remote=remote, refspec=['git-annex']))
-
-
-def _check_and_update_remote_server_info(ds, remote):
-    # if we managed to copy to "http" url  we should should try to trigger git
-    # update-server-info hook on the remote if there was ssh annexurl defined
-    # for it. Apparently we do that already in create_sibling ones, but here
-    # we need more checks and preparation
-    remote_url = ds.repo.config.get('remote.%s.url' % remote, None)
-    if remote_url:
-        remote_url = RI(remote_url)
-        if isinstance(remote_url, URL) and remote_url.scheme in (
-        'http', 'https'):
-            remote_annexurl = ds.repo.config.get('remote.%s.annexurl' % remote,
-                                                 None)
-            if remote_annexurl:
-                remote_annexurl_ri = RI(remote_annexurl)
-                if is_ssh(remote_annexurl_ri):
-                    ssh = ssh_manager.get_connection(remote_annexurl_ri)
-                    ssh('git -C {} update-server-info'.format(
-                        sh_quote(remote_annexurl_ri.path)))
-                    return True
-                else:
-                    lgr.debug(
-                        "There is no annexurl defined but not ssh: %s, "
-                        "dunno if "
-                        "we could/should do anything", remote_annexurl
-                    )
-    return False
-
-
-def _publish_dataset(ds, remote, refspec, paths, annex_copy_options, force=False, jobs=None,
-                     transfer_data='auto', **kwargs):
-    # TODO: this setup is now quite ugly. The only way `refspec` can come
-    # in, is when there is a tracking branch, and we get its state via
-    # `refspec`
-
-    # define config var name for potential publication dependencies
-    depvar = 'remote.{}.datalad-publish-depends'.format(remote)
-    # list of remotes that are publication dependencies for the
-    # target remote
-    publish_depends = assure_list(ds.config.get(depvar, []))
-
-    # remote might be set to be ignored by annex, or we might not even know yet its uuid
-    # make sure we are up-to-date on this topic on all affected remotes, before
-    # we start making decisions
-    for r in publish_depends + [remote]:
-        if not ds.config.get('.'.join(('remote', remote, 'annex-uuid')), None):
-            lgr.debug("Obtain remote annex info from '%s'", r)
-            ds.repo.fetch(remote=r)
-            # in order to be able to use git's config to determine what to push,
-            # we need to annex merge first. Otherwise a git push might be
-            # rejected if involving all matching branches for example.
-            # NOTE we should not use a precomputed 'is_annex' test here, as
-            # each fetch could give evidence that there is an annex
-            # somewhere and replace the repo class...
-            if isinstance(ds.repo, AnnexRepo):
-                ds.repo.merge_annex(r)
-    ds.config.reload()
-
-    # anything that follows will not change the repo type anymore, cache
-    is_annex_repo = isinstance(ds.repo, AnnexRepo)
-
-    # Plan:
-    # 1. Check if there is anything to push, and if so
-    #    2. process push dependencies
-    #    3. fetch and merge annex branch
-    #    4. push non-annex branch(es)
-    # 5. copy data to the remote if paths are provided or it wants something generally
-
-    # upstream refspec needed for update (merge) and subsequent push,
-    # in case there is no.
-    # no tracking refspec yet?
-
-    # TODO: i think this whole modification detection could be done by path
-    # annotation at the very beginning -- keeping it for now to not get too
-    # dizzy in the forehead....
-
-    # if forced -- we push regardless if there are differences or not
-    diff = True if force else has_diff(ds, refspec, remote, paths)
-
-    # We might have got new information in git-annex branch although no other
-    # changes
-    if not diff and is_annex_repo:
-        try:
-            git_annex_commit = next(ds.repo.get_branch_commits('git-annex'))
-        except StopIteration:
-            git_annex_commit = None
-        #diff = _get_remote_diff(ds, [], git_annex_commit, remote, 'git-annex')
-        diff = _get_remote_diff(ds, git_annex_commit, remote, 'git-annex')
-        if diff:
-            lgr.info("Will publish updated git-annex")
-
-    #
-    # publish data (annex copy --to)
-    #
-    # # remote might be set to be ignored by annex, or we might not even know yet its uuid
-    # annex_ignore = ds.config.getbool('remote.{}.annex-ignore'.format(remote), None)
-    # annex_uuid = ds.config.get('remote.{}.annex-uuid'.format(remote), None)
-    # if not annex_ignore:
-    #     if annex_uuid is None:
-    #         # most probably not yet 'known' and might require some annex
-
-    copied_data = False
-    # skip right away if data transfer is not desired
-    if transfer_data != 'none' and isinstance(ds.repo, AnnexRepo):
-        # publishing of `remote` might depend on publishing other
-        # remote(s) first, so they need to receive the data first:
-        for d in publish_depends:
-            lgr.info("Transferring data to configured publication dependency: '%s'" % d)
-            # properly initialized remote annex -> publish data
-            for r in _publish_data(
-                    ds,
-                    d,
-                    paths,
-                    annex_copy_options,
-                    force,
-                    transfer_data,
-                    **kwargs):
-                # note if we published any data, notify to sync annex branch below
-                if r['status'] == 'ok' and r['action'] == 'publish' and \
-                        r.get('type', None) == 'file':
-                    copied_data = True
-                yield r
-        # and for the main target
-        for r in _publish_data(
-                ds,
-                remote,
-                paths,
-                annex_copy_options,
-                force,
-                transfer_data,
-                **kwargs):
-            # note if we published any data, notify to sync annex branch below
-            if r['status'] == 'ok' and r['action'] == 'publish' and \
-                    r.get('type', None) == 'file':
-                copied_data = True
-            yield r
-
-    #
-    # publish dataset (git push)
-    #
-    if not diff and not copied_data:
-        lgr.debug("No changes detected with respect to state of '%s'", remote)
-        yield get_status_dict(ds=ds, status='notneeded', **kwargs)
-    else:
-        # publishing of `remote` might depend on publishing other
-        # remote(s) first:
-        for d in publish_depends:
-            lgr.info("Publishing to configured dependency: '%s'" % d)
-            # call this again to take care of the dependency first,
-            # but keep the paths the same, as the goal is to publish those
-            # to the primary remote, and not anything elase to a dependency
-            for r in _publish_dataset(
-                    ds,
-                    d,
-                    # should get the same as the base dataset
-                    refspec,
-                    paths,
-                    annex_copy_options,
-                    force=force,
-                    jobs=jobs,
-                    transfer_data=transfer_data,
-                    **kwargs):
-                yield r
-
-        if is_annex_repo and \
-                ds.repo.is_special_annex_remote(remote):
-            # There is nothing else to "publish"
-            lgr.debug(
-                "{0} is a special annex remote, no git push is needed".format(remote)
-            )
-            return
-
-        lgr.info("Publishing {0} to {1}".format(ds, remote))
-        # in order to be able to use git's config to determine what to push,
-        # we need to annex merge first. Otherwise a git push might be
-        # rejected if involving all matching branches for example
-        # even if we already fetched above we need to do it again
-        if is_annex_repo:
-            lgr.debug("Obtain remote annex info from '%s'", remote)
-            ds.repo.fetch(remote=remote)
-            ds.repo.merge_annex(remote)
-
-        # Note: git's push.default is 'matching', which doesn't work for first
-        # time publication (a branch, that doesn't exist on remote yet)
-        # But if we want to respect remote.*.push entries, etc. we need to
-        # not pass a specific refspec (like active branch) to `git push`
-        # by default.
-        # hence we amend any existing config on the fly
-        # TODO: what else to push by default?
-        # consider also: --follow-tags, --tags, --atomic
-        # make sure we push
-        things2push = []
-        current_branch = ds.repo.get_active_branch()
-        if current_branch:  # possibly make this conditional on a switch
-            # TODO: this should become it own helper
-            if is_annex_repo:
-                # annex could manage this branch
-                if current_branch.startswith('annex/direct') \
-                        and ds.config.getbool('annex', 'direct', default=False):
-                    # this is a "fake" annex direct mode branch
-                    # we want to publish the underlying branch
-                    current_branch = current_branch[12:]
-                match_adjusted = re.match(
-                    'adjusted/(.*)\([a-z]*\)',
-                    current_branch)
-                if match_adjusted:
-                    # adjusted/master(...)
-                    # TODO:  this code is not tested
-                    # see https://codecov.io/gh/datalad/datalad/src/17e67045a088ae0372b38aa4d8d46ecf7c821cb7/datalad/distribution/publish.py#L156
-                    # and thus probably broken -- test me!
-                    current_branch = match_adjusted.group(1)
-            things2push.append(current_branch)
-        if is_annex_repo:
-            things2push.append('git-annex')
-        # check that all our magic found valid branches
-        things2push = [t for t in things2push if t in ds.repo.get_branches()]
-        # check that we don't ask to push things that are already configured
-        # -> would cause error
-        # TODO need to find a way to properly do this, when wildcards are used
-        # in the push configuration variable
-        things2push = [t for t in things2push
-                       if t not in ds.config.get('remote.{}.push'.format(remote), [])]
-        # now we know what to push where
-        status, msg = _push(ds, remote, things2push, force)
-        yield get_status_dict(ds=ds, status=status, message=msg, **kwargs)
-
-
-def _get_remote_info(ds_path, ds_remote_info, to, missing):
-    """Returns None if desired info was obtained, or a tuple (status, message)
-    if not"""
-    ds = Dataset(ds_path)
-    if ds.repo is None:
-        # There is no repository, nothing could be done
-        return ('impossible',
-                'No repository found for %s' % ds)
-    if to is None:
-        # we need an upstream remote, if there's none given. We could
-        # wait for git push to complain, but we need to explicitly
-        # figure it out for pushing annex branch anyway and we might as
-        # well fail right here.
-        track_remote, track_refspec = ds.repo.get_tracking_branch()
-        if not track_remote:
-            # no tracking remote configured, but let try one more
-            # if we only have one remote, and it has a push target
-            # configured that is "good enough" for us
-            cand_remotes = [r for r in ds.repo.get_remotes()
-                            if 'remote.{}.push'.format(r) in ds.config]
-            if len(cand_remotes) > 1:
-                lgr.warning('Target sibling ambiguous, please specific via --to')
-            elif len(cand_remotes) == 1:
-                track_remote = cand_remotes[0]
-            else:
-                return ('impossible',
-                        'No target sibling configured for default publication, '
-                        'please specific via --to')
-        if track_remote:
-            ds_remote_info[ds_path] = dict(zip(
-                ('remote', 'refspec'),
-                (track_remote, track_refspec)))
-        elif missing == 'skip':
-            ds_remote_info[ds_path] = None
-            return ('notneeded',
-                    'Cannot determine target sibling, skipping publication')
-        else:
-            # we have no remote given and no upstream
-            return 'error', 'Cannot determine a default target sibling for publication'
-    elif to not in ds.repo.get_remotes():
-        # unknown given remote
-        if missing == 'skip':
-            ds_remote_info[ds_path] = None
-            return ('notneeded',
-                    ("Unknown target sibling '%s', skipping publication", to))
-        elif missing == 'inherit':
-            superds = ds.get_superdataset()
-            if not superds:
-                return ('error',
-                        ("No super-dataset to inherit settings for remote %s", to))
-            # XXX due to difference between create-sibling and create-sibling-github
-            # would not be as transparent to inherit for -github
-            lgr.info("Will try to create a sibling inheriting settings from %s", superds)
-            # XXX explicit None as sshurl for now
-            # TODO this is not good: e.g. #1344
-            ds.create_sibling(None, name=to, inherit=True)
-            ds_remote_info[ds_path] = {'remote': to}
-        else:
-            return ('error',
-                    ("Unknown target sibling '%s' for publication", to))
-    else:
-        # all good: remote given and is known
-        ds_remote_info[ds_path] = {'remote': to}
-
-
-
-def _get_remote_diff(ds, current_commit, remote, remote_branch_name):
-#def _get_remote_diff(ds, paths, current_commit, remote, remote_branch_name):
-    """Helper to check if remote has different state of the branch"""
-    if remote_branch_name in ds.repo.repo.remotes[remote].refs:
-        lgr.debug("Testing for changes with respect to '%s' of remote '%s'",
-                  remote_branch_name, remote)
-        if current_commit is None:
-            current_commit = ds.repo.repo.commit()
-        remote_ref = ds.repo.repo.remotes[remote].refs[remote_branch_name]
-        # XXX: ATM nothing calls this function with a non-empty `paths` arg
-        #if paths:
-        #    # if there were custom paths, we will look at the diff
-        #    lgr.debug("Since paths provided, looking at diff")
-        #    diff = current_commit.diff(
-        #        remote_ref,
-        #        paths=paths
-        #    )
-        #else:
-        # if commits differ at all
-        lgr.debug("Since no paths provided, comparing commits")
-        diff = current_commit != remote_ref.commit
-    else:
-        lgr.debug("Remote '%s' has no branch matching %r. Will publish",
-                  remote, remote_branch_name)
-        # we don't have any remote state, need to push for sure
-        diff = True
-
-    return diff
+lgr = logging.getLogger('datalad.core.distributed.push')
 
 
 @build_doc
-class Publish(Interface):
-    """Publish a dataset to a known :term:`sibling`.
+class Push(Interface):
+    """Push a dataset to a known :term:`sibling`.
 
-    This makes the last saved state of a dataset available to a sibling
-    or special remote data store of a dataset. Any target sibling must already
-    exist and be known to the dataset.
-
-    Optionally, it is possible to limit publication to change sets relative
-    to a particular point in the version history of a dataset (e.g. a release
-    tag). By default, the state of the local dataset is evaluated against the
-    last known state of the target sibling. Actual publication is only attempted
-    if there was a change compared to the reference state, in order to speed up
-    processing of large collections of datasets. Evaluation with respect to
-    a particular "historic" state is only supported in conjunction with a
-    specified reference dataset. Change sets are also evaluated recursively, i.e.
-    only those subdatasets are published where a change was recorded that is
-    reflected in to current state of the top-level reference dataset.
-    See "since" option for more information.
-
-    Only publication of saved changes is supported. Any unsaved changes in a
-    dataset (hierarchy) have to be saved before publication.
+    This makes a saved state of a dataset available to a sibling or special
+    remote data store of a dataset. Any target sibling must already exist and
+    be known to the dataset.
+
+    || REFLOW >>
+    By default, all files tracked in the last saved state (of the current
+    branch) will be copied to the target location. Optionally, it is possible
+    to limit a push to changes relative to a particular point in the version
+    history of a dataset (e.g. a release tag) using the
+    [CMD: --since CMD][PY: since PY] option in conjunction with the
+    specification of a reference dataset. In recursive mode subdatasets will also be
+    evaluated, and only those subdatasets are pushed where a change was
+    recorded that is reflected in the current state of the top-level reference
+    dataset.
+    << REFLOW ||
 
     .. note::
-      Power-user info: This command uses :command:`git push`, and :command:`git annex copy`
-      to publish a dataset. Publication targets are either configured remote
-      Git repositories, or git-annex special remotes (if their support data
-      upload).
+      Power-user info: This command uses :command:`git push`, and :command:`git
+      annex copy` to push a dataset. Publication targets are either configured
+      remote Git repositories, or git-annex special remotes (if they support
+      data upload).
     """
-    # XXX prevent common args from being added to the docstring
-    _no_eval_results = True
-    # TODO: Figure out, how to tell about tracking branch/upstream
-    #      (and the respective remote)
-    #      - it is used, when no destination is given
-    #      - it is configured to be the given destination, if there was no
-    #        upstream set up before, so you can use just "datalad publish" next
-    #        time.
+
+    # TODO add examples
 
     _params_ = dict(
         dataset=Parameter(
             args=("-d", "--dataset"),
-            metavar='DATASET',
-            doc="""specify the (top-level) dataset to be published. If no dataset
-            is given, the datasets are determined based on the input arguments""",
+            doc="""specify the dataset to push""",
             constraints=EnsureDataset() | EnsureNone()),
         to=Parameter(
             args=("--to",),
-            metavar='LABEL',
+            metavar='SIBLING',
             doc="""name of the target sibling. If no name is given an attempt is
             made to identify the target based on the dataset's configuration
             (i.e. a configured tracking branch, or a single sibling that is
-            configured for publication)""",
-            # TODO: See TODO at top of class!
+            configured for push)""",
             constraints=EnsureStr() | EnsureNone()),
         since=Parameter(
             args=("--since",),
             constraints=EnsureStr() | EnsureNone(),
-            doc="""When publishing dataset(s), specifies commit (treeish, tag, etc)
-            from which to look for changes
-            to decide either updated publishing is necessary for this and which children.
-            If empty argument is provided, then we would take from the previously 
-            published to that remote/sibling state (for the current branch)"""),
-        # since: commit => .gitmodules diff to head => submodules to publish
-        missing=missing_sibling_opt,
+            doc="""specifies commit-ish (tag, shasum, etc.) from which to look for
+            changes to decide whether pushing is necessary.
+            If '^' is given, the last state of the current branch at the sibling
+            is taken as a starting point."""),
         path=Parameter(
             args=("path",),
             metavar='PATH',
-            # TODO this description is no longer correct
-            doc="path(s), that may point to file handle(s) to publish including "
-                "their actual content or to subdataset(s) to be published. If a "
-                "file handle is published with its data, this implicitly means "
-                "to also publish the (sub)dataset it belongs to. '.' as a path "
-                "is treated in a special way in the sense, that it is passed "
-                "to subdatasets in case `recursive` is also given.",
-            constraints=EnsureStr() | EnsureNone(),
-            nargs='*'),
+            doc="""path to constrain a push to. If given, only
+            data or changes for those paths are considered for a push.""",
+            nargs='*',
+            constraints=EnsureStr() | EnsureNone()),
+        data=Parameter(
+            args=("--data",),
+            doc="""what to do with (annex'ed) data. 'anything' would cause
+            transfer of all annexed content, 'nothing' would avoid call to
+            `git annex copy` altogether. 'auto' would use 'git annex copy' with
+            '--auto' thus transferring only data which would satisfy "wanted"
+            or "numcopies" settings for the remote (thus "nothing" otherwise).
+            'auto-if-wanted' would enable '--auto' mode only if there is a 
+            "wanted" setting for the remote, and transfer 'anything' otherwise.
+            """,
+            constraints=EnsureChoice(
+                'anything', 'nothing', 'auto', 'auto-if-wanted')),
         force=Parameter(
+            # multi-mode option https://github.com/datalad/datalad/issues/3414
             args=("-f", "--force",),
-            doc="""enforce doing publish activities (git push etc) regardless of
-            the analysis if they seemed needed""",
-            action='store_true'),
-        # TODO add option to decide what branch/repo to push
-        transfer_data=Parameter(
-            args=("--transfer-data",),
-            doc="""ADDME""",
-            constraints=EnsureChoice('auto', 'none', 'all')),
+            doc="""force particular operations, possibly overruling safety
+            protections or optimizations: use --force with git-push ('gitpush');
+            do not use --fast with git-annex copy ('checkdatapresent');
+            combine all force modes ('all').""",
+            constraints=EnsureChoice(
+                'all', 'gitpush', 'checkdatapresent', None)),
         recursive=recursion_flag,
         recursion_limit=recursion_limit,
-        git_opts=git_opts,
-        annex_opts=annex_opts,
-        annex_copy_opts=annex_copy_opts,
         jobs=jobs_opt,
     )
 
+    # Desired features:
+    # - let Git do it's thing (push multiple configured refs without the need
+    #                          to specific anything on the command line
+    #   - compilication: we need publication dependencies (i.e. publish what
+    #     would be published by Git to a different remote first, hence we
+    #     cannot simply watch Git do it, and later act on it.)
+    #   - https://github.com/datalad/datalad/issues/1284
+    #   - https://github.com/datalad/datalad/issues/4006
+    # - make differences between remotes and various types of special remotes
+    #   opaque
+    #   - https://github.com/datalad/datalad/issues/3127
+    # - informative and comprehensive (error) reporting
+    #   - https://github.com/datalad/datalad/issues/2000
+    #   - https://github.com/datalad/datalad/issues/1682
+    #   - https://github.com/datalad/datalad/issues/2029
+    #   - https://github.com/datalad/datalad/issues/2855
+    #   - https://github.com/datalad/datalad/issues/3412
+    #   - https://github.com/datalad/datalad/issues/3424
+    # - ensure robust behavior in multi-lateral push scenarios (updating
+    #   a dataset that was updated by a 3rd-party after the last known
+    #   fetched change
+    #   - https://github.com/datalad/datalad/issues/2636
+    # - should NOT mimic `publish` and that it mixes `create-sibling` and
+    #   `push` into a single operation. This would fold the complexity
+    #   of all possible ways a local dataset hierarchy could possibly
+    #   connected to remote ends into this command. It would be lost battle
+    #   from the start.
+    #   - not tackle: https://github.com/datalad/datalad/issues/2186
+    # - maintain standard setup, and not reflect procedural aspects
+    #   onto the resulting outcomes
+    #   - https://github.com/datalad/datalad/issues/2001
+    # - do a straight push, nothing like 'sync'. If a remote has something that
+    #   needs merging first, fail and let users update. Any diff we are missing
+    #   locally can impact decision making via --since and friends.
+
     @staticmethod
-    @datasetmethod(name='publish')
+    @datasetmethod(name='push')
     @eval_results
     def __call__(
             path=None,
+            *,
             dataset=None,
             to=None,
             since=None,
-            missing='fail',
-            force=False,
-            transfer_data='auto',
+            data='auto-if-wanted',
+            force=None,
             recursive=False,
             recursion_limit=None,
-            git_opts=None,
-            annex_opts=None,
-            annex_copy_opts=None,
-            jobs=None
+            jobs=None):
+        # push uses '^' to annotate the previous pushed committish, and None for default
+        # behavior. '' was/is (to be deprecated) used in `publish`. Alert user about the mistake
+        if since == '':
+            raise ValueError("'since' should point to commitish or use '^'.")
+        # we resolve here, because we need to perform inspection on what was given
+        # as an input argument further down
+        paths = [resolve_path(p, dataset) for p in ensure_list(path)]
+
+        ds = require_dataset(
+            dataset, check_installed=True, purpose='push')
+        ds_repo = ds.repo
+
+        res_kwargs = dict(
+            action='publish',
+            refds=ds.path,
+            logger=lgr,
+        )
+
+        get_remote_kwargs = {'exclude_special_remotes': False} \
+            if isinstance(ds_repo, AnnexRepo) else {}
+        if to and to not in ds_repo.get_remotes(**get_remote_kwargs):
+            # get again for proper error:
+            sr = ds_repo.get_remotes(**get_remote_kwargs)
+            # yield an error result instead of raising a ValueError,
+            # to enable the use case of pushing to a target that
+            # a superdataset doesn't know, but some subdatasets to
+            # (in combination with '--on-failure ignore')
+            yield dict(
+                res_kwargs,
+                status='error',
+                path=ds.path,
+                message="Unknown push target '{}'. {}".format(
+                    to,
+                    'Known targets: {}.'.format(', '.join(repr(s) for s in sr))
+                    if sr
+                    else 'No targets configured in dataset.'))
+            return
+        if since == '^':
+            # figure out state of remote branch and set `since`
+            since = _get_corresponding_remote_state(ds_repo, to)
+            if not since:
+                lgr.info(
+                    "No tracked remote for active branch, "
+                    "detection of last pushed state not in effect.")
+        elif since:
+            # will blow with ValueError if unusable
+            ds_repo.get_hexsha(since)
+
+
+        # obtain a generator for information on the datasets to process
+        # idea is to turn the `paths` argument into per-dataset
+        # content listings that can be acted upon
+        ds_spec = _datasets_since_(
+            # important to pass unchanged dataset arg
+            dataset,
+            since,
+            paths,
+            recursive,
+            recursion_limit)
+
+        # instead of a loop, this could all be done in parallel
+        matched_anything = False
+        for dspath, dsrecords in ds_spec:
+            matched_anything = True
+            lgr.debug('Pushing Dataset at %s', dspath)
+            pbars = {}
+            yield from _push(
+                dspath, dsrecords, to, data, force, jobs, res_kwargs.copy(), pbars,
+                got_path_arg=True if path else False)
+            # take down progress bars for this dataset
+            for i, ds in pbars.items():
+                log_progress(lgr.info, i, 'Finished push of %s', ds)
+        if not matched_anything:
+            potential_remote = False
+            if not to and len(paths) == 1:
+                # if we get a remote name without --to, provide a hint
+                sr = ds_repo.get_remotes(**get_remote_kwargs)
+                potential_remote = [
+                    p for p in ensure_list(path) if p in sr
+                ]
+            if potential_remote:
+                if len(potential_remote) == 1:
+                    # present as a single value to make hint even more human
+                    # friendly
+                    potential_remote = potential_remote[0]
+                hint = "{} matches a sibling name and not a path. " \
+                      "Forgot --to?".format(potential_remote)
+                yield dict(
+                    res_kwargs,
+                    status='notneeded',
+                    message=hint,
+                    hints=hint,
+                    type='dataset',
+                    path=ds.path,
+                )
+                # there's no matching path and we have generated a hint on
+                # fixing the call - we can return now
+                return
+            yield dict(
+                res_kwargs,
+                status='notneeded',
+                message='Given constraints did not match any changes to publish',
+                type='dataset',
+                path=ds.path,
+            )
+
+    custom_result_summary_renderer_pass_summary = True
+
+    @staticmethod
+    def custom_result_summary_renderer(results, action_summary):  # pragma: more cover
+        render_action_summary(action_summary)
+        # report on any hints at the end
+        # get all unique hints
+        hints = set([r.get('hints', None) for r in results])
+        hints = [hint for hint in hints if hint is not None]
+        if hints:
+            from datalad.ui import ui
+            from datalad.support import ansi_colors
+            intro = ansi_colors.color_word(
+                "Hints: ",
+                ansi_colors.YELLOW)
+            ui.message(intro)
+            [ui.message("{}: {}".format(
+                ansi_colors.color_word(id + 1, ansi_colors.YELLOW), hint))
+                for id, hint in enumerate(hints)]
+
+
+
+def _datasets_since_(dataset, since, paths, recursive, recursion_limit):
+    """Generator"""
+    # rely on diff() reporting sequentially across datasets
+    cur_ds = None
+    ds_res = None
+    for res in diff_dataset(
+            dataset=dataset,
+            fr=since,
+            # we never touch unsaved content
+            to='HEAD',
+            constant_refs=False,
+            path=paths,
+            # we need to know what is around locally to be able
+            # to report something that should have been pushed
+            # but could not, because we don't have a copy.
+            # however, getting this info here is needlessly
+            # expensive, we will do it at the latest possible stage
+            # in _push_data()
+            annex=None,
+            recursive=recursive,
+            recursion_limit=recursion_limit,
+            # TODO?: expose order as an option for diff and push
+            # since in some cases breadth-first would be sufficient
+            # and result in "taking action faster"
+            reporting_order='bottom-up'
     ):
+        if res.get('action', None) != 'diff':
+            # we don't care right now
+            continue
+        if res.get('status', None) != 'ok':
+            # we cannot handle this situation, report it in panic
+            raise RuntimeError(
+                'Cannot handle non-OK diff result: {}'.format(res))
+        parentds = res.get('parentds', None)
+        if not parentds:
+            raise RuntimeError(
+                'Cannot handle diff result without a parent dataset '
+                'property: {}'.format(res))
+        if res.get('type', None) == 'dataset':
+            # a subdataset record in another dataset
+            # this could be here, because
+            # - this dataset was explicitly requested by path
+            #   -> should get a dedicated dataset record -- even without recursion
+            # - a path within an existing subdataset was given
+            # - a path within an non-existing subdataset was given
+            #   locally or not)
+            #   -> it should be ignored, but should not cause the branch in the
+            #   superdataset not to be pushed, if this was the only change
+            p = Path(res['path'])
+            # was given as an explicit path argument
+            if any(arg == p for arg in paths) and \
+                    not GitRepo.is_valid_repo(res['path']):
+                raise ValueError(
+                    'Cannot publish subdataset, not present: {}'.format(res['path']))
+
+        if parentds != cur_ds:
+            if ds_res:
+                # we switch to another dataset, yield this one so outside
+                # code can start processing immediately
+                yield (cur_ds, ds_res)
+            # clean start
+            ds_res = []
+            cur_ds = parentds
+        ds_res.append({
+            k: v for k, v in res.items()
+            if k in (
+                # let's keep 'state' in for now, it would make it possible
+                # to implement a "sync"-type push downstream that actually
+                # pulls 'deleted' files
+                'state',
+                # 'file' to copy-to, and subdataset records to possibly
+                # act on
+                'type',
+                # essential
+                'path')
+        })
+
+    # if we have something left to report, do it
+    # importantly do not test for ds_res, even if we had only seen subdataset
+    # records to be changes, we would still want to push the git branches
+    if cur_ds:
+        yield (cur_ds, ds_res)
+
+
+@todo_interface_for_extensions
+def _transfer_data(repo, ds, target, content, data, force, jobs, res_kwargs,
+                   got_path_arg):
+    yield from _push_data(
+        ds,
+        target,
+        content,
+        data,
+        force,
+        jobs,
+        res_kwargs.copy(),
+        got_path_arg=got_path_arg,
+    )
 
-        # if ever we get a mode, for "with-data" we would need this
-        #if dataset and not path:
-        #    # act on the whole dataset if nothing else was specified
-        #    path = dataset.path if isinstance(dataset, Dataset) else dataset
-
-        if not dataset and not path:
-            # try to find a dataset in PWD
-            dataset = require_dataset(
-                None, check_installed=True, purpose='publishing')
-
-        if since and not dataset:
-            raise InsufficientArgumentsError(
-                'Modification detection (--since) without a base dataset '
-                'is not supported')
-
-        if dataset and since == '':
-            # only update since last update so we figure out what was the last update
-            active_branch = dataset.repo.get_active_branch()
-            if to:
-                # XXX here we assume one to one mapping of names from local branches
-                # to the remote
-                since = '%s/%s' % (to, active_branch)
-            else:
-                # take tracking remote for the active branch
-                tracked_remote, tracked_refspec = dataset.repo.get_tracking_branch()
-                if tracked_remote:
-                    if tracked_refspec.startswith('refs/heads/'):
-                        tracked_refspec = tracked_refspec[len('refs/heads/'):]
-                    #to = tracked_remote
-                    since = '%s/%s' % (tracked_remote, tracked_refspec)
-                else:
-                    lgr.info(
-                        "No tracked remote for %s. since option is of no effect",
-                        active_branch
+
+def _push(dspath, content, target, data, force, jobs, res_kwargs, pbars,
+          got_path_arg=False):
+    force_git_push = force in ('all', 'gitpush')
+
+    # nothing recursive in here, we only need a repo to work with
+    ds = Dataset(dspath)
+    repo = ds.repo
+
+    res_kwargs.update(type='dataset', path=dspath)
+
+    # content will be unique for every push (even on the same dataset)
+    pbar_id = 'push-{}-{}'.format(target, id(content))
+    # register for final orderly take down
+    pbars[pbar_id] = ds
+    log_progress(
+        lgr.info, pbar_id,
+        'Determine push target',
+        unit=' Steps',
+        label='Push',
+        total=4,
+    )
+    # pristine input arg
+    _target = target
+    # verified or auto-detected
+    target = None
+    if not _target:
+        try:
+            try:
+                # let Git figure out what needs doing
+                # we will reuse the result further down again, so nothing is wasted
+                wannabe_gitpush = repo.push(remote=None, git_options=['--dry-run'])
+                # we did not get an explicit push target, get it from Git
+                target = set(p.get('remote', None) for p in wannabe_gitpush)
+                # handle case where a pushinfo record did not have a 'remote'
+                # property -- should not happen, but be robust
+                target.discard(None)
+            except CommandError as e:
+                if 'Please make sure you have the correct access rights' in e.stderr:
+                    # there is a default push target but we have no permission
+                    yield dict(
+                        res_kwargs,
+                        status='impossible',
+                        message='Attempt to push to default target resulted in following '
+                                'error.  Address the error or specify different target with --to: '
+                                + e.stderr,
                     )
-                    since = None
+                    return
+                raise
+        except Exception as e:
+            lgr.debug(
+                'Dry-run push to determine default push target failed, '
+                'assume no configuration: %s', e)
+            target = set()
+        if not len(target):
+            yield dict(
+                res_kwargs,
+                status='impossible',
+                message='No push target given, and none could be '
+                        'auto-detected, please specify via --to',
+            )
+            return
+        elif len(target) > 1:
+            # dunno if this can ever happen, but if it does, report
+            # nicely
+            yield dict(
+                res_kwargs,
+                status='error',
+                message=(
+                    'No push target given, '
+                    'multiple candidates auto-detected: %s',
+                    list(target),
+                )
+            )
+            return
+        else:
+            # can only be a single one at this point
+            target = target.pop()
 
-        # here is the plan
-        # 1. figure out remote to publish to
-        # 2. figure out which content needs to be published to this remote
-        # 3. look for any pre-publication dependencies of that remote
-        #    (i.e. remotes that need to be published to before)
-        # 4. publish the content needed to go to the primary remote to
-        #    the dependencies first, and to the primary afterwards
-        ds_remote_info = {}
-
-        refds_path = Interface.get_refds_path(dataset)
-        res_kwargs = dict(refds=refds_path, logger=lgr, action='publish')
-
-        to_process = []
-        for ap in AnnotatePaths.__call__(
-                dataset=refds_path,
-                path=path,
-                recursive=recursive,
-                recursion_limit=recursion_limit,
-                action='publish',
-                unavailable_path_status='impossible',
-                nondataset_path_status='error',
-                modified=since,
-                return_type='generator',
-                on_failure='ignore',
-                force_no_revision_change_discovery=False, # we cannot publish what was not committed
-                force_untracked_discovery=False  # we cannot publish untracked
-        ):
-            if ap.get('status', None):
-                # this is done
-                yield ap
-                continue
-            remote_info_result = None
-            if ap.get('type', ap.get('type_src', 'dataset')) != 'dataset':
-                # for everything that is not a dataset get the remote info
-                # for the parent
-                parentds = ap.get('parentds', None)
-                if parentds and parentds not in ds_remote_info:
-                    remote_info_result = _get_remote_info(
-                        parentds, ds_remote_info, to, missing)
-            else:
-                # this is a dataset
-                if ap.get('state', None) == 'absent':
-                    continue
-                # get the remote info for itself
-                remote_info_result = _get_remote_info(
-                    ap['path'], ds_remote_info, to, missing)
-                ap['process_content'] = True
-            if remote_info_result is not None:
-                ap['status'] = remote_info_result[0]
-                ap['message'] = remote_info_result[1]
-                yield ap
-                continue
-            to_process.append(ap)
-
-        content_by_ds, ds_props, completed, nondataset_paths = \
-            annotated2content_by_ds(
-                to_process,
-                refds_path=refds_path)
-        assert(not completed)
+    if not target:
+        if _target not in repo.get_remotes():
+            yield dict(
+                res_kwargs,
+                status='error',
+                message=(
+                    "Unknown target sibling '%s'.", _target))
+            return
+        target = _target
+
+    log_progress(
+        lgr.info, pbar_id, "Push refspecs",
+        label="Push to '{}'".format(target), update=1, total=4)
+
+    # define config var name for potential publication dependencies
+    depvar = 'remote.{}.datalad-publish-depends'.format(target)
+    # list of remotes that are publication dependencies for the
+    # target remote
+    publish_depends = ensure_list(ds.config.get(depvar, [], get_all=True))
+    if publish_depends:
+        lgr.debug("Discovered publication dependencies for '%s': %s'",
+                  target, publish_depends)
 
+    # cache repo type
+    is_annex_repo = isinstance(ds.repo, AnnexRepo)
+
+    # TODO prevent this when `target` is a special remote
+    # (possibly redo) a push attempt to figure out what needs pushing
+    # do this on the main target only, and apply the result to all
+    # dependencies
+    try:
+        if _target:
+            # only do it when an explicit target was given, otherwise
+            # we can reuse the result from the auto-probing above
+            wannabe_gitpush = repo.push(
+                remote=target,
+                git_options=['--dry-run'])
+    except Exception as e:
         lgr.debug(
-            "Evaluating %i dataset publication candidate(s)",
-            len(content_by_ds))
-        # TODO: fancier sorting, so we still follow somewhat the hierarchy
-        #       in sorted order, e.g.
-        #  d1/sub1/sub1
-        #  d1/sub1
-        #  d1
-        #  d2/sub1
-        #  d2
-        content_by_ds = OrderedDict(
-            (d, content_by_ds[d]) for d in sorted(content_by_ds, reverse=True)
+            'Dry-run push to check push configuration failed, '
+            'assume no configuration: %s', e)
+        wannabe_gitpush = []
+    refspecs2push = [
+        # if an upstream branch is set, go with it
+        p['from_ref']
+        if ds.config.get(
+            # refs come in as refs/heads/<branchname>
+            # need to cut the prefix
+            'branch.{}.remote'.format(p['from_ref'][11:]),
+            None) == target and ds.config.get(
+                'branch.{}.merge'.format(p['from_ref'][11:]),
+                None)
+        # if not, define target refspec explicitly to avoid having to
+        # set an upstream branch, which would happen implicitly from
+        # a users POV, and may also be hard to decide when publication
+        # dependencies are present
+        else '{}:{}'.format(p['from_ref'], p['to_ref'])
+        for p in wannabe_gitpush
+        # TODO: what if a publication dependency doesn't have it yet
+        # should we not attempt to push, because the main target has it?
+        if 'uptodate' not in p['operations'] and (
+            # cannot think of a scenario where we would want to push a
+            # managed branch directly, instead of the corresponding branch
+            'refs/heads/adjusted' not in p['from_ref'])
+    ]
+    # TODO this is not right with managed branches
+    active_branch = repo.get_active_branch()
+    if active_branch and is_annex_repo:
+        # we could face a managed branch, in which case we need to
+        # determine the actual one and make sure it is sync'ed with the
+        # managed one, and push that one instead. following methods can
+        # be called unconditionally
+        repo.localsync(managed_only=True)
+        active_branch = repo.get_corresponding_branch(
+            active_branch) or active_branch
+
+    if not refspecs2push and not active_branch:
+        # nothing was set up for push, and we have no active branch
+        # this is a weird one, let's confess and stop here
+        # I don't think we need to support such a scenario
+        if not active_branch:
+            yield dict(
+                res_kwargs,
+                status='impossible',
+                message=
+                'There is no active branch, cannot determine remote '
+                'branch'
+            )
+            return
+
+    # make sure that we always push the active branch (the context for the
+    # potential path arguments) and the annex branch -- because we claim
+    # to know better than any git config
+    must_have_branches = [active_branch] if active_branch else []
+    if is_annex_repo:
+        must_have_branches.append('git-annex')
+    for branch in must_have_branches:
+        _append_branch_to_refspec_if_needed(ds, refspecs2push, branch)
+
+    # we know what to push and where, now dependency processing first
+    for r in publish_depends:
+        # simply make a call to this function again, all the same, but
+        # target is different
+        yield from _push(
+            dspath,
+            content,
+            # to this particular dependency
+            r,
+            data,
+            force,
+            jobs,
+            res_kwargs.copy(),
+            pbars,
+            got_path_arg=got_path_arg,
         )
 
-        lgr.debug("Attempt to publish %i datasets", len(content_by_ds))
-        for ds_path in content_by_ds:
-            remote_info = ds_remote_info.get(ds_path, None)
-            if remote_info is None:
-                # maybe this dataset wasn't annotated above, try to get info
-                # MIH: I think this entire if-branch is practically impossible
-                # to reach. It is certainly untested, but I think this is due
-                # to mutually exclusive conditions during remote_info detection
-                remote_info_result = _get_remote_info(
-                    ds_path, ds_remote_info, to, missing)
-                if remote_info_result is not None:
-                    yield get_status_dict(
-                        type='dataset',
-                        path=ds_path,
-                        status=remote_info_result[0],
-                        message=remote_info_result[1],
-                        **res_kwargs)
-                    continue
-                # continue with freshly obtained info
-                remote_info = ds_remote_info[ds_path]
-                # condition above must catch all other cases
-                assert remote_info
-            # and publish
-            ds = Dataset(ds_path)
-            for r in _publish_dataset(
-                    ds,
-                    remote=remote_info['remote'],
-                    refspec=remote_info.get('refspec', None),
-                    # only send paths that were explicitly requested
-                    paths=[p for p in content_by_ds[ds_path]
-                           # do not feed (sub)dataset paths into the beast
-                           # makes no sense to try to annex copy them
-                           # for the base dataset itself let `transfer_data`
-                           # decide
-                           if p.get('type', None) != 'dataset'],
-                    annex_copy_options=annex_copy_opts,
-                    force=force,
-                    jobs=jobs,
-                    transfer_data=transfer_data,
-                    **res_kwargs):
-                yield r
+    # and lastly the primary push target
+    target_is_git_remote = repo.config.get(
+        'remote.{}.url'.format(target), None) is not None
+
+    # git-annex data copy
+    #
+    if is_annex_repo:
+        if data != "nothing":
+            log_progress(
+                lgr.info, pbar_id, "Transfer data",
+                label="Transfer data to '{}'".format(target), update=2, total=4)
+            yield from _transfer_data(
+                repo,
+                ds,
+                target,
+                content,
+                data,
+                force,
+                jobs,
+                res_kwargs.copy(),
+                got_path_arg=got_path_arg,
+            )
+        else:
+            lgr.debug("Data transfer to '%s' disabled by argument", target)
+    else:
+        lgr.debug("No data transfer: %s is not a git annex repository", repo)
+
+    if not target_is_git_remote:
+        # there is nothing that we need to push or sync with on the git-side
+        # of things with this remote
+        return
+
+    log_progress(
+        lgr.info, pbar_id, "Update availability information",
+        label="Update availability for '{}'".format(target), update=3, total=4)
+
+    # TODO fetch is only needed if anything was actually transferred. Collect this
+    # info and make the following conditional on it
+
+    # after file transfer the remote might have different commits to
+    # the annex branch. They have to be merged locally, otherwise a
+    # push of it further down will fail
+    try:
+        # fetch remote, let annex sync them locally, so that the push
+        # later on works.
+        # We have to fetch via the push url (if there is any),
+        # not a pull url.
+        # The latter might be dumb and without the execution of a
+        # post-update hook we might not be able to retrieve the
+        # server-side git-annex branch updates (and git-annex does
+        # not trigger the hook on copy), but we know we have
+        # full access via the push url -- we have just used it to copy.
+        lgr.debug("Fetching 'git-annex' branch updates from '%s'", target)
+        fetch_cmd = ['fetch', target, 'git-annex']
+        pushurl = repo.config.get(
+            'remote.{}.pushurl'.format(target), None)
+        if pushurl:
+            # for some reason overwriting remote.{target}.url
+            # does not have any effect...
+            fetch_cmd = [
+                '-c',
+                'url.{}.insteadof={}'.format(
+                    pushurl,
+                    repo.config.get(
+                        'remote.{}.url'.format(target), None)
+                )
+            ] + fetch_cmd
+            lgr.debug(
+                "Sync local annex branch from pushurl after remote "
+                'availability update.')
+        repo.call_git(fetch_cmd)
+        # If no CommandError was raised, it means that remote has git-annex
+        # but local repo might not be an annex yet. Since there is nothing to "sync"
+        # from us, we just skip localsync without mutating repo into an AnnexRepo
+        if is_annex_repo:
+            repo.localsync(target)
+    except CommandError as e:
+        # it is OK if the remote doesn't have a git-annex branch yet
+        # (e.g. fresh repo)
+        # TODO is this possible? we just copied? Maybe check if anything
+        # was actually copied?
+        if "fatal: couldn't find remote ref git-annex" not in e.stderr.lower():
+            raise
+        lgr.debug('Remote does not have a git-annex branch: %s', e)
+
+    if not refspecs2push:
+        lgr.debug('No refspecs found that need to be pushed')
+        return
+
+    # and push all relevant branches, plus the git-annex branch to announce
+    # local availability info too
+    yield from _push_refspecs(
+        repo,
+        target,
+        refspecs2push,
+        force_git_push,
+        res_kwargs.copy(),
+    )
+
+
+def _append_branch_to_refspec_if_needed(ds, refspecs, branch):
+    # try to anticipate any flavor of an idea of a branch ending up in a refspec
+    looks_like_that_branch = re.compile(
+        r'((^|.*:)refs/heads/|.*:|^){}$'.format(branch))
+    if all(not looks_like_that_branch.match(r) for r in refspecs):
+        refspecs.append(
+            branch
+            if ds.config.get('branch.{}.merge'.format(branch), None)
+            else '{branch}:{branch}'.format(branch=branch)
+        )
+
+
+def _push_refspecs(repo, target, refspecs, force_git_push, res_kwargs):
+    push_res = repo.push(
+        remote=target,
+        refspec=refspecs,
+        git_options=['--force'] if force_git_push else None,
+    )
+    # TODO maybe compress into a single message whenever everything is
+    # OK?
+    for pr in push_res:
+        ops = pr['operations']
+        status = (
+            'error'
+            if any(o in ops for o in (
+                'error', 'no-match', 'rejected', 'remote-rejected',
+                'remote-failure'))
+            else 'notneeded'
+            if 'uptodate' in pr['operations']
+            else 'ok'
+            if any(o in ops for o in (
+                'new-tag', 'new-branch', 'forced-update', 'fast-forward'))
+            # no really a good fit, but we have tested all relevant
+            # operations above, so in some sense this condition should be
+            # impossible to achieve
+            else 'impossible'
+        )
+        refspec = '{}:{}'.format(pr['from_ref'], pr['to_ref'])
+        yield dict(
+            res_kwargs,
+            status=status,
+            target=pr['remote'],
+            refspec=refspec,
+            operations=ops,
+            hints=pr.get('hints', None),
+            # seems like a good idea to pass on Git's native message
+            # TODO maybe implement a dedicated result renderer, instead
+            # of duplicating information only so that the default one
+            # can make sense at all
+            message='{}->{}:{} {}'.format(
+                pr['from_ref'],
+                pr['remote'],
+                pr['to_ref'],
+                pr['note']),
+        )
+
+
+def _push_data(ds, target, content, data, force, jobs, res_kwargs,
+               got_path_arg=False):
+    if ds.config.getbool('remote.{}'.format(target), 'annex-ignore', False):
+        lgr.debug(
+            "Target '%s' is set to annex-ignore, exclude from data-push.",
+            target,
+        )
+        return
+
+    ds_repo = ds.repo
+
+    res_kwargs['target'] = target
+    if not ds.config.get('.'.join(('remote', target, 'annex-uuid')), None):
+        # this remote either isn't an annex,
+        # or hasn't been properly initialized
+        # given that there was no annex-ignore, let's try to init it
+        # see https://github.com/datalad/datalad/issues/5143 for the story
+        ds_repo.localsync(target)
+
+        if not ds.config.get('.'.join(('remote', target, 'annex-uuid')), None):
+            # still nothing
+            # rather than barfing tons of messages for each file, do one
+            # for the entire dataset
+            yield dict(
+                res_kwargs,
+                action='copy',
+                status='impossible'
+                if force in ('all', 'checkdatapresent')
+                else 'notneeded',
+                message=(
+                    "Target '%s' does not appear to be an annex remote",
+                    target)
+            )
+            return
+
+    # it really looks like we will transfer files, get info on what annex
+    # has in store
+    # paths must be recoded to a dataset REPO root (in case of a symlinked
+    # location
+    annex_info_init = \
+        {ds_repo.pathobj / Path(c['path']).relative_to(ds.pathobj): c
+         for c in content} if ds.pathobj != ds_repo.pathobj else \
+        {Path(c['path']): c for c in content}
+    content = ds.repo.get_content_annexinfo(
+        # paths are taken from `annex_info_init`
+        paths=None,
+        init=annex_info_init,
+        ref='HEAD',
+        # this is an expensive operation that is only needed
+        # to perform a warning below, and for more accurate
+        # progress reporting (exclude unavailable content).
+        # limit to cases with explicit paths provided
+        eval_availability=True if got_path_arg else False,
+    )
+    # figure out which of the reported content (after evaluating
+    # `since` and `path` arguments needs transport
+    to_transfer = [
+        c
+        for c in content.values()
+        # by force
+        if ((force in ('all', 'checkdatapresent') or
+             # or by modification report
+             c.get('state', None) not in ('clean', 'deleted'))
+            # only consider annex'ed files
+            and 'key' in c
+        )
+    ]
+    if got_path_arg:
+        for c in [c for c in to_transfer if not c.get('has_content', False)]:
+            yield dict(
+                res_kwargs,
+                type=c['type'],
+                path=c['path'],
+                action='copy',
+                status='impossible',
+                message='Slated for transport, but no content present',
+            )
+
+    cmd = ['copy', '--batch', '-z', '--to', target]
+
+    if jobs:
+        cmd.extend(['--jobs', str(jobs)])
+
+    # Since we got here - we already have some  data != "nothing"
+    if (data == 'auto') or \
+        (
+            (data == 'auto-if-wanted') and
+            ds_repo.get_preferred_content('wanted', target)
+        ):
+        lgr.debug("Invoking copy --auto")
+        cmd.append('--auto')
+
+    if force not in ('all', 'checkdatapresent'):
+        # if we force, we do not trust local knowledge and do the checks
+        cmd.append('--fast')
+
+    lgr.debug("Pushing data from %s to '%s'", ds, target)
+
+    # input has type=dataset, but now it is about files
+    res_kwargs.pop('type', None)
+
+    # A set and a dict is used to track files pointing to the
+    # same key.  The set could be dropped, using a single dictionary
+    # that has an entry for every seen key and a (likely empty) list
+    # of redundant files, but that would mean looping over potentially
+    # many keys to yield likely few if any notneeded results.
+    seen_keys = set()
+    repkey_paths = dict()
+
+    # produce final path list. use knowledge that annex command will
+    # run in the root of the dataset and compact paths to be relative
+    # to this location
+    file_list = b''
+    nbytes = 0
+    for c in to_transfer:
+        key = c['key']
+        if key in seen_keys:
+            repkey_paths.setdefault(key, []).append(c['path'])
+        else:
+            file_list += bytes(Path(c['path']).relative_to(ds.pathobj))
+            file_list += b'\0'
+            nbytes += c.get('bytesize', 0)
+            seen_keys.add(key)
+    lgr.debug('Counted %d bytes of annex data to transfer',
+              nbytes)
+
+    # and go
+    res = ds_repo._call_annex_records(
+        cmd,
+        git_options=[
+            "-c",
+            "annex.retry={}".format(
+                ds_repo.config.obtain("datalad.annex.retry"))]
+            if ds_repo.config.get("annex.retry") else None,
+        stdin=file_list,
+        progress=True,
+        # tailor the progress protocol with the total number of files
+        # to be transferred
+        total_nbytes=nbytes)
+    for j in res:
+        yield annexjson2result(j, ds, type='file', **res_kwargs)
+
+    for annex_key, paths in repkey_paths.items():
+        for path in paths:
+            yield dict(
+                res_kwargs, action='copy', type='file', status='notneeded',
+                path=path, annexkey=annex_key,
+                message='Another file points to the same key')
+    return
+
+
+def _get_corresponding_remote_state(repo, to):
+    since = None
+    # for managed branches we cannot assume a matching one at the remote end
+    # instead we target the corresponding branch
+    active_branch = repo.get_corresponding_branch() or repo.get_active_branch()
+
+    if to:
+        # XXX here we assume one to one mapping of names from local branches
+        # to the remote
+        since = '%s/%s' % (to, active_branch)
+    else:
+        # take tracking remote for the active branch
+        tracked_remote, tracked_refspec = repo.get_tracking_branch()
+        if tracked_remote:
+            if tracked_refspec.startswith('refs/heads/'):
+                tracked_refspec = tracked_refspec[len('refs/heads/'):]
+            #to = tracked_remote
+            since = '%s/%s' % (tracked_remote, tracked_refspec)
+    return since
```

### Comparing `datalad-0.9.3/datalad/distribution/create_sibling.py` & `datalad-1.0.0/datalad/distributed/drop.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,807 +1,836 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""High-level interface for creation of publication target via SSH
+"""High-level interface for dropping dataset content
+
 """
 
 __docformat__ = 'restructuredtext'
 
-from six import text_type
-from distutils.version import LooseVersion
-from glob import glob
 import logging
-from os.path import join as opj, relpath, normpath, dirname, curdir
+from itertools import chain
+import warnings
 
-import datalad
-from datalad import ssh_manager
-from datalad.cmd import CommandError
-from datalad.consts import WEB_HTML_DIR, WEB_META_LOG
-from datalad.consts import TIMESTAMP_FMT
-from datalad.dochelpers import exc_str
-from datalad.distribution.siblings import Siblings
-from datalad.distribution.siblings import _DelayedSuper
-from datalad.distribution.add_sibling import _check_deps
-from datalad.distribution.dataset import EnsureDataset, Dataset, \
-    datasetmethod, require_dataset
-from datalad.interface.annotate_paths import AnnotatePaths
-from datalad.interface.base import Interface
-from datalad.interface.base import build_doc
-from datalad.interface.utils import eval_results
-from datalad.interface.common_opts import recursion_limit, recursion_flag
-from datalad.interface.common_opts import as_common_datasrc
-from datalad.interface.common_opts import publish_by_default
-from datalad.interface.common_opts import publish_depends
-from datalad.interface.common_opts import inherit_opt
-from datalad.interface.common_opts import annex_wanted_opt
-from datalad.interface.common_opts import annex_group_opt
-from datalad.interface.common_opts import annex_groupwanted_opt
+from datalad.core.local.status import get_paths_by_ds
+from datalad.distribution.dataset import (
+    Dataset,
+    EnsureDataset,
+    datasetmethod,
+    require_dataset,
+)
+from datalad.interface.base import (
+    Interface,
+    build_doc,
+    eval_results,
+)
+from datalad.interface.common_opts import (
+    jobs_opt,
+    recursion_flag,
+    recursion_limit,
+)
+from datalad.interface.results import (
+    annexjson2result,
+    results_from_annex_noinfo,
+    success_status_map,
+)
+from datalad.runner.exception import CommandError
 from datalad.support.annexrepo import AnnexRepo
-from datalad.support.constraints import EnsureStr, EnsureNone, EnsureBool
-from datalad.support.constraints import EnsureChoice
-from datalad.support.exceptions import InsufficientArgumentsError
-from datalad.support.exceptions import MissingExternalDependency
-from datalad.support.network import RI
-from datalad.support.network import is_ssh
-from datalad.support.sshconnector import sh_quote
+from datalad.support.constraints import (
+    EnsureChoice,
+    EnsureNone,
+    EnsureStr,
+)
+from datalad.support.exceptions import CapturedException
 from datalad.support.param import Parameter
-from datalad.utils import make_tempfile
-from datalad.utils import not_supported_on_windows
-from datalad.utils import _path_
-from datalad.utils import slash_join
-from datalad.utils import assure_list
-
-
-lgr = logging.getLogger('datalad.distribution.create_sibling')
-
-
-def _create_dataset_sibling(
-        name,
-        ds,
-        hierarchy_basepath,
-        ssh,
-        replicate_local_structure,
-        ssh_url,
-        target_dir,
-        target_url,
-        target_pushurl,
-        existing,
-        shared,
-        publish_depends,
-        publish_by_default,
-        as_common_datasrc,
-        annex_wanted,
-        annex_group,
-        annex_groupwanted,
-        inherit
-):
-    """Everyone is very smart here and could figure out the combinatorial
-    affluence among provided tiny (just slightly over a dozen) number of options
-    and only a few pages of code
-    """
-    localds_path = ds.path
-    ds_name = relpath(localds_path, start=hierarchy_basepath)
-    if not replicate_local_structure:
-        ds_name = '' if ds_name == curdir \
-            else '-{}'.format(ds_name.replace("/", "-"))
-        remoteds_path = target_dir.replace(
-            "%RELNAME",
-            ds_name)
-    else:
-        # TODO: opj depends on local platform, not the remote one.
-        # check how to deal with it. Does windows ssh server accept
-        # posix paths? vice versa? Should planned SSH class provide
-        # tools for this issue?
-        # see gh-1188
-        remoteds_path = normpath(opj(target_dir, ds_name))
-
-    # construct a would-be ssh url based on the current dataset's path
-    ssh_url.path = remoteds_path
-    ds_sshurl = ssh_url.as_str()
-    # configure dataset's git-access urls
-    ds_target_url = target_url.replace('%RELNAME', ds_name) \
-        if target_url else ds_sshurl
-    # push, configure only if needed
-    ds_target_pushurl = None
-    if ds_target_url != ds_sshurl:
-        # not guaranteed that we can push via the primary URL
-        ds_target_pushurl = target_pushurl.replace('%RELNAME', ds_name) \
-            if target_pushurl else ds_sshurl
-
-    lgr.info("Considering to create a target dataset {0} at {1} of {2}".format(
-        localds_path, remoteds_path, ssh_url.hostname))
-    # Must be set to True only if exists and existing='reconfigure'
-    # otherwise we might skip actions if we say existing='reconfigure'
-    # but it did not even exist before
-    only_reconfigure = False
-    if remoteds_path != '.':
-        # check if target exists
-        # TODO: Is this condition valid for != '.' only?
-        path_exists = True
-        try:
-            out, err = ssh("ls {}".format(sh_quote(remoteds_path)))
-        except CommandError as e:
-            if "No such file or directory" in e.stderr and \
-                    remoteds_path in e.stderr:
-                path_exists = False
-            else:
-                raise  # It's an unexpected failure here
-
-        if path_exists:
-            _msg = "Target path %s already exists." % remoteds_path
-            # path might be existing but be an empty directory, which should be
-            # ok to remove
-            try:
-                lgr.debug(
-                    "Trying to rmdir %s on remote since might be an empty dir",
-                    remoteds_path
-                )
-                # should be safe since should not remove anything unless an empty dir
-                ssh("rmdir {}".format(sh_quote(remoteds_path)))
-                path_exists = False
-            except CommandError as e:
-                # If fails to rmdir -- either contains stuff no permissions
-                # TODO: fixup encode/decode dance again :-/ we should have got
-                # unicode/str here by now.  I guess it is the same as
-                # https://github.com/ReproNim/niceman/issues/83
-                # where I have reused this Runner thing
-                try:
-                    # ds_name is unicode which makes _msg unicode so we must be
-                    # unicode-ready
-                    err_str = text_type(e.stderr)
-                except UnicodeDecodeError:
-                    err_str = e.stderr.decode(errors='replace')
-                _msg += " And it fails to rmdir (%s)." % (err_str.strip(),)
-
-        if path_exists:
-            if existing == 'error':
-                raise RuntimeError(_msg)
-            elif existing == 'skip':
-                lgr.info(_msg + " Skipping")
-                return
-            elif existing == 'replace':
-                lgr.info(_msg + " Replacing")
-                # enable write permissions to allow removing dir
-                ssh("chmod +r+w -R {}".format(sh_quote(remoteds_path)))
-                # remove target at path
-                ssh("rm -rf {}".format(sh_quote(remoteds_path)))
-                # if we succeeded in removing it
-                path_exists = False
-                # Since it is gone now, git-annex also should forget about it
-                remotes = ds.repo.get_remotes()
-                if name in remotes:
-                    # so we had this remote already, we should announce it dead
-                    # XXX what if there was some kind of mismatch and this name
-                    # isn't matching the actual remote UUID?  should have we
-                    # checked more carefully?
-                    lgr.info(
-                        "Announcing existing remote %s dead to annex and removing",
-                        name
-                    )
-                    if isinstance(ds.repo, AnnexRepo):
-                        ds.repo.set_remote_dead(name)
-                    ds.repo.remove_remote(name)
-            elif existing == 'reconfigure':
-                lgr.info(_msg + " Will only reconfigure")
-                only_reconfigure = True
-            else:
-                raise ValueError(
-                    "Do not know how to handle existing={}".format(
-                        repr(existing)))
-
-        if not path_exists:
-            ssh("mkdir -p {}".format(sh_quote(remoteds_path)))
-
-    if inherit and shared is None:
-        # here we must analyze current_ds's super, not the super_ds
-        delayed_super = _DelayedSuper(ds)
-        # inherit from the setting on remote end
-        shared = CreateSibling._get_ds_remote_shared_setting(
-            delayed_super, name, ssh)
-
-    # don't (re-)initialize dataset if existing == reconfigure
-    if not only_reconfigure:
-        # init git and possibly annex repo
-        if not CreateSibling.init_remote_repo(
-                remoteds_path, ssh, shared, ds,
-                description=target_url):
-            return
-
-        if target_url and not is_ssh(target_url):
-            # we are not coming in via SSH, hence cannot assume proper
-            # setup for webserver access -> fix
-            ssh('git -C {} update-server-info'.format(sh_quote(remoteds_path)))
-    else:
-        # TODO -- we might still want to reconfigure 'shared' setting!
-        pass
-
-    # at this point we have a remote sibling in some shape or form
-    # -> add as remote
-    lgr.debug("Adding the siblings")
-    # TODO generator, yield the now swallowed results
-    Siblings.__call__(
-        'configure',
-        dataset=ds,
-        name=name,
-        url=ds_target_url,
-        pushurl=ds_target_pushurl,
-        recursive=False,
-        fetch=True,
-        as_common_datasrc=as_common_datasrc,
-        publish_by_default=publish_by_default,
-        publish_depends=publish_depends,
-        annex_wanted=annex_wanted,
-        annex_group=annex_group,
-        annex_groupwanted=annex_groupwanted,
-        inherit=inherit
-    )
-
-    # check git version on remote end
-    lgr.info("Adjusting remote git configuration")
-    if ssh.get_git_version() and ssh.get_git_version() >= LooseVersion("2.4"):
-        # allow for pushing to checked out branch
-        try:
-            ssh("git -C {} config receive.denyCurrentBranch updateInstead".format(
-                sh_quote(remoteds_path)))
-        except CommandError as e:
-            lgr.error("git config failed at remote location %s.\n"
-                      "You will not be able to push to checked out "
-                      "branch. Error: %s", remoteds_path, exc_str(e))
-    else:
-        lgr.error("Git version >= 2.4 needed to configure remote."
-                  " Version detected on server: %s\nSkipping configuration"
-                  " of receive.denyCurrentBranch - you will not be able to"
-                  " publish updates to this repository. Upgrade your git"
-                  " and run with --existing=reconfigure",
-                  ssh.get_git_version())
-
-    # enable metadata refresh on dataset updates to publication server
-    lgr.info("Enabling git post-update hook ...")
-    try:
-        CreateSibling.create_postupdate_hook(
-            remoteds_path, ssh, ds)
-    except CommandError as e:
-        lgr.error("Failed to add json creation command to post update "
-                  "hook.\nError: %s" % exc_str(e))
+from datalad.utils import (
+    ensure_list,
+    rmtree,
+)
 
-    return remoteds_path
+lgr = logging.getLogger('datalad.distributed.drop')
 
 
 @build_doc
-class CreateSibling(Interface):
-    """Create a dataset sibling on a UNIX-like SSH-accessible machine
+class Drop(Interface):
+    """Drop content of individual files or entire (sub)datasets
+
+    This command is the antagonist of 'get'. It can undo the retrieval of file
+    content, and the installation of subdatasets.
 
-    Given a local dataset, and SSH login information this command creates
-    a remote dataset repository and configures it as a dataset sibling to
-    be used as a publication target (see `publish` command).
-
-    Various properties of the remote sibling can be configured (e.g. name
-    location on the server, read and write access URLs, and access
-    permissions.
-
-    Optionally, a basic web-viewer for DataLad datasets can be installed
-    at the remote location.
-
-    This command supports recursive processing of dataset hierarchies, creating
-    a remote sibling for each dataset in the hierarchy. By default, remote
-    siblings are created in hierarchical structure that reflects the
-    organization on the local file system. However, a simple templating
-    mechanism is provided to produce a flat list of datasets (see
-    --target-dir).
+    Dropping is a safe-by-default operation. Before dropping any information,
+    the command confirms the continued availability of file-content (see e.g.,
+    configuration 'annex.numcopies'), and the state of all dataset branches
+    from at least one known dataset sibling. Moreover, prior removal of an
+    entire dataset annex, that it is confirmed that it is no longer marked
+    as existing in the network of dataset siblings.
+
+    Importantly, all checks regarding version history availability and local
+    annex availability are performed using the current state of remote
+    siblings as known to the local dataset. This is done for performance
+    reasons and for resilience in case of absent network connectivity. To
+    ensure decision making based on up-to-date information, it is advised to
+    execute a dataset update before dropping dataset components.
     """
-    # XXX prevent common args from being added to the docstring
-    _no_eval_results = True
+    _examples_ = [
+        {'text': "Drop single file content",
+         'code_py': "drop('path/to/file')",
+         'code_cmd': "datalad drop <path/to/file>"},
+        {'text': "Drop all file content in the current dataset",
+         'code_py': "drop('.')",
+         'code_cmd': "datalad drop"},
+        {'text': "Drop all file content in a dataset and all its subdatasets",
+         'code_py': "drop(dataset='.', recursive=True)",
+         'code_cmd': "datalad drop -d <path/to/dataset> -r"},
+        {'text': "Disable check to ensure the configured minimum number of "
+                 "remote sources for dropped data",
+         'code_py': "drop(path='path/to/content', reckless='availability')",
+         'code_cmd': "datalad drop <path/to/content> --reckless availability"},
+        {'text': 'Drop (uninstall) an entire dataset '
+                 '(will fail with subdatasets present)',
+         'code_py': "drop(what='all')",
+         'code_cmd': "datalad drop --what all"},
+        {'text': 'Kill a dataset recklessly with any existing subdatasets too'
+                 '(this will be fast, but will disable any and all safety '
+                 'checks)',
+         'code_py': "drop(what='all', reckless='kill', recursive=True)",
+         'code_cmd': "datalad drop --what all, --reckless kill --recursive"},
+    ]
 
     _params_ = dict(
-        # TODO: Figure out, whether (and when) to use `sshurl` as push url
         dataset=Parameter(
-            args=("--dataset", "-d",),
-            doc="""specify the dataset to create the publication target for. If
-                no dataset is given, an attempt is made to identify the dataset
-                based on the current working directory""",
+            args=("-d", "--dataset"),
+            metavar="DATASET",
+            doc="""specify the dataset to perform drop from.
+            If no dataset is given, the current working directory is used
+            as operation context""",
             constraints=EnsureDataset() | EnsureNone()),
-        sshurl=Parameter(
-            args=("sshurl",),
-            metavar='SSHURL',
-            nargs='?',
-            doc="""Login information for the target server. This can be given
-                as a URL (ssh://host/path) or SSH-style (user@host:path).
-                Unless overridden, this also serves the future dataset's access
-                URL and path on the server.""",
-            constraints=EnsureStr()),
-        name=Parameter(
-            args=('-s', '--name',),
-            metavar='NAME',
-            doc="""sibling name to create for this publication target.
-                If `recursive` is set, the same name will be used to label all
-                the subdatasets' siblings. When creating a target dataset fails,
-                no sibling is added""",
-            constraints=EnsureStr() | EnsureNone(),
-            nargs="?"),
-        target_dir=Parameter(
-            args=('--target-dir',),
-            metavar='PATH',
-            doc="""path to the directory *on the server* where the dataset
-                shall be created. By default the SSH access URL is used to
-                identify this directory. If a relative path is provided here,
-                it is interpreted as being relative to the user's home
-                directory on the server.\n
-                Additional features are relevant for recursive processing of
-                datasets with subdatasets. By default, the local
-                dataset structure is replicated on the server. However, it is
-                possible to provide a template for generating different target
-                directory names for all (sub)datasets. Templates can contain
-                certain placeholder that are substituted for each (sub)dataset.
-                For example: "/mydirectory/dataset%%RELNAME".\nSupported
-                placeholders:\n
-                %%RELNAME - the name of the datasets, with any slashes replaced by
-                dashes\n""",
-            constraints=EnsureStr() | EnsureNone()),
-        target_url=Parameter(
-            args=('--target-url',),
-            metavar='URL',
-            doc=""""public" access URL of the to-be-created target dataset(s)
-                (default: `sshurl`). Accessibility of this URL determines the
-                access permissions of potential consumers of the dataset.
-                As with `target_dir`, templates (same set of placeholders)
-                are supported.  Also, if specified, it is provided as the annex
-                description\n""",
-            constraints=EnsureStr() | EnsureNone()),
-        target_pushurl=Parameter(
-            args=('--target-pushurl',),
-            metavar='URL',
-            doc="""In case the `target_url` cannot be used to publish to the
-                dataset, this option specifies an alternative URL for this
-                purpose. As with `target_url`, templates (same set of
-                placeholders) are supported.\n""",
+        path=Parameter(
+            args=("path",),
+            metavar="PATH",
+            doc="path of a dataset or dataset component to be dropped",
+            nargs="*",
             constraints=EnsureStr() | EnsureNone()),
+        reckless=Parameter(
+            args=("--reckless",),
+            doc="""disable individual or all data safety measures that would
+            normally prevent potentially irreversible data-loss.
+            With 'modification', unsaved modifications in a dataset will not be
+            detected. This improves performance at the cost of permitting
+            potential loss of unsaved or untracked dataset components.
+            With 'availability', detection of dataset/branch-states that are
+            only available in the local dataset, and detection of an
+            insufficient number of file-content copies will be disabled.
+            Especially the latter is a potentially expensive check which might
+            involve numerous network transactions.
+            With 'undead', detection of whether a to-be-removed local annex is
+            still known to exist in the network of dataset-clones is disabled.
+            This could cause zombie-records of invalid file availability.
+            With 'kill', all safety-checks are disabled.""",
+            constraints=EnsureChoice(
+                'modification', 'availability', 'undead', 'kill', None)),
+        what=Parameter(
+            args=("--what",),
+            doc="""select what type of items shall be dropped.
+            With 'filecontent', only the file content (git-annex keys) of files
+            in a dataset's worktree will be dropped.
+            With 'allkeys', content of any version of any file in any branch
+            (including, but not limited to the worktree) will be dropped. This
+            effectively empties the annex of a local dataset.
+            With 'datasets', only complete datasets will be dropped (implies
+            'allkeys' mode for each such dataset), but no filecontent will be
+            dropped for any files in datasets that are not dropped entirely.
+            With 'all', content for any matching file or dataset will be dropped
+            entirely.
+            """,
+            # TODO add 'unwanted'
+            constraints=EnsureChoice('filecontent', 'allkeys', 'datasets', 'all')),
         recursive=recursion_flag,
         recursion_limit=recursion_limit,
-        existing=Parameter(
-            args=("--existing",),
-            constraints=EnsureChoice('skip', 'replace', 'error', 'reconfigure'),
-            metavar='MODE',
-            doc="""action to perform, if a sibling is already configured under the
-            given name and/or a target directory already exists.
-            In this case, a dataset can be skipped ('skip'), an existing target
-            directory be forcefully re-initialized, and the sibling (re-)configured
-            ('replace', implies 'reconfigure'), the sibling configuration be updated
-            only ('reconfigure'), or to error ('error').""",),
-        inherit=inherit_opt,
-        shared=Parameter(
-            args=("--shared",),
-            metavar='false|true|umask|group|all|world|everybody|0xxx',
-            doc="""if given, configures the access permissions on the server
-            for multi-users (this could include access by a webserver!).
-            Possible values for this option are identical to those of
-            `git init --shared` and are described in its documentation.""",
-            constraints=EnsureStr() | EnsureBool() | EnsureNone()),
-        ui=Parameter(
-            args=("--ui",),
-            metavar='false|true|html_filename',
-            doc="""publish a web interface for the dataset with an
-            optional user-specified name for the html at publication
-            target. defaults to `index.html` at dataset root""",
-            constraints=EnsureBool() | EnsureStr()),
-        as_common_datasrc=as_common_datasrc,
-        publish_depends=publish_depends,
-        publish_by_default=publish_by_default,
-        annex_wanted=annex_wanted_opt,
-        annex_group=annex_group_opt,
-        annex_groupwanted=annex_groupwanted_opt,
-        since=Parameter(
-            args=("--since",),
-            constraints=EnsureStr() | EnsureNone(),
-            doc="""limit processing to datasets that have been changed since a given
-            state (by tag, branch, commit, etc). This can be used to create siblings
-            for recently added subdatasets."""),
+        jobs=jobs_opt,
+        check=Parameter(
+            args=("--nocheck",),
+            doc="""DEPRECATED: use '--reckless availability'""",
+            action="store_false",
+            dest='check'),
+        if_dirty=Parameter(
+            args=("--if-dirty",),
+            doc="""DEPRECATED and IGNORED: use --reckless instead""",),
     )
 
     @staticmethod
-    @datasetmethod(name='create_sibling')
+    @datasetmethod(name='drop')
     @eval_results
-    def __call__(sshurl, name=None, target_dir=None,
-                 target_url=None, target_pushurl=None,
-                 dataset=None,
-                 recursive=False,
-                 recursion_limit=None,
-                 existing='error', shared=None, ui=False,
-                 as_common_datasrc=None,
-                 publish_by_default=None,
-                 publish_depends=None,
-                 annex_wanted=None, annex_group=None, annex_groupwanted=None,
-                 inherit=False,
-                 since=None):
-
-        # there is no point in doing anything further
-        not_supported_on_windows(
-            "Support for SSH connections is not yet implemented in Windows")
-
-        #
-        # nothing without a base dataset
-        #
-        ds = require_dataset(dataset, check_installed=True,
-                             purpose='creating a sibling')
-        refds_path = ds.path
-
-        #
-        # all checks that are possible before we start parsing the dataset
-        #
-
-        # possibly use sshurl to get the name in case if not specified
-        if not sshurl:
-            if not inherit:
-                raise InsufficientArgumentsError(
-                    "needs at least an SSH URL, if no inherit option"
-                )
-            if name is None:
-                raise ValueError(
-                    "Neither SSH URL, nor the name of sibling to inherit from "
-                    "was specified"
-                )
-            # It might well be that we already have this remote setup
+    def __call__(
+            path=None,
+            *,
+            what='filecontent',
+            reckless=None,
+            dataset=None,
+            recursive=False,
+            recursion_limit=None,
+            jobs=None,
+            # deprecated
+            check=None,
+            if_dirty=None):
+
+        # TODO if reckless is None, initialize from a potential config setting
+        # left behind by a reckless clone
+
+        # proper spelling of mode switches is critical for implementation
+        # below. double-check, also in Python API usage
+        # TODO consider making a generic helper
+        for label, value in (('what', what), ('reckless', reckless)):
             try:
-                sshurl = CreateSibling._get_remote_url(ds, name)
-            except Exception as exc:
-                lgr.debug('%s does not know about url for %s: %s', ds, name, exc_str(exc))
-        elif inherit:
-            raise ValueError(
-                "For now, for clarity not allowing specifying a custom sshurl "
-                "while inheriting settings"
-            )
-            # may be could be safely dropped -- still WiP
+                Drop._params_[label].constraints(value)
+            except ValueError as e:
+                raise ValueError(
+                    f"Invalid '{label}' parameter value of: "
+                    f"{repr(value)} [{str(e)}]") from e
+
+        if check is not None:
+            warnings.warn(
+                "The `check` argument of `datalad drop` is deprecated, "
+                "use the `reckless` argument instead.",
+                DeprecationWarning)
+        if if_dirty is not None:
+            warnings.warn(
+                "The `if_dirty` argument of `datalad drop` is ignored, "
+                "it can be removed for a safe-by-default behavior. For "
+                "other cases consider the `reckless` argument.",
+                DeprecationWarning)
 
-        if not sshurl:
-            # TODO: may be more back up before _prep?
-            super_ds = ds.get_superdataset()
-            if not super_ds:
+        if check is False:
+            if reckless is not None:
                 raise ValueError(
-                    "Could not determine super dataset for %s to inherit URL"
-                    % ds
-                )
-            super_url = CreateSibling._get_remote_url(super_ds, name)
-            # for now assuming hierarchical setup
-            # (TODO: to be able to destinguish between the two, probably
-            # needs storing datalad.*.target_dir to have %RELNAME in there)
-            sshurl = slash_join(super_url, relpath(ds.path, super_ds.path))
-
-        # check the login URL
-        sshri = RI(sshurl)
-        if not is_ssh(sshri):
-            raise ValueError(
-                "Unsupported SSH URL: '{0}', "
-                "use ssh://host/path or host:path syntax".format(sshurl))
+                    'Must not use deprecated `check` argument, and new '
+                    '`reckless` argument together with `datalad drop`.')
+            reckless = 'availability'
 
-        if not name:
-            # use the hostname as default remote name
-            name = sshri.hostname
-            lgr.debug(
-                "No sibling name given, use URL hostname '%s' as sibling name",
-                name)
-
-        if since == '':
-            # consider creating siblings only since the point of
-            # the last update
-            # XXX here we assume one to one mapping of names from local branches
-            # to the remote
-            active_branch = ds.repo.get_active_branch()
-            since = '%s/%s' % (name, active_branch)
-
-        #
-        # parse the base dataset to find all subdatasets that need processing
-        #
-        to_process = []
-        for ap in AnnotatePaths.__call__(
-                dataset=refds_path,
-                # only a single path!
-                path=refds_path,
+        if what in ('all', 'datasets') and reckless == 'kill' and not recursive:
+            raise ValueError(
+                'A reckless kill is requested but no recursion flag is set. '
+                "With 'kill' no checks for subdatasets will be made, "
+                'acknowledge by setting the recursive flag')
+
+        # we cannot test for what=='allkeys' and path==None here,
+        # on per each dataset. otherwise we will not be able to drop
+        # from a subdataset, by given its path -- desirable MIH thinks
+
+        ds = require_dataset(dataset, check_installed=True, purpose='drop')
+
+        res_props = dict(
+            logger=lgr,
+            refds=ds.path,
+        )
+        # if not paths are given, there will still be a single dataset record
+        # with paths==None
+        paths_by_ds, errors = get_paths_by_ds(
+            ds,
+            dataset,
+            ensure_list(path),
+            # XXX this needs more thought!! Maybe this is what the mode should be
+            # in general?!
+            # when we want to drop entire datasets, it is much more useful
+            # to have subdatasets be their own record
+            subdsroot_mode='sub'
+            if what in ('all', 'datasets')
+            else 'rsync',
+        )
+        for e in errors:
+            yield dict(
+                action='drop',
+                path=str(e),
+                status='error',
+                message=('path not underneath the reference dataset %s', ds),
+                **res_props)
+            # we are not returning, a caller could decide on failure mode
+
+        if what in ('all', 'datasets') and _paths_atunder_dir(
+                paths_by_ds, ds.pathobj.cwd()):
+            raise RuntimeError(
+                'refuse to perform actions that would remove the current '
+                'working directory')
+
+        lgr.debug('Discovered %i datasets to drop (from)', len(paths_by_ds))
+
+        # a dataset paths are normalized and guaranteed to be under the same
+        # root dataset
+        # we want to start from the leave datasets, such that all discovered
+        # dataset can be processed independently
+        for dpath in sorted(paths_by_ds.keys(), reverse=True):
+            d = Dataset(dpath)
+            lgr.debug('Starting to drop %s at %s', what, d)
+            for res in _drop_dataset(
+                    d,
+                    paths_by_ds[dpath],
+                    what=what,
+                    reckless=reckless,
+                    # recursion from any of the given paths!
+                    recursive=recursive,
+                    recursion_limit=recursion_limit,
+                    jobs=jobs):
+                yield dict(res, **res_props)
+            lgr.debug('Finished dropping %s at %s', what, d)
+        return
+
+
+def _paths_atunder_dir(pbd, dirpath):
+    """Whether any of the paths is at or under a reference path
+
+    Parameters
+    ----------
+    pbd: dict
+      Dataset path dict is produced by get_paths_by_ds()
+    dirpath: Path
+      Reference path
+
+    Returns
+    -------
+    bool
+    """
+    for dpath, paths in pbd.items():
+        for p in ([dpath] if paths is None else paths):
+            if p == dirpath or p in dirpath.parents:
+                return True
+    return False
+
+
+def _drop_dataset(ds, paths, what, reckless, recursive, recursion_limit, jobs):
+    lgr.debug('Start dropping for %s', ds)
+    # we know that any given path is part of `ds` and not any of its
+    # subdatasets!
+
+    # by-passing this completely with reckless=kill
+    if recursive and not reckless == 'kill':
+        # process subdatasets first with recursion
+        for sub in ds.subdatasets(
+                # must be resolved!
+                path=paths or None,
+                # nothing to drop with unavailable subdatasets
+                state='present',
+                # we can use the full recursion depth, only the first layer
+                # of calls to _drop_dataset() must/can have recursive=True
                 recursive=recursive,
                 recursion_limit=recursion_limit,
-                action='create_sibling',
-                # both next should not happen anyways
-                unavailable_path_status='impossible',
-                nondataset_path_status='error',
-                modified=since,
+                # start reporting with the leaves
+                bottomup=True,
+                result_xfm='datasets',
+                on_failure='ignore',
                 return_type='generator',
-                on_failure='ignore'):
-            if ap.get('status', None):
-                # this is done
-                yield ap
-                continue
-            if ap.get('type', None) != 'dataset' or ap.get('state', None) == 'absent':
-                # this can happen when there is `since`, but we have no
-                # use for anything but datasets here
-                continue
-            checkds_remotes = Dataset(ap['path']).repo.get_remotes() \
-                if ap.get('state', None) != 'absent' \
-                else []
-            if publish_depends:
-                # make sure dependencies are valid
-                # TODO: inherit -- we might want to automagically create
-                # those dependents as well???
-                unknown_deps = set(assure_list(publish_depends)).difference(checkds_remotes)
-                if unknown_deps:
-                    ap['status'] = 'error'
-                    ap['message'] = (
-                        'unknown sibling(s) specified as publication dependency: %s',
-                        unknown_deps)
-                    yield ap
-                    continue
-            if name in checkds_remotes and existing in ('error', 'skip'):
-                ap['status'] = 'error' if existing == 'error' else 'notneeded'
-                ap['message'] = (
-                    "sibling '%s' already configured (specify alternative name, or force "
-                    "reconfiguration via --existing",
-                    name)
-                yield ap
-                continue
-            to_process.append(ap)
+                result_renderer='disabled'):
+            yield from _drop_dataset(
+                ds=sub,
+                # everything, the entire subdataset is matching a given path
+                paths=None,
+                what=what,
+                reckless=reckless,
+                recursive=False,
+                recursion_limit=None,
+                jobs=jobs)
+
+    if not ds.pathobj.exists():
+        # basic protection against something having wiped it out already.
+        # should not happen, but better than a crash, if it does
+        yield dict(
+            action='drop',
+            path=ds.path,
+            status='notneeded',
+            message="does not exist",
+            type='dataset',
+        )
+        return
+
+    if paths is not None and paths != [ds.pathobj] and what == 'all':
+        if recursive and reckless == 'kill':
+            # check if any paths contains a subdataset, and if so, drop it to
+            # ensure its not left behind
+            for sub in ds.subdatasets(
+                    # just check for subds at the provided path
+                    path=paths,
+                    state='present',
+                    recursive=recursive,
+                    recursion_limit=recursion_limit,
+                    result_xfm='datasets',
+                    on_failure='ignore',
+                    return_type='generator',
+                    result_renderer='disabled'):
+                if sub is not None:
+                    # there is a subdataset underneath the given path
+                    yield from _drop_dataset(
+                        ds=sub,
+                        # everything underneath the subds can go
+                        paths=None,
+                        what=what,
+                        reckless=reckless,
+                        recursive=False,
+                        recursion_limit=None,
+                        jobs=jobs)
+        # so we have paths constraints that prevent dropping the full dataset
+        lgr.debug('Only dropping file content for given paths in %s, '
+                  'allthough instruction was to drop %s', ds, what)
+        what = 'filecontent'
+
+    repo = ds.repo
+    is_annex = isinstance(repo, AnnexRepo)
+
+    # first critical checks that might prevent further operation
+    had_fatality = False
+    for res in _fatal_pre_drop_checks(
+            ds, repo, paths, what, reckless, is_annex):
+        had_fatality = True
+        yield res
+    if had_fatality:
+        return
+
+    # next check must come AFTER the modification checks above, otherwise
+    # remove() could not rely on the modification detection above
+    if paths is not None and paths != [ds.pathobj] and what == 'datasets':
+        # so we have paths constraints that prevent dropping the full dataset
+        # there is nothing to do here, but to drop keys, which we must not
+        # done
+        return
+
+    # now conditional/informative checks
+    yield from _pre_drop_checks(ds, repo, paths, what, reckless, is_annex)
+
+    if is_annex and what == 'filecontent':
+        yield from _drop_files(
+            ds,
+            repo,
+            # give paths or '.' with no constraint
+            paths=[str(p.relative_to(ds.pathobj))
+                   for p in paths] if paths else '.',
+            force=reckless in ('availability', 'kill'),
+            jobs=jobs,
+        )
+        # end it here for safety, the rest of the function deals with
+        # dropping more and more fundamentally
+        return
+
+    drop_all_errored = False
+    if is_annex and what in ('allkeys', 'datasets', 'all') \
+            and not reckless == 'kill':
+        for r in _drop_allkeys(
+                ds,
+                repo,
+                force=reckless in ('availability',),
+                jobs=jobs):
+            res = dict(
+                action='drop',
+                type='key',
+                # use the path of the containing dataset
+                # using the location of the key does not add any
+                # practical value, and is expensive to obtain
+                path=ds.path,
+                status='ok' if r.get('success') else 'error',
+                key=r.get('key'),
+            )
+            # pull any note, and rename recommended parameter to
+            # avoid confusion
+            message = r.get('note', '').replace(
+                '--force',
+                '--reckless availability')
+            if message:
+                res['message'] = message
+            error_messages = r.get('error-messages')
+            if error_messages:
+                res['error_message'] = '\n'.join(
+                    m.strip() for m in error_messages
+                )
+            # play safe, if there is no status, assume error
+            if res.get('status', 'error') != 'ok':
+                drop_all_errored = True
+            yield res
+
+    if drop_all_errored:
+        # end it here, if there is any indication that wiping out the
+        # repo is unsafe
+        return
+
+    if what in ('all', 'datasets'):
+        yield from _kill_dataset(ds)
+    lgr.debug('Done dropping for %s', ds)
+    return
+
+
+def _fatal_pre_drop_checks(ds, repo, paths, what, reckless, is_annex):
+    if what == 'allkeys' and paths is not None \
+            and paths != [ds.pathobj]:
+        yield dict(
+            action='drop',
+            path=ds.path,
+            type='dataset',
+            status='impossible',
+            message=(
+                'cannot drop %s, with path constraints given: %s',
+                what, paths),
+        )
+        return
+
+    if what in ('all', 'datasets') and not reckless == 'kill':
+        # we must not have subdatasets anymore
+        # if we do, --recursive was forgotten
+        subdatasets = ds.subdatasets(
+            path=paths,
+            # we only care about the present ones
+            state='present',
+            # first-level is enough, if that has none, there will be none
+            recursive=False,
+            result_xfm='paths',
+            result_renderer='disabled')
+        if subdatasets:
+            yield dict(
+                action='uninstall',
+                path=ds.path,
+                type='dataset',
+                status='error',
+                message=('cannot drop dataset, subdataset(s) still present '
+                         '(forgot --recursive?): %s', subdatasets)
+            )
+            # this is fatal
+            return
+
+    if what in ('all', 'datasets') \
+            and reckless not in ('availability', 'kill') \
+            and (paths is None or paths == [ds.pathobj]):
+        unpushed = _detect_unpushed_revs(repo, is_annex)
+        if unpushed:
+            yield dict(
+                action='uninstall',
+                path=ds.path,
+                type='dataset',
+                status='error',
+                message=(
+                    "to-be-dropped dataset has revisions that "
+                    "are not available at any known sibling. Use "
+                    "`datalad push --to ...` to push "
+                    "these before dropping the local dataset, "
+                    "or ignore via `--reckless availability`. "
+                    "Unique revisions: %s",
+                    unpushed)
+            )
+            # this is fatal
+            return
 
-        if not to_process:
-            # we ruled out all possibilities
-            # TODO wait for gh-1218 and make better return values
-            lgr.info("No datasets qualify for sibling creation. "
-                     "Consider different settings for --existing "
-                     "or --since if this is unexpected")
+    if is_annex and what in ('all', 'datasets') \
+            and reckless not in ('undead', 'kill'):
+        # this annex is about to die, test if it is still considered
+        # not-dead. if so, complain to avoid generation of zombies
+        # (annexed that are floating around, but are actually dead).
+        # if repo.uuid is None, git annex init never ran, and we can skip this
+        remotes_that_know_this_annex = None if repo.uuid is None else [
+            r
+            for r in _detect_nondead_annex_at_remotes(repo, repo.uuid)
+            # filter out "here"
+            if r is not None
+        ]
+        if remotes_that_know_this_annex:
+            yield dict(
+                action='uninstall',
+                path=ds.path,
+                type='dataset',
+                status='error',
+                message=(
+                    "to-be-deleted local annex not declared 'dead' at the "
+                    "following siblings. Announce death "
+                    "(`git annex dead here` + `datalad push --to ...`), "
+                    "or ignore via `--reckless undead`: %s",
+                    remotes_that_know_this_annex)
+            )
+            # this is fatal
             return
 
-        if target_dir is None:
-            if sshri.path:
-                target_dir = sshri.path
+    if reckless not in ('modification', 'kill'):
+        # do a cheaper status run to discover any kind of modification and
+        # generate results based on the `what` mode of operation
+        for res in ds.status(
+                path=paths,
+                # untracked content will not be reported on further down
+                # must catch it here
+                untracked='normal',
+                # downstream code can handle non-annex/annex distinction
+                # avoid expensive evaluation here
+                annex=False,
+                # we only need to inspect a subdataset's state in case
+                # we want to drop it completely
+                eval_subdataset_state='commit'
+                if what in ('all', 'datasets') else 'no',
+                # recursion is handled outside this function
+                recursive=False,
+                result_renderer='disabled',
+                return_type='generator',
+                on_failure='ignore'):
+            state = res.get('state')
+            if state == 'clean':
+                # nothing to worry about, nothing to communicate
+                continue
+            elif state in ('modified', 'untracked'):
+                yield dict(
+                    res,
+                    status='impossible',
+                    action='drop',
+                    message=f'cannot drop {state} content, save first',
+                )
+                # we are not aborting nevetheless. in normal conditions
+                # the above result will stop processing, but if desired
+                # we could go on
             else:
-                target_dir = '.'
+                lgr.debug(
+                    'Status record not considered for drop '
+                    'state inspection: %s', res)
 
-        # TODO: centralize and generalize template symbol handling
-        replicate_local_structure = "%RELNAME" not in target_dir
 
-        # request ssh connection:
-        lgr.info("Connecting ...")
-        assert(sshurl is not None)  # delayed anal verification
-        ssh = ssh_manager.get_connection(sshurl)
-        if not ssh.get_annex_version():
-            raise MissingExternalDependency(
-                'git-annex',
-                msg='on the remote system')
-
-        #
-        # all checks done and we have a connection, now do something
-        #
-
-        # loop over all datasets, ordered from top to bottom to make test
-        # below valid (existing directories would cause the machinery to halt)
-        # But we need to run post-update hook in depth-first fashion, so
-        # would only collect first and then run (see gh #790)
-        yielded = set()
-        remote_repos_to_run_hook_for = []
-        for currentds_ap in \
-                sorted(to_process, key=lambda x: x['path'].count('/')):
-            current_ds = Dataset(currentds_ap['path'])
-
-            path = _create_dataset_sibling(
-                name,
-                current_ds,
-                ds.path,
-                ssh,
-                replicate_local_structure,
-                sshri,
-                target_dir,
-                target_url,
-                target_pushurl,
-                existing,
-                shared,
-                publish_depends,
-                publish_by_default,
-                as_common_datasrc,
-                annex_wanted,
-                annex_group,
-                annex_groupwanted,
-                inherit
+def _pre_drop_checks(ds, repo, paths, what, reckless, is_annex):
+    if not is_annex and reckless not in ('datasets', 'kill'):
+        # we cannot drop content in non-annex repos, issue same
+        # 'notneeded' as for git-file in annex repo
+        for p in paths or [ds.path]:
+            yield dict(
+                action='drop',
+                path=str(p),
+                status='notneeded',
+                message="no annex'ed content",
             )
-            if not path:
-                # nothing new was created
-                # TODO is 'notneeded' appropriate in this case?
-                currentds_ap['status'] = 'notneeded'
-                # TODO explain status in 'message'
-                yield currentds_ap
-                yielded.add(currentds_ap['path'])
-                continue
-            remote_repos_to_run_hook_for.append((path, currentds_ap))
+        # continue, this is nothing fatal
 
-            # publish web-interface to root dataset on publication server
-            if current_ds.path == ds.path and ui:
-                lgr.info("Uploading web interface to %s" % path)
-                try:
-                    CreateSibling.upload_web_interface(path, ssh, shared, ui)
-                except CommandError as e:
-                    currentds_ap['status'] = 'error'
-                    currentds_ap['message'] = (
-                        "failed to push web interface to the remote datalad repository (%s)",
-                        exc_str(e))
-                    yield currentds_ap
-                    yielded.add(currentds_ap['path'])
-                    continue
-
-        # in reverse order would be depth first
-        lgr.info("Running post-update hooks in all created siblings")
-        # TODO: add progressbar
-        for path, currentds_ap in remote_repos_to_run_hook_for[::-1]:
-            # Trigger the hook
-            lgr.debug("Running hook for %s", path)
-            try:
-                ssh("cd {} && hooks/post-update".format(
-                    sh_quote(_path_(path, ".git"))))
-            except CommandError as e:
-                currentds_ap['status'] = 'error'
-                currentds_ap['message'] = (
-                    "failed to run post-update hook under remote path %s (%s)",
-                    path, exc_str(e))
-                yield currentds_ap
-                yielded.add(currentds_ap['path'])
-                continue
-            if not currentds_ap['path'] in yielded:
-                # if we were silent until now everything is just splendid
-                currentds_ap['status'] = 'ok'
-                yield currentds_ap
-
-    @staticmethod
-    def _get_ds_remote_shared_setting(ds, name, ssh):
-        """Figure out setting of sharedrepository for dataset's `name` remote"""
-        shared = None
+    if not is_annex and what in ('allkeys', 'unwanted') \
+            and not reckless == 'kill':
+        # these drop modes are meaningless without an annex
+        yield dict(
+            action='drop',
+            path=ds.path,
+            status='notneeded',
+            message="dataset with no annex",
+            type='dataset',
+        )
+        # continue, this is nothing fatal
+
+
+def _detect_unpushed_revs(repo, consider_managed_branches):
+    """Check if all local branch states (and HEAD) are available at a remote
+
+    There need not be a 1:1 correspondence. What is tested is whether
+    each commit corresponding to a local branch tip (or HEAD), is also an
+    ancestor of any remote branch. It is not required that there is a single
+    remote that has all commits.
+
+    This only uses the state of remotes known to the local remote state.
+    No remote synchronization is performed.
+
+    Parameters
+    ----------
+    repo: GitRepo
+      Repository to evaluated
+    consider_managed_branches: bool
+      Whether to enable handling of managed branches.
+
+    Returns
+    -------
+    list
+      Names of local states/refs that are no available at a remote.
+    """
+    if consider_managed_branches:
+        # consolidate corresponding branches to get reliable detection
+        repo.localsync(managed_only=True)
+    # we do not want to check this for any managed branches
+    # that are not meant to be pushed without consolidation
+    # or even at all (incl. git-annex, it can behave in complex ways)
+    local_refs = [
+        lb for lb in repo.get_branches()
+        if not (not consider_managed_branches
+            or lb == 'git-annex' or repo.is_managed_branch(lb))]
+    if not repo.get_active_branch():
+        # check for HEAD, in case we are on a detached HEAD
+        local_refs.append('HEAD')
+    # extend to tags?
+    remote_refs = repo.get_remote_branches()
+
+    unpushed_refs = [
+        local_ref
+        for local_ref in local_refs
+        if not any(repo.is_ancestor(local_ref, remote_ref)
+                   for remote_ref in remote_refs)
+    ]
+    return unpushed_refs
+
+
+def _detect_nondead_annex_at_remotes(repo, annex_uuid):
+    """Return list of remote names that know about a given (not-dead) annex
+
+    This only uses the state of remotes known to the local remote state.
+    No remote synchronization is performed.
+
+    Parameters
+    ----------
+    repo: AnnexRepo or GitRepo
+      Repository to evaluated
+    annex_uuid: str
+      UUID string of a particular annex
+
+    Returns
+    -------
+    list
+      Names of any matching remote, the local repository is indicated using
+      a `None` label.
+    """
+    # build the refs for all remotes and local
+    remotes_w_registration = []
+    for remote in chain([''], repo.get_remotes()):
+        refprefix = '{}{}git-annex:'.format(
+            remote,
+            '/' if remote else '',
+        )
+        uuid_known = False
         try:
-            current_super_url = CreateSibling._get_remote_url(
-                ds, name)
-            current_super_ri = RI(current_super_url)
-            out, err = ssh('git -C {} config --get core.sharedrepository'.format(
-                # TODO -- we might need to expanduser taking .user into account
-                # but then it must be done also on remote side
-                sh_quote(current_super_ri.path))
-            )
-            shared = out.strip()
-            if err:
-                lgr.warning("Got stderr while calling ssh: %s", err)
+            for line in repo.call_git_items_(
+                    ['cat-file', '-p', refprefix + 'uuid.log']):
+                if line.startswith(annex_uuid):
+                    # use None to label the local repo
+                    uuid_known = True
+                    break
         except CommandError as e:
-            lgr.debug(
-                "Could not figure out remote shared setting of %s for %s due "
-                "to %s",
-                ds, name, exc_str(e)
-            )
-            # could well be ok if e.g. not shared
-            # TODO: more detailed analysis may be?
-        return shared
-
-    @staticmethod
-    def _get_remote_url(ds, name):
-        """A little helper to get url from pushurl or from url if not defined"""
-        # take pushurl if present, if not -- just a url
-        url = ds.config.get('remote.%s.pushurl' % name) or \
-            ds.config.get('remote.%s.url' % name)
-        if not url:
-            raise ValueError(
-                "%s had neither pushurl or url defined for %s" % (ds, name)
-            )
-        return url
+            CapturedException(e)
+            # this is not a problem per-se, logged above, just continue
+            continue
+        if not uuid_known:
+            # if an annex id is not even in the uuid.log, we can stop here
+            # (for this remote)
+            continue
 
-    @staticmethod
-    def init_remote_repo(path, ssh, shared, dataset, description=None):
-        cmd = "git -C {} init{}".format(
-            sh_quote(path),
-            " --shared='{}'".format(sh_quote(shared)) if shared else '')
+        # annex is known, but maybe is declared dead already, must check
+        # trust.log in addition
         try:
-            ssh(cmd)
+            for line in repo.call_git_items_(
+                    ['cat-file', '-p', refprefix + 'trust.log']):
+                columns = line.split()
+                if columns[0] == annex_uuid:
+                    # not known if dead
+                    uuid_known = False if columns[1] == 'X' else True
+                    break
         except CommandError as e:
-            lgr.error("Initialization of remote git repository failed at %s."
-                      "\nError: %s\nSkipping ..." % (path, exc_str(e)))
-            return False
+            CapturedException(e)
+            # this is not a problem per-se, logged above, just continue
+            continue
+        finally:
+            if uuid_known:
+                remotes_w_registration.append(remote or None)
+    return(remotes_w_registration)
 
-        if isinstance(dataset.repo, AnnexRepo):
-            # init remote git annex repo (part fix of #463)
-            try:
-                ssh(
-                    "git -C {} annex init {}".format(
-                        sh_quote(path),
-                        sh_quote(description)
-                        if description else '')
-                )
-            except CommandError as e:
-                lgr.error("Initialization of remote git annex repository failed at %s."
-                          "\nError: %s\nSkipping ..." % (path, exc_str(e)))
-                return False
-        return True
 
-    @staticmethod
-    def create_postupdate_hook(path, ssh, dataset):
-        # location of post-update hook file, logs folder on remote target
-        hooks_remote_dir = opj(path, '.git', 'hooks')
-        # make sure hooks directory exists (see #1251)
-        ssh('mkdir -p {}'.format(sh_quote(hooks_remote_dir)))
-        hook_remote_target = opj(hooks_remote_dir, 'post-update')
-
-        # create json command for current dataset
-        log_filename = 'datalad-publish-hook-$(date +%s).log' % TIMESTAMP_FMT
-        hook_content = r'''#!/bin/bash
+def _kill_dataset(ds):
+    """This is a harsh internal helper: it will wipe out a dataset, no checks
+    """
+    # figure out whether we should be nice to a superdataset later on
+    has_super = ds.get_superdataset(topmost=False, registered_only=True)
+    # Close any possibly associated process etc with underlying repo.
+    # Otherwise - rmtree could fail to remove e.g. under NFS which would
+    # still have some files opened by them (thus having .nfs00000xxxx
+    # files) forbidding rmdir to work in rmtree
+    ds.close()
+    rmtree(ds.path)
+    # invalidate loaded ConfigManager -- datasets are singletons!!
+    ds._cfg = None
+    if has_super:
+        # recreate an empty mountpoint to make Git happier
+        ds.pathobj.mkdir(exist_ok=True)
+    yield dict(
+        # keep uninstall to please the gods of a distant past
+        #action='drop',
+        action='uninstall',
+        path=ds.path,
+        type='dataset',
+        status='ok',
+    )
 
-git update-server-info
 
-#
-# DataLad
-#
-# (Re)generate meta-data for DataLad Web UI and possibly init new submodules
-dsdir="$(dirname $0)/../.."
-logfile="$dsdir/{WEB_META_LOG}/{log_filename}"
-
-if [ ! -e "$dsdir/.git" ]; then
-  echo Assumption of being under .git has failed >&2
-  exit 1
-fi
-
-mkdir -p "$dsdir/{WEB_META_LOG}"  # assure logs directory exists
-
-( which datalad > /dev/null \
-  && ( cd "$dsdir"; GIT_DIR="$PWD/.git" datalad ls -a --json file .; ) \
-  || echo "E: no datalad found - skipping generation of indexes for web frontend"; \
-) &> "$logfile"
-
-# Some submodules might have been added and thus we better init them
-( cd "$dsdir"; git submodule update --init || : ; ) >> "$logfile" 2>&1
-'''.format(WEB_META_LOG=WEB_META_LOG, **locals())
-
-        with make_tempfile(content=hook_content) as tempf:
-            # create post_update hook script
-            # upload hook to dataset
-            ssh.copy(tempf, hook_remote_target)
-        # and make it executable
-        ssh('chmod +x {}'.format(sh_quote(hook_remote_target)))
+def _drop_allkeys(ds, repo, force=False, jobs=None):
+    """
+    """
+    assert not (repo.dot_git / 'annex').is_symlink(), \
+        "Dropping from a symlinked annex is unsupported to prevent data-loss"
 
-    @staticmethod
-    def upload_web_interface(path, ssh, shared, ui):
-        # path to web interface resources on local
-        webui_local = opj(dirname(datalad.__file__), 'resources', 'website')
-        # local html to dataset
-        html_local = opj(webui_local, "index.html")
-
-        # name and location of web-interface html on target
-        html_targetname = {True: ui, False: "index.html"}[isinstance(ui, str)]
-        html_target = opj(path, html_targetname)
-
-        # upload ui html to target
-        ssh.copy(html_local, html_target)
-
-        # upload assets to the dataset
-        webresources_local = opj(webui_local, 'assets')
-        webresources_remote = opj(path, WEB_HTML_DIR)
-        ssh('mkdir -p {}'.format(sh_quote(webresources_remote)))
-        ssh.copy(webresources_local, webresources_remote, recursive=True)
-
-        # minimize and upload js assets
-        for js_file in glob(opj(webresources_local, 'js', '*.js')):
-            with open(js_file) as asset:
-                try:
-                    from jsmin import jsmin
-                    # jsmin = lambda x: x   # no minimization
-                    minified = jsmin(asset.read())                      # minify asset
-                except ImportError:
-                    lgr.warning(
-                        "Will not minify web interface javascript, no jsmin available")
-                    minified = asset.read()                             # no minify available
-                with make_tempfile(content=minified) as tempf:          # write minified to tempfile
-                    js_name = js_file.split('/')[-1]
-                    ssh.copy(tempf, opj(webresources_remote, 'assets', 'js', js_name))  # and upload js
-
-        # explicitly make web+metadata dir of dataset world-readable, if shared set to 'all'
-        mode = None
-        if shared in (True, 'true', 'all', 'world', 'everybody'):
-            mode = 'a+rX'
-        elif shared == 'group':
-            mode = 'g+rX'
-        elif str(shared).startswith('0'):
-            mode = shared
-
-        if mode:
-            ssh('chmod {} -R {} {}'.format(
-                mode,
-                sh_quote(dirname(webresources_remote)),
-                sh_quote(opj(path, 'index.html'))))
+    cmd = ['drop', '--all']
+    if force:
+        cmd.append('--force')
+    if jobs:
+        cmd.extend(['--jobs', str(jobs)])
+
+    try:
+        yield from repo._call_annex_records_items_(cmd)
+    except CommandError as e:
+        # pick up the results captured so far and yield them
+        # the error will be amongst them
+        yield from e.kwargs.get('stdout_json', [])
+
+
+def _drop_files(ds, repo, paths, force=False, jobs=None):
+    """Helper to drop content in datasets.
+
+    Parameters
+    ----------
+    repo : AnnexRepo
+    paths : list
+      for which files to drop content
+    check : bool
+      whether to instruct annex to perform minimum copy availability
+      checks
+
+    Yields
+    ------
+    dict
+    """
+    assert not (repo.dot_git / 'annex').is_symlink(), \
+        "Dropping from a symlinked annex is unsupported to prevent data-loss"
+    cmd = ['drop']
+    if force:
+        cmd.append('--force')
+    if jobs:
+        cmd.extend(['--jobs', str(jobs)])
+
+    respath_by_status = {}
+    try:
+        yield from (
+            _postproc_annexdrop_result(res, respath_by_status, ds)
+            for res in repo._call_annex_records_items_(cmd, files=paths)
+        )
+    except CommandError as e:
+        # pick up the results captured so far and yield them
+        # the error will be amongst them
+        yield from (
+            _postproc_annexdrop_result(res, respath_by_status, ds)
+            for res in e.kwargs.get('stdout_json', [])
+        )
+    # report on things requested that annex was silent about
+    for r in results_from_annex_noinfo(
+            ds, paths, respath_by_status,
+            dir_fail_msg='could not drop some content in %s %s',
+            noinfo_dir_msg='nothing to drop from %s',
+            noinfo_file_msg="no annex'ed content"):
+        r['action'] = 'drop'
+        yield r
+
+
+def _postproc_annexdrop_result(res, respath_by_status, ds, **kwargs):
+    res = annexjson2result(
+        # annex reports are always about files
+        res, ds, type='file', **kwargs)
+    success = success_status_map[res['status']]
+    respath_by_status[success] = \
+        respath_by_status.get(success, []) + [res['path']]
+    if res["status"] == "error" and res["action"] == "drop":
+        msg = res.get("message", None)
+        if isinstance(msg, str) and "Use --force to" in msg:
+            # Avoid confusing datalad-drop callers with git-annex-drop's
+            # suggestion to use --force.
+            # Just mention reckless itself, do not go into the details
+            # of which mode. This is likely changing over time and
+            # adjusting this replacement will be forgotten.
+            res["message"] = msg.replace(
+                "--force",
+                "--reckless availability")
+    return res
```

### Comparing `datalad-0.9.3/datalad/distribution/subdatasets.py` & `datalad-1.0.0/datalad/local/subdatasets.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,178 +1,134 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 """Plumbing command for reporting subdatasets"""
 
 __docformat__ = 'restructuredtext'
 
 
 import logging
 import re
 import os
-from os.path import join as opj
-from os.path import normpath
-from os.path import relpath
-from os.path import exists
-
-from git import GitConfigParser
-
-from datalad.interface.base import Interface
-from datalad.interface.utils import eval_results
-from datalad.interface.base import build_doc
+import warnings
+
+from datalad.interface.utils import generic_result_renderer
+from datalad.interface.base import (
+    Interface,
+    build_doc,
+    eval_results,
+)
 from datalad.interface.results import get_status_dict
-from datalad.support.constraints import EnsureBool
-from datalad.support.constraints import EnsureStr
-from datalad.support.constraints import EnsureNone
+from datalad.support.constraints import (
+    EnsureStr,
+    EnsureNone,
+    NoneDeprecated,
+)
 from datalad.support.param import Parameter
-from datalad.support.gitrepo import InvalidGitRepositoryError
-from datalad.support.exceptions import CommandError
-from datalad.interface.common_opts import recursion_flag
-from datalad.interface.common_opts import recursion_limit
-from datalad.distribution.dataset import Dataset
-from datalad.distribution.dataset import require_dataset
-from datalad.cmd import GitRunner
+from datalad.support.exceptions import (
+    CapturedException,
+    CommandError
+)
+from datalad.interface.common_opts import (
+    contains,
+    dataset_state,
+    fulfilled,
+    recursion_flag,
+    recursion_limit,
+)
+from datalad.distribution.dataset import (
+    Dataset,
+    require_dataset,
+)
 from datalad.support.gitrepo import GitRepo
-from datalad.utils import with_pathsep as _with_sep
-from datalad.utils import path_startswith
-from datalad.utils import assure_list
-from datalad.dochelpers import exc_str
-
-from .dataset import EnsureDataset
-from .dataset import datasetmethod
-from .dataset import resolve_path
+from datalad.utils import (
+    ensure_list,
+    getpwd,
+    partition,
+    Path,
+)
+
+from datalad.distribution.dataset import (
+    EnsureDataset,
+    datasetmethod,
+    resolve_path,
+)
 
-lgr = logging.getLogger('datalad.distribution.subdatasets')
+lgr = logging.getLogger('datalad.local.subdatasets')
 
 
-submodule_full_props = re.compile(r'([0-9a-f]+) (.*) \((.*)\)$')
-submodule_nodescribe_props = re.compile(r'([0-9a-f]+) (.*)$')
 valid_key = re.compile(r'^[A-Za-z][-A-Za-z0-9]*$')
 
-status_map = {
-    ' ': 'clean',
-    '+': 'modified',
-    '-': 'absent',
-    'U': 'conflict',
-}
-
-
-def _parse_gitmodules(dspath):
-    gitmodule_path = opj(dspath, ".gitmodules")
-    parser = GitConfigParser(gitmodule_path)
-    mods = {}
-    for sec in parser.sections():
-        try:
-            modpath = parser.get(sec, 'path')
-        except Exception:
-            lgr.debug("Failed to get '%s.path', skipping section", sec)
-            continue
-        if not modpath or not sec.startswith('submodule '):
-            continue
-        modpath = normpath(opj(dspath, modpath))
-        modprops = {'gitmodule_{}'.format(opt): parser.get_value(sec, opt)
-                    for opt in parser.options(sec)
-                    if not (opt.startswith('__') or opt == 'path')}
-        modprops['gitmodule_name'] = sec[11:-1]
-        mods[modpath] = modprops
-    # make sure we let go of any resources held be the parser
-    # we cannot rely on __del__
-    parser.release()
-    return mods
 
-
-def _parse_git_submodules(dspath, recursive):
+def _parse_git_submodules(ds, paths, cache):
     """All known ones with some properties"""
-    if not exists(opj(dspath, ".gitmodules")):
+    ds_pathobj = ds.pathobj
+    if not (ds_pathobj / ".gitmodules").exists():
         # easy way out. if there is no .gitmodules file
         # we cannot have (functional) subdatasets
         return
 
-    # this will not work in direct mode, need better way #1422
-    cmd = ['git', '--work-tree=.', 'submodule', 'status']
-    if recursive:
-        cmd.append('--recursive')
-
-    # need to go rogue  and cannot use proper helper in GitRepo
-    # as they also pull in all of GitPython's magic
-    try:
-        stdout, stderr = GitRunner(cwd=dspath).run(
-            cmd,
-            log_stderr=True,
-            log_stdout=True,
-            # not sure why exactly, but log_online has to be false!
-            log_online=False,
-            expect_stderr=False,
-            shell=False,
-            # we don't want it to scream on stdout
-            expect_fail=True)
-    except CommandError as e:
-        raise InvalidGitRepositoryError(exc_str(e))
-
-    for line in stdout.split('\n'):
-        if not line:
-            continue
-        sm = {}
-
-        props = submodule_full_props.match(line[1:])
-        if props:
-            sm['revision'] = props.group(1)
-            sm['path'] = opj(dspath, props.group(2))
-            sm['revision_descr'] = props.group(3)
-        else:
-            props = submodule_nodescribe_props.match(line[1:])
-            sm['revision'] = props.group(1)
-            sm['path'] = opj(dspath, props.group(2))
-
-        sm['state'] = status_map[line[0]]
-        if sm['state'] == 'clean':
-            # with git 2.16 we see " " for uninstalled dataset
-            # and ((null)) in the trailing of the line
-            # This code is RFed completely away in datalad 0.10
-            if '((null))' in line:
-                sm['state'] = 'absent'
-
-        yield sm
+    if paths:
+        paths_outside, paths_at_or_in = partition(
+            paths,
+            lambda p: ds_pathobj == p or ds_pathobj in p.parents)
+        paths = [p.relative_to(ds_pathobj) for p in paths_at_or_in]
+        if not paths:
+            if any(p for p in paths_outside if p in ds_pathobj.parents):
+                # The dataset is directly under some specified path, so include
+                # it.
+                paths = None
+            else:
+                # we had path constraints, but none matched this dataset
+                return
+    # can we use the reported as such, or do we need to recode wrt to the
+    # query context dataset?
+    cache['repo'] = repo = ds.repo
+    if ds_pathobj == repo.pathobj:
+        yield from repo.get_submodules_(paths=paths)
+    else:
+        for props in repo.get_submodules_(paths=paths):
+            props['path'] = ds_pathobj / props['path'].relative_to(repo.pathobj)
+            yield props
 
 
 @build_doc
 class Subdatasets(Interface):
-    """Report subdatasets and their properties.
+    r"""Report subdatasets and their properties.
 
     The following properties are reported (if possible) for each matching
     subdataset record.
 
     "name"
         Name of the subdataset in the parent (often identical with the
         relative path in the parent dataset)
 
     "path"
         Absolute path to the subdataset
 
     "parentds"
         Absolute path to the parent dataset
 
-    "revision"
+    "gitshasum"
         SHA1 of the subdataset commit recorded in the parent dataset
 
     "state"
-        Condition of the subdataset: 'clean', 'modified', 'absent', 'conflict'
-        as reported by `git submodule`
-
-    "revision_descr"
-        Output of `git describe` for the subdataset
+        Condition of the subdataset: 'absent', 'present'
 
     "gitmodule_url"
         URL of the subdataset recorded in the parent
 
+    "gitmodule_name"
+        Name of the subdataset recorded in the parent
+
     "gitmodule_<label>"
         Any additional configuration property on record.
 
     Performance note: Property modification, requesting `bottomup` reporting
     order, or a particular numerical `recursion_limit` implies an internal
     switch to an alternative query implementation for recursive query that is
     more flexible, but also notably slower (performs one call to Git per
@@ -183,40 +139,40 @@
 
     "datalad-recursiveinstall"
         If set to 'skip', the respective subdataset is skipped when DataLad
         is recursively installing its superdataset. However, the subdataset
         remains installable when explicitly requested, and no other features
         are impaired.
 
-
-
+    "datalad-url"
+        If a subdataset was originally established by cloning, 'datalad-url'
+        records the URL that was used to do so. This might be different from
+        'url' if the URL contains datalad specific pieces like any URL of the
+        form "ria+<some protocol>...".
     """
     _params_ = dict(
         dataset=Parameter(
             args=("-d", "--dataset"),
             doc="""specify the dataset to query.  If
             no dataset is given, an attempt is made to identify the dataset
             based on the input and/or the current working directory""",
             constraints=EnsureDataset() | EnsureNone()),
-        fulfilled=Parameter(
-            args=("--fulfilled",),
-            doc="""if given, must be a boolean flag indicating whether
-            to report either only locally present or absent datasets.
-            By default subdatasets are reported regardless of their
-            status""",
-            constraints=EnsureBool() | EnsureNone()),
-        recursive=recursion_flag,
-        recursion_limit=recursion_limit,
-        contains=Parameter(
-            args=('--contains',),
+        path=Parameter(
+            args=("path",),
             metavar='PATH',
-            doc="""limit report to the subdatasets containing the
-            given path. If a root path of a subdataset is given the last
-            reported dataset will be the subdataset itself.""",
+            doc="""path/name to query for subdatasets. Defaults to the
+            current directory[PY: , or the entire dataset if called as
+            a dataset method PY].""",
+            nargs='*',
             constraints=EnsureStr() | EnsureNone()),
+        state=dataset_state,
+        fulfilled=fulfilled,
+        recursive=recursion_flag,
+        recursion_limit=recursion_limit,
+        contains=contains,
         bottomup=Parameter(
             args=("--bottomup",),
             action="store_true",
             doc="""whether to report subdatasets in bottom-up order along
             each branch in the dataset tree, and not top-down."""),
         set_property=Parameter(
             args=('--set-property',),
@@ -241,192 +197,256 @@
             metavar='NAME',
             action='append',
             doc="""Name of one or more subdataset properties to be removed
             from the parent dataset's .gitmodules file.[CMD:  This
             option can be given multiple times. CMD]""",
             constraints=EnsureStr() | EnsureNone()))
 
+    result_renderer = "tailored"
+
     @staticmethod
     @datasetmethod(name='subdatasets')
     @eval_results
     def __call__(
+            path=None,
+            *,
             dataset=None,
-            fulfilled=None,
+            state='any',
+            fulfilled=NoneDeprecated,
             recursive=False,
             recursion_limit=None,
             contains=None,
             bottomup=False,
             set_property=None,
             delete_property=None):
-        dataset = require_dataset(
-            dataset, check_installed=False, purpose='subdataset reporting/modification')
-        refds_path = dataset.path
-
-        # XXX this seems strange, but is tested to be the case -- I'd rather set
-        # `check_installed` to true above and fail
-        if not GitRepo.is_valid_repo(refds_path):
-            return
+        if fulfilled is not NoneDeprecated:
+            # the two mirror options do not agree and the deprecated one is
+            # not at default value
+            warnings.warn("subdatasets's `fulfilled` option is deprecated "
+                          "and will be removed in a future release, "
+                          "use the `state` option instead.",
+                          DeprecationWarning)
+            if state != 'any':
+                raise ValueError("Do not specify both 'fulfilled' and 'state', use 'state'")
+            # honor the old option for now
+            state = {
+                None: 'any',
+                True: 'present',
+                False: 'absent',
+            }[fulfilled]
+        # Path of minimal resistance/code-change - internally here we will reuse fulfilled
+        fulfilled = {
+            'any': None,
+            'present': True,
+            'absent': False,
+        }[state]
+        ds = require_dataset(
+            dataset, check_installed=True, purpose='report on subdataset(s)')
+
+        paths = resolve_path(ensure_list(path), dataset, ds) if path else None
+
+        # no constraints given -> query subdatasets under curdir
+        if not paths and dataset is None:
+            cwd = Path(getpwd())
+            paths = None if cwd == ds.pathobj else [cwd]
+
+        lgr.debug('Query subdatasets of %s', dataset)
+        if paths is not None:
+            lgr.debug('Query subdatasets underneath paths: %s', paths)
+        refds_path = ds.path
 
         # return as quickly as possible
         if isinstance(recursion_limit, int) and (recursion_limit <= 0):
             return
 
         if set_property:
             for k, v in set_property:
                 if valid_key.match(k) is None:
                     raise ValueError(
-                        "key '%s' is invalid (alphanumeric plus '-' only, must start with a letter)",
-                        k)
-        try:
-            if not (bottomup or contains or set_property or delete_property or \
-                    (recursive and recursion_limit is not None)):
-                # FAST IMPLEMENTATION FOR THE STRAIGHTFORWARD CASE
-                # as fast as possible (just a single call to Git)
-                # need to track current parent
-                stack = [refds_path]
-                modinfo_cache = {}
-                for sm in _parse_git_submodules(refds_path, recursive=recursive):
-                    # unwind the parent stack until we find the right one
-                    # this assumes that submodules come sorted
-                    while not path_startswith(sm['path'], stack[-1]):
-                        stack.pop()
-                    parent = stack[-1]
-                    if parent not in modinfo_cache:
-                        # read the parent .gitmodules, if not done yet
-                        modinfo_cache[parent] = _parse_gitmodules(parent)
-                    # get URL info, etc.
-                    sm.update(modinfo_cache[parent].get(sm['path'], {}))
-                    subdsres = get_status_dict(
-                        'subdataset',
-                        status='ok',
-                        type='dataset',
-                        refds=refds_path,
-                        logger=lgr)
-                    subdsres.update(sm)
-                    subdsres['parentds'] = parent
-                    if (fulfilled is None or
-                            GitRepo.is_valid_repo(sm['path']) == fulfilled):
-                        yield subdsres
-                    # for the next "parent" commit this subdataset to the stack
-                    stack.append(sm['path'])
-                # MUST RETURN: the rest of the function is doing another implementation
-                return
-        except InvalidGitRepositoryError as e:
-            lgr.debug("fast subdataset query failed, trying slow robust one (%s)",
-                      exc_str(e))
-
-        # MORE ROBUST, FLEXIBLE, BUT SLOWER IMPLEMENTATION
-        # slow but flexible (one Git call per dataset), but deals with subdatasets in
-        # direct mode
+                        "key '%s' is invalid (alphanumeric plus '-' only, must "
+                        "start with a letter)" % k)
         if contains:
-            contains = resolve_path(contains, dataset)
+            contains = resolve_path(ensure_list(contains), dataset, ds)
+            # expand all test cases for the contains test in the loop below
+            # leads to ~20% speedup per loop iteration of a non-match
+            expanded_contains = [[c] + list(c.parents) for c in contains]
+        else:
+            expanded_contains = []
+        contains_hits = set()
         for r in _get_submodules(
-                dataset.path, fulfilled, recursive, recursion_limit,
-                contains, bottomup, set_property, delete_property,
+                ds, paths, fulfilled, recursive, recursion_limit,
+                expanded_contains, bottomup, set_property, delete_property,
                 refds_path):
+            # a boat-load of ancient code consumes this and is ignorant of
+            # Path objects
+            r['path'] = str(r['path'])
             # without the refds_path cannot be rendered/converted relative
             # in the eval_results decorator
             r['refds'] = refds_path
+            if 'contains' in r:
+                contains_hits.update(r['contains'])
+                r['contains'] = [str(c) for c in r['contains']]
             yield r
+        if contains:
+            for c in set(contains).difference(contains_hits):
+                yield get_status_dict(
+                    'subdataset',
+                    path=str(c),
+                    status='impossible',
+                    message='path not contained in any matching subdataset',
+                    # we do not want to log such an event, because it is a
+                    # legit query to check for matching subdatasets simply
+                    # for the purpose of further decision making
+                    # user communication in front-end scenarios will happen
+                    # via result rendering
+                    #logger=lgr
+                )
+
+    @staticmethod
+    def custom_result_renderer(res, **kwargs):
+        generic_result_renderer(res)
 
 
 # internal helper that needs all switches, simply to avoid going through
 # the main command interface with all its decorators again
-def _get_submodules(dspath, fulfilled, recursive, recursion_limit,
+def _get_submodules(ds, paths, fulfilled, recursive, recursion_limit,
                     contains, bottomup, set_property, delete_property,
                     refds_path):
-    if not GitRepo.is_valid_repo(dspath):
-        return
-    modinfo = _parse_gitmodules(dspath)
-    # write access parser
-    parser = None
-    # TODO bring back in more global scope from below once segfaults are
-    # figured out
-    #if set_property or delete_property:
-    #    gitmodule_path = opj(dspath, ".gitmodules")
-    #    parser = GitConfigParser(
-    #        gitmodule_path, read_only=False, merge_includes=False)
-    #    parser.read()
+    lookup_cache = {}
+    # it should be OK to skip the extra check, because _parse_git_submodules()
+    # we specifically look for .gitmodules and the rest of the function
+    # is on its results
+    #if not GitRepo.is_valid_repo(dspath):
+    #    return
     # put in giant for-loop to be able to yield results before completion
-    for sm in _parse_git_submodules(dspath, recursive=False):
-        if contains and \
-                not (contains == sm['path'] or
-                     path_startswith(contains, sm['path'])):
-            # we are not looking for this subds, because it doesn't
-            # match the target path
-            continue
-        sm.update(modinfo.get(sm['path'], {}))
-        if set_property or delete_property:
-            gitmodule_path = opj(dspath, ".gitmodules")
-            parser = GitConfigParser(
-                gitmodule_path, read_only=False, merge_includes=False)
-            parser.read()
-            # do modifications now before we read the info out for reporting
-            # use 'submodule "NAME"' section ID style as this seems to be the default
-            submodule_section = 'submodule "{}"'.format(sm['gitmodule_name'])
+    for sm in _parse_git_submodules(ds, paths, lookup_cache):
+        repo = lookup_cache['repo']
+        sm_path = sm['path']
+        contains_hits = None
+        if contains:
+            contains_hits = [c[0] for c in contains if sm_path in c]
+            if not contains_hits:
+                # we are not looking for this subds, because it doesn't
+                # match the target path
+                continue
+        # the following used to be done by _parse_git_submodules()
+        # but is expensive and does not need to be done for submodules
+        # not matching `contains`
+        if not sm_path.exists() or not GitRepo.is_valid_repo(sm_path):
+            sm['state'] = 'absent'
+        else:
+            assert 'state' not in sm
+            sm['state'] = 'present'
+        # do we just need this to recurse into subdatasets, or is this a
+        # real results?
+        to_report = paths is None \
+            or any(p == sm_path or p in sm_path.parents
+                   for p in paths)
+        if to_report and (set_property or delete_property):
             # first deletions
-            for dprop in assure_list(delete_property):
-                parser.remove_option(submodule_section, dprop)
+            for dprop in ensure_list(delete_property):
+                try:
+                    repo.call_git(
+                        ['config', '--file', '.gitmodules',
+                         '--unset-all',
+                         'submodule.{}.{}'.format(sm['gitmodule_name'], dprop),
+                        ]
+                    )
+                except CommandError:
+                    yield get_status_dict(
+                        'subdataset',
+                        status='impossible',
+                        message=(
+                            "Deleting subdataset property '%s' failed for "
+                            "subdataset '%s', possibly did "
+                            "not exist",
+                            dprop, sm['gitmodule_name']),
+                        logger=lgr,
+                        **sm)
                 # also kick from the info we just read above
                 sm.pop('gitmodule_{}'.format(dprop), None)
             # and now setting values
-            for sprop in assure_list(set_property):
+            for sprop in ensure_list(set_property):
                 prop, val = sprop
                 if val.startswith('<') and val.endswith('>') and '{' in val:
                     # expand template string
                     val = val[1:-1].format(
                         **dict(
                             sm,
-                            refds_relpath=relpath(sm['path'], refds_path),
-                            refds_relname=relpath(sm['path'], refds_path).replace(os.sep, '-')))
-                parser.set_value(
-                    submodule_section,
-                    prop,
-                    val)
+                            refds_relpath=sm_path.relative_to(refds_path),
+                            refds_relname=str(
+                                sm_path.relative_to(refds_path)
+                            ).replace(os.sep, '-')))
+                try:
+                    repo.call_git(
+                        ['config', '--file', '.gitmodules',
+                         '--replace-all',
+                         'submodule.{}.{}'.format(sm['gitmodule_name'], prop),
+                         str(val),
+                        ]
+                    )
+                except CommandError as e:  # pragma: no cover
+                    # this conditional may not be possible to reach, as
+                    # variable name validity is checked before and Git
+                    # replaces the file completely, resolving any permission
+                    # issues, if the file could be read (already done above)
+                    ce = CapturedException(e)
+                    yield get_status_dict(
+                        'subdataset',
+                        status='error',
+                        message=("Failed to set property '%s': %s", prop, ce),
+                        exception=ce,
+                        type='dataset',
+                        logger=lgr,
+                        **sm)
+                    # it is up to parent code to decide whether we would continue
+                    # after this
+
                 # also add to the info we just read above
                 sm['gitmodule_{}'.format(prop)] = val
-            Dataset(dspath).add(
+            yield from ds.save(
                 '.gitmodules', to_git=True,
-                message='[DATALAD] modified subdataset properties')
-            # let go of resources, locks, ...
-            parser.release()
+                message='[DATALAD] modified subdataset properties',
+                result_renderer='disabled',
+                return_type='generator')
 
         #common = commonprefix((with_pathsep(subds), with_pathsep(path)))
         #if common.endswith(sep) and common == with_pathsep(subds):
         #    candidates.append(common)
         subdsres = get_status_dict(
             'subdataset',
             status='ok',
             type='dataset',
             logger=lgr)
         subdsres.update(sm)
-        subdsres['parentds'] = dspath
-        if not bottomup and \
+        subdsres['parentds'] = ds.path
+        if to_report:
+            if contains_hits:
+                subdsres['contains'] = contains_hits
+            if (not bottomup and \
                 (fulfilled is None or
-                 GitRepo.is_valid_repo(sm['path']) == fulfilled):
-            yield subdsres
+                 GitRepo.is_valid_repo(sm_path) == fulfilled)):
+                yield subdsres
 
         # expand list with child submodules. keep all paths relative to parent
         # and convert jointly at the end
         if recursive and \
                 (recursion_limit in (None, 'existing') or
                  (isinstance(recursion_limit, int) and
                   recursion_limit > 1)):
             for r in _get_submodules(
-                    sm['path'],
+                    Dataset(sm_path),
+                    paths,
                     fulfilled, recursive,
                     (recursion_limit - 1)
                     if isinstance(recursion_limit, int)
                     else recursion_limit,
                     contains,
                     bottomup,
                     set_property,
                     delete_property,
                     refds_path):
                 yield r
-        if bottomup and \
+        if to_report and (bottomup and \
                 (fulfilled is None or
-                 GitRepo.is_valid_repo(sm['path']) == fulfilled):
+                 GitRepo.is_valid_repo(sm_path) == fulfilled)):
             yield subdsres
-    if parser is not None:
-        # release parser lock manually, auto-cleanup is not reliable in PY3
-        parser.release()
```

### Comparing `datalad-0.9.3/datalad/distribution/siblings.py` & `datalad-1.0.0/datalad/distribution/siblings.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,67 +1,85 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 """Plumbing command for managing sibling configuration"""
 
 __docformat__ = 'restructuredtext'
 
 
 import logging
-
 import os
-from os.path import basename
-from os.path import relpath
-
-from six.moves.urllib.parse import urlparse
+import os.path as op
+from urllib.parse import urlparse
 
-from datalad.interface.base import Interface
-from datalad.interface.utils import eval_results
-from datalad.interface.base import build_doc
+import datalad.support.ansi_colors as ac
+from datalad.distribution.dataset import (
+    Dataset,
+    require_dataset,
+)
+from datalad.distribution.update import Update
+from datalad.downloaders.credentials import UserPassword
+from datalad.interface.base import (
+    Interface,
+    build_doc,
+    eval_results,
+)
+from datalad.interface.common_opts import (
+    annex_group_opt,
+    annex_groupwanted_opt,
+    annex_required_opt,
+    annex_wanted_opt,
+    as_common_datasrc,
+    inherit_opt,
+    location_description,
+    publish_by_default,
+    publish_depends,
+    recursion_flag,
+    recursion_limit,
+)
 from datalad.interface.results import get_status_dict
+from datalad.interface.utils import generic_result_renderer
 from datalad.support.annexrepo import AnnexRepo
-from datalad.support.constraints import EnsureStr
-from datalad.support.constraints import EnsureChoice
-from datalad.support.constraints import EnsureNone
-from datalad.support.constraints import EnsureBool
-from datalad.support.param import Parameter
-from datalad.support.exceptions import CommandError
-from datalad.support.exceptions import InsufficientArgumentsError
-from datalad.support.exceptions import AccessDeniedError
-from datalad.support.exceptions import AccessFailedError
-from datalad.support.exceptions import RemoteNotAvailableError
-from datalad.support.network import RI
-from datalad.support.network import URL
+from datalad.support.constraints import (
+    EnsureBool,
+    EnsureChoice,
+    EnsureNone,
+    EnsureStr,
+)
+from datalad.support.exceptions import (
+    AccessDeniedError,
+    AccessFailedError,
+    CapturedException,
+    CommandError,
+    DownloadError,
+    InsufficientArgumentsError,
+    RemoteNotAvailableError,
+)
 from datalad.support.gitrepo import GitRepo
-from datalad.interface.common_opts import recursion_flag
-from datalad.interface.common_opts import recursion_limit
-from datalad.interface.common_opts import as_common_datasrc
-from datalad.interface.common_opts import publish_depends
-from datalad.interface.common_opts import publish_by_default
-from datalad.interface.common_opts import annex_wanted_opt
-from datalad.interface.common_opts import annex_required_opt
-from datalad.interface.common_opts import annex_group_opt
-from datalad.interface.common_opts import annex_groupwanted_opt
-from datalad.interface.common_opts import inherit_opt
-from datalad.interface.common_opts import location_description
-from datalad.downloaders.credentials import UserPassword
-from datalad.distribution.dataset import require_dataset
-from datalad.distribution.dataset import Dataset
-from datalad.distribution.update import Update
-from datalad.utils import assure_list
-from datalad.utils import slash_join
-from datalad.dochelpers import exc_str
-
-from .dataset import EnsureDataset
-from .dataset import datasetmethod
+from datalad.support.network import (
+    RI,
+    URL,
+    PathRI,
+)
+from datalad.support.param import Parameter
+from datalad.utils import (
+    Path,
+    ensure_list,
+    slash_join,
+)
+
+from .dataset import (
+    EnsureDataset,
+    datasetmethod,
+)
 
 lgr = logging.getLogger('datalad.distribution.siblings')
 
 
 def _mangle_urls(url, ds_name):
     if not url:
         return url
@@ -118,25 +136,24 @@
             doc="""specify the dataset to configure.  If
             no dataset is given, an attempt is made to identify the dataset
             based on the input and/or the current working directory""",
             constraints=EnsureDataset() | EnsureNone()),
         name=Parameter(
             args=('-s', '--name',),
             metavar='NAME',
-            doc="""name of the sibling. For sibling removal this option is
-            mandatory, otherwise the hostname part of a given URL is used as a
-            default. This option can be used to limit 'query' to a specific
-            sibling.""",
+            doc="""name of the sibling. For addition with path "URLs" and
+            sibling removal this option is mandatory, otherwise the hostname
+            part of a given URL is used as a default. This option can be used
+            to limit 'query' to a specific sibling.""",
             constraints=EnsureStr() | EnsureNone()),
         action=Parameter(
             args=('action',),
             nargs='?',
-            metavar='ACTION',
             doc="""command action selection (see general documentation)""",
-            constraints=EnsureChoice('query', 'add', 'remove', 'configure', 'enable') | EnsureNone()),
+            constraints=EnsureChoice('query', 'add', 'remove', 'configure', 'enable')),
         url=Parameter(
             args=('--url',),
             doc="""the URL of or path to the dataset sibling named by
                 `name`. For recursive operation it is required that
                 a template string for building subdataset sibling URLs
                 is given.\n List of currently available placeholders:\n
                 %%NAME\tthe name of the dataset, where slashes are replaced by
@@ -155,15 +172,21 @@
         # --template/cfgfrom gh-1462 (maybe also for a one-time inherit)
         # --wanted gh-925 (also see below for add_sibling approach)
 
         fetch=Parameter(
             args=("--fetch",),
             action="store_true",
             doc="""fetch the sibling after configuration"""),
-        as_common_datasrc=as_common_datasrc,
+        as_common_datasrc=Parameter(
+            args=("--as-common-datasrc",),
+            metavar='NAME',
+            doc="""configure a sibling as a common data source of the
+            dataset that can be automatically used by all consumers of the
+            dataset. The sibling must be a regular Git remote with a
+            configured HTTP(S) URL."""),
         publish_depends=publish_depends,
         publish_by_default=publish_by_default,
         annex_wanted=annex_wanted_opt,
         annex_required=annex_required_opt,
         annex_group=annex_group_opt,
         annex_groupwanted=annex_groupwanted_opt,
         inherit=inherit_opt,
@@ -177,14 +200,15 @@
         recursion_limit=recursion_limit)
 
     @staticmethod
     @datasetmethod(name='siblings')
     @eval_results
     def __call__(
             action='query',
+            *,
             dataset=None,
             name=None,
             url=None,
             pushurl=None,
             description=None,
             # TODO consider true, for now like add_sibling
             fetch=False,
@@ -217,80 +241,103 @@
         }
         # all worker strictly operate on a single dataset
         # anything that deals with hierarchies and/or dataset
         # relationships in general should be dealt with in here
         # at the top-level and vice versa
         worker = action_worker_map[action]
 
-        dataset = require_dataset(
-            dataset, check_installed=False, purpose='sibling configuration')
-        refds_path = dataset.path
+        ds = require_dataset(
+            dataset,
+            # it makes no sense to use this command without a dataset
+            check_installed=True,
+            purpose='configure sibling')
+        refds_path = ds.path
 
         res_kwargs = dict(refds=refds_path, logger=lgr)
 
-        ds_name = basename(dataset.path)
+        ds_name = op.basename(ds.path)
 
         # do not form single list of datasets (with recursion results) to
         # give fastest possible response, for the precise of a long-all
         # function call
-        ds = dataset
-        for r in worker(
-                # always copy signature to below to avoid bugs!
-                ds, name,
-                ds.repo.get_remotes(),
-                # for top-level dataset there is no layout questions
-                _mangle_urls(url, ds_name),
-                _mangle_urls(pushurl, ds_name),
-                fetch, description,
-                as_common_datasrc, publish_depends, publish_by_default,
-                annex_wanted, annex_required, annex_group, annex_groupwanted,
-                inherit, get_annex_info,
-                **res_kwargs):
-            yield r
+
+        # minimize expensive calls to .repo
+        ds_repo = ds.repo
+
+        # prepare common parameterization package for all worker calls
+        worker_kwargs = dict(
+            name=name,
+            fetch=fetch,
+            description=description,
+            as_common_datasrc=as_common_datasrc,
+            publish_depends=publish_depends,
+            publish_by_default=publish_by_default,
+            annex_wanted=annex_wanted,
+            annex_required=annex_required,
+            annex_group=annex_group,
+            annex_groupwanted=annex_groupwanted,
+            inherit=inherit,
+            get_annex_info=get_annex_info,
+            res_kwargs=res_kwargs,
+        )
+        yield from worker(
+            ds=ds,
+            repo=ds_repo,
+            known_remotes=ds_repo.get_remotes(),
+            # for top-level dataset there is no layout questions
+            url=_mangle_urls(url, ds_name),
+            pushurl=_mangle_urls(pushurl, ds_name),
+            **worker_kwargs)
         if not recursive:
             return
 
         # do we have instructions to register siblings with some alternative
         # layout?
         replicate_local_structure = url and "%NAME" not in url
 
-        for subds in dataset.subdatasets(
-                fulfilled=True,
+        subds_pushurl = None
+        for subds in ds.subdatasets(
+                state='present',
                 recursive=recursive, recursion_limit=recursion_limit,
                 result_xfm='datasets'):
-            subds_name = relpath(subds.path, start=dataset.path)
+            subds_repo = subds.repo
+            subds_name = op.relpath(subds.path, start=ds.path)
             if replicate_local_structure:
                 subds_url = slash_join(url, subds_name)
-                subds_pushurl = slash_join(pushurl, subds_name)
+                if pushurl:
+                    subds_pushurl = slash_join(pushurl, subds_name)
             else:
                 subds_url = \
                     _mangle_urls(url, '/'.join([ds_name, subds_name]))
                 subds_pushurl = \
                     _mangle_urls(pushurl, '/'.join([ds_name, subds_name]))
-            for r in worker(
-                    # always copy signature from above to avoid bugs
-                    subds, name,
-                    subds.repo.get_remotes(),
-                    subds_url,
-                    subds_pushurl,
-                    fetch,
-                    description,
-                    as_common_datasrc, publish_depends, publish_by_default,
-                    annex_wanted, annex_required, annex_group, annex_groupwanted,
-                    inherit, get_annex_info,
-                    **res_kwargs):
-                yield r
+            yield from worker(
+                ds=subds,
+                repo=subds_repo,
+                known_remotes=subds_repo.get_remotes(),
+                url=subds_url,
+                pushurl=subds_pushurl,
+                **worker_kwargs)
 
     @staticmethod
     def custom_result_renderer(res, **kwargs):
         from datalad.ui import ui
+
+        # should we attempt to remove an unknown sibling, complain like Git does
+        if res['status'] == 'notneeded' and res['action'] == 'remove-sibling':
+            ui.message(
+                '{warn}: No sibling "{name}" in dataset {path}'.format(
+                    warn=ac.color_word('Warning', ac.LOG_LEVEL_COLORS['WARNING']),
+                    **res)
+            )
+            return
         if res['status'] != 'ok' or not res.get('action', '').endswith('-sibling') :
-            # logging complained about this already
+            generic_result_renderer(res)
             return
-        path = relpath(res['path'],
+        path = op.relpath(res['path'],
                        res['refds']) if res.get('refds', None) else res['path']
         got_url = 'url' in res
         spec = '{}{}{}{}'.format(
             res.get('url', ''),
             ' (' if got_url else '',
             res.get('annex-externaltype', 'git'),
             ')' if got_url else '')
@@ -301,39 +348,45 @@
                 # TODO report '+' for special remotes
                 with_annex='+' if 'annex-uuid' in res \
                     else ('-' if res.get('annex-ignore', None) else '?'),
                 spec=spec)))
 
 
 # always copy signature from above to avoid bugs
-def _add_remote(
-        ds, name, known_remotes, url, pushurl, fetch, description,
-        as_common_datasrc, publish_depends, publish_by_default,
-        annex_wanted, annex_required, annex_group, annex_groupwanted,
-        inherit, get_annex_info,
-        **res_kwargs):
+def _add_remote(ds, repo, name, known_remotes, url, pushurl, as_common_datasrc,
+                res_kwargs, **unused_kwargs):
     # TODO: allow for no url if 'inherit' and deduce from the super ds
     #       create-sibling already does it -- generalize/use
     #  Actually we could even inherit/deduce name from the super by checking
     #  which remote it is actively tracking in current branch... but may be
     #  would be too much magic
 
     # it seems that the only difference is that `add` should fail if a remote
     # already exists
     if (url is None and pushurl is None):
         raise InsufficientArgumentsError(
             """insufficient information to add a sibling
             (needs at least a dataset, and any URL).""")
+
+    # a pushurl should always be able to fill in for a not
+    # specified url, however, only when adding new remotes,
+    # not when configuring existing remotes (to avoid undesired
+    # overwriting of configurations), hence done here only
     if url is None:
         url = pushurl
 
     if not name:
         urlri = RI(url)
         # use the hostname as default remote name
-        name = urlri.hostname
+        try:
+            name = urlri.hostname
+        except AttributeError:
+            raise InsufficientArgumentsError(
+                "cannot derive a default remote name from '{}', "
+                "please specify a name.".format(url))
         lgr.debug(
             "No sibling name given, use URL hostname '%s' as sibling name",
             name)
 
     if not name:
         raise InsufficientArgumentsError("no sibling name given")
     if name in known_remotes:
@@ -342,37 +395,42 @@
             status='error',
             path=ds.path,
             type='sibling',
             name=name,
             message=("sibling is already known: %s, use `configure` instead?", name),
             **res_kwargs)
         return
+    # XXX this check better be done in configure too
+    # see https://github.com/datalad/datalad/issues/5914
+    if as_common_datasrc == name:
+        raise ValueError('Sibling name ({}) and common data source name ({}) '
+                         'can not be identical.'.format(name, as_common_datasrc))
+    if isinstance(RI(url), PathRI):
+        # make sure any path URL is stored in POSIX conventions for consistency
+        # with git's behavior (e.g. origin configured by clone)
+        url = Path(url).as_posix()
     # this remote is fresh: make it known
     # just minimalistic name and URL, the rest is coming from `configure`
-    ds.repo.add_remote(name, url)
+    repo.add_remote(name, url)
     known_remotes.append(name)
     # always copy signature from above to avoid bugs
     for r in _configure_remote(
-            ds, name, known_remotes, url, pushurl, fetch, description,
-            as_common_datasrc, publish_depends, publish_by_default,
-            annex_wanted, annex_required, annex_group, annex_groupwanted,
-            inherit, get_annex_info,
-            **res_kwargs):
+            ds=ds, repo=repo, name=name, known_remotes=known_remotes, url=url,
+            pushurl=pushurl, as_common_datasrc=as_common_datasrc,
+            res_kwargs=res_kwargs, **unused_kwargs):
         if r['action'] == 'configure-sibling':
             r['action'] = 'add-sibling'
         yield r
 
 
-# always copy signature from above to avoid bugs
 def _configure_remote(
-        ds, name, known_remotes, url, pushurl, fetch, description,
+        ds, repo, name, known_remotes, url, pushurl, fetch, description,
         as_common_datasrc, publish_depends, publish_by_default,
         annex_wanted, annex_required, annex_group, annex_groupwanted,
-        inherit, get_annex_info,
-        **res_kwargs):
+        inherit, res_kwargs, **unused_kwargs):
     result_props = dict(
         action='configure-sibling',
         path=ds.path,
         type='sibling',
         name=name,
         **res_kwargs)
     if name is None:
@@ -380,39 +438,39 @@
         result_props['message'] = 'need sibling `name` for configuration'
         yield result_props
         return
 
     if name != 'here':
         # do all configure steps that are not meaningful for the 'here' sibling
         # AKA the local repo
-        if name not in known_remotes:
+        if name not in known_remotes and url:
             # this remote is fresh: make it known
             # just minimalistic name and URL, the rest is coming from `configure`
-            ds.repo.add_remote(name, url)
+            repo.add_remote(name, url)
             known_remotes.append(name)
         elif url:
             # not new, override URl if given
-            ds.repo.set_remote_url(name, url)
+            repo.set_remote_url(name, url)
 
         # make sure we have a configured fetch expression at this point
         fetchvar = 'remote.{}.fetch'.format(name)
-        if fetchvar not in ds.repo.config:
+        if fetchvar not in repo.config:
             # place default fetch refspec in config
             # same as `git remote add` would have added
-            ds.repo.config.add(
+            repo.config.add(
                 fetchvar,
                 '+refs/heads/*:refs/remotes/{}/*'.format(name),
-                where='local')
+                scope='local')
 
         if pushurl:
-            ds.repo.set_remote_url(name, pushurl, push=True)
+            repo.set_remote_url(name, pushurl, push=True)
 
         if publish_depends:
             # Check if all `deps` remotes are known to the `repo`
-            unknown_deps = set(assure_list(publish_depends)).difference(
+            unknown_deps = set(ensure_list(publish_depends)).difference(
                 known_remotes)
             if unknown_deps:
                 result_props['status'] = 'error'
                 result_props['message'] = (
                     'unknown sibling(s) specified as publication dependency: %s',
                     unknown_deps)
                 yield result_props
@@ -422,46 +480,46 @@
         depvar = 'remote.{}.datalad-publish-depends'.format(name)
         # and default pushes
         dfltvar = "remote.{}.push".format(name)
 
         if fetch:
             # fetch the remote so we are up to date
             for r in Update.__call__(
-                    dataset=res_kwargs['refds'],
-                    path=[dict(path=ds.path, type='dataset')],
+                    dataset=ds.path,
                     sibling=name,
                     merge=False,
                     recursive=False,
                     on_failure='ignore',
                     return_type='generator',
                     result_xfm=None):
                 # fixup refds
                 r.update(res_kwargs)
                 yield r
 
-        if inherit:
+        delayed_super = _DelayedSuper(repo)
+        if inherit and delayed_super.super is not None:
             # Adjust variables which we should inherit
-            delayed_super = _DelayedSuper(ds.repo)
             publish_depends = _inherit_config_var(
                 delayed_super, depvar, publish_depends)
             publish_by_default = _inherit_config_var(
                 delayed_super, dfltvar, publish_by_default)
             # Copy relevant annex settings for the sibling
             # makes sense only if current AND super are annexes, so it is
             # kinda a boomer, since then forbids having a super a pure git
-            if isinstance(ds.repo, AnnexRepo) and \
-                    isinstance(delayed_super.repo, AnnexRepo):
+            if isinstance(repo, AnnexRepo) and \
+                    isinstance(delayed_super.repo, AnnexRepo) and \
+                    name in delayed_super.repo.get_remotes():
                 if annex_wanted is None:
                     annex_wanted = _inherit_annex_var(
                         delayed_super, name, 'wanted')
                 if annex_required is None:
                     annex_required = _inherit_annex_var(
                         delayed_super, name, 'required')
                 if annex_group is None:
-                    # I think it might be worth inheritting group regardless what
+                    # I think it might be worth inheriting group regardless what
                     # value is
                     #if annex_wanted in {'groupwanted', 'standard'}:
                     annex_group = _inherit_annex_var(
                         delayed_super, name, 'group'
                     )
                 if annex_wanted == 'groupwanted' and annex_groupwanted is None:
                     # we better have a value for the expression for that group
@@ -469,128 +527,140 @@
                         delayed_super, name, 'groupwanted'
                     )
 
         if publish_depends:
             if depvar in ds.config:
                 # config vars are incremental, so make sure we start from
                 # scratch
-                ds.config.unset(depvar, where='local', reload=False)
-            for d in assure_list(publish_depends):
+                ds.config.unset(depvar, scope='local', reload=False)
+            for d in ensure_list(publish_depends):
                 lgr.info(
                     'Configure additional publication dependency on "%s"',
                     d)
-                ds.config.add(depvar, d, where='local', reload=False)
+                ds.config.add(depvar, d, scope='local', reload=False)
             ds.config.reload()
 
         if publish_by_default:
             if dfltvar in ds.config:
-                ds.config.unset(dfltvar, where='local', reload=False)
-            for refspec in assure_list(publish_by_default):
+                ds.config.unset(dfltvar, scope='local', reload=False)
+            for refspec in ensure_list(publish_by_default):
                 lgr.info(
                     'Configure additional default publication refspec "%s"',
                     refspec)
                 ds.config.add(dfltvar, refspec, 'local')
             ds.config.reload()
 
-        assert isinstance(ds.repo, GitRepo)  # just against silly code
-        if isinstance(ds.repo, AnnexRepo):
+        assert isinstance(repo, GitRepo)  # just against silly code
+        if isinstance(repo, AnnexRepo):
             # we need to check if added sibling an annex, and try to enable it
             # another part of the fix for #463 and #432
             try:
+                exc = None
                 if not ds.config.obtain(
                         'remote.{}.annex-ignore'.format(name),
                         default=False,
                         valtype=EnsureBool(),
                         store=False):
-                    ds.repo.enable_remote(name)
-            except CommandError as exc:
+                    repo.enable_remote(name)
+            except (CommandError, DownloadError) as exc:
+                # Note: CommandError happens with git-annex
+                # 6.20180416+gitg86b18966f-1~ndall+1 (prior 6.20180510, from
+                # which starts to fail with AccessFailedError) if URL is bogus,
+                # so enableremote fails. E.g. as "tested" in test_siblings
                 # TODO yield
-                # this is unlikely to ever happen, now done for AnnexRepo instances
-                # only
-                lgr.info("Failed to enable annex remote %s, "
-                         "could be a pure git" % name)
-                lgr.debug("Exception was: %s" % exc_str(exc))
+                ce = CapturedException(exc)
+                repo.config.reload()
+                if repo.is_remote_annex_ignored(name):
+                    # Only inform user about the failure, if it's actually
+                    # consequential, because annex decided to set
+                    # annex-ignore=true.
+                    lgr.info("Could not annex-enable %s: %s", name, exc.stderr)
+
             if as_common_datasrc:
-                ri = RI(url)
+                # we need a fully configured remote here
+                # do not reuse `url`, but ask for the remote config
+                # that git-annex will use too
+                remote_url = repo.config.get(f'remote.{name}.url')
+                ri = RI(remote_url)
                 if isinstance(ri, URL) and ri.scheme in ('http', 'https'):
                     # XXX what if there is already a special remote
                     # of this name? Above check for remotes ignores special
                     # remotes. we need to `git annex dead REMOTE` on reconfigure
                     # before we can init a new one
                     # XXX except it is not enough
 
                     # make special remote of type=git (see #335)
-                    ds.repo._run_annex_command(
+                    repo.call_annex([
                         'initremote',
-                        annex_options=[
-                            as_common_datasrc,
-                            'type=git',
-                            'location={}'.format(url),
-                            'autoenable=true'])
+                        as_common_datasrc,
+                        'type=git',
+                        'location={}'.format(remote_url),
+                        'autoenable=true'])
                 else:
                     yield dict(
                         status='impossible',
-                        name=name,
                         message='cannot configure as a common data source, '
                                 'URL protocol is not http or https',
                         **result_props)
     #
     # place configure steps that also work for 'here' below
     #
-    if isinstance(ds.repo, AnnexRepo):
+    if isinstance(repo, AnnexRepo):
         for prop, var in (('wanted', annex_wanted),
                           ('required', annex_required),
                           ('group', annex_group)):
             if var is not None:
-                ds.repo.set_preferred_content(prop, var, '.' if name =='here' else name)
+                repo.set_preferred_content(prop, var, '.' if name =='here' else name)
         if annex_groupwanted:
-            ds.repo.set_groupwanted(annex_group, annex_groupwanted)
+            repo.set_groupwanted(annex_group, annex_groupwanted)
 
     if description:
-        if not isinstance(ds.repo, AnnexRepo):
+        if not isinstance(repo, AnnexRepo):
             result_props['status'] = 'impossible'
             result_props['message'] = 'cannot set description of a plain Git repository'
             yield result_props
             return
-        ds.repo._run_annex_command('describe', annex_options=[name, description])
+        repo.call_annex(['describe', name, description])
 
     # report all we know at once
-    info = list(_query_remotes(ds, name, known_remotes, get_annex_info=get_annex_info))[0]
+    info = list(_query_remotes(ds, repo, name, known_remotes, **unused_kwargs))[0]
     info.update(dict(status='ok', **result_props))
     yield info
 
 
-# always copy signature from above to avoid bugs
-def _query_remotes(
-        ds, name, known_remotes, url=None, pushurl=None, fetch=None, description=None,
-        as_common_datasrc=None, publish_depends=None, publish_by_default=None,
-        annex_wanted=None, annex_required=None, annex_group=None, annex_groupwanted=None,
-        inherit=None, get_annex_info=True,
-        **res_kwargs):
+def _query_remotes(ds, repo, name, known_remotes, get_annex_info=True,
+                   res_kwargs=None, **unused_kwargs):
+    res_kwargs = res_kwargs or {}
     annex_info = {}
     available_space = None
-    if get_annex_info and isinstance(ds.repo, AnnexRepo):
+    want_annex_info = get_annex_info and isinstance(repo, AnnexRepo)
+    if want_annex_info:
         # pull repo info from annex
         try:
             # need to do in safety net because of gh-1560
-            raw_info = ds.repo.repo_info(fast=True)
+            raw_info = repo.repo_info(fast=True)
         except CommandError:
             raw_info = {}
         available_space = raw_info.get('available local disk space', None)
         for trust in ('trusted', 'semitrusted', 'untrusted'):
             ri = raw_info.get('{} repositories'.format(trust), [])
             for r in ri:
                 uuid = r.get('uuid', '00000000-0000-0000-0000-00000000000')
                 if uuid.startswith('00000000-0000-0000-0000-00000000000'):
                     continue
                 ainfo = annex_info.get(uuid, {})
                 ainfo['description'] = r.get('description', None)
                 annex_info[uuid] = ainfo
     # treat the local repo as any other remote using 'here' as a label
     remotes = [name] if name else ['here'] + known_remotes
+    special_remote_info = None
+    if want_annex_info:
+        # query it once here, and inspect per-remote further down
+        special_remote_info = repo.get_special_remotes()
+
     for remote in remotes:
         info = get_status_dict(
             action='query-sibling',
             path=ds.path,
             type='sibling',
             name=remote,
             **res_kwargs)
@@ -617,109 +687,105 @@
                 info['available_local_disk_space'] = available_space
         else:
             # common case: actual remotes
             for remotecfg in [k for k in ds.config.keys()
                               if k.startswith('remote.{}.'.format(remote))]:
                 info[remotecfg[8 + len(remote):]] = ds.config[remotecfg]
         if get_annex_info and info.get('annex-uuid', None):
-            ainfo = annex_info.get(info['annex-uuid'])
+            ainfo = annex_info.get(info['annex-uuid'], {})
             annex_description = ainfo.get('description', None)
             if annex_description is not None:
                 info['annex-description'] = annex_description
-        if get_annex_info and isinstance(ds.repo, AnnexRepo):
-            if not ds.repo.is_remote_annex_ignored(remote):
+        if want_annex_info:
+            if not repo.is_remote_annex_ignored(remote):
                 try:
                     for prop in ('wanted', 'required', 'group'):
-                        var = ds.repo.get_preferred_content(
+                        var = repo.get_preferred_content(
                             prop, '.' if remote == 'here' else remote)
                         if var:
                             info['annex-{}'.format(prop)] = var
-                    groupwanted = ds.repo.get_groupwanted(remote)
+                    groupwanted = repo.get_groupwanted(remote)
                     if groupwanted:
                         info['annex-groupwanted'] = groupwanted
                 except CommandError as exc:
-                    if 'cannot determine uuid' in str(exc):
-                        # not an annex (or no connection), would be marked as
-                        #  annex-ignore
-                        msg = "Failed to determine if %s carries annex." % remote
-                        ds.repo.config.reload()
-                        if ds.repo.is_remote_annex_ignored(remote):
-                            msg += " Remote was marked by annex as annex-ignore.  " \
-                                   "Edit .git/config to reset if you think that was done by mistake due to absent connection etc"
-                        lgr.warning(msg)
-                        info['annex-ignore'] = True
+                    if 'cannot determine uuid' in exc.stderr:
+                        ce = CapturedException(exc)
+                        repo.config.reload()
+                        if repo.is_remote_annex_ignored(remote):
+                            lgr.warning(
+                                "%s was marked by git-annex as annex-ignore."
+                                "Edit .git/config to reset if you think that "
+                                "was done by mistake due to absent "
+                                "connection etc.",
+                                remote)
+                            info['annex-ignore'] = True
                     else:
                         raise
             else:
                 info['annex-ignore'] = True
 
+        if special_remote_info:
+            # pull out special remote info for this remote, if there is any
+            for k, v in special_remote_info.get(
+                    info.get('annex-uuid'), {}).items():
+                info[f'annex-{k}'] = v
+
         info['status'] = 'ok'
         yield info
 
 
-def _remove_remote(
-        ds, name, known_remotes, url, pushurl, fetch, description,
-        as_common_datasrc, publish_depends, publish_by_default,
-        annex_wanted, annex_required, annex_group, annex_groupwanted,
-        inherit, get_annex_info,
-        **res_kwargs):
+def _remove_remote(ds, repo, name, res_kwargs, **unused_kwargs):
     if not name:
         # TODO we could do ALL instead, but that sounds dangerous
         raise InsufficientArgumentsError("no sibling name given")
     result_props = dict(
         action='remove-sibling',
         path=ds.path,
         type='sibling',
         name=name,
         **res_kwargs)
     try:
         # failure can happen and is OK
-        ds.repo.remove_remote(name)
+        repo.remove_remote(name)
     except RemoteNotAvailableError as e:
         yield get_status_dict(
             # result-oriented! given remote is absent already
             status='notneeded',
             **result_props)
         return
 
     yield get_status_dict(
         status='ok',
         **result_props)
 
 
-# always copy signature from above to avoid bugs
-def _enable_remote(
-        ds, name, known_remotes, url, pushurl, fetch, description,
-        as_common_datasrc, publish_depends, publish_by_default,
-        annex_wanted, annex_required, annex_group, annex_groupwanted,
-        inherit, get_annex_info,
-        **res_kwargs):
+def _enable_remote(ds, repo, name, res_kwargs, **unused_kwargs):
     result_props = dict(
         action='enable-sibling',
         path=ds.path,
         type='sibling',
         name=name,
         **res_kwargs)
 
-    if not isinstance(ds.repo, AnnexRepo):
+    if not isinstance(repo, AnnexRepo):
         yield dict(
             result_props,
             status='impossible',
             message='cannot enable sibling of non-annex dataset')
         return
 
     if name is None:
         yield dict(
             result_props,
             status='error',
             message='require `name` of sibling to enable')
         return
 
     # get info on special remote
-    sp_remotes = {v['name']: dict(v, uuid=k) for k, v in ds.repo.get_special_remotes().items()}
+    sp_remotes = {v['name']: dict(v, uuid=k) for k, v in repo.get_special_remotes().items()}
     remote_info = sp_remotes.get(name, None)
 
     if remote_info is None:
         yield dict(
             result_props,
             status='impossible',
             message=("cannot enable sibling '%s', not known", name))
@@ -757,15 +823,15 @@
             # we need to pass a complete env because of #1776
             env = dict(
                 os.environ,
                 WEBDAV_USERNAME=creds['user'],
                 WEBDAV_PASSWORD=creds['password'])
 
     try:
-        ds.repo.enable_remote(name, env=env)
+        repo.enable_remote(name, env=env)
         result_props['status'] = 'ok'
     except AccessDeniedError as e:
         # credentials are wrong, wipe them out
         if cred and cred.is_known:
             cred.delete()
         result_props['status'] = 'error'
         result_props['message'] = str(e)
@@ -800,37 +866,40 @@
                 ds, var)
     return var
 
 
 class _DelayedSuper(object):
     """A helper to delay deduction on super dataset until needed
 
-    But if asked and not found -- blow up
+    But if asked and not found -- would return None for everything
     """
 
     def __init__(self, repo):
         self._child_dataset = Dataset(repo.path)
         self._super = None
+        self._super_tried = False
 
     def __str__(self):
         return str(self.super)
 
     @property
     def super(self):
-        if self._super is None:
+        if not self._super_tried:
+            self._super_tried = True
             # here we must analyze current_ds's super, not the super_ds
             self._super = self._child_dataset.get_superdataset()
             if not self._super:
-                raise RuntimeError(
+                lgr.warning(
                     "Cannot determine super dataset for %s, thus "
-                    "cannot inherit anything" % self._child_dataset
+                    "probably nothing would be inherited where desired"
+                    % self._child_dataset
                 )
         return self._super
 
     # Lean proxies going through .super
     @property
     def config(self):
-        return self.super.config
+        return self.super.config if self.super else None
 
     @property
     def repo(self):
-        return self.super.repo
+        return self.super.repo if self.super else None
```

### Comparing `datalad-0.9.3/datalad/distribution/remove.py` & `datalad-1.0.0/datalad/distribution/install.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,310 +1,429 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""High-level interface for removing dataset content
+"""High-level interface for dataset (component) installation
 
 """
 
-__docformat__ = 'restructuredtext'
-
-import os
 import logging
+from os import curdir
 
-from os.path import exists
-from os.path import relpath
-from os.path import pardir
-from os.path import join as opj
-from datalad.support.param import Parameter
-from datalad.support.constraints import EnsureStr, EnsureNone
-from datalad.support.exceptions import InsufficientArgumentsError
-from datalad.support.gitrepo import GitRepo
-from datalad.distribution.dataset import Dataset, \
-    datasetmethod
-from datalad.interface.annotate_paths import AnnotatePaths
-from datalad.interface.annotate_paths import annotated2content_by_ds
 from datalad.interface.base import Interface
-from datalad.interface.common_opts import if_dirty_opt
-from datalad.interface.common_opts import recursion_flag
-from datalad.interface.common_opts import nosave_opt
-from datalad.interface.common_opts import save_message_opt
-from datalad.interface.utils import path_is_under
-from datalad.interface.utils import eval_results
-from datalad.interface.base import build_doc
-from datalad.interface.results import get_status_dict
-from datalad.interface.save import Save
-from datalad.distribution.drop import _drop_files
-from datalad.distribution.drop import dataset_argument
-from datalad.distribution.drop import check_argument
-from datalad.distribution.uninstall import _uninstall_dataset
-from datalad.distribution.uninstall import Uninstall
+from datalad.interface.common_opts import (
+    recursion_flag,
+    recursion_limit,
+    location_description,
+    jobs_opt,
+    reckless_opt,
+)
+from datalad.interface.results import (
+    get_status_dict,
+    YieldDatasets,
+    is_result_matching_pathsource_argument,
+)
+from datalad.interface.base import (
+    build_doc,
+    eval_results,
+)
+from datalad.support.constraints import (
+    EnsureNone,
+    EnsureStr,
+)
+from datalad.support.exceptions import (
+    CapturedException,
+    InsufficientArgumentsError,
+)
+from datalad.support.param import Parameter
+from datalad.support.network import (
+    RI,
+    PathRI,
+)
+from datalad.utils import ensure_list
+
+from datalad.distribution.dataset import (
+    datasetmethod,
+    resolve_path,
+    require_dataset,
+    EnsureDataset,
+)
+from datalad.distribution.get import Get
+from datalad.core.distributed.clone import Clone
 
+__docformat__ = 'restructuredtext'
 
-lgr = logging.getLogger('datalad.distribution.remove')
+lgr = logging.getLogger('datalad.distribution.install')
 
 
 @build_doc
-class Remove(Interface):
-    """Remove components from datasets
-
-    This command can remove any components (subdatasets, and (directories with)
-    files) from datasets. Removing a component implies any present content to
-    be dropped, and any associated subdatasets to be uninstalled. Subsequently,
-    the component is "unregistered" from the respective dataset. This means
-    that the respective component is no longer present on the file system.
-
-    By default, the availability of at least one remote copy is verified, by
-    default, before file content is dropped. As these checks could lead to slow
-    operation (network latencies, etc), they can be disabled.
-
-    Any number of paths to process can be given as input. Recursion into
-    subdatasets needs to be explicitly enabled, while recursion in
-    subdirectories within a dataset as always done automatically. An optional
-    recursion limit is applied relative to each given input path.
-
-    Examples:
-
-      Permanently remove a subdataset from a dataset and wipe out the subdataset
-      association too::
+class Install(Interface):
+    """Install one or many datasets from remote URL(s) or local PATH source(s).
 
-        ~/some/dataset$ datalad remove somesubdataset1
+    This command creates local :term:`sibling`\\(s) of existing dataset(s) from
+    (remote) locations specified as URL(s) or path(s). Optional recursion into
+    potential subdatasets, and download of all referenced data is supported.
+    The new dataset(s) can be optionally registered in an existing
+    :term:`superdataset` by identifying it via the `dataset` argument (the new
+    dataset's path needs to be located within the superdataset for that).
+
+    || REFLOW >>
+    If no explicit [CMD: -s|--source CMD][PY: `source` PY] option is specified,
+    then all positional URL-OR-PATH
+    arguments are considered to be "sources" if they are URLs or target locations
+    if they are paths.
+    If a target location path corresponds to a submodule, the source location for it
+    is figured out from its record in the `.gitmodules`.
+    If [CMD: -s|--source CMD][PY: `source` PY] is specified, then a single optional
+    positional PATH would be taken as the destination path for that dataset.
+    << REFLOW ||
+
+    It is possible to provide a brief description to label the dataset's
+    nature *and* location, e.g. "Michael's music on black laptop". This helps
+    humans to identify data locations in distributed scenarios.  By default an
+    identifier comprised of user and machine name, plus path will be generated.
+
+    When only partial dataset content shall be obtained, it is recommended to
+    use this command without the `get-data` flag, followed by a
+    :func:`~datalad.api.get` operation to obtain the desired data.
+
+    .. note::
+      Power-user info: This command uses :command:`git clone`, and
+      :command:`git annex init` to prepare the dataset. Registering to a
+      superdataset is performed via a :command:`git submodule add` operation
+      in the discovered superdataset.
     """
-    _action = 'remove'
+
+    # very frequently this command will yield exactly one installed dataset
+    # spare people the pain of going through a list by default
+    return_type = 'item-or-list'
+    # as discussed in #1409 and #1470, we want to return dataset instances
+    # matching what is actually available after command completion (and
+    # None for any failed dataset installation)
+    # TODO actually need success(containing)dataset-or-none
+    result_xfm = 'successdatasets-or-none'
+    # we also want to limit the returned result to explicit input arguments
+    # (paths/source) and not report any implicit action, like intermediate
+    # datasets
+    result_filter = is_result_matching_pathsource_argument
+
+    _examples_ = [
+        dict(text="Install a dataset from GitHub into the current directory",
+             code_py="install("
+             "source='https://github.com/datalad-datasets/longnow"
+             "-podcasts.git')",
+             code_cmd="datalad install "
+             "https://github.com/datalad-datasets/longnow-podcasts.git"),
+        dict(text="Install a dataset as a subdataset into the current dataset",
+             code_py="""\
+             install(dataset='.',
+                     source='https://github.com/datalad-datasets/longnow-podcasts.git')""",
+             code_cmd="""\
+             datalad install -d . \\
+             --source='https://github.com/datalad-datasets/longnow-podcasts.git'"""),
+        dict(text="Install a dataset into 'podcasts' (not 'longnow-podcasts') directory,"
+                  " and get all content right away",
+             code_py="""\
+             install(path='podcasts',
+                     source='https://github.com/datalad-datasets/longnow-podcasts.git',
+                     get_data=True)""",
+             code_cmd="""\
+             datalad install --get-data \\
+             -s https://github.com/datalad-datasets/longnow-podcasts.git podcasts"""),
+        dict(text="Install a dataset with all its subdatasets",
+             code_py="""\
+             install(source='https://github.com/datalad-datasets/longnow-podcasts.git',
+                     recursive=True)""",
+             code_cmd="""\
+             datalad install -r \\
+             https://github.com/datalad-datasets/longnow-podcasts.git"""),
+    ]
 
     _params_ = dict(
-        dataset=dataset_argument,
+        dataset=Parameter(
+            args=("-d", "--dataset"),
+            # TODO: this probably changes to install into the dataset (add_to_super)
+            # and to install the thing 'just there' without operating 'on' a dataset.
+            # Adapt doc.
+            # MIH: `shouldn't this be the job of `add`?
+            doc="""specify the dataset to perform the install operation on.  If
+            no dataset is given, an attempt is made to identify the dataset
+            in a parent directory of the current working directory and/or the
+            `path` given""",
+            constraints=EnsureDataset() | EnsureNone()),
         path=Parameter(
             args=("path",),
-            metavar="PATH",
-            doc="path/name of the component to be removed",
+            metavar='URL-OR-PATH',
             nargs="*",
+            # doc: TODO
+            doc="""path/name of the installation target.  If no `path` is
+            provided a destination path will be derived from a source URL
+            similar to :command:`git clone`"""),
+        source=Parameter(
+            args=("-s", "--source"),
+            metavar='URL-OR-PATH',
+            doc="URL or local path of the installation source",
+            constraints=EnsureStr() | EnsureNone()),
+        branch=Parameter(
+            args=("--branch",),
+            doc="""Clone source at this branch or tag. This option applies only
+            to the top-level dataset not any subdatasets that may be cloned
+            when installing recursively. Note that if the source is a RIA URL
+            with a version, it takes precedence over this option.""",
             constraints=EnsureStr() | EnsureNone()),
+        get_data=Parameter(
+            args=("-g", "--get-data",),
+            doc="""if given, obtain all data content too""",
+            action="store_true"),
+        description=location_description,
         recursive=recursion_flag,
-        check=check_argument,
-        save=nosave_opt,
-        message=save_message_opt,
-        if_dirty=if_dirty_opt,
+        recursion_limit=recursion_limit,
+        reckless=reckless_opt,
+        jobs=jobs_opt,
     )
 
     @staticmethod
-    @datasetmethod(name=_action)
+    @datasetmethod(name='install')
     @eval_results
     def __call__(
             path=None,
+            *,
+            source=None,
             dataset=None,
+            get_data=False,
+            description=None,
             recursive=False,
-            check=True,
-            save=True,
-            message=None,
-            if_dirty='save-before'):
-        res_kwargs = dict(action='remove', logger=lgr)
-        if not dataset and not path:
+            recursion_limit=None,
+            reckless=None,
+            jobs="auto",
+            branch=None):
+
+        # normalize path argument to be equal when called from cmdline and
+        # python and nothing was passed into `path`
+        path = ensure_list(path)
+
+        if not source and not path:
             raise InsufficientArgumentsError(
-                "insufficient information for `remove`: requires at least a path or dataset")
-        refds_path = Interface.get_refds_path(dataset)
-        res_kwargs['refds'] = refds_path
-        if refds_path and not path and not GitRepo.is_valid_repo(refds_path):
-            # nothing here, nothing to remove
-            yield get_status_dict(path=refds_path, status='notneeded', **res_kwargs)
-            return
-        if refds_path and not path:
-            # act on the whole dataset if nothing else was specified
-            # TODO i think that would happen automatically in annotation?
-            path = refds_path
-
-        to_process = []
-
-        for ap in AnnotatePaths.__call__(
-                path=path,
-                dataset=refds_path,
-                recursive=recursive,
-                # we only ever want to discover immediate subdatasets, the rest
-                # will happen in `uninstall`
-                recursion_limit=1,
-                action='remove',
-                unavailable_path_status='',
-                nondataset_path_status='error',
-                return_type='generator',
-                on_failure='ignore'):
-            if ap.get('status', None):
-                # this is done
-                yield ap
-                continue
-            if ap.get('state', None) == 'absent' and \
-                    ap.get('parentds', None) is None:
-                # nothing exists at location, and there is no parent to
-                # remove from
-                ap['status'] = 'notneeded'
-                ap['message'] = "path does not exist and is not in a dataset"
-                yield ap
-                continue
-            if ap.get('raw_input', False) and ap.get('type', None) == 'dataset':
-                # make sure dataset sorting yields a dedicted entry for this one
-                ap['process_content'] = True
-            to_process.append(ap)
+                "Please provide at least a source or a path")
 
-        if not to_process:
-            # nothing left to do, potentially all errored before
+        #  Common kwargs to pass to underlying git/install calls.
+        #  They might need adjustments (e.g. for recursion_limit, but
+        #  otherwise would be applicable throughout
+        #
+        # There should have been more of common options!
+        # since underneath get could do similar installs
+        common_kwargs = dict(
+            get_data=get_data,
+            recursive=recursive,
+            recursion_limit=recursion_limit,
+            # git_opts=git_opts,
+            # annex_opts=annex_opts,
+            reckless=reckless,
+            jobs=jobs,
+        )
+
+        # did we explicitly get a dataset to install into?
+        # if we got a dataset, path will be resolved against it.
+        # Otherwise path will be resolved first.
+        ds = None
+        if dataset is not None:
+            ds = require_dataset(dataset, check_installed=True,
+                                 purpose='install')
+            common_kwargs['dataset'] = dataset
+        # pre-compute for results below
+        refds_path = ds if ds is None else ds.path
+
+        # switch into the two scenarios without --source:
+        # 1. list of URLs
+        # 2. list of (sub)dataset content
+        if source is None:
+            # we need to collect URLs and paths
+            to_install = []
+            to_get = []
+            # TODO: this approach is problematic, it disrupts the order of input args.
+            # consequently results will be returned in an unexpected order when a
+            # mixture of source URL and paths is given. Reordering is only possible when
+            # everything in here is fully processed before any results can be yielded.
+            # moreover, I think the semantics of the status quo implementation are a
+            # bit complicated: in a mixture list a source URL will lead to a new dataset
+            # at a generated default location, but a path will lead to a subdataset
+            # at that exact location
+            for urlpath in path:
+                ri = RI(urlpath)
+                (to_get if isinstance(ri, PathRI) else to_install).append(urlpath)
+
+            # 1. multiple source URLs
+            for s in to_install:
+                lgr.debug("Install passes into install source=%s", s)
+                for r in Install.__call__(
+                        source=s,
+                        description=description,
+                        # we need to disable error handling in order to have it done at
+                        # the very top, otherwise we are not able to order a global
+                        # "ignore-and-keep-going"
+                        on_failure='ignore',
+                        return_type='generator',
+                        result_renderer='disabled',
+                        result_xfm=None,
+                        result_filter=None,
+                        branch=branch,
+                        **common_kwargs):
+                    # no post-processing of the installed content on disk
+                    # should be necessary here, all done by code further
+                    # down that deals with an install from an actual `source`
+                    # any necessary fixes should go there too!
+                    r['refds'] = refds_path
+                    yield r
+
+            # 2. one or more dataset content paths
+            if to_get:
+                lgr.debug("Install passes into get %d items", len(to_get))
+                # all commented out hint on inability to pass those options
+                # into underlying install-related calls.
+                # Also need to pass from get:
+                #  annex_get_opts
+
+                for r in Get.__call__(
+                        to_get,
+                        # TODO should pass-through description, not sure why disabled
+                        # description=description,
+                        # we need to disable error handling in order to have it done at
+                        # the very top, otherwise we are not able to order a global
+                        # "ignore-and-keep-going"
+                        on_failure='ignore',
+                        return_type='generator',
+                        result_xfm=None,
+                        result_renderer='disabled',
+                        result_filter=None,
+                        **common_kwargs):
+                    # no post-processing of get'ed content on disk should be
+                    # necessary here, this is the responsibility of `get`
+                    # (incl. adjusting parent's gitmodules when submodules end
+                    # up in an "updated" state (done in get helpers)
+                    # any required fixes should go there!
+                    r['refds'] = refds_path
+                    yield r
+
+            # we are done here
+            # the rest is about install from a `source`
             return
 
-        if path_is_under([ap['path'] for ap in to_process]):
-            # behave like `rm` and refuse to remove where we are
+        # an actual `source` was given
+        if source and path and len(path) > 1:
+            # exception is ok here, if this fails it is either direct user error
+            # or we fucked up one of our internal calls
             raise ValueError(
-                "refusing to uninstall current or parent directory")
+                "install needs a single PATH when source is provided.  "
+                "Was given multiple PATHs: %s" % str(path))
+
+        # parameter constraints:
+        if not source:
+            # exception is ok here, if this fails it is either direct user error
+            # or we fucked up one of our internal calls
+            raise InsufficientArgumentsError(
+                "a `source` is required for installation")
 
-        # now sort into datasets so we can process them one by one
-        content_by_ds, ds_props, completed, nondataset_paths = \
-            annotated2content_by_ds(
-                to_process,
-                refds_path=refds_path)
-        assert(not completed)
-
-        # iterate over all datasets, starting at the bottom
-        # to make the removal of dataset content known upstairs
-        to_save = []
-        # track which submodules we have removed in the process, to avoid
-        # failure in case we revisit them due to a subsequent path argument
-        subm_removed = []
-        for ds_path in sorted(content_by_ds, reverse=True):
-            ds = Dataset(ds_path)
-            paths = content_by_ds[ds_path]
-            to_reporemove = dict()
-            # PLAN any dataset that was not raw_input, uninstall (passing recursive flag)
-            # if dataset itself is in paths, skip any nondataset
-            # sort reverse so we get subdatasets first
-            for ap in sorted(paths, key=lambda x: x['path'], reverse=True):
-                if ap.get('type', None) == 'dataset':
-                    # entire dataset needs to go, uninstall if present, pass recursive!
-                    uninstall_failed = False
-                    if ap['path'] == refds_path or \
-                            (refds_path is None and ap.get('raw_input', False)):
-                        # top-level handling, cannot use regular uninstall call, as
-                        # it will refuse to uninstall a top-level dataset
-                        # and rightfully so, it is really a remove in that case
-                        # bypass all the safety by using low-level helper
-                        for r in _uninstall_dataset(ds, check=check, has_super=False,
-                                                    **res_kwargs):
-                            if r['status'] in ('impossible', 'error'):
-                                # we need to inspect if something went wrong, in order
-                                # to prevent failure from removing a non-empty dir below,
-                                # but at the same time allow for continued processing
-                                uninstall_failed = True
-                            r['refds'] = refds_path
-                            yield r
-                    # recheck that it wasn't removed during a previous iteration
-                    elif ap.get('state', None) != 'absent' and GitRepo.is_valid_repo(ap['path']):
-                        # anything that is not the top-level -> regular uninstall
-                        # this is for subdatasets of the to-be-removed dataset
-                        # we want to simply uninstall them in a regular manner
-                        for r in Uninstall.__call__(
-                                # use annotate path as input, but pass a copy because
-                                # we cannot rely on it being unaltered by reannotation
-                                # TODO maybe adjust annotate_path to do that
-                                [ap.copy()],
-                                dataset=refds_path, recursive=recursive, check=check,
-                                if_dirty=if_dirty, result_xfm=None, result_filter=None,
-                                on_failure='ignore'):
-                            if r['status'] in ('impossible', 'error'):
-                                # we need to inspect if something went wrong, in order
-                                # to prevent failure from removing a non-empty dir below,
-                                # but at the same time allow for continued processing
-                                uninstall_failed = True
-                            yield r
-                    if not ap.get('raw_input', False):
-                        # we only ever want to actually unregister subdatasets that
-                        # were given explicitly
-                        continue
-                    if not uninstall_failed and \
-                            not ap['path'] in subm_removed and \
-                            refds_path and \
-                            ap.get('parentds', None) and \
-                            not (relpath(ap['path'], start=refds_path).startswith(pardir) or
-                                 ap['path'] == refds_path) and \
-                            ap.get('registered_subds', False):
-                        # strip from superdataset, but only if a dataset was given explcitly
-                        # as in "remove from this dataset", but not when just a path was given
-                        # as in "remove from the filesystem"
-                        subds_relpath = relpath(ap['path'], start=ap['parentds'])
-                        # remove submodule reference
-                        parentds = Dataset(ap['parentds'])
-                        # play safe, will fail on dirty
-                        parentds.repo.deinit_submodule(ap['path'])
-                        # remove now empty submodule link
-                        parentds.repo.remove(ap['path'])
-                        # make a record that we removed this already, should it be
-                        # revisited via another path argument, because do not reannotate
-                        # the paths after every removal
-                        subm_removed.append(ap['path'])
-                        yield dict(ap, status='ok', **res_kwargs)
-                        # need .gitmodules update in parent
-                        to_save.append(dict(
-                            path=opj(parentds.path, '.gitmodules'),
-                            parents=parentds.path,
-                            type='file'))
-                        # and the removal itself needs to be committed
-                        # inform `save` that it is OK that this path
-                        # doesn't exist on the filesystem anymore
-                        ap['unavailable_path_status'] = ''
-                        ap['process_content'] = False
-                        to_save.append(ap)
-                    if not uninstall_failed and exists(ap['path']):
-                        # could be an empty dir in case an already uninstalled subdataset
-                        # got removed
-                        os.rmdir(ap['path'])
-                else:
-                    # anything that is not a dataset can simply be passed on
-                    to_reporemove[ap['path']] = ap
-            # avoid unnecessary git calls when there is nothing to do
-            if to_reporemove:
-                if check and hasattr(ds.repo, 'drop'):
-                    for r in _drop_files(ds, [p for p in to_reporemove],
-                                         check=True):
-                        if r['status'] == 'error':
-                            # if drop errored on that path, we can't remove it
-                            to_reporemove.pop(r['path'], 'avoidKeyError')
-                        yield r
-
-                if to_reporemove:
-                    for r in ds.repo.remove([p for p in to_reporemove], r=True):
-                        # these were removed, but we still need to save the
-                        # removal
-
-                        r_abs = opj(ds.path, r)
-                        if r_abs in to_reporemove:
-                            ap = to_reporemove[r_abs]
-                        else:
-                            ap = {'path': r_abs,
-                                  'parentds': ds.path,
-                                  'refds': refds_path
-                                  }
-                        ap['unavailable_path_status'] = ''
-                        to_save.append(ap)
-                        yield get_status_dict(
-                            status='ok',
-                            path=r,
-                            **res_kwargs)
+        # code below deals with a single path only
+        path = path[0] if path else None
 
-        if not to_save:
-            # nothing left to do, potentially all errored before
-            return
-        if not save:
-            lgr.debug('Not calling `save` as instructed')
+        if source == path:
+            # even if they turn out to be identical after resolving symlinks
+            # and more sophisticated witchcraft, it would still happily say
+            # "it appears to be already installed", so we just catch an
+            # obviously pointless input combination
+            yield get_status_dict(
+                'install', path=path, status='impossible', logger=lgr,
+                source_url=source, refds=refds_path,
+                message="installation `source` and destination `path` are identical. "
+                "If you are trying to add a subdataset simply use the `save` command")
             return
 
-        for res in Save.__call__(
-                # TODO compose hand-selected annotated paths
-                path=to_save,
-                # we might have removed the reference dataset by now, recheck
-                dataset=refds_path
-                        if (refds_path and GitRepo.is_valid_repo(refds_path))
-                        else None,
-                message=message if message else '[DATALAD] removed content',
-                return_type='generator',
-                result_xfm=None,
-                result_filter=None,
-                on_failure='ignore'):
-            yield res
+        # resolve the target location (if local) against the provided dataset
+        # or CWD:
+        if path is not None:
+            # MIH everything in here is highly similar to what common
+            # interface helpers do (or should/could do), but at the same
+            # is very much tailored to just apply to `install` -- I guess
+            # it has to stay special
+
+            # Should work out just fine for regular paths, so no additional
+            # conditioning is necessary
+            try:
+                path_ri = RI(path)
+            except Exception as e:
+                ce = CapturedException(e)
+                raise ValueError(
+                    "invalid path argument {}: ({})".format(path, ce))
+            try:
+                # Wouldn't work for SSHRI ATM, see TODO within SSHRI
+                # yoh: path should be a local path, and mapping note within
+                #      SSHRI about mapping localhost:path to path is kinda
+                #      a peculiar use-case IMHO
+                # TODO Stringification can be removed once PY35 is no longer
+                # supported
+                path = str(resolve_path(path_ri.localpath, dataset))
+                # any `path` argument that point to something local now
+                # resolved and is no longer a URL
+            except ValueError:
+                # `path` is neither a valid source nor a local path.
+                # TODO: The only thing left is a known subdataset with a
+                # name, that is not a path; Once we correctly distinguish
+                # between path and name of a submodule, we need to consider
+                # this.
+                # For now: Just raise
+                raise ValueError("Invalid path argument {0}".format(path))
+        # `path` resolved, if there was any.
+
+        # clone dataset, will also take care of adding to superdataset, if one
+        # is given
+        res = Clone.__call__(
+            source, path, dataset=ds, description=description,
+            reckless=reckless,
+            git_clone_opts=["--branch=" + branch] if branch else None,
+            # we need to disable error handling in order to have it done at
+            # the very top, otherwise we are not able to order a global
+            # "ignore-and-keep-going"
+            result_xfm=None,
+            return_type='generator',
+            result_renderer='disabled',
+            result_filter=None,
+            on_failure='ignore')
+        # helper
+        as_ds = YieldDatasets()
+        destination_dataset = None
+        for r in res:
+            if r['action'] == 'install' and r['type'] == 'dataset':
+                # make sure logic below is valid, only one dataset result is
+                # coming back
+                assert(destination_dataset is None)
+                destination_dataset = as_ds(r)
+            r['refds'] = refds_path
+            yield r
+        assert(destination_dataset)
+
+        # Now, recursive calls:
+        if recursive or get_data:
+            # dataset argument must not be passed inside since we use bound .get
+            # It is ok to do "inplace" as long as we still return right
+            # after the loop ends
+            common_kwargs.pop('dataset', '')
+            for r in destination_dataset.get(
+                    curdir,
+                    description=description,
+                    # we need to disable error handling in order to have it done at
+                    # the very top, otherwise we are not able to order a global
+                    # "ignore-and-keep-going"
+                    on_failure='ignore',
+                    return_type='generator',
+                    result_xfm=None,
+                    result_renderer='disabled',
+                    **common_kwargs):
+                r['refds'] = refds_path
+                yield r
+        # at this point no further post-processing should be necessary,
+        # `clone` and `get` must have done that (incl. parent handling)
+        # if not, bugs should be fixed in those commands
+        return
```

### Comparing `datalad-0.9.3/datalad/distribution/create_test_dataset.py` & `datalad-1.0.0/datalad/distribution/create_test_dataset.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 """A Helper to initiate arbitrarily small/large test meta-dataset
@@ -67,40 +67,41 @@
     ----------
     path : str
       Path to the top directory under which dataset will be created.
       If relative -- relative to current directory
     levels : list of list
       List of specifications for :func:`random.randint` call per each level.
     ds : Dataset, optional
-      Super-dataset which would contain a new dataset (thus its path whould be
+      Super-dataset which would contain a new dataset (thus its path would be
       a parent of path. Note that ds needs to be installed.
     max_leading_dirs : int, optional
-      Up to how many leading directories withing a dataset could lead to a
+      Up to how many leading directories within a dataset could lead to a
       sub-dataset
 
     Yields
     ------
     str
        Path to the generated dataset(s)
 
     """
     # we apparently can't import api functionality within api
-    from datalad.api import add
+    from datalad.api import save
     # To simplify managing all the file paths etc
     if not isabs(path):
         path = abspath(path)
     # make it a git (or annex??) repository... ok - let's do randomly one or another ;)
     RepoClass = GitRepo if random.randint(0, 1) else AnnexRepo
     lgr.info("Generating repo of class %s under %s", RepoClass, path)
     repo = RepoClass(path, create=True)
     # let's create some dummy file and add it to the beast
     fn = opj(path, "file%d.dat" % random.randint(1, 1000))
     with open(fn, 'w') as f:
         f.write(fn)
-    repo.add(fn, git=True, commit=True, msg="Added %s" % fn, _datalad_msg=True)
+    repo.add(fn, git=True)
+    repo.commit(msg="Added %s" % fn)
 
     yield path
 
     if levels:
         # make a dataset for that one since we want to add sub datasets
         ds_ = Dataset(path)
         # Process the levels
@@ -114,26 +115,24 @@
             subds_fpath = opj(path, subds_path)
             # yield all under
             for d in _makeds(subds_fpath, levels_, ds=ds_):
                 yield d
 
     if ds:
         assert ds.is_installed()
-        out = add(
+        out = save(
             path,
             dataset=ds,
         )
 
 
 @build_doc
 class CreateTestDataset(Interface):
     """Create test (meta-)dataset.
     """
-    # XXX prevent common args from being added to the docstring
-    _no_eval_results = True
 
     _params_ = dict(
         path=Parameter(
             args=("path",),
             doc="path/name where to create (if specified, must not exist)",
             constraints=EnsureStr() | EnsureNone()),
         spec=Parameter(
@@ -150,15 +149,15 @@
             args=("--seed",),
             doc="""seed for rng""",
             constraints=EnsureInt() | EnsureNone()),
 
     )
 
     @staticmethod
-    def __call__(path=None, spec=None, seed=None):
+    def __call__(path=None, *, spec=None, seed=None):
         levels = _parse_spec(spec)
 
         if seed is not None:
             # TODO: if to be used within a bigger project we shouldn't seed main RNG
             random.seed(seed)
         if path is None:
             kw = get_tempfile_kwargs({}, prefix="ds")
```

### Comparing `datalad-0.9.3/datalad/distribution/create_sibling_github.py` & `datalad-1.0.0/datalad/local/configuration.py`

 * *Files 18% similar despite different names*

```diff
@@ -2,401 +2,411 @@
 # ex: set sts=4 ts=4 sw=4 noet:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""High-level interface for creating a publication target on GitHub
-"""
+"""Frontend for the DataLad config"""
 
 __docformat__ = 'restructuredtext'
 
 
 import logging
-import re
+from textwrap import wrap
 
-from os.path import join as opj
-from os.path import relpath
-from datalad import cfg
-
-from datalad.interface.common_opts import recursion_flag, recursion_limit
-from datalad.interface.common_opts import publish_depends
-from datalad.downloaders.credentials import UserPassword
-from datalad.dochelpers import exc_str
-from datalad.utils import assure_list
+import datalad.support.ansi_colors as ac
+from datalad import cfg as dlcfg
+from datalad.distribution.dataset import (
+    Dataset,
+    EnsureDataset,
+    datasetmethod,
+    require_dataset,
+)
+from datalad.interface.base import (
+    Interface,
+    build_doc,
+    eval_results,
+)
+from datalad.interface.common_cfg import definitions as cfg_defs
+from datalad.interface.common_opts import (
+    recursion_flag,
+    recursion_limit,
+)
+from datalad.interface.results import get_status_dict
+from datalad.interface.utils import default_result_renderer
+from datalad.support.constraints import (
+    EnsureChoice,
+    EnsureNone,
+)
+from datalad.support.exceptions import (
+    CommandError,
+    NoDatasetFound,
+)
 from datalad.support.param import Parameter
-from datalad.support.network import URL
-from datalad.support.constraints import EnsureStr, EnsureNone
-from datalad.support.constraints import EnsureChoice
-from datalad.support.exceptions import MissingExternalDependency
-from ..interface.base import Interface
-from datalad.interface.base import build_doc
-from datalad.distribution.dataset import EnsureDataset, datasetmethod, \
-    require_dataset, Dataset
-from datalad.distribution.siblings import Siblings
-
-lgr = logging.getLogger('datalad.distribution.create_sibling_github')
-
-
-def get_repo_url(repo, access_protocol, github_login):
-    """Report the repository access URL for Git matching the protocol"""
-    prop = {
-        'https': repo.clone_url,
-        'ssh': repo.ssh_url
-    }[access_protocol]
-    if access_protocol == 'https' and github_login:
-        # we were provided explicit github login.  For ssh access it is
-        # impossible to specify different login within ssh RI, but it is
-        # possible to do so for https logins
-        url = URL(prop)
-        assert url.scheme in ('http', 'https')
-        url.username = github_login
-        prop = url.as_str()
-    return prop
-
-
-def _get_github_entity(gh, cred, github_login, github_passwd, github_organization):
-    if github_login == 'disabledloginfortesting':
-        raise gh.BadCredentialsException(403, 'no login specified')
-    if not (github_login and github_passwd):
-        # we don't have both
-        # check if there is an oauth token from
-        # https://github.com/sociomantic/git-hub
-        token = False
-        if not cred.is_known:
-            if not github_login:
-                # try find a token as login
-                github_login = cfg.get('hub.oauthtoken', None)
-                token = True
-            if not (github_login and (github_passwd or token)):
-                # still at least one missing, utilize the credential store
-                # to get auth info, pass potential passwd value along
-                cred.enter_new(
-                    user=github_login,
-                    password=github_passwd)
-        # now we should really have it
-        creds = cred()
-        github_login = creds['user']
-        github_passwd = creds['password']
-
-    if not github_login:
-        raise gh.BadCredentialsException(403, 'no login specified')
-
-    # this will always succeed, but it might later throw an exception
-    # if the credentials were wrong
-    # and this case, known credentials are wiped out again below
-    authed_gh = gh.Github(
-        github_login,
-        password=github_passwd)
+from datalad.utils import (
+    Path,
+    ensure_list,
+)
 
-    try:
-        if github_organization:
-            try:
-                entity = authed_gh.get_organization(github_organization)
-            except gh.UnknownObjectException as e:
-                raise ValueError('unknown organization "{}" [{}]'.format(
-                                 github_organization,
-                                 exc_str(e)))
-        else:
-            entity = authed_gh.get_user()
-    except gh.BadCredentialsException as e:
-        # things blew up, wipe out cred store, if anything is in it
-        if cred.is_known:
-            cred.delete()
-        raise e
-
-    return entity
-
-
-def _make_github_repos(
-        gh, github_login, github_passwd, github_organization, rinfo, existing,
-        access_protocol, dryrun):
-    # make it per user if github_login was provided. People might want to use
-    # different credentials etc
-    cred_identity = "%s@github" % github_login if github_login else "github"
-    cred = UserPassword(cred_identity, 'https://github.com/login')
-
-    # determine the entity under which to create the repos
-    entity = _get_github_entity(
-        gh,
-        cred,
-        github_login,
-        github_passwd,
-        github_organization)
-
-    res = []
-    for ds, reponame in rinfo:
-        try:
-            access_url, existed = _make_github_repo(
-                gh,
-                github_login,
-                entity,
-                reponame,
-                existing,
-                access_protocol,
-                dryrun)
-            res.append((ds, access_url, existed))
-        except gh.BadCredentialsException as e:
-            # things blew up, wipe out cred store, if anything is in it
-            if cred.is_known:
-                # to avoid surprises "who ate my creds?", warn the user
-                lgr.warning(
-                    "Authentication failed, deleting stored credential %s",
-                    cred_identity
-                )
-                cred.delete()
-            raise e
-    return res
-
-
-def _make_github_repo(gh, github_login, entity, reponame, existing, access_protocol, dryrun):
-    repo = None
-    try:
-        repo = entity.get_repo(reponame)
-    except gh.GithubException as e:
-        if e.status != 404:
-            # this is not a not found message, raise
-            raise e
-        lgr.debug(
-            'To be created repository "%s" does not yet exist on Github',
-            reponame)
-
-    if repo is not None:
-        if existing in ('skip', 'reconfigure'):
-            access_url = get_repo_url(repo, access_protocol, github_login)
-            return access_url, existing == 'skip'
-        elif existing == 'error':
-            msg = 'repository "{}" already exists on Github'.format(reponame)
-            if dryrun:
-                lgr.error(msg)
-            else:
-                raise ValueError(msg)
-        else:
-            RuntimeError('to must not happen')
-
-    if repo is None and not dryrun:
-        try:
-            repo = entity.create_repo(
-                reponame,
-                # TODO description='',
-                # TODO homepage='',
-                # TODO private=False,
-                has_issues=False,
-                has_wiki=False,
-                has_downloads=False,
-                auto_init=False)
-        except gh.GithubException as e:
-            msg = "Github {}: {}".format(
-                e.data.get('message', 'unknown'),
-                ', '.join([err.get('message')
-                           for err in e.data.get('errors', [])
-                           if 'message' in err]))
-            raise RuntimeError(msg)
-
-    if repo is None and not dryrun:
-        raise RuntimeError(
-            'something went wrong, we got no Github repository')
+lgr = logging.getLogger('datalad.local.configuration')
 
-    if dryrun:
-        return '{}:github/.../{}'.format(access_protocol, reponame), False
-    else:
-        # report URL for given access protocol
-        return get_repo_url(repo, access_protocol, github_login), False
-
-
-# presently only implemented method to turn subdataset paths into Github
-# compliant repository name suffixes
-template_fx = lambda x: re.sub(r'\s+', '_', re.sub(r'[/\\]+', '-', x))
+config_actions = ('dump', 'get', 'set', 'unset')
 
 
 @build_doc
-class CreateSiblingGithub(Interface):
-    """Create dataset sibling on Github.
+class Configuration(Interface):
+    """Get and set dataset, dataset-clone-local, or global configuration
 
-    A repository can be created under a user's Github account, or any
-    organization a user is a member of (given appropriate permissions).
+    This command works similar to git-config, but some features are not
+    supported (e.g., modifying system configuration), while other features
+    are not available in git-config (e.g., multi-configuration queries).
+
+    Query and modification of three distinct configuration scopes is
+    supported:
+
+    - 'branch': the persistent configuration in .datalad/config of a dataset
+      branch
+    - 'local': a dataset clone's Git repository configuration in .git/config
+    - 'global': non-dataset-specific configuration (usually in $USER/.gitconfig)
+
+    Modifications of the persistent 'branch' configuration will not be saved
+    by this command, but have to be committed with a subsequent `save`
+    call.
+
+    Rules of precedence regarding different configuration scopes are the same
+    as in Git, with two exceptions: 1) environment variables can be used to
+    override any datalad configuration, and have precedence over any other
+    configuration scope (see below). 2) the 'branch' scope is considered in
+    addition to the standard git configuration scopes. Its content has lower
+    precedence than Git configuration scopes, but it is committed to a branch,
+    hence can be used to ship (default and branch-specific) configuration with
+    a dataset.
+
+    Besides storing configuration settings statically via this command or ``git
+    config``, DataLad also reads any :envvar:`DATALAD_*` environment on process
+    startup or import, and maps it to a configuration item.  Their values take
+    precedence over any other specification. In variable names ``_`` encodes a
+    ``.`` in the configuration name, and ``__`` encodes a ``-``, such that
+    ``DATALAD_SOME__VAR`` is mapped to ``datalad.some-var``.  Additionally, a
+    :envvar:`DATALAD_CONFIG_OVERRIDES_JSON` environment variable is
+    queried, which may contain configuration key-value mappings as a
+    JSON-formatted string of a JSON-object::
+
+      DATALAD_CONFIG_OVERRIDES_JSON='{"datalad.credential.example_com.user": "jane", ...}'
+
+    This is useful when characters are part of the configuration key that
+    cannot be encoded into an environment variable name. If both individual
+    configuration variables *and* JSON-overrides are used, the former take
+    precedent over the latter, overriding the respective *individual* settings
+    from configurations declared in the JSON-overrides.
 
-    Recursive sibling creation for subdatasets is supported. A dataset
-    hierarchy is represented as a flat list of Github repositories.
-
-    Github cannot host dataset content. However, in combination with
-    other data sources (and siblings), publishing a dataset to Github can
-    facilitate distribution and exchange, while still allowing any dataset
-    consumer to obtain actual data content from alternative sources.
-
-    For Github authentication user credentials can be given as arguments.
-    Alternatively, they are obtained interactively or queried from the systems
-    credential store. Lastly, an *oauth* token stored in the Git
-    configuration under variable *hub.oauthtoken* will be used automatically.
-    Such a token can be obtained, for example, using the commandline Github
-    interface (https://github.com/sociomantic/git-hub) by running:
-    :kbd:`git hub setup`.
+    This command supports recursive operation for querying and modifying
+    configuration across a hierarchy of datasets.
     """
-    # XXX prevent common args from being added to the docstring
-    _no_eval_results = True
+    _examples_ = [
+        dict(text="Dump the effective configuration, including an annotation for common items",
+             code_py="configuration()",
+             code_cmd="datalad configuration"),
+        dict(text="Query two configuration items",
+             code_py="configuration('get', ['user.name', 'user.email'])",
+             code_cmd="datalad configuration get user.name user.email"),
+        dict(text="Recursively set configuration in all (sub)dataset repositories",
+             code_py="configuration('set', [('my.config.name', 'value')], recursive=True)",
+             code_cmd="datalad configuration -r set my.config=value"),
+        dict(text="Modify the persistent branch configuration (changes are not committed)",
+             code_py="configuration('set', [('my.config.name', 'value')], scope='branch')",
+             code_cmd="datalad configuration --scope branch set my.config=value"),
+    ]
+
+    result_renderer = 'tailored'
 
     _params_ = dict(
         dataset=Parameter(
-            args=("--dataset", "-d",),
-            doc="""specify the dataset to create the publication target for. If
-                no dataset is given, an attempt is made to identify the dataset
-                based on the current working directory""",
+            args=("-d", "--dataset"),
+            doc="""specify the dataset to query or to configure""",
             constraints=EnsureDataset() | EnsureNone()),
-        reponame=Parameter(
-            args=('reponame',),
-            metavar='REPONAME',
-            doc="""Github repository name. When operating recursively,
-            a suffix will be appended to this name for each subdataset""",
-            constraints=EnsureStr()),
+        action=Parameter(
+            args=("action",),
+            nargs='?',
+            doc="""which action to perform""",
+            constraints=EnsureChoice(*config_actions)),
+        scope=Parameter(
+            args=("--scope",),
+            doc="""scope for getting or setting
+            configuration. If no scope is declared for a query, all
+            configuration sources (including overrides via environment
+            variables) are considered according to the normal
+            rules of precedence. For action 'get' only 'branch' and 'local'
+            (which include 'global' here) are supported. For action 'dump',
+            a scope selection is ignored and all available scopes are
+            considered.""",
+            constraints=EnsureChoice('global', 'local', 'branch', None)),
+        spec=Parameter(
+            args=("spec",),
+            doc="""configuration name (for actions 'get' and 'unset'),
+            or name/value pair (for action 'set')""",
+            nargs='*',
+            metavar='name[=value]'),
         recursive=recursion_flag,
         recursion_limit=recursion_limit,
-        name=Parameter(
-            args=('-s', '--name',),
-            metavar='NAME',
-            doc="""name to represent the Github repository in the local
-            dataset installation""",
-            constraints=EnsureStr()),
-        existing=Parameter(
-            args=("--existing",),
-            constraints=EnsureChoice('skip', 'error', 'reconfigure'),
-            metavar='MODE',
-            doc="""desired behavior when already existing or configured
-            siblings are discovered. 'skip': ignore; 'error': fail immediately;
-            'reconfigure': use the existing repository and reconfigure the
-            local dataset to use it as a sibling""",),
-        github_login=Parameter(
-            args=('--github-login',),
-            constraints=EnsureStr() | EnsureNone(),
-            metavar='NAME',
-            doc="""Github user name or access token"""),
-        github_passwd=Parameter(
-            args=('--github-passwd',),
-            constraints=EnsureStr() | EnsureNone(),
-            metavar='PASSWORD',
-            doc="""Github user password"""),
-        github_organization=Parameter(
-            args=('--github-organization',),
-            constraints=EnsureStr() | EnsureNone(),
-            metavar='NAME',
-            doc="""If provided, the repository will be created under this
-            Github organization. The respective Github user needs appropriate
-            permissions."""),
-        access_protocol=Parameter(
-            args=("--access-protocol",),
-            constraints=EnsureChoice('https', 'ssh'),
-            doc="""Which access protocol/URL to configure for the sibling"""),
-        publish_depends=publish_depends,
-        dryrun=Parameter(
-            args=("--dryrun",),
-            action="store_true",
-            doc="""If this flag is set, no communication with Github is
-            performed, and no repositories will be created. Instead
-            would-be repository names are reported for all relevant datasets
-            """),
     )
 
     @staticmethod
-    @datasetmethod(name='create_sibling_github')
+    @datasetmethod(name='configuration')
+    @eval_results
     def __call__(
-            reponame,
+            action='dump',
+            spec=None,
+            *,
+            scope=None,
             dataset=None,
             recursive=False,
-            recursion_limit=None,
-            name='github',
-            existing='error',
-            github_login=None,
-            github_passwd=None,
-            github_organization=None,
-            access_protocol='https',
-            publish_depends=None,
-            dryrun=False):
-        try:
-            # this is an absolute leaf package, import locally to avoid
-            # unnecessary dependencies
-            import github as gh
-        except ImportError:
-            raise MissingExternalDependency(
-                'PyGitHub',
-                msg='GitHub-related functionality is unavailable without this package')
-
-        # what to operate on
-        ds = require_dataset(
-            dataset, check_installed=True, purpose='create Github sibling')
-        # gather datasets and essential info
-        # dataset instance and mountpoint relative to the top
-        toprocess = [(ds, '')]
-        if recursive:
-            for sub in ds.subdatasets(
-                    fulfilled=None,  # we want to report on missing dataset in here
-                    recursive=recursive,
-                    recursion_limit=recursion_limit,
-                    result_xfm='datasets'):
-                if not sub.is_installed():
-                    lgr.info('Ignoring unavailable subdataset %s', sub)
-                    continue
-                toprocess.append((sub, relpath(sub.path, start=ds.path)))
-
-        # check for existing remote configuration
-        filtered = []
-        for d, mp in toprocess:
-            if name in d.repo.get_remotes():
-                if existing == 'error':
-                    msg = '{} already had a configured sibling "{}"'.format(
-                        d, name)
-                    if dryrun:
-                        lgr.error(msg)
-                    else:
-                        raise ValueError(msg)
-                elif existing == 'skip':
-                    continue
-            gh_reponame = '{}{}{}'.format(
-                reponame,
-                '-' if mp else '',
-                template_fx(mp))
-            filtered.append((d, gh_reponame))
-
-        if not filtered:
-            # all skipped
-            return []
-
-        # actually make it happen on Github
-        rinfo = _make_github_repos(
-            gh, github_login, github_passwd, github_organization, filtered,
-            existing, access_protocol, dryrun)
-
-        # lastly configure the local datasets
-        for d, url, existed in rinfo:
-            if not dryrun:
-                # first make sure that annex doesn't touch this one
-                # but respect any existing config
-                ignore_var = 'remote.{}.annex-ignore'.format(name)
-                if not ignore_var in d.config:
-                    d.config.add(ignore_var, 'true', where='local')
-                Siblings()(
-                    'configure',
-                    dataset=d,
-                    name=name,
-                    url=url,
-                    recursive=False,
-                    # TODO fetch=True, maybe only if one existed already
-                    publish_depends=publish_depends)
+            recursion_limit=None):
 
-        # TODO let submodule URLs point to Github (optional)
-        return rinfo
+        # check conditions
+        # - global and recursion makes no sense
+
+        if action == 'dump':
+            if scope:
+                raise ValueError(
+                    'Scope selection is not supported for dumping')
+
+        # normalize variable specifications
+        specs = []
+        for s in ensure_list(spec):
+            if isinstance(s, tuple):
+                specs.append((str(s[0]), str(s[1])))
+            elif '=' not in s:
+                specs.append((str(s),))
+            else:
+                specs.append(tuple(s.split('=', 1)))
+
+        if action == 'set':
+            missing_values = [s[0] for s in specs if len(s) < 2]
+            if missing_values:
+                raise ValueError(
+                    'Values must be provided for all configuration '
+                    'settings. Missing: {}'.format(missing_values))
+            invalid_names = [s[0] for s in specs if '.' not in s[0]]
+            if invalid_names:
+                raise ValueError(
+                    'Name must contain a section (i.e. "section.name"). '
+                    'Invalid: {}'.format(invalid_names))
+
+        ds = None
+        if scope != 'global' or recursive:
+            try:
+                ds = require_dataset(
+                    dataset,
+                    check_installed=True,
+                    purpose='configure')
+            except NoDatasetFound:
+                if action != 'dump' or dataset:
+                    raise
+
+        res_kwargs = dict(
+            action='configuration',
+            logger=lgr,
+        )
+        if ds:
+            res_kwargs['refds'] = ds.path
+        yield from configuration(action, scope, specs, res_kwargs, ds)
+
+        if not recursive:
+            return
+
+        for subds in ds.subdatasets(
+                state='present',
+                recursive=True,
+                recursion_limit=recursion_limit,
+                on_failure='ignore',
+                return_type='generator',
+                result_renderer='disabled'):
+            yield from configuration(
+                action, scope, specs, res_kwargs, Dataset(subds['path']))
 
     @staticmethod
-    def result_renderer_cmdline(res, args):
+    def custom_result_renderer(res, **kwargs):
+        if (res['status'] != 'ok' or
+                res['action'] not in ('get_configuration',
+                                      'dump_configuration')):
+            if 'message' not in res and 'name' in res:
+                suffix = '={}'.format(res['value']) if 'value' in res else ''
+                res['message'] = '{}{}'.format(
+                    res['name'],
+                    suffix)
+            default_result_renderer(res)
+            return
+        # TODO source
         from datalad.ui import ui
-        res = assure_list(res)
-        if args.dryrun:
-            ui.message('DRYRUN -- Anticipated results:')
-        if not len(res):
-            ui.message("Nothing done")
+        name = res['name']
+        if res['action'] == 'dump_configuration':
+            for key in ('purpose', 'description'):
+                s = res.get(key)
+                if s:
+                    ui.message('\n'.join(wrap(
+                        s,
+                        initial_indent='# ',
+                        subsequent_indent='# ',
+                    )))
+
+        if kwargs.get('recursive', False):
+            have_subds = res['path'] != res['refds']
+            # we need to mark up from which dataset results are reported
+            prefix = '<ds>{}{}:'.format(
+                '/' if have_subds else '',
+                Path(res['path']).relative_to(res['refds']).as_posix()
+                if have_subds else '',
+            )
         else:
-            for d, url, existed in res:
-                ui.message(
-                    "'{}'{} configured as sibling '{}' for {}".format(
-                        url,
-                        " (existing repository)" if existed else '',
-                        args.name,
-                        d))
+            prefix = ''
+
+        if kwargs.get('action', None) == 'dump':
+            if 'value_type' in res:
+                value_type = res['value_type']
+                vtype = value_type.short_description() \
+                    if hasattr(value_type, 'short_description') else str(value_type)
+                vtype = f'Value constraint: {vtype}'
+                ui.message('\n'.join(wrap(
+                    vtype,
+                    initial_indent='# ',
+                    subsequent_indent='#                    ',
+                    break_on_hyphens=False,
+                )))
+            else:
+                vtype = ''
+            value = res['value'] if res['value'] is not None else ''
+            if value in (True, False):
+                # normalize booleans for git-config syntax
+                value = str(value).lower()
+            ui.message(f'{prefix}{ac.color_word(name, ac.BOLD)}={value}')
+        else:
+            ui.message('{}{}'.format(
+                prefix,
+                res['value'] if res['value'] is not None else '',
+            ))
+
+
+def configuration(action, scope, specs, res_kwargs, ds=None):
+    if scope == 'global' or (action == 'dump' and ds is None):
+        cfg = dlcfg
+    else:
+        cfg = ds.config
+
+    if action not in config_actions:
+        raise ValueError("Unsupported action '{}'".format(action))
+
+    if action == 'dump':
+        if not specs:
+            # dumping is querying for all known keys
+            specs = [(n,) for n in sorted(set(cfg_defs.keys()).union(cfg.keys()))]
+        scope = None
+
+    for spec in specs:
+        if '.' not in spec[0]:
+            yield get_status_dict(
+                ds=ds,
+                status='error',
+                message=(
+                    "Configuration key without a section: '%s'",
+                    spec[0],
+                ),
+                **res_kwargs)
+            continue
+        # TODO without get-all there is little sense in having add
+        #if action == 'add':
+        #    res = _add(cfg, scope, spec)
+        if action == 'get':
+            res = _get(cfg, scope, spec[0])
+        elif action == 'dump':
+            res = _dump(cfg, spec[0])
+        # TODO this should be there, if we want to be comprehensive
+        # however, we turned this off by default in the config manager
+        # because we hardly use it, and the handling in ConfigManager
+        # is not really well done.
+        #elif action == 'get-all':
+        #    res = _get_all(cfg, scope, spec)
+        elif action == 'set':
+            res = _set(cfg, scope, *spec)
+        elif action == 'unset':
+            res = _unset(cfg, scope, spec[0])
+
+        if ds:
+            res['path'] = ds.path
+
+        if 'status' not in res:
+            res['status'] = 'ok'
+
+        yield dict(res_kwargs, **res)
+
+    if action in ('add', 'set', 'unset'):
+        # we perform a single reload, rather than one for each modification
+        # TODO: can we detect a call from cmdline? We could skip the reload.
+        cfg.reload(force=True)
+
+
+def _dump(cfg, name):
+    value = cfg.get(
+        name,
+        # pull a default from the config definitions
+        # if we have no value, but a key
+        cfg_defs.get(name, {}).get('default', None))
+
+    res = dict(
+        action='dump_configuration',
+        name=name,
+        value=value,
+    )
+    if name in cfg_defs:
+        ui_def = cfg_defs[name].get('ui', [None, {}])[1]
+        for s, key in (
+                (ui_def.get('title'), 'purpose'),
+                (ui_def.get('text'), 'description'),
+                (cfg_defs[name].get('type'), 'value_type')):
+            if s:
+                res[key] = s
+    return res
+
+
+def _get(cfg, scope, name):
+    value = cfg.get_from_source(scope, name) \
+        if scope else cfg.get(
+            name,
+            # pull a default from the config definitions
+            # if we have no value, but a key (i.e. in dump mode)
+            cfg_defs.get(name, {}).get('default', None))
+    return dict(
+        action='get_configuration',
+        name=name,
+        value=value,
+    )
+
+
+def _set(cfg, scope, name, value):
+    cfg.set(name, value, scope=scope, force=True, reload=False)
+    return dict(
+        action='set_configuration',
+        name=name,
+        value=value,
+    )
+
+
+def _unset(cfg, scope, name):
+    try:
+        cfg.unset(name, scope=scope, reload=False)
+    except CommandError as e:
+        # we could also check if the option exists in the merged/effective
+        # config first, but then we would have to make sure that there could
+        # be no valid way of overriding a setting in a particular scope.
+        # seems safer to do it this way
+        if e.code == 5:
+            return dict(
+                status='error',
+                action='unset_configuration',
+                name=name,
+                message=("configuration '%s' does not exist (%s)", name, e),
+            )
+    return dict(
+        action='unset_configuration',
+        name=name,
+    )
```

### Comparing `datalad-0.9.3/datalad/distribution/drop.py` & `datalad-1.0.0/datalad/local/export_archive.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,224 +1,166 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""High-level interface for dropping dataset content
-
-"""
+"""export a dataset as a compressed TAR/ZIP archive"""
 
 __docformat__ = 'restructuredtext'
 
-import logging
-
-from os.path import join as opj
-from os.path import isabs
-from os.path import normpath
-
-from datalad.utils import assure_list
-from datalad.support.param import Parameter
-from datalad.support.constraints import EnsureStr, EnsureNone
-from datalad.support.exceptions import InsufficientArgumentsError
-from datalad.support.gitrepo import GitRepo
-from datalad.distribution.dataset import Dataset, EnsureDataset, \
-    datasetmethod
-from datalad.interface.annotate_paths import AnnotatePaths
-from datalad.interface.annotate_paths import annotated2content_by_ds
 from datalad.interface.base import Interface
-from datalad.interface.common_opts import if_dirty_opt
-from datalad.interface.common_opts import recursion_flag
-from datalad.interface.common_opts import recursion_limit
-from datalad.interface.results import get_status_dict
-from datalad.interface.results import annexjson2result
-from datalad.interface.results import success_status_map
-from datalad.interface.results import results_from_annex_noinfo
-from datalad.interface.utils import handle_dirty_dataset
-from datalad.interface.utils import eval_results
 from datalad.interface.base import build_doc
-
-lgr = logging.getLogger('datalad.distribution.drop')
-
-dataset_argument = Parameter(
-    args=("-d", "--dataset"),
-    metavar="DATASET",
-    doc="""specify the dataset to perform the operation on.
-    If no dataset is given, an attempt is made to identify a dataset
-    based on the `path` given""",
-    constraints=EnsureDataset() | EnsureNone())
-
-
-check_argument = Parameter(
-    args=("--nocheck",),
-    doc="""whether to perform checks to assure the configured minimum
-    number (remote) source for data.[CMD:  Give this
-    option to skip checks CMD]""",
-    action="store_false",
-    dest='check')
-
-
-def _drop_files(ds, paths, check, noannex_iserror=False, **kwargs):
-    """Helper to drop content in datasets.
-
-    Parameters
-    ----------
-    ds : Dataset
-    paths : path or list(path)
-      which content to drop
-    check : bool
-      whether to instruct annex to perform minimum copy availability
-      checks
-    noannex_iserror : bool
-      whether calling this function on a pur Git repo results in an
-      'impossible' or 'notneeded' result.
-    **kwargs
-      additional payload for the result dicts
-    """
-    if 'action' not in kwargs:
-        kwargs['action'] = 'drop'
-    # always need to make sure that we pass a list
-    # `normalize_paths` decorator will otherwise screw all logic below
-    paths = assure_list(paths)
-    if not hasattr(ds.repo, 'drop'):
-        for p in paths:
-            r = get_status_dict(
-                status='impossible' if noannex_iserror else 'notneeded',
-                path=p if isabs(p) else normpath(opj(ds.path, p)),
-                message="no annex'ed content",
-                **kwargs)
-            r['action'] = 'drop'
-            yield r
-        return
-
-    opts = ['--force'] if not check else []
-    respath_by_status = {}
-    for res in ds.repo.drop(paths, options=opts):
-        res = annexjson2result(
-            # annex reports are always about files
-            res, ds, type='file', **kwargs)
-        success = success_status_map[res['status']]
-        respath_by_status[success] = \
-            respath_by_status.get(success, []) + [res['path']]
-        yield res
-    # report on things requested that annex was silent about
-    for r in results_from_annex_noinfo(
-            ds, paths, respath_by_status,
-            dir_fail_msg='could not drop some content in %s %s',
-            noinfo_dir_msg='nothing to drop from %s',
-            noinfo_file_msg="no annex'ed content",
-            **kwargs):
-        r['action'] = 'drop'
-        yield r
-
+from pathlib import Path
 
 @build_doc
-class Drop(Interface):
-    """Drop file content from datasets
-
-    This command takes any number of paths of files and/or directories. If
-    a common (super)dataset is given explicitly, the given paths are
-    interpreted relative to this dataset.
-
-    Recursion into subdatasets needs to be explicitly enabled, while recursion
-    in subdirectories within a dataset as always done automatically. An
-    optional recursion limit is applied relative to each given input path.
-
-    By default, the availability of at least one remote copy is verified,
-    before file content is dropped. As these checks could lead to slow
-    operation (network latencies, etc), they can be disabled.
-
-    Examples:
-
-      Drop all file content in a dataset::
-
-        ~/some/dataset$ datalad drop
-
-      Drop all file content in a dataset and all its subdatasets::
-
-        ~/some/dataset$ datalad drop --recursive
-
+class ExportArchive(Interface):
+    """Export the content of a dataset as a TAR/ZIP archive.
     """
-    _action = 'drop'
+    from datalad.support.param import Parameter
+    from datalad.distribution.dataset import datasetmethod
+    from datalad.interface.base import eval_results
+    from datalad.distribution.dataset import EnsureDataset
+    from datalad.support.constraints import (
+        EnsureChoice,
+        EnsureNone,
+        EnsureStr,
+    )
 
     _params_ = dict(
-        dataset=dataset_argument,
-        path=Parameter(
-            args=("path",),
+        dataset=Parameter(
+            args=("-d", "--dataset"),
+            doc=""""specify the dataset to export. If no dataset is given, an
+            attempt is made to identify the dataset based on the current
+            working directory.""",
+            constraints=EnsureDataset() | EnsureNone()),
+        filename=Parameter(
+            args=("filename",),
             metavar="PATH",
-            doc="path/name of the component to be dropped",
-            nargs="*",
+            nargs='?',
+            doc="""File name of the generated TAR archive. If no file name is
+            given the archive will be generated in the current directory and
+            will be named: datalad_<dataset_uuid>.(tar.*|zip). To generate that
+            file in a different directory, provide an existing directory as the
+            file name.""",
             constraints=EnsureStr() | EnsureNone()),
-        recursive=recursion_flag,
-        recursion_limit=recursion_limit,
-        check=check_argument,
-        if_dirty=if_dirty_opt,
+        archivetype=Parameter(
+            args=("-t", "--archivetype"),
+            doc="""Type of archive to generate.""",
+            constraints=EnsureChoice("tar", "zip")),
+        compression=Parameter(
+            args=("-c", "--compression"),
+            doc="""Compression method to use.  'bz2' is not supported for ZIP
+            archives.  No compression is used when an empty string is
+            given.""",
+            constraints=EnsureChoice("gz", "bz2", "")),
+        missing_content=Parameter(
+            args=("--missing-content",),
+            doc="""By default, any discovered file with missing content will
+            result in an error and the export is aborted. Setting this to
+            'continue' will issue warnings instead of failing on error. The
+            value 'ignore' will only inform about problem at the 'debug' log
+            level. The latter two can be helpful when generating a TAR archive
+            from a dataset where some file content is not available
+            locally.""",
+            constraints=EnsureChoice("error", "continue", "ignore")),
     )
 
     @staticmethod
-    @datasetmethod(name=_action)
+    @datasetmethod(name='export_archive')
     @eval_results
-    def __call__(
-            path=None,
-            dataset=None,
-            recursive=False,
-            recursion_limit=None,
-            check=True,
-            if_dirty='save-before'):
-
-        if not dataset and not path:
-            raise InsufficientArgumentsError(
-                "insufficient information for `drop`: requires at least a path or dataset")
-        refds_path = Interface.get_refds_path(dataset)
-        res_kwargs = dict(action='drop', logger=lgr, refds=refds_path)
-        if dataset and not path:
-            # act on the whole dataset if nothing else was specified
-            path = refds_path
-        to_drop = []
-        for ap in AnnotatePaths.__call__(
-                dataset=refds_path,
-                path=path,
-                recursive=recursive,
-                recursion_limit=recursion_limit,
-                action='drop',
-                # justification for status:
-                # content need not be dropped where there is none
-                unavailable_path_status='notneeded',
-                nondataset_path_status='error',
-                return_type='generator',
-                on_failure='ignore'):
-            if ap.get('status', None):
-                # this is done
-                yield ap
-                continue
-            if ap.get('type', None) == 'dataset' and \
-                    GitRepo.is_valid_repo(ap['path']) and \
-                    not ap['path'] == refds_path:
-                ap['process_content'] = True
-            if ap.get('registered_subds', False) and ap.get('state', None) == 'absent':
-                # nothing to drop in an absent subdataset, don't be annoying
-                # and skip silently
-                continue
-            to_drop.append(ap)
-
-        content_by_ds, ds_props, completed, nondataset_paths = \
-            annotated2content_by_ds(
-                to_drop,
-                refds_path=refds_path)
-        assert(not completed)
-
-        # iterate over all datasets, order doesn't matter
-        for ds_path in content_by_ds:
-            ds = Dataset(ds_path)
-            # TODO generator
-            # this should yield what it did
-            handle_dirty_dataset(ds, mode=if_dirty)
-            # ignore submodule entries
-            content = [ap['path'] for ap in content_by_ds[ds_path]
-                       if ap.get('type', None) != 'dataset' or ap['path'] == ds.path]
-            if not content:
-                continue
-            for r in _drop_files(ds, content, check=check, **res_kwargs):
-                yield r
-        # there is nothing to save at the end
+    def __call__(filename=None,
+                 *,
+                 dataset=None,
+                 archivetype='tar',
+                 compression='gz',
+                 missing_content='error'):
+        import tarfile
+        import zipfile
+        from unittest.mock import patch
+
+        from datalad.distribution.dataset import require_dataset
+        from datalad.utils import file_basename
+        from datalad.support.annexrepo import AnnexRepo
+
+        import logging
+        lgr = logging.getLogger('datalad.local.export_archive')
+
+        dataset = require_dataset(dataset, check_installed=True,
+                                  purpose='export archive')
+
+        repo = dataset.repo
+        committed_date = repo.get_commit_date()
+
+        # could be used later on to filter files by some criterion
+        def _filter_tarinfo(ti):
+            # Reset the date to match the one of the last commit, not from the
+            # filesystem since git doesn't track those at all
+            # TODO: use the date of the last commit when any particular
+            # file was changed -- would be the most kosher yoh thinks to the
+            # degree of our abilities
+            ti.mtime = committed_date
+            return ti
+        tar_args = dict(recursive=False, filter=_filter_tarinfo)
+
+        file_extension = '.{}{}'.format(
+            archivetype,
+            '{}{}'.format(
+                '.' if compression else '',
+                compression) if archivetype == 'tar' else '')
+
+        default_filename = "datalad_{.id}".format(dataset)
+        if filename is not None:
+            filename = Path(filename)
+        if filename is None:
+            filename = Path(default_filename)  # in current directory
+        elif filename.exists() and filename.is_dir():
+            filename = filename / default_filename # under given directory
+        if filename.suffix != file_extension:
+            filename = filename.with_suffix(file_extension)
+
+        root = dataset.path
+        # use dir inside matching the output filename without suffix(es)
+        # TODO: could be an option to the export plugin allowing empty value
+        # for no leading dir
+        leading_dir = file_basename(filename)
+
+        # workaround for inability to pass down the time stamp
+        with patch('time.time', return_value=committed_date), \
+                tarfile.open(filename, "w:{}".format(compression)) \
+                if archivetype == 'tar' \
+                else zipfile.ZipFile(
+                    filename, 'w',
+                    zipfile.ZIP_STORED if not compression else zipfile.ZIP_DEFLATED) \
+                as archive:
+            add_method = archive.add if archivetype == 'tar' else archive.write
+
+
+            repo_files = repo.get_content_info(ref='HEAD', untracked='no')
+            if isinstance(repo, AnnexRepo):
+                # add availability (has_content) info
+                repo_files = repo.get_content_annexinfo(ref='HEAD',
+                                                        init=repo_files,
+                                                        eval_availability=True)
+            for p, props in repo_files.items():
+                if 'key' in props and not props.get('has_content', False):
+                    if missing_content in ('ignore', 'continue'):
+                        (lgr.warning if missing_content == 'continue' else lgr.debug)(
+                            'File %s has no content available, skipped', p)
+                        continue
+                    else:
+                        raise IOError('File %s has no content available' % p)
+                # name in the archive
+                aname = Path(leading_dir) / p.relative_to(repo.pathobj)
+                add_method(
+                    p if 'key' not in props else props['objloc'],
+                    arcname=aname,
+                    **(tar_args if archivetype == 'tar' else {}))
+
+        yield dict(
+            status='ok',
+            path=filename.resolve(),
+            type='file',
+            action='export_archive',
+            logger=lgr)
```

### Comparing `datalad-0.9.3/datalad/distribution/dataset.py` & `datalad-1.0.0/datalad/distributed/export_to_figshare.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,505 +1,387 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""Implements class Dataset
-"""
+"""export a dataset as a TAR/ZIP archive to figshare"""
 
-import logging
-from os.path import abspath
-from os.path import curdir
-from os.path import exists
-from os.path import join as opj
-from os.path import normpath, isabs
-from os.path import pardir
-from os.path import realpath
-from os.path import relpath
-from weakref import WeakValueDictionary
-from six import PY2
-from six import string_types
-from six import add_metaclass
-import wrapt
-
-from datalad.config import ConfigManager
-from datalad.consts import LOCAL_CENTRAL_PATH
-from datalad.dochelpers import exc_str
-from datalad.support.annexrepo import AnnexRepo
-from datalad.support.constraints import Constraint
-from datalad.support.exceptions import NoDatasetArgumentFound
-from datalad.support.gitrepo import GitRepo
-from datalad.support.gitrepo import InvalidGitRepositoryError
-from datalad.support.gitrepo import NoSuchPathError
-from datalad.support.repo import Flyweight
-from datalad.support.network import RI
-
-from datalad.utils import getpwd
-from datalad.utils import optional_args, expandpath, is_explicit_path
-from datalad.utils import get_dataset_root
-from datalad.distribution.utils import get_git_dir
-
-
-lgr = logging.getLogger('datalad.dataset')
-lgr.log(5, "Importing dataset")
-
-
-# TODO: use the same piece for resolving paths against Git/AnnexRepo instances
-#       (see normalize_path)
-def resolve_path(path, ds=None):
-    """Resolve a path specification (against a Dataset location)
-
-    Any explicit path (absolute or relative) is returned as an absolute path.
-    In case of an explicit relative path, the current working directory is
-    used as a reference. Any non-explicit relative path is resolved against
-    as dataset location, i.e. considered relative to the location of the
-    dataset. If no dataset is provided, the current working directory is
-    used.
-
-    Returns
-    -------
-    Absolute path
-    """
-    path = expandpath(path, force_absolute=False)
-    # TODO: normpath?!
-    if is_explicit_path(path):
-        return abspath(path)
-    # no dataset given, use CWD as reference
-    # note: abspath would disregard symlink in CWD
-    top_path = getpwd() \
-        if ds is None else ds.path if isinstance(ds, Dataset) else ds
-    return normpath(opj(top_path, path))
-
-
-@add_metaclass(Flyweight)
-class Dataset(object):
-    """Representation of a DataLad dataset/repository
-
-    This is the core data type of DataLad: a representation of a dataset.
-    At its core, datasets are (git-annex enabled) Git repositories. This
-    class provides all operations that can be performed on a dataset.
-
-    Creating a dataset instance is cheap, all actual operations are
-    delayed until they are actually needed. Creating multiple `Dataset`
-    class instances for the same Dataset location will automatically
-    yield references to the same object.
-
-    A dataset instance comprises of two major components: a `repo`
-    attribute, and a `config` attribute. The former offers access to
-    low-level functionality of the Git or git-annex repository. The
-    latter gives access to a dataset's configuration manager.
-
-    Most functionality is available via methods of this class, but also
-    as stand-alone functions with the same name in `datalad.api`.
-    """
-    # Begin Flyweight
-    _unique_instances = WeakValueDictionary()
-
-    @classmethod
-    def _flyweight_id_from_args(cls, *args, **kwargs):
-
-        if args:
-            # to a certain degree we need to simulate an actual call to __init__
-            # and make sure, passed arguments are fitting:
-            # TODO: Figure out, whether there is a cleaner way to do this in a
-            # generic fashion
-            assert('path' not in kwargs)
-            path = args[0]
-            args = args[1:]
-        elif 'path' in kwargs:
-            path = kwargs.pop('path')
-        else:
-            raise TypeError("__init__() requires argument `path`")
-
-        if path is None:
-            raise AttributeError
+__docformat__ = 'restructuredtext'
 
-        # Custom handling for few special abbreviations
-        path_ = path
-        if path == '^':
-            # get the topmost dataset from current location. Note that 'zsh'
-            # might have its ideas on what to do with ^, so better use as -d^
-            path_ = Dataset(curdir).get_superdataset(topmost=True).path
-        elif path == '///':
-            # TODO: logic/UI on installing a central dataset could move here
-            # from search?
-            path_ = LOCAL_CENTRAL_PATH
-        if path != path_:
-            lgr.debug("Resolved dataset alias %r to path %r", path, path_)
-
-        # Sanity check for argument `path`:
-        # raise if we cannot deal with `path` at all or
-        # if it is not a local thing:
-        path_ = RI(path_).localpath
-
-        # we want an absolute path, but no resolved symlinks
-        if not isabs(path_):
-            path_ = opj(getpwd(), path_)
-
-        # use canonical paths only:
-        path_ = normpath(path_)
-        kwargs['path'] = path_
-        return path_, args, kwargs
-    # End Flyweight
+from datalad.utils import unlink
+from datalad.interface.base import Interface
+from datalad.interface.base import build_doc
+from datalad.interface.results import get_status_dict
 
-    def __init__(self, path):
-        """
-        Parameters
-        ----------
-        path : str
-          Path to the dataset location. This location may or may not exist
-          yet.
-        """
-        self._path = path
-        self._repo = None
-        self._id = None
-        self._cfg = None
-
-    def __repr__(self):
-        return "<Dataset path=%s>" % self.path
-
-    def __eq__(self, other):
-        if not hasattr(other, 'path'):
-            return False
-        return realpath(self.path) == realpath(other.path)
-
-    def close(self):
-        """Perform operations which would close any possible process using this Dataset
-        """
-        repo = self._repo
-        self._repo = None
-        if repo:
-            # might take care about lingering batched processes etc
-            del repo
-
-    @property
-    def path(self):
-        """path to the dataset"""
-        return self._path
+import logging
+lgr = logging.getLogger('datalad.distributed.export_to_figshare')
 
-    @property
-    def repo(self):
-        """Get an instance of the version control system/repo for this dataset,
-        or None if there is none yet.
-
-        If creating an instance of GitRepo is guaranteed to be really cheap
-        this could also serve as a test whether a repo is present.
-
-        Returns
-        -------
-        GitRepo
-        """
 
-        # Note: lazy loading was disabled, since this is provided by the
-        # flyweight pattern already and a possible invalidation of an existing
-        # instance has to be done therein.
-        # TODO: Still this is somewhat problematic. We can't invalidate strong
-        # references
-
-        for cls, ckw, kw in (
-                # TODO: Do we really want to allow_noninitialized=True here?
-                # And if so, leave a proper comment!
-                (AnnexRepo, {'allow_noninitialized': True}, {'init': False}),
-                (GitRepo, {}, {})
-        ):
-            if cls.is_valid_repo(self._path, **ckw):
-                try:
-                    lgr.log(5, "Detected %s at %s", cls, self._path)
-                    self._repo = cls(self._path, create=False, **kw)
-                    break
-                except (InvalidGitRepositoryError, NoSuchPathError) as exc:
-                    lgr.log(5,
-                            "Oops -- guess on repo type was wrong?: %s",
-                            exc_str(exc))
-                    pass
-                # version problems come as RuntimeError: DO NOT CATCH!
-        if self._repo is None:
-            # Often .repo is requested to 'sense' if anything is installed
-            # under, and if so -- to proceed forward. Thus log here only
-            # at DEBUG level and if necessary "complaint upstairs"
-            lgr.log(5, "Failed to detect a valid repo at %s", self.path)
+class FigshareRESTLaison(object):
+    """A little helper to provide minimal interface to interact with Figshare
+    """
+    API_URL = 'https://api.figshare.com/v2'
 
-        return self._repo
+    def __init__(self):
+        self._token = None
+        from datalad.ui import ui
+        self.ui = ui  # we will be chatty here
 
     @property
-    def id(self):
-        """Identifier of the dataset.
+    def token(self):
+        if self._token is None:
+            from datalad.downloaders.providers import Providers
+            providers = Providers.from_config_files()
+            provider = providers.get_provider(self.API_URL)
+            credential = provider.credential
+            self._token = credential().get('token')
+        return self._token
+
+    def __call__(self, m, url, data=None, success=None, binary=False,
+                 headers=None, return_json=True):
+        """A wrapper around requests calls
 
-        This identifier is supposed to be unique across datasets, but identical
-        for different versions of the same dataset (that have all been derived
-        from the same original dataset repository).
-
-        Returns
-        -------
-        str
-          This is either a stored UUID, or `None`.
+        to interpolate deposition_id, do basic checks and conversion
         """
-        if self._id is None:
-            # if we have one on record, stick to it!
-            self.config.reload()
-            self._id = self.config.get('datalad.dataset.id', None)
-        return self._id
+        import json
+        if '://' not in url:
+            url_ = self.API_URL + '/' + url
+        else:
+            url_ = url
 
-    @property
-    def config(self):
-        """Get an instance of the parser for the persistent dataset configuration.
+        headers = headers or {}
+        if data is not None and not binary:
+            data = json.dumps(data)
+            headers["Content-Type"] = "application/json"
+        headers['Authorization'] = "token %s" % self.token
+
+        lgr.debug(
+            "Submitting %s request to %s with data %s (headers: %s)",
+            m.__name__, url_, data, 'sanitized'  # headers
+        )
+        r = m(url_, data=data, headers=headers)
+        status_code = r.status_code
+        if (success != "donotcheck") and \
+                ((success and status_code not in success)
+                 or (not success and status_code >= 400)):
+            msg = "Got return code %(status_code)s for %(m)s(%(url_)s." \
+                  % locals()
+            raise RuntimeError("Error status %s" % msg)
 
-        Returns
-        -------
-        ConfigManager
-        """
-        if self._cfg is None:
-            if self.repo is None:
-                # associate with this dataset and read the entire config hierarchy
-                self._cfg = ConfigManager(dataset=self, dataset_only=False)
-            else:
-                self._cfg = self.repo.config
-        return self._cfg
-
-    def get_subdatasets(self, pattern=None, fulfilled=None, absolute=False,
-                        recursive=False, recursion_limit=None, edges=False):
-        """DEPRECATED: use `subdatasets()`"""
-        # TODO wipe this function out completely once we are comfortable
-        # with it. Internally we don't need or use it anymore.
-        import inspect
-        lgr.warning('%s still uses Dataset.get_subdatasets(). RF to use `subdatasets` command', inspect.stack()[1][3])
-        from datalad.api import subdatasets
-        if edges:
-            return [(r['parentpath'] if absolute else relpath(r['parentpath'], start=self.path),
-                     r['path'] if absolute else relpath(r['path'], start=self.path))
-                    for r in subdatasets(
-                        dataset=self,
-                        fulfilled=fulfilled,
-                        recursive=recursive,
-                        recursion_limit=recursion_limit,
-                        bottomup=True)]
+        if return_json:
+            return r.json() if r.content else {}
         else:
-            return subdatasets(
-                dataset=self,
-                fulfilled=fulfilled,
-                recursive=recursive,
-                recursion_limit=recursion_limit,
-                bottomup=True,
-                result_xfm='{}paths'.format('' if absolute else 'rel'))
-
-    def recall_state(self, whereto):
-        """Something that can be used to checkout a particular state
-        (tag, commit) to "undo" a change or switch to a otherwise desired
-        previous state.
-
-        Parameters
-        ----------
-        whereto: str
-        """
-        if not self.is_installed():
-            raise RuntimeError(
-                "cannot remember a state when a dataset is not yet installed")
-        self.repo.checkout(whereto)
-
-    def is_installed(self):
-        """Returns whether a dataset is installed.
-
-        A dataset is installed when a repository for it exists on the filesystem.
-
-        Returns
-        -------
-        bool
-        """
-        # do early check manually if path exists to not even ask git at all
-        exists_now = exists(self.path)
+            return r.content
 
-        was_once_installed = None
-        if exists_now:
-            was_once_installed = self.path is not None and \
-                                 self.repo is not None
-
-        if not exists_now or \
-                (was_once_installed and not GitRepo.is_valid_repo(self.path)):
-            # repo gone now, reset
-            self._repo = None
-            return False
+    def put(self, *args, **kwargs):
+        import requests
+        return self(requests.put, *args, **kwargs)
+
+    def post(self, *args, **kwargs):
+        import requests
+        return self(requests.post, *args, **kwargs)
+
+    def get(self, *args, **kwargs):
+        import requests
+        return self(requests.get, *args, **kwargs)
+
+    def upload_file(self, fname, files_url):
+        # In v2 API seems no easy way to "just upload".  Need to initiate,
+        # do uploads
+        # and finalize
+        # TODO: check if the file with the same name already available, and offer
+        # to remove/prune it
+        import os
+        from datalad.utils import md5sum
+        from datalad.ui import ui
+        file_rec = {'md5': md5sum(fname),
+                    'name': os.path.basename(fname),
+                    'size': os.stat(fname).st_size
+                    }
+        # Initiate upload
+        j = self.post(files_url, file_rec)
+        file_endpoint = j['location']
+        file_info = self.get(file_endpoint)
+        file_upload_info = self.get(file_info['upload_url'])
+
+        pbar = ui.get_progressbar(label=fname,  # fill_text=f.name,
+                                  total=file_rec['size'])
+        with open(fname, 'rb') as f:
+            for part in file_upload_info['parts']:
+                udata = dict(file_info, **part)
+                if part['status'] == 'PENDING':
+                    f.seek(part['startOffset'])
+                    data = f.read(part['endOffset'] - part['startOffset'] + 1)
+                    url = '{upload_url}/{partNo}'.format(**udata)
+                    ok = self.put(url, data=data, binary=True, return_json=False)
+                    assert ok == b'OK'
+                pbar.update(part['endOffset'], increment=False)
+            pbar.finish()
+
+        # complete upload
+        jcomplete = self.post(file_endpoint, return_json=False)
+        return file_info
+
+    def get_article_ids(self):
+        articles = self.get('account/articles')
+        ids = []
+        for item in articles or []:
+            self.ui.message(' {id} {url} - {title}'.format(**item))
+            ids.append(item['id'])
+        return ids
+
+    def create_article(self, title):
+        data = {
+            'title': title
+        }
+        # we could prefill more fields interactively if desired
+        result = self.post('account/articles', data=data)
+        result = self.get(result['location'])
+        return result
+
+
+def _get_default_title(dataset):
+    """Create default title as dataset directory[#UUID][@version]
+    with any of [] missing if not defined
+    """
+    from ..support.path import basename
+    title = basename(dataset.path)
+    if dataset.id:
+        title += "#{dataset.id}".format(**locals())
+    version = dataset.repo.describe()
+    if version:
+        title += "@{version}".format(**locals())
+    # 3 is minimal length. Just in case there is no UUID or version and dir
+    # is short
+    if len(title) < 3:
+        title += "0"*(3 - len(title))
+    return title
+
+
+def _enter_title(ui, dataset):
+    default = _get_default_title(dataset)
+    while True:
+        title = ui.question(
+            "Please enter the title (must be at least 3 characters long).",
+            title="New article",
+            default=default
+        )
+        if len(title) < 3:
+            ui.error("Title must be at least 3 characters long.")
         else:
-            return was_once_installed
+            return title
 
-    def get_superdataset(self, datalad_only=False, topmost=False,
-                         registered_only=False):
-        """Get the dataset's superdataset
-
-        Parameters
-        ----------
-        datalad_only : bool, optional
-          Either to consider only "datalad datasets" (with non-None
-          id), or (if False, which is default) - any git repository
-        topmost : bool, optional
-          Return the topmost super-dataset. Might then be the current one.
-        registered_only : bool, optional
-          Test whether any discovered superdataset actually contains the
-          dataset in question as a registered subdataset (as opposed to
-          just being located in a subdirectory without a formal relationship).
-
-        Returns
-        -------
-        Dataset or None
-        """
-        # TODO: return only if self is subdataset of the superdataset
-        #       (meaning: registered as submodule)?
-        path = self.path
-        sds_path = path if topmost else None
-        while path:
-            # normalize the path after adding .. so we guaranteed to not
-            # follow into original directory if path itself is a symlink
-            par_path = normpath(opj(path, pardir))
-            sds_path_ = get_dataset_root(par_path)
-            if sds_path_ is None:
-                # no more parents, use previous found
-                break
-
-            sds = Dataset(sds_path_)
-            if datalad_only:
-                # test if current git is actually a dataset?
-                # can't use ATM since we just autogenerate and ID, see
-                # https://github.com/datalad/datalad/issues/986
-                # if not sds.id:
-                if not sds.config.get('datalad.dataset.id', None):
-                    break
-            if registered_only:
-                if path not in sds.subdatasets(
-                        recursive=False, result_xfm='paths'):
-                    break
-
-            # That was a good candidate
-            sds_path = sds_path_
-            path = par_path
-            if not topmost:
-                # no looping
-                break
-
-        if sds_path is None:
-            # None was found
-            return None
-
-        # No postprocessing now should be necessary since get_toppath
-        # tries its best to not resolve symlinks now
-
-        return Dataset(sds_path)
-
-
-@optional_args
-def datasetmethod(f, name=None, dataset_argname='dataset'):
-    """Decorator to bind functions to Dataset class.
-
-    The decorated function is still directly callable and additionally serves
-    as method `name` of class Dataset.  To achieve this, the first positional
-    argument is redirected to original keyword argument 'dataset_argname'. All
-    other arguments stay in order (and keep their names, of course). That
-    means, that the signature of the bound function is name(self, a, b) if the
-    original signature is name(a, dataset, b) for example.
 
-    The decorator has no effect on the actual function decorated with it.
-    """
-    if not name:
-        name = f.func_name if PY2 else f.__name__
+@build_doc
+class ExportToFigshare(Interface):
+    """Export the content of a dataset as a ZIP archive to figshare
 
-    @wrapt.decorator
-    def apply_func(wrapped, instance, args, kwargs):
-        # Wrapper function to assign arguments of the bound function to
-        # original function.
-        #
-        # Note
-        # ----
-        # This wrapper is NOT returned by the decorator, but only used to bind
-        # the function `f` to the Dataset class.
-
-        kwargs = kwargs.copy()
-        from inspect import getargspec
-        orig_pos = getargspec(f).args
-
-        # If bound function is used with wrong signature (especially by
-        # explicitly passing a dataset, let's raise a proper exception instead
-        # of a 'list index out of range', that is not very telling to the user.
-        if len(args) >= len(orig_pos):
-            raise TypeError("{0}() takes at most {1} arguments ({2} given):"
-                            " {3}".format(name, len(orig_pos), len(args),
-                                          ['self'] + [a for a in orig_pos
-                                                      if a != dataset_argname]))
-        if dataset_argname in kwargs:
-            raise TypeError("{}() got an unexpected keyword argument {}"
-                            "".format(name, dataset_argname))
-        kwargs[dataset_argname] = instance
-        ds_index = orig_pos.index(dataset_argname)
-        for i in range(0, len(args)):
-            if i < ds_index:
-                kwargs[orig_pos[i]] = args[i]
-            elif i >= ds_index:
-                kwargs[orig_pos[i+1]] = args[i]
-        return f(**kwargs)
-
-    setattr(Dataset, name, apply_func(f))
-    return f
-
-
-# Note: Cannot be defined within constraints.py, since then dataset.py needs to
-# be imported from constraints.py, which needs to be imported from dataset.py
-# for another constraint
-class EnsureDataset(Constraint):
-
-    def __call__(self, value):
-        if isinstance(value, Dataset):
-            return value
-        elif isinstance(value, string_types):
-            return Dataset(path=value)
-        else:
-            raise ValueError("Can't create Dataset from %s." % type(value))
+    Very quick and dirty approach.  Ideally figshare should be supported as
+    a proper git annex special remote.  Unfortunately, figshare does not support
+    having directories, and can store only a flat list of files.  That makes
+    it impossible for any sensible publishing of complete datasets.
 
-    def short_description(self):
-        return "Dataset"
-
-    def long_description(self):
-        return """Value must be a Dataset or a valid identifier of a Dataset
-        (e.g. a path)"""
-
-
-def require_dataset(dataset, check_installed=True, purpose=None):
-    """Helper function to resolve a dataset.
-
-    This function tries to resolve a dataset given an input argument,
-    or based on the process' working directory, if `None` is given.
-
-    Parameters
-    ----------
-    dataset : None or path or Dataset
-      Some value identifying a dataset or `None`. In the latter case
-      a dataset will be searched based on the process working directory.
-    check_installed : bool, optional
-      If True, an optional check whether the resolved dataset is
-      properly installed will be performed.
-    purpose : str, optional
-      This string will be inserted in error messages to make them more
-      informative. The pattern is "... dataset for <STRING>".
-
-    Returns
-    -------
-    Dataset
-      Or raises an exception (InsufficientArgumentsError).
+    The only workaround is to publish dataset as a zip-ball, where the entire
+    content is wrapped into a .zip archive for which figshare would provide a
+    navigator.
     """
-    if dataset is not None and not isinstance(dataset, Dataset):
-        dataset = Dataset(dataset)
-
-    if dataset is None:  # possible scenario of cmdline calls
-        dspath = get_dataset_root(getpwd())
-        if not dspath:
-            raise NoDatasetArgumentFound("No dataset found")
-        dataset = Dataset(dspath)
-
-    assert(dataset is not None)
-    lgr.debug("Resolved dataset{0}: {1}".format(
-        ' for {}'.format(purpose) if purpose else '',
-        dataset))
-
-    if check_installed and not dataset.is_installed():
-        raise ValueError("No installed dataset found at "
-                         "{0}.".format(dataset.path))
 
-    return dataset
+    from datalad.support.param import Parameter
+    from datalad.distribution.dataset import datasetmethod
+    from datalad.interface.base import eval_results
+    from datalad.distribution.dataset import EnsureDataset
+    from datalad.support.constraints import (
+        EnsureChoice,
+        EnsureInt,
+        EnsureNone,
+        EnsureStr,
+    )
+
+    _params_ = dict(
+        dataset=Parameter(
+            args=("-d", "--dataset"),
+            doc=""""specify the dataset to export. If no dataset is given, an
+            attempt is made to identify the dataset based on the current
+            working directory.""",
+            constraints=EnsureDataset() | EnsureNone()),
+        filename=Parameter(
+            args=("filename",),
+            metavar="PATH",
+            nargs='?',
+            doc="""File name of the generated ZIP archive. If no file name is
+            given the archive will be generated in the top directory
+            of the dataset and will be named: datalad_<dataset_uuid>.zip.""",
+            constraints=EnsureStr() | EnsureNone()),
+        no_annex=Parameter(
+            args=("--no-annex",),
+            action="store_true",
+            doc="""By default the generated .zip file would be added to annex,
+            and all files would get registered in git-annex to be available
+            from such a tarball. Also upon upload we will register for that
+            archive to be a possible source for it in annex. Setting this flag
+            disables this behavior."""),
+        missing_content=Parameter(
+            args=("--missing-content",),
+            doc="""By default, any discovered file with missing content will
+            result in an error and the plugin is aborted. Setting this to
+            'continue' will issue warnings instead of failing on error. The
+            value 'ignore' will only inform about problem at the 'debug' log
+            level. The latter two can be helpful when generating a TAR archive
+            from a dataset where some file content is not available
+            locally.""",
+            constraints=EnsureChoice("error", "continue", "ignore")),
+        # article_id=Parameter(
+        #     args=("--project-id",),
+        #     metavar="ID",
+        #     doc="""If given, article (if article_id is not provided) will be
+        #     created in that project.""",
+        #     constraints=EnsureInt() | EnsureNone()),
+        article_id=Parameter(
+            args=("--article-id",),
+            metavar="ID",
+            doc="""Which article to publish to.""",
+            constraints=EnsureInt() | EnsureNone()),
+    )
+
+    @staticmethod
+    @datasetmethod(name='export_to_figshare')
+    @eval_results
+    # TODO*: yet another former plugin with dataset first -- do we need that???
+    def __call__(filename=None,
+                 *,
+                 dataset=None,
+                 missing_content='error', no_annex=False,
+                 # TODO: support working with projects and articles within them
+                 # project_id=None,
+                 article_id=None):
+        import logging
+        lgr = logging.getLogger('datalad.plugin.export_to_figshare')
+
+        from datalad.ui import ui
+        from datalad.api import add_archive_content
+        from datalad.api import export_archive
+        from datalad.distribution.dataset import require_dataset
+        from datalad.support.annexrepo import AnnexRepo
+
+        dataset = require_dataset(dataset, check_installed=True,
+                                  purpose='export to figshare')
+
+        if not isinstance(dataset.repo, AnnexRepo):
+            raise ValueError(
+                "%s is not an annex repo, so annexification could be done"
+                % dataset
+            )
+
+        if dataset.repo.dirty:
+            yield get_status_dict(
+                'export_to_figshare',
+                ds=dataset,
+                status='impossible',
+                message=(
+                    'clean dataset required to export; '
+                    'use `datalad status` to inspect unsaved changes'))
+            return
+        if filename is None:
+            filename = dataset.path
+        lgr.info(
+            "Exporting current tree as an archive under %s since figshare "
+            "does not support directories",
+            filename
+        )
+        archive_out = next(
+            export_archive(
+                dataset=dataset,
+                filename=filename,
+                archivetype='zip',
+                missing_content=missing_content,
+                return_type="generator"
+            )
+        )
+        assert archive_out['status'] == 'ok'
+        fname = str(archive_out['path'])
+
+        lgr.info("Uploading %s to figshare", fname)
+        figshare = FigshareRESTLaison()
+
+        if not article_id:
+            # TODO: ask if it should be an article within a project
+            if ui.is_interactive:
+                # or should we just upload to a new article?
+                if ui.yesno(
+                    "Would you like to create a new article to upload to?  "
+                    "If not - we will list existing articles",
+                    title="Article"
+                ):
+                    article = figshare.create_article(
+                        title=_enter_title(ui, dataset)
+                    )
+                    lgr.info(
+                        "Created a new (private) article %(id)s at %(url_private_html)s. "
+                        "Please visit it, enter additional meta-data and make public",
+                        article
+                    )
+                    article_id = article['id']
+                else:
+                    article_id = int(ui.question(
+                        "Which of the articles should we upload to.",
+                        choices=list(map(str, figshare.get_article_ids()))
+                    ))
+            if not article_id:
+                raise ValueError("We need an article to upload to.")
+
+        file_info = figshare.upload_file(
+            fname,
+            files_url='account/articles/%s/files' % article_id
+        )
+
+        if no_annex:
+            lgr.info("Removing generated tarball")
+            unlink(fname)
+        else:
+            # I will leave all the complaining etc to the dataset add if path
+            # is outside etc
+            lgr.info("'Registering' %s within annex", fname)
+            repo = dataset.repo
+            repo.add(fname, git=False)
+            key = repo.get_file_annexinfo(fname)['key']
+            lgr.info("Adding URL %(download_url)s for it", file_info)
+            repo.call_annex([
+                "registerurl", '-c', 'annex.alwayscommit=false',
+                key, file_info['download_url']])
+
+            lgr.info("Registering links back for the content of the archive")
+            add_archive_content(
+                fname,
+                dataset=dataset,
+                delete_after=True,  # just remove extracted into a temp dir
+                allow_dirty=True,  # since we have a tarball
+                commit=False  # we do not want to commit anything we have done here
+            )
+
+            lgr.info("Removing generated and now registered in annex archive")
+            repo.drop(key, key=True, options=['--force'])
+            repo.remove(fname, force=True)  # remove the tarball
+
+            # if annex in {'delete'}:
+            #     dataset.repo.remove(fname)
+            # else:
+            #     # kinda makes little sense I guess.
+            #     # Made more sense if export_archive could export an arbitrary treeish
+            #     # so we could create a branch where to dump and export to figshare
+            #     # (kinda closer to my idea)
+            #     dataset.save(fname, message="Added the entire dataset into a zip file")
+
+        # TODO: add to downloader knowledge about figshare token so it could download-url
+        # those zipballs before they go public
+        yield dict(
+            status='ok',
+            # TODO: add article url (which needs to be queried if only ID is known
+            message="Published archive {}".format(
+                file_info['download_url']),
+            file_info=file_info,
+            path=dataset,
+            action='export_to_figshare',
+            logger=lgr
+        )
```

### Comparing `datalad-0.9.3/datalad/distribution/get.py` & `datalad-1.0.0/datalad/core/local/status.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,675 +1,657 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""High-level interface for getting dataset content
+"""Report status of a dataset (hierarchy)'s work tree"""
 
-"""
-
-import logging
-
-from os.path import join as opj
-from os.path import relpath
+__docformat__ = 'restructuredtext'
 
-from six.moves.urllib.parse import quote as urlquote
 
-from datalad.interface.base import Interface
-from datalad.interface.annotate_paths import AnnotatePaths
-from datalad.interface.annotate_paths import annotated2content_by_ds
-from datalad.interface.utils import eval_results
-from datalad.interface.base import build_doc
-from datalad.interface.results import get_status_dict
-from datalad.interface.results import results_from_paths
-from datalad.interface.results import annexjson2result
-from datalad.interface.results import count_results
-from datalad.interface.results import success_status_map
-from datalad.interface.results import results_from_annex_noinfo
-from datalad.interface.common_opts import recursion_flag
-# from datalad.interface.common_opts import git_opts
-# from datalad.interface.common_opts import annex_opts
-# from datalad.interface.common_opts import annex_get_opts
-from datalad.interface.common_opts import location_description
-from datalad.interface.common_opts import jobs_opt
-from datalad.interface.common_opts import reckless_opt
-from datalad.interface.common_opts import verbose
-from datalad.support.constraints import EnsureInt
-from datalad.support.constraints import EnsureChoice
-from datalad.support.constraints import EnsureStr
-from datalad.support.constraints import EnsureNone
+import logging
+import os
+import os.path as op
+from collections import OrderedDict
+import warnings
+
+from datalad.utils import (
+    bytes2human,
+    ensure_list,
+    ensure_unicode,
+    get_dataset_root,
+)
+from datalad.interface.base import (
+    Interface,
+    build_doc,
+    eval_results,
+)
+from datalad.interface.common_opts import (
+    recursion_limit,
+    recursion_flag,
+)
+from datalad.interface.utils import generic_result_renderer
+import datalad.support.ansi_colors as ac
 from datalad.support.param import Parameter
-from datalad.support.annexrepo import AnnexRepo
-from datalad.support.gitrepo import GitRepo
-from datalad.support.exceptions import InsufficientArgumentsError
-from datalad.support.exceptions import InstallFailedError
-from datalad.support.exceptions import IncompleteResultsError
-from datalad.support.network import URL
-from datalad.support.network import RI
-from datalad.dochelpers import exc_str
-from datalad.dochelpers import single_or_plural
-from datalad.utils import get_dataset_root
-from datalad.utils import with_pathsep as _with_sep
-from datalad.utils import unique
-from datalad.utils import path_startswith
-from datalad.utils import path_is_subpath
-
-from .dataset import Dataset
-from .dataset import EnsureDataset
-from .dataset import datasetmethod
-from .clone import Clone
-from .utils import _get_flexible_source_candidates
-from .utils import _get_tracking_source
-from .utils import _fixup_submodule_dotgit_setup
+from datalad.support.constraints import (
+    EnsureChoice,
+    EnsureNone,
+    EnsureStr,
+)
+from datalad.distribution.dataset import (
+    Dataset,
+    EnsureDataset,
+    datasetmethod,
+    require_dataset,
+    resolve_path,
+    path_under_rev_dataset,
+)
 
-__docformat__ = 'restructuredtext'
+import datalad.utils as ut
 
-lgr = logging.getLogger('datalad.distribution.get')
+from datalad.dochelpers import single_or_plural
 
+lgr = logging.getLogger('datalad.core.local.status')
 
-def _get_flexible_source_candidates_for_submodule(ds, sm_path, sm_url=None):
-    """Retrieve candidates from where to install the submodule
+_common_diffstatus_params = dict(
+    dataset=Parameter(
+        args=("-d", "--dataset"),
+        doc="""specify the dataset to query.  If
+        no dataset is given, an attempt is made to identify the dataset
+        based on the current working directory""",
+        constraints=EnsureDataset() | EnsureNone()),
+    annex=Parameter(
+        args=('--annex',),
+        # the next two enable a sole `--annex` that auto-translates to
+        # `--annex basic`
+        const='basic',
+        nargs='?',
+        constraints=EnsureChoice(None, 'basic', 'availability', 'all'),
+        doc="""Switch whether to include information on the annex
+        content of individual files in the status report, such as
+        recorded file size. By default no annex information is reported
+        (faster). Three report modes are available: basic information
+        like file size and key name ('basic'); additionally test whether
+        file content is present in the local annex ('availability';
+        requires one or two additional file system stat calls, but does
+        not call git-annex), this will add the result properties
+        'has_content' (boolean flag) and 'objloc' (absolute path to an
+        existing annex object file); or 'all' which will report all
+        available information (presently identical to 'availability').
+        [CMD: The 'basic' mode will be assumed when this option is given,
+        but no mode is specified. CMD]
+        """),
+    untracked=Parameter(
+        args=('--untracked',),
+        constraints=EnsureChoice('no', 'normal', 'all'),
+        doc="""If and how untracked content is reported when comparing
+        a revision to the state of the working tree. 'no': no untracked
+        content is reported; 'normal': untracked files and entire
+        untracked directories are reported as such; 'all': report
+        individual files even in fully untracked directories."""),
+    recursive=recursion_flag,
+    recursion_limit=recursion_limit)
+
+
+STATE_COLOR_MAP = {
+    'untracked': ac.RED,
+    'modified': ac.RED,
+    'deleted': ac.RED,
+    'added': ac.GREEN,
+    'unknown': ac.YELLOW,
+}
+
+
+def yield_dataset_status(ds, paths, annexinfo, untracked, recursion_limit,
+                         queried, eval_submodule_state, eval_filetype, cache,
+                         reporting_order):
+    """Internal helper to obtain status information on a dataset
 
-    Even if url for submodule is provided explicitly -- first tries urls under
-    parent's module tracking branch remote.
+    Parameters
+    ----------
+    ds : Dataset
+      Dataset to get the status of.
+    path : Path-like, optional
+      Paths to constrain the status to (see main status() command).
+    annexinfo : str
+      Annex information reporting mode (see main status() command).
+    untracked : str, optional
+      Reporting mode for untracked content (see main status() command).
+    recursion_limit : int, optional
+    queried : set
+      Will be populated with a Path instance for each queried dataset.
+    eval_submodule_state : str
+      Submodule evaluation mode setting for Repo.diffstatus().
+    eval_filetype : bool, optional
+      THIS OPTION IS IGNORED. It will be removed in a future release.
+    cache : dict
+      Cache to be passed on to all Repo.diffstatus() calls to avoid duplicate
+      queries.
+    reporting_order : {'depth-first', 'breadth-first'}, optional
+      By default, subdataset content records are reported after the record
+      on the subdataset's submodule in a superdataset (depth-first).
+      Alternatively, report all superdataset records first, before reporting
+      any subdataset content records (breadth-first).
+
+    Yields
+    ------
+    dict
+      DataLad result records.
     """
-    clone_urls = []
+    if eval_filetype is not None:
+        warnings.warn(
+            "yield_dataset_status(eval_filetype=) no longer supported, "
+            "and will be removed in a future release",
+            DeprecationWarning)
 
-    # should be our first candidate
-    tracking_remote, tracking_branch = ds.repo.get_tracking_branch()
-    candidate_remotes = [tracking_remote] if tracking_remote else []
-
-    # if we have a remote, let's check the location of that remote
-    # for the presence of the desired submodule
-    try:
-        last_commit = next(ds.repo._get_files_history(sm_path)).hexsha
-        # ideally should also give preference to the remotes which have
-        # the same branch checked out I guess
-        candidate_remotes += list(ds.repo._get_remotes_having_commit(last_commit))
-    except StopIteration:
-        # no commit for it known yet, ... oh well
-        pass
-
-    for remote in unique(candidate_remotes):
-        remote_url = ds.repo.get_remote_url(remote, push=False)
-
-        # Directly on parent's ds url
-        if remote_url:
-            # attempt: submodule checkout at parent remote URL
-            # We might need to quote sm_path portion, e.g. for spaces etc
-            if isinstance(RI(remote_url), URL):
-                sm_path_url = urlquote(sm_path)
-            else:
-                sm_path_url = sm_path
+    if reporting_order not in ('depth-first', 'breadth-first'):
+        raise ValueError('Unknown reporting order: {}'.format(reporting_order))
 
-            clone_urls.extend(
-                _get_flexible_source_candidates(
-                    # alternate suffixes are tested by `clone` anyways
-                    sm_path_url, remote_url, alternate_suffix=False))
-
-            # attempt: provided (configured?) submodule URL
-            # TODO: consider supporting DataLadRI here?  or would confuse
-            #  git and we wouldn't want that (i.e. not allow pure git clone
-            #  --recursive)
-            if sm_url:
-                clone_urls += _get_flexible_source_candidates(
-                    sm_url,
-                    remote_url,
-                    alternate_suffix=False
+    if ds.pathobj in queried:
+        # do not report on a single dataset twice
+        return
+    # take the dataset that went in first
+    repo = ds.repo
+    repo_path = repo.pathobj
+    lgr.debug('Querying %s.diffstatus() for paths: %s', repo, paths)
+    # recode paths with repo reference for low-level API
+    paths = [repo_path / p.relative_to(ds.pathobj) for p in paths] if paths else None
+    status = repo.diffstatus(
+        fr='HEAD' if repo.get_hexsha() else None,
+        to=None,
+        paths=paths,
+        untracked=untracked,
+        eval_submodule_state=eval_submodule_state,
+        _cache=cache)
+    if annexinfo and hasattr(repo, 'get_content_annexinfo'):
+        if paths:
+            # when an annex query has been requested for specific paths,
+            # exclude untracked files from the annex query (else gh-7032)
+            untracked = [k for k, v in status.items() if
+                         v['state'] == 'untracked']
+            lgr.debug(
+                'Skipping %s.get_content_annexinfo() for untracked paths: %s',
+                repo, paths)
+            [paths.remove(p) for p in untracked]
+        lgr.debug('Querying %s.get_content_annexinfo() for paths: %s', repo, paths)
+        # this will amend `status`
+        repo.get_content_annexinfo(
+            paths=paths,
+            init=status,
+            eval_availability=annexinfo in ('availability', 'all'),
+            ref=None)
+    # potentially collect subdataset status call specs for the end
+    # (if order == 'breadth-first')
+    subds_statuscalls = []
+    for path, props in status.items():
+        cpath = ds.pathobj / path.relative_to(repo_path)
+        yield dict(
+            props,
+            path=str(cpath),
+            # report the dataset path rather than the repo path to avoid
+            # realpath/symlink issues
+            parentds=ds.path,
+        )
+        queried.add(ds.pathobj)
+        if recursion_limit and props.get('type', None) == 'dataset':
+            if cpath == ds.pathobj:
+                # ATM can happen if there is something wrong with this repository
+                # We will just skip it here and rely on some other exception to bubble up
+                # See https://github.com/datalad/datalad/pull/4526 for the usecase
+                lgr.debug("Got status for itself, which should not happen, skipping %s", path)
+                continue
+            subds = Dataset(str(cpath))
+            if subds.is_installed():
+                call_args = (
+                    subds,
+                    None,
+                    annexinfo,
+                    untracked,
+                    recursion_limit - 1,
+                    queried,
+                    eval_submodule_state,
+                    None,
+                    cache,
                 )
+                call_kwargs = dict(
+                    reporting_order='depth-first',
+                )
+                if reporting_order == 'depth-first':
+                    yield from yield_dataset_status(*call_args, **call_kwargs)
+                else:
+                    subds_statuscalls.append((call_args, call_kwargs))
+
+    # deal with staged subdataset status calls
+    for call_args, call_kwargs in subds_statuscalls:
+        yield from yield_dataset_status(*call_args, **call_kwargs)
 
-    # Do based on the ds.path as the last resort
-    if sm_url:
-        clone_urls += _get_flexible_source_candidates(
-            sm_url,
-            ds.path,
-            alternate_suffix=False)
-
-    return unique(clone_urls)
-
-
-def _install_subds_from_flexible_source(
-        ds, sm_path, sm_url, reckless, description=None):
-    """Tries to obtain a given subdataset from several meaningful locations"""
-    # TODO remove this assertion eventually, for now it assures intented
-    # usage of this helper function
-    assert(sm_path in ds.subdatasets(recursive=False, result_xfm='relpaths'))
-
-    # compose a list of candidate clone URLs
-    clone_urls = _get_flexible_source_candidates_for_submodule(
-        ds, sm_path, sm_url)
-
-    # prevent inevitable exception from `clone`
-    dest_path = opj(ds.path, sm_path)
-    clone_urls = [src for src in clone_urls if src != dest_path]
-
-    if not clone_urls:
-        raise InstallFailedError(
-            msg="Have got no candidates to install subdataset {} from".format(
-                sm_path))
-
-    # now loop over all candidates and try to clone
-    subds = None
-    try:
-        subds = Clone.__call__(
-            clone_urls[0],
-            path=dest_path,
-            # pretend no parent -- we don't want clone to add to ds
-            # because this is a submodule already!
-            dataset=None,
-            reckless=reckless,
-            # if we have more than one source, pass as alternatives
-            alt_sources=clone_urls[1:],
-            description=description,
-            result_xfm='datasets',
-            # not really need, but should protect against future RF
-            on_failure='stop',
-            result_renderer='disabled',
-            return_type='item-or-list')
-        # failure will raise an exception, hence if we got here we can
-        # leave the loop and have a successful clone
-    except IncompleteResultsError:
-        # details of the failure are logged already by common code
-        pass
-    except Exception as exc:
-        lgr.warning("Something went wrong while installing %s: %s",
-                    sm_path, exc_str(exc))
-    if subds is None:
-        raise InstallFailedError(
-            msg="Failed to install dataset from{}: {}".format(
-                ' any of' if len(clone_urls) > 1 else '',
-                clone_urls))
-
-    assert(subds.is_installed())
-    _fixup_submodule_dotgit_setup(ds, sm_path)
-
-    # do fancy update
-    lgr.debug("Update cloned subdataset {0} in parent".format(subds))
-    # TODO: move all of that into update_submodule ??
-    # TODO: direct mode ramifications?
-    # track branch originally cloned
-    subrepo = subds.repo
-    branch = subrepo.get_active_branch()
-    branch_hexsha = subrepo.get_hexsha(branch)
-    ds.repo.update_submodule(sm_path, init=True)
-    updated_branch = subrepo.get_active_branch()
-    if branch and not updated_branch:
-        # got into 'detached' mode
-        # trace if current state is a predecessor of the branch_hexsha
-        lgr.debug(
-            "Detected detached HEAD after updating submodule %s which was "
-            "in %s branch before", subds.path, branch)
-        detached_hexsha = subrepo.get_hexsha()
-        if subrepo.get_merge_base(
-                [branch_hexsha, detached_hexsha]) == detached_hexsha:
-            # TODO: config option?
-            # in all likely event it is of the same branch since
-            # it is an ancestor -- so we could update that original branch
-            # to point to the state desired by the submodule, and update
-            # HEAD to point to that location
-            lgr.info(
-                "Submodule HEAD got detached. Resetting branch %s to point "
-                "to %s. Original location was %s",
-                branch, detached_hexsha[:8], branch_hexsha[:8]
-            )
-            branch_ref = 'refs/heads/%s' % branch
-            subrepo.update_ref(branch_ref, detached_hexsha)
-            assert(subrepo.get_hexsha(branch) == detached_hexsha)
-            subrepo.update_ref('HEAD', branch_ref, symbolic=True)
-            assert(subrepo.get_active_branch() == branch)
-        else:
-            lgr.warning(
-                "%s has a detached HEAD since cloned branch %s has another common ancestor with %s",
-                subrepo.path, branch, detached_hexsha[:8]
-            )
-    return subds
-
-
-def _install_necessary_subdatasets(
-        ds, path, reckless, refds_path, description=None):
-    """Installs subdatasets of `ds`, that are necessary to obtain in order
-    to have access to `path`.
-
-    Gets the subdataset containing `path` regardless of whether or not it was
-    already installed. While doing so, installs everything necessary in between
-    the uppermost installed one and `path`.
 
-    Note: `ds` itself has to be installed.
+@build_doc
+class Status(Interface):
+    """Report on the state of dataset content.
 
-    Parameters
-    ----------
-    ds: Dataset
-    path: str
-    reckless: bool
-    """
-    # figuring out what dataset to start with, --contains limits --recursive
-    # to visit only subdataset on the trajectory to the target path
-    subds_trail = ds.subdatasets(contains=path, recursive=True)
-    if not subds_trail:
-        # there is not a single known subdataset (installed or not)
-        # for this path -- job done
-        return
-    # otherwise we start with the one deepest down
-    cur_subds = subds_trail[-1]
+    This is an analog to `git status` that is simultaneously crippled and more
+    powerful. It is crippled, because it only supports a fraction of the
+    functionality of its counter part and only distinguishes a subset of the
+    states that Git knows about. But it is also more powerful as it can handle
+    status reports for a whole hierarchy of datasets, with the ability to
+    report on a subset of the content (selection of paths) across any number
+    of datasets in the hierarchy.
 
-    while not GitRepo.is_valid_repo(cur_subds['path']):
-        # install using helper that give some flexibility regarding where to
-        # get the module from
-        try:
-            sd = _install_subds_from_flexible_source(
-                Dataset(cur_subds['parentds']),
-                relpath(cur_subds['path'], start=cur_subds['parentds']),
-                cur_subds['gitmodule_url'],
-                reckless,
-                description=description)
-        except Exception as e:
-            # skip all of downstairs, if we didn't manage to install subdataset
-            yield get_status_dict(
-                'install', path=cur_subds['path'], type='dataset',
-                status='error', logger=lgr, refds=refds_path,
-                message=("Installation of subdatasets %s failed with exception: %s",
-                         cur_subds['path'], exc_str(e)))
-            return
+    *Path conventions*
 
-        # report installation, whether it helped or not
-        yield get_status_dict(
-            'install', ds=sd, status='ok', logger=lgr, refds=refds_path,
-            message=("Installed subdataset in order to get %s", path))
-
-        # now check whether the just installed subds brought us any closer to
-        # the target path
-        subds_trail = sd.subdatasets(contains=path, recursive=False)
-        if not subds_trail:
-            # no (newly available) subdataset get's us any closer
-            return
-        # next round
-        cur_subds = subds_trail[-1]
+    All reports are guaranteed to use absolute paths that are underneath the
+    given or detected reference dataset, regardless of whether query paths are
+    given as absolute or relative paths (with respect to the working directory,
+    or to the reference dataset, when such a dataset is given explicitly).
+    Moreover, so-called "explicit relative paths" (i.e. paths that start with
+    '.' or '..') are also supported, and are interpreted as relative paths with
+    respect to the current working directory regardless of whether a reference
+    dataset with specified.
 
+    When it is necessary to address a subdataset record in a superdataset
+    without causing a status query for the state _within_ the subdataset
+    itself, this can be achieved by explicitly providing a reference dataset
+    and the path to the root of the subdataset like so::
 
-def _recursive_install_subds_underneath(ds, recursion_limit, reckless, start=None,
-                                        refds_path=None, description=None):
-    if isinstance(recursion_limit, int) and recursion_limit <= 0:
-        return
-    # install using helper that give some flexibility regarding where to
-    # get the module from
-    for sub in ds.subdatasets(
-            return_type='generator', result_renderer='disabled'):
-        subds = Dataset(sub['path'])
-        if sub.get('gitmodule_datalad-recursiveinstall', '') == 'skip':
-            lgr.debug(
-                "subdataset %s is configured to be skipped on recursive installation",
-                sub['path'])
-            continue
-        if start is not None and not path_is_subpath(subds.path, start):
-            # this one we can ignore, not underneath the start path
-            continue
-        if sub['state'] != 'absent':
-            # dataset was already found to exist
-            yield get_status_dict(
-                'install', ds=subds, status='notneeded', logger=lgr,
-                refds=refds_path)
-            # do not continue, even if an intermediate dataset exists it
-            # does not imply that everything below it does too
-        else:
-            # try to get this dataset
-            try:
-                subds = _install_subds_from_flexible_source(
-                    ds,
-                    relpath(sub['path'], start=ds.path),
-                    sub['gitmodule_url'],
-                    reckless,
-                    description=description)
-                yield get_status_dict(
-                    'install', ds=subds, status='ok', logger=lgr, refds=refds_path,
-                    message=("Installed subdataset %s", subds), parentds=ds.path)
-            except Exception as e:
-                # skip all of downstairs, if we didn't manage to install subdataset
-                yield get_status_dict(
-                    'install', ds=subds, status='error', logger=lgr, refds=refds_path,
-                    message=("Installation of subdatasets %s failed with exception: %s",
-                             subds, exc_str(e)))
-                continue
-        # otherwise recurse
-        # we can skip the start expression, we know we are within
-        for res in _recursive_install_subds_underneath(
-                subds,
-                recursion_limit=recursion_limit - 1 if isinstance(recursion_limit, int) else recursion_limit,
-                reckless=reckless,
-                refds_path=refds_path):
-            yield res
+      datalad status --dataset . subdspath
 
+    In contrast, when the state of the subdataset within the superdataset is
+    not relevant, a status query for the content of the subdataset can be
+    obtained by adding a trailing path separator to the query path (rsync-like
+    syntax)::
 
-@build_doc
-class Get(Interface):
-    """Get any dataset content (files/directories/subdatasets).
+      datalad status --dataset . subdspath/
+
+    When both aspects are relevant (the state of the subdataset content
+    and the state of the subdataset within the superdataset), both queries
+    can be combined::
 
-    This command only operates on dataset content. To obtain a new independent
-    dataset from some source use the `install` command.
+      datalad status --dataset . subdspath subdspath/
 
-    By default this command operates recursively within a dataset, but not
-    across potential subdatasets, i.e. if a directory is provided, all files in
-    the directory are obtained. Recursion into subdatasets is supported too. If
-    enabled, relevant subdatasets are detected and installed in order to
-    fulfill a request.
-
-    Known data locations for each requested file are evaluated and data are
-    obtained from some available location (according to git-annex configuration
-    and possibly assigned remote priorities), unless a specific source is
-    specified.
-
-    .. note::
-      Power-user info: This command uses :command:`git annex get` to fulfill
-      file handles.
+    When performing a recursive status query, both status aspects of subdataset
+    are always included in the report.
+
+
+    *Content types*
+
+    The following content types are distinguished:
+
+    - 'dataset' -- any top-level dataset, or any subdataset that is properly
+      registered in superdataset
+    - 'directory' -- any directory that does not qualify for type 'dataset'
+    - 'file' -- any file, or any symlink that is placeholder to an annexed
+      file when annex-status reporting is enabled
+    - 'symlink' -- any symlink that is not used as a placeholder for an annexed
+      file
+
+    *Content states*
+
+    The following content states are distinguished:
+
+    - 'clean'
+    - 'added'
+    - 'modified'
+    - 'deleted'
+    - 'untracked'
     """
+    # make the custom renderer the default one, as the global default renderer
+    # does not yield meaningful output for this command
+    result_renderer = 'tailored'
+    _examples_ = [
+        dict(text="Report on the state of a dataset",
+             code_py="status()",
+             code_cmd="datalad status"),
+        dict(text="Report on the state of a dataset and all subdatasets",
+             code_py="status(recursive=True)",
+             code_cmd="datalad status -r"),
+        dict(text="Address a subdataset record in a superdataset without "
+                  "causing a status query for the state _within_ the subdataset "
+                  "itself",
+             code_py="status(dataset='.', path='mysubdataset')",
+             code_cmd="datalad status -d . mysubdataset"),
+        dict(text="Get a status query for the state within the subdataset "
+                  "without causing a status query for the superdataset (using trailing "
+                  "path separator in the query path):",
+             code_py="status(dataset='.', path='mysubdataset/')",
+             code_cmd="datalad status -d . mysubdataset/"),
+        dict(text="Report on the state of a subdataset in a superdataset and "
+                  "on the state within the subdataset",
+             code_py="status(dataset='.', path=['mysubdataset', 'mysubdataset/'])",
+             code_cmd="datalad status -d . mysubdataset mysubdataset/"),
+        dict(text="Report the file size of annexed content in a dataset",
+             code_py="status(annex=True)",
+             code_cmd="datalad status --annex")
+    ]
 
     _params_ = dict(
-        dataset=Parameter(
-            args=("-d", "--dataset"),
-            metavar="PATH",
-            doc="""specify the dataset to perform the add operation on, in
-            which case `path` arguments are interpreted as being relative
-            to this dataset.  If no dataset is given, an attempt is made to
-            identify a dataset for each input `path`""",
-            constraints=EnsureDataset() | EnsureNone()),
+        _common_diffstatus_params,
         path=Parameter(
             args=("path",),
             metavar="PATH",
-            doc="""path/name of the requested dataset component. The component
-            must already be known to a dataset. To add new components to a
-            dataset use the `add` command""",
+            doc="""path to be evaluated""",
             nargs="*",
             constraints=EnsureStr() | EnsureNone()),
-        source=Parameter(
-            args=("-s", "--source",),
-            metavar="LABEL",
-            doc="""label of the data source to be used to fulfill requests.
-            This can be the name of a dataset :term:`sibling` or another known
-            source""",
-            constraints=EnsureStr() | EnsureNone()),
-        recursive=recursion_flag,
-        recursion_limit=Parameter(
-            args=("--recursion-limit",),
-            metavar="LEVELS",
-            constraints=EnsureInt() | EnsureChoice('existing') | EnsureNone(),
-            doc="""limit recursion into subdataset to the given number of levels.
-            Alternatively, 'existing' will limit recursion to subdatasets that already
-            existed on the filesystem at the start of processing, and prevent new
-            subdatasets from being obtained recursively."""),
-        get_data=Parameter(
-            args=("-n", "--no-data",),
-            dest='get_data',
-            action='store_false',
-            doc="""whether to obtain data for all file handles. If disabled, `get`
-            operations are limited to dataset handles.[CMD:  This option prevents data
-            for file handles from being obtained CMD]"""),
-        description=location_description,
-        reckless=reckless_opt,
-        # git_opts=git_opts,
-        # annex_opts=annex_opts,
-        # annex_get_opts=annex_get_opts,
-        jobs=jobs_opt,
-        verbose=verbose)
-
-    # Note: May be use 'git annex find --not --in here' to have a list of all
-    # files to actually get and give kind of a progress in terms of number
-    # files processed ...
+        eval_subdataset_state=Parameter(
+            args=("-e", "--eval-subdataset-state",),
+            constraints=EnsureChoice('no', 'commit', 'full'),
+            doc="""Evaluation of subdataset state (clean vs.
+            modified) can be expensive for deep dataset hierarchies
+            as subdataset have to be tested recursively for
+            uncommitted modifications. Setting this option to
+            'no' or 'commit' can substantially boost performance
+            by limiting what is being tested. With 'no' no state
+            is evaluated and subdataset result records typically do
+            not contain a 'state' property.
+            With 'commit' only a discrepancy of the HEAD commit
+            shasum of a subdataset and the shasum recorded in the
+            superdataset's record is evaluated,
+            and the 'state' result property only reflects this
+            aspect. With 'full' any other modification is considered
+            too (see the 'untracked' option for further tailoring
+            modification testing)."""),
+        report_filetype=Parameter(
+            args=("-t", "--report-filetype",),
+            constraints=EnsureChoice('raw', 'eval', None),
+            doc="""THIS OPTION IS IGNORED. It will be removed in a future
+            release. Dataset component types are always reported
+            as-is (previous 'raw' mode), unless annex-reporting is enabled
+            with the [CMD: --annex CMD][PY: `annex` PY] option, in which
+            case symlinks that represent annexed files will be reported
+            as type='file'."""),
+    )
 
     @staticmethod
-    @datasetmethod(name='get')
+    @datasetmethod(name='status')
     @eval_results
     def __call__(
             path=None,
-            source=None,
+            *,
             dataset=None,
+            annex=None,
+            untracked='normal',
             recursive=False,
             recursion_limit=None,
-            get_data=True,
-            description=None,
-            reckless=False,
-            #git_opts=None,
-            #annex_opts=None,
-            #annex_get_opts=None,
-            jobs=None,
-            verbose=False,
-    ):
-        # IMPLEMENTATION CONCEPT:
-        #
-        # 1. Sort the world into existing handles and the rest
-        # 2. Try locate missing handles (obtain subdatasets along the way)
-        # 3. Expand into subdatasets with recursion enables (potentially
-        #    obtain even more subdatasets
-        # 4. Shoot info of which handles to get in each subdataset to,
-        #    git-annex, once at the very end
-
-        refds_path = Interface.get_refds_path(dataset)
-        if not (dataset or path):
-            raise InsufficientArgumentsError(
-                "Neither dataset nor target path(s) provided")
-        if dataset and not path:
-            # act on the whole dataset if nothing else was specified
-            path = refds_path
-
-        # remember which results we already reported, to avoid duplicates
-        yielded_ds = []
-        to_get = []
-        unavailable_paths = []
-        for ap in AnnotatePaths.__call__(
-                path=path,
-                dataset=refds_path,
-                recursive=recursive,
-                recursion_limit=recursion_limit,
-                action='get',
-                # NOTE: Do not act upon unavailable paths yet! Done below after
-                # testing which ones could be obtained
-                unavailable_path_status='',
-                nondataset_path_status='impossible',
-                return_type='generator',
-                on_failure='ignore'):
-            if ap.get('status', None):
-                # we know what to report already
-                yield ap
-                continue
-            if ap.get('state', None) == 'absent' and ap.get('raw_input', False):
-                # if this wasn't found, but directly requested, queue for further
-                # exploration
-                unavailable_paths.append(ap)
-                continue
-            if ap.get('type', None) == 'dataset' and \
-                    GitRepo.is_valid_repo(ap['path']) and \
-                    not ap['path'] == refds_path:
-                # do not report what hasn't arived yet
-                # also do not report the base dataset that is already
-                # present -- no surprise
-                yield dict(ap, status='notneeded', logger=lgr,
-                           message='already installed')
-                yielded_ds.append(ap['path'])
-                ap['process_content'] = get_data
-            to_get.append(ap)
-
-        # explore the unknown
-        for ap in sorted(unavailable_paths, key=lambda x: x['path']):
-            lgr.debug("Investigate yet unavailable path %s", ap)
-            # how close can we get?
-            dspath = ap.get('parentds', get_dataset_root(ap['path']))
-            if dspath is None:
-                # nothing we can do for this path
-                continue
-            lgr.debug("Found containing dataset %s for path %s", dspath, ap['path'])
-            ds = Dataset(dspath)
-            # now actually obtain whatever is necessary to get to this path
-            containing_ds = [dspath]
-            for res in _install_necessary_subdatasets(
-                    ds, ap['path'], reckless, refds_path, description=description):
-                # yield immediately so errors could be acted upon outside, before
-                # we continue
-                if not (res['type'] == 'dataset' and res['path'] in yielded_ds):
-                    # unless we reported on this dataset before
-                    if res['type'] == 'dataset':
-                        # make a record, recursive below might now want to report
-                        # a 'notneeded'
-                        yielded_ds.append(res['path'])
-                    yield res
-                # update to the current innermost dataset
-                containing_ds.append(res['path'])
-
-            if len(containing_ds) < 2:
-                # no subdataset was installed, hence if the path was unavailable
-                # before it still is, no need to bother git annex
-                ap.update(status='impossible',
-                          message='path does not exist')
-                yield ap
+            eval_subdataset_state='full',
+            report_filetype=None):
+        if report_filetype is not None:
+            warnings.warn(
+                "status(report_filetype=) no longer supported, and will be removed "
+                "in a future release",
+                DeprecationWarning)
+
+        # To the next white knight that comes in to re-implement `status` as a
+        # special case of `diff`. There is one fundamental difference between
+        # the two commands: `status` can always use the worktree as evident on
+        # disk as a constraint (e.g. to figure out which subdataset a path is
+        # in) `diff` cannot do that (everything need to be handled based on a
+        # "virtual" representation of a dataset hierarchy).
+        # MIH concludes that while `status` can be implemented as a special case
+        # of `diff` doing so would complicate and slow down both `diff` and
+        # `status`. So while the apparent almost code-duplication between the
+        # two commands feels wrong, the benefit is speed. Any future RF should
+        # come with evidence that speed does not suffer, and complexity stays
+        # on a manageable level
+        ds = require_dataset(
+            dataset, check_installed=True, purpose='report status')
+        ds_path = ds.path
+        queried = set()
+        content_info_cache = {}
+        for res in _yield_paths_by_ds(ds, dataset, ensure_list(path)):
+            if 'status' in res:
+                # this is an error
+                yield res
                 continue
-            # important to only do the next for the innermost subdataset
-            # as the `recursive` logic below relies on that!
-            # set the correct parent, for a dataset this would be the second-last
-            # reported subdataset
-            ap.update(parentds=containing_ds[-1])
-            if containing_ds[-1] == ap['path']:
-                # the path actually refers to the last installed dataset
-                ap.update(parentds=containing_ds[-2],
-                          process_content=get_data,
-                          type='dataset')
-            to_get.append(ap)
-
-        # results of recursive installation of yet undiscovered datasets
-        rec_get = []
-        if recursive and not recursion_limit == 'existing':
-            # obtain any subdatasets underneath the paths given inside the
-            # subdatasets that we know already exist
-            # unless we do not want recursion into not-yet-installed datasets
-            for ap in sorted(to_get, key=lambda x: x['path']):
-                if ap['type'] not in ('dataset', 'directory') or not ap.get('raw_input', False):
-                    # a non-directory cannot have content underneath
-                    # also we do NOT want to recurse into anything that was specifically
-                    # requested, to avoid duplication
-                    continue
-                subds = Dataset(ap['path'] if ap['type'] == 'dataset' else ap['parentds'])
-                lgr.info(
-                    "Installing %s%s recursively",
-                    subds,
-                    (" underneath %s" % ap['path']
-                     if subds.path != ap['path']
-                     else ""))
-                for res in _recursive_install_subds_underneath(
-                        subds,
-                        # `ap['path']` was explicitly given as input
-                        # we count recursions from the input, hence we
-                        # can start with the full number
-                        recursion_limit,
-                        reckless,
-                        start=ap['path'],
-                        refds_path=refds_path,
-                        description=description):
-                    # yield immediately so errors could be acted upon
-                    # outside, before we continue
-                    if not (res['type'] == 'dataset' and res['path'] in yielded_ds):
-                        # unless we reported on this dataset before
-                        if res['type'] == 'dataset':
-                            # make a record
-                            yielded_ds.append(res['path'])
-                    yield res
-                    if not (res['status'] == 'ok' and res['type'] == 'dataset'):
-                        # not a dataset that was just installed, we just reported it
-                        # upstairs, and can ignore it from now on
-                        continue
-                    # paranoia, so popular these days...
-                    assert GitRepo.is_valid_repo(res['path'])
-                    # keep a copy of the install record for `get` later on
-                    get_ap = {k: v for k, v in res.items()
-                              if not k == 'status'}
-                    get_ap['process_content'] = get_data
-                    rec_get.append(get_ap)
+            for r in yield_dataset_status(
+                    res['ds'],
+                    res['paths'],
+                    annex,
+                    untracked,
+                    recursion_limit
+                    if recursion_limit is not None else -1
+                    if recursive else 0,
+                    queried,
+                    eval_subdataset_state,
+                    None,
+                    content_info_cache,
+                    reporting_order='depth-first'):
+                if 'status' not in r:
+                    r['status'] = 'ok'
+                yield dict(
+                    r,
+                    refds=ds_path,
+                    action='status',
+                )
 
-        if not get_data:
-            # done already
+    @staticmethod
+    def custom_result_renderer(res, **kwargs):  # pragma: more cover
+        if (res['status'] == 'ok' and res['action'] in ('status', 'diff')
+                and res.get('state') == 'clean'):
+            # this renderer will be silent for clean status|diff results
             return
+        if res['status'] != 'ok' or res['action'] not in ('status', 'diff'):
+            # whatever this renderer cannot account for, send to generic
+            generic_result_renderer(res)
+            return
+        from datalad.ui import ui
+        # when to render relative paths:
+        #  1) if a dataset arg was given
+        #  2) if CWD is the refds
+        refds = res.get('refds', None)
+        refds = refds if kwargs.get('dataset', None) is not None \
+            or refds == os.getcwd() else None
+        path = res['path'] if refds is None \
+            else str(ut.Path(res['path']).relative_to(refds))
+        type_ = res.get('type', res.get('type_src', ''))
+        max_len = len('untracked')
+        state = res.get('state', 'unknown')
+        ui.message(u'{fill}{state}: {path}{type_}'.format(
+            fill=' ' * max(0, max_len - len(state)),
+            state=ac.color_word(
+                state,
+                STATE_COLOR_MAP.get(res.get('state', 'unknown'))),
+            path=path,
+            type_=' ({})'.format(
+                ac.color_word(type_, ac.MAGENTA) if type_ else '')))
 
-        # merge the two AP lists
-        to_get.extend(rec_get)
-
-        # sort into datasets
-        content_by_ds, ds_props, completed, nondataset_paths = \
-            annotated2content_by_ds(
-                to_get,
-                refds_path=refds_path)
-        assert(not completed)
-
-        # hand over to git-annex, get files content,
-        # report files in git as 'notneeded' to get
-        for ds_path in sorted(content_by_ds.keys()):
-            ds = Dataset(ds_path)
-            # grab content, ignore subdataset entries
-            content = [ap['path'] for ap in content_by_ds[ds_path]
-                       if ap.get('type', None) != 'dataset' or ap['path'] == ds.path]
-            if not content:
-                # cut this short should there be nothing
-                continue
-            # needs to be an annex to get content
-            if not isinstance(ds.repo, AnnexRepo):
-                for r in results_from_paths(
-                        content, status='notneeded',
-                        message="no dataset annex, content already present",
-                        action='get', logger=lgr,
-                        refds=refds_path):
-                    yield r
-                continue
-            respath_by_status = {}
-            for res in ds.repo.get(
-                    content,
-                    options=['--from=%s' % source] if source else [],
-                    jobs=jobs):
-                res = annexjson2result(res, ds, type='file', logger=lgr,
-                                       refds=refds_path)
-                success = success_status_map[res['status']]
-                # TODO: in case of some failed commands (e.g. get) there might
-                # be no path in the record.  yoh has only vague idea of logic
-                # here so just checks for having 'path', but according to
-                # results_from_annex_noinfo, then it would be assumed that
-                # `content` was acquired successfully, which is not the case
-                if 'path' in res:
-                    respath_by_status[success] = \
-                        respath_by_status.get(success, []) + [res['path']]
-                yield res
+    @staticmethod
+    def custom_result_summary_renderer(results):  # pragma: more cover
+        # fish out sizes of annexed files. those will only be present
+        # with --annex ...
+        annexed = [
+            (r.get('bytesize', None), r.get('has_content', None), r['path'])
+            for r in results
+            if r.get('action', None) == 'status' \
+            and 'key' in r]
+        if annexed:
+            # convert to int and interrogate files with content but
+            # with unknown size (e.g. for --relaxed URLs), and drop 'path'
+            annexed = [
+                (int(bytesize) if bytesize is not None else (
+                    int(os.stat(path).st_size) if has_content else 0
+                 ), has_content)
+                for bytesize, has_content, path in annexed
+            ]
+            have_availability = any(a[1] is not None for a in annexed)
+            total_size = bytes2human(sum(a[0] for a in annexed))
+            # we have availability info encoded in the results
+            from datalad.ui import ui
+            if have_availability:
+                ui.message(
+                    "{} annex'd {} ({}/{} present/total size)".format(
+                        len(annexed),
+                        single_or_plural('file', 'files', len(annexed)),
+                        bytes2human(sum(a[0] for a in annexed if a[1])),
+                        total_size))
+            else:
+                ui.message(
+                    "{} annex'd {} ({} recorded total size)".format(
+                        len(annexed),
+                        single_or_plural('file', 'files', len(annexed)),
+                        total_size))
+        if all(r.get('action', None) == 'status'
+               and r.get('state', None) == 'clean'
+               for r in results):
+            from datalad.ui import ui
+            ui.message("nothing to save, working tree clean")
+
+
+def get_paths_by_ds(refds, dataset_arg, paths, subdsroot_mode='rsync'):
+    """Resolve and sort any paths into their containing datasets
+
+    Any path will be associated (sorted into) its nearest containing dataset.
+    It is irrelevant whether or not a path presently exists on the file system.
+    However, only datasets that exist on the file system are used for
+    sorting/association -- known, but non-existent subdatasets are not
+    considered.
 
-            for r in results_from_annex_noinfo(
-                    ds,
-                    content,
-                    respath_by_status,
-                    dir_fail_msg='could not get some content in %s %s',
-                    noinfo_dir_msg='nothing to get from %s',
-                    noinfo_file_msg='already present',
-                    action='get',
-                    logger=lgr,
-                    refds=refds_path):
-                yield r
+    Parameters
+    ----------
+    refds: Dataset
+    dataset_arg: Dataset or str or Path or None
+      Any supported value given to a command's `dataset` argument. Given
+      to `resolve_path()`.
+    paths: list
+      Any number of absolute or relative paths, in str-form or as
+      Path instances, to be sorted into their respective datasets. See also
+      the `subdsroot_mode` parameter.
+    subdsroot_mode: {'rsync', 'super', 'sub'}
+      Switch behavior for paths that are the root of a subdataset. By default
+      ('rsync'), such a path is associated with its parent/superdataset,
+      unless the path ends with a trailing directory separator, in which case
+      it is sorted into the subdataset record (this resembles the path
+      semantics of rsync, hence the label). In 'super' mode, the path is always
+      placed with the superdataset record. Likewise, in 'sub' mode the path
+      is always placed into the subdataset record.
+
+    Returns
+    -------
+    dict, list
+      The first return value is the main result, a dictionary with root
+      directories of all discovered datasets as keys and a list of the
+      associated paths inside these datasets as values.  Keys and values are
+      normalized to be Path instances of absolute paths.
+      The second return value is a list of all paths (again Path instances)
+      that are not located underneath the reference dataset.
+    """
+    ds_path = refds.path
+    paths_by_ds = dict()
+    errors = []
+
+    if not paths:
+        # that was quick
+        paths_by_ds[refds.pathobj] = None
+        return paths_by_ds, errors
+
+    # in order to guarantee proper path sorting, we first need to resolve all
+    # of them (some may be str, some Path, some relative, some absolute)
+    # step 1: normalize to unicode
+    paths = map(ensure_unicode, paths)
+    # step 2: resolve
+    # for later comparison, we need to preserve the original value too
+    paths = [(resolve_path(p, dataset_arg), str(p)) for p in paths]
+    # OPT: store cache for dataset roots for each directory directly
+    #      listed in paths, or containing the path (if file)
+    roots_cache = {}
+    # sort any path argument into the respective subdatasets
+    # sort by comparing the resolved Path instances, this puts top-level
+    # paths first, leading to their datasets to be injected into the result
+    # dict first
+    for p, orig_path in sorted(paths, key=lambda x: x[0]):
+        # TODO (left from implementing caching OPT):
+        # Logic here sounds duplicated with discover_dataset_trace_to_targets
+        # and even get_tree_roots of save.
+        str_p = str(p)
+
+        # query get_dataset_root caching for repeated queries within the same
+        # directory
+        if p.is_dir():
+            p_dir = str(p)
+        else:  # symlink, file, whatnot - seems to match logic in get_dataset_root
+            p_dir = str(p.parent)
 
-    @staticmethod
-    def custom_result_summary_renderer(res):
-        from datalad.ui import ui
-        from os import linesep
-        if not len(res):
-            ui.message("Got nothing new")
-            return
+        try:
+            root = roots_cache[p_dir]
+        except KeyError:
+            root = roots_cache[p_dir] = get_dataset_root(p_dir)
+
+        # to become the root of the dataset that contains the path in question
+        # in the context of (same basepath) as the reference dataset
+        qds_inrefds = None
+        if root is not None:
+            qds_inrefds = path_under_rev_dataset(refds, root)
+        if root is None or qds_inrefds is None:
+            # no root, not possibly underneath the refds
+            # or root that is not underneath/equal the reference dataset root
+            errors.append(p)
+            continue
 
-        nfiles = count_results(res, type='file')
-        nsuccess_file = count_results(res, type='file', status='ok')
-        nfailure = nfiles - nsuccess_file
-        msg = "Tried to get %d %s that had no content yet." % (
-            nfiles, single_or_plural("file", "files", nfiles))
-        if nsuccess_file:
-            msg += " Successfully obtained %d. " % nsuccess_file
-        if nfailure:
-            msg += " %d (failed)." % (nfailure,)
-        ui.message(msg)
-
-        # if just a few or less than initially explicitly requested
-        if len(res) < 10:
-            msg = linesep.join([
-                "{path}{type} ... {suc}".format(
-                    suc=item.get('status'),
-                    path=item.get('path'),
-                    type=' [{}]'.format(item['type']) if 'type' in item else '')
-                for item in res])
-            ui.message(msg)
+        if root != qds_inrefds:
+            # try to recode the dataset path wrt to the reference
+            # dataset
+            # the path that it might have been located by could
+            # have been a resolved path or another funky thing
+            # the path this dataset was located by is not how it would
+            # be referenced underneath the refds (possibly resolved
+            # realpath) -> recode all paths to be underneath the refds
+            p = qds_inrefds / p.relative_to(root)
+            root = qds_inrefds
+
+        # Note: Compare to Dataset(root).path rather
+        # than root to get same path normalization.
+        if root == str_p and not Dataset(root).path == ds_path and (
+                subdsroot_mode == 'super' or (
+                subdsroot_mode == 'rsync' and dataset_arg and
+                not orig_path.endswith(op.sep))):
+            # the given path is pointing to a subdataset
+            # and we are either in 'super' mode, or in 'rsync' and found
+            # rsync-link syntax to identify the dataset as whole
+            # (e.g. 'ds') vs its content (e.g. 'ds/')
+            root_dir = op.dirname(root)
+            try:
+                super_root = roots_cache[root_dir]
+            except KeyError:
+                super_root = roots_cache[root_dir] = get_dataset_root(root_dir)
+            if super_root:
+                # the dataset identified by the path argument
+                # is contained in a superdataset, and no
+                # trailing path separator was found in the
+                # argument -> user wants to address the dataset
+                # as a whole (in the superdataset)
+                root = super_root
+
+        root = ut.Path(root)
+        ps = paths_by_ds.get(root, [])
+        ps.append(p)
+        paths_by_ds[root] = ps
+    return paths_by_ds, errors
+
+
+def _yield_paths_by_ds(refds, dataset_arg, paths):
+    """Status-internal helper to yield get_paths_by_ds() items"""
+    paths_by_ds, errors = get_paths_by_ds(refds, dataset_arg, paths)
+    # communicate all the problems
+    for e in errors:
+        yield dict(
+            path=str(e),
+            action='status',
+            refds=refds.path,
+            status='error',
+            message=('path not underneath the reference dataset %s',
+                     refds.path),
+            logger=lgr)
+
+    while paths_by_ds:
+        # gh-6566 advised replacement of OrderedDicts with dicts for performance
+        # The previous qdspath, qpaths = paths_by_ds.popitem(last=False) used an
+        # OrderedDict specific function (returns k, v in FIFO order). Below is a
+        # less pretty replacement for this functionality with a pure dict
+        qdspath = next(iter(paths_by_ds.keys()))
+        qpaths = paths_by_ds.pop(qdspath)
+        if qpaths and qdspath in qpaths:
+            # this is supposed to be a full status query, save some
+            # cycles sifting through the actual path arguments
+            qpaths = []
+        yield dict(ds=Dataset(qdspath), paths=qpaths)
```

### Comparing `datalad-0.9.3/datalad/distribution/update.py` & `datalad-1.0.0/datalad/local/unlock.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,217 +1,210 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""High-level interface for updating a dataset
+"""For now just a proxy to git annex unlock
 
 """
 
 __docformat__ = 'restructuredtext'
 
-
 import logging
-from os.path import lexists, join as opj
+import os.path as op
+from collections import defaultdict
 
-from datalad.interface.base import Interface
-from datalad.interface.utils import eval_results
-from datalad.interface.base import build_doc
+from datalad.core.local.status import Status
+from datalad.distribution.dataset import (
+    Dataset,
+    EnsureDataset,
+    datasetmethod,
+    require_dataset,
+    resolve_path,
+)
+from datalad.interface.base import (
+    Interface,
+    build_doc,
+    eval_results,
+)
+from datalad.interface.common_opts import (
+    recursion_flag,
+    recursion_limit,
+)
 from datalad.interface.results import get_status_dict
-from datalad.support.constraints import EnsureStr
-from datalad.support.constraints import EnsureNone
-from datalad.support.annexrepo import AnnexRepo
+from datalad.log import log_progress
+from datalad.support.constraints import (
+    EnsureNone,
+    EnsureStr,
+)
 from datalad.support.param import Parameter
-from datalad.interface.annotate_paths import AnnotatePaths
-from datalad.interface.common_opts import recursion_flag
-from datalad.interface.common_opts import recursion_limit
-from datalad.distribution.dataset import require_dataset
-
-from .dataset import Dataset
-from .dataset import EnsureDataset
-from .dataset import datasetmethod
+from datalad.utils import (
+    Path,
+    ensure_list,
+)
 
-lgr = logging.getLogger('datalad.distribution.update')
+lgr = logging.getLogger('datalad.local.unlock')
 
 
 @build_doc
-class Update(Interface):
-    """Update a dataset from a sibling.
+class Unlock(Interface):
+    """Unlock file(s) of a dataset
 
+    Unlock files of a dataset in order to be able to edit the actual content
     """
-    # TODO: adjust docs to say:
-    # - update from just one sibling at a time
 
     _params_ = dict(
         path=Parameter(
             args=("path",),
-            metavar="PATH",
-            doc="path to be updated",
+            doc="""file(s) to unlock""",
             nargs="*",
             constraints=EnsureStr() | EnsureNone()),
-        sibling=Parameter(
-            args=("-s", "--sibling",),
-            doc="""name of the sibling to update from""",
-            constraints=EnsureStr() | EnsureNone()),
         dataset=Parameter(
             args=("-d", "--dataset"),
-            doc=""""specify the dataset to update.  If
+            doc=""""specify the dataset to unlock files in. If
             no dataset is given, an attempt is made to identify the dataset
-            based on the input and/or the current working directory""",
+            based on the current working directory.""",
             constraints=EnsureDataset() | EnsureNone()),
-        merge=Parameter(
-            args=("--merge",),
-            action="store_true",
-            doc="""merge obtained changes from the given or the
-            default sibling""", ),
         recursive=recursion_flag,
         recursion_limit=recursion_limit,
-        fetch_all=Parameter(
-            args=("--fetch-all",),
-            action="store_true",
-            doc="fetch updates from all known siblings", ),
-        reobtain_data=Parameter(
-            args=("--reobtain-data",),
-            action="store_true",
-            doc="TODO"), )
+    )
+
+    _examples_ = [
+        dict(text="Unlock a single file",
+             code_py="unlock(path='path/to/file')",
+             code_cmd="datalad unlock <path/to/file>"),
+        dict(text="Unlock all contents in the dataset",
+             code_py="unlock('.')",
+             code_cmd="datalad unlock ."),
+    ]
 
     @staticmethod
-    @datasetmethod(name='update')
+    @datasetmethod(name='unlock')
     @eval_results
     def __call__(
             path=None,
-            sibling=None,
-            merge=False,
+            *,
             dataset=None,
             recursive=False,
-            recursion_limit=None,
-            fetch_all=False,
-            reobtain_data=False):
-        """
-        """
-
-        if not dataset and not path:
-            # try to find a dataset in PWD
-            dataset = require_dataset(
-                None, check_installed=True, purpose='updating')
-        refds_path = Interface.get_refds_path(dataset)
-        if dataset and not path:
-            # act on the whole dataset if nothing else was specified
-            path = refds_path
-
-        for ap in AnnotatePaths.__call__(
-                dataset=refds_path,
-                path=path,
+            recursion_limit=None):
+        refds = require_dataset(dataset, check_installed=True,
+                                purpose="unlock")
+
+        # Before passing the results to status()
+        #   * record explicitly specified non-directory paths so that we can
+        #     decide whether to yield a result for reported paths
+        #   * filter out and yield results for paths that don't exist
+        res_paths_nondir = set()
+        paths_lexist = None
+        res_paths = list()
+        if path:
+            # Note, that we need unresolved versions of the path input to be
+            # passed on to status. See gh-5456 for example.
+            path = ensure_list(path)
+            res_paths = resolve_path(path, ds=dataset)
+            paths_lexist = []
+            res_paths_lexist = []
+            for p, p_r in zip(path, res_paths):
+                if p_r.exists() or p_r.is_symlink():
+                    paths_lexist.append(p)
+                    res_paths_lexist.append(p_r)
+                if not p_r.is_dir():
+                    res_paths_nondir.add(p_r)
+
+        res_kwargs = dict(action='unlock', logger=lgr, refds=refds.path)
+        if res_paths:
+            for p in set(res_paths).difference(set(res_paths_lexist)):
+                yield get_status_dict(
+                    status="impossible",
+                    path=str(p),
+                    type="file",
+                    message="path does not exist",
+                    **res_kwargs)
+        if not (paths_lexist or paths_lexist is None):
+            return
+
+        # Collect information on the paths to unlock.
+        to_unlock = defaultdict(list)  # ds => paths (relative to ds)
+        for res in Status()(
+                # ATTN: it is vital to pass the `dataset` argument as it,
+                # and not a dataset instance in order to maintain the path
+                # semantics between here and the status() call
+                dataset=dataset,
+                path=paths_lexist,
+                untracked="normal" if res_paths_nondir else "no",
+                annex="availability",
                 recursive=recursive,
                 recursion_limit=recursion_limit,
-                action='update',
-                unavailable_path_status='impossible',
-                nondataset_path_status='error',
-                return_type='generator',
-                on_failure='ignore'):
-            if ap.get('status', None):
-                # this is done
-                yield ap
-                continue
-            if not ap.get('type', None) == 'dataset':
-                ap.update(
-                    status='impossible',
-                    message="can only update datasets")
-                yield ap
-                continue
-            # this is definitely as dataset from here on
-            ds = Dataset(ap['path'])
-            if not ds.is_installed():
-                lgr.debug("Skipping update since not installed %s", ds)
-                continue
-            repo = ds.repo
-            # prepare return value
-            # TODO reuse AP for return props
-            res = get_status_dict('update', ds=ds, logger=lgr, refds=refds_path)
-            # get all remotes which have references (would exclude
-            # special remotes)
-            remotes = repo.get_remotes(
-                **({'exclude_special_remotes': True} if isinstance(repo, AnnexRepo) else {}))
-            if not remotes:
-                res['message'] = ("No siblings known to dataset at %s\nSkipping",
-                                  repo.path)
-                res['status'] = 'notneeded'
+                result_renderer="disabled",
+                return_type="generator",
+                on_failure="ignore"):
+            if res["action"] != "status" or res["status"] != "ok":
                 yield res
                 continue
-            if not sibling:
-                # nothing given, look for tracking branch
-                sibling_ = repo.get_tracking_branch()[0]
-            else:
-                sibling_ = sibling
-            if sibling_ and sibling_ not in remotes:
-                res['message'] = ("'%s' not known to dataset %s\nSkipping",
-                                  sibling_, repo.path)
-                res['status'] = 'impossible'
-                yield res
-                continue
-            if not sibling_ and len(remotes) == 1:
-                # there is only one remote, must be this one
-                sibling_ = remotes[0]
-            if not sibling_ and len(remotes) > 1 and merge:
-                lgr.debug("Found multiple siblings:\n%s" % remotes)
-                res['status'] = 'impossible'
-                res['error'] = NotImplementedError(
-                    "Multiple siblings, please specify from which to update.")
-                yield res
-                continue
-            lgr.info("Updating dataset '%s' ..." % repo.path)
-            # fetch remote
-            repo.fetch(
-                remote=None if fetch_all else sibling_,
-                all_=fetch_all,
-                prune=True)  # prune to not accumulate a mess over time
-            # NOTE if any further acces to `repo` is needed, reevaluate
-            # ds.repo again, as it might have be converted from an GitRepo
-            # to an AnnexRepo
-            if merge:
-                for fr in _update_repo(ds, sibling_, reobtain_data):
-                    yield fr
-            res['status'] = 'ok'
-            yield res
-
-
-def _update_repo(ds, remote, reobtain_data):
-    repo = ds.repo
-
-    lgr.info("Merging updates...")
-    if isinstance(repo, AnnexRepo):
-        if reobtain_data:
-            # get all annexed files that have data present
-            lgr.info('Recording file content availability to re-obtain updated files later on')
-            reobtain_data = \
-                [opj(ds.path, p)
-                 for p in repo.get_annexed_files(with_content_only=True)]
-        # this runs 'annex sync' and should deal with anything
-        repo.sync(remotes=remote, push=False, pull=True, commit=False)
-        if reobtain_data:
-            reobtain_data = [p for p in reobtain_data if lexists(p)]
-        if reobtain_data:
-            lgr.info('Ensure content availability for %i previously available files', len(reobtain_data))
-            for res in ds.get(
-                    reobtain_data, recursive=False, return_type='generator'):
-                yield res
-    else:
-        # handle merge in plain git
-        active_branch = repo.get_active_branch()
-        if active_branch is None:
-            # I guess we need to fetch, and then let super-dataset to update
-            # into the state it points to for this submodule, but for now let's
-            # just blow I guess :-/
-            lgr.warning(
-                "No active branch in %s - we just fetched and not changing state",
-                repo
+            has_content = res.get("has_content")
+            if has_content:
+                parentds = res["parentds"]
+                to_unlock[parentds].append(op.relpath(res["path"], parentds))
+            elif res_paths_nondir and Path(res["path"]) in res_paths_nondir:
+                if has_content is False:
+                    msg = "no content present"
+                    status = "impossible"
+                elif res["state"] == "untracked":
+                    msg = "untracked"
+                    status = "impossible"
+                else:
+                    # This is either a regular git file or an unlocked annex
+                    # file.
+                    msg = "non-annex file"
+                    status = "notneeded"
+                yield get_status_dict(
+                    status=status,
+                    path=res["path"],
+                    type="file",
+                    message="{}; cannot unlock".format(msg),
+                    **res_kwargs)
+
+        # Do the actual unlocking.
+        for ds_path, files in to_unlock.items():
+            # register for final orderly take down
+            pbar_id = f'unlock-{ds_path}'
+            nfiles = len(files)
+            log_progress(
+                lgr.info, pbar_id,
+                'Unlocking files',
+                unit=' Files',
+                label='Unlocking',
+                total=nfiles,
+                noninteractive_level=logging.INFO,
             )
-        else:
-            if repo.config.get('branch.{}.remote'.format(remote), None) == remote:
-                # the branch love this remote already, let git pull do its thing
-                repo.pull(remote=remote)
-            else:
-                # no marriage yet, be specific
-                repo.pull(remote=remote, refspec=active_branch)
+            ds = Dataset(ds_path)
+            for r in ds.repo._call_annex_records_items_(
+                    ["unlock"],
+                    files=files,
+            ):
+                log_progress(
+                    lgr.info, pbar_id,
+                    "Files to unlock %i", nfiles,
+                    update=1, increment=True,
+                    noninteractive_level=logging.DEBUG)
+                nfiles -= 1
+                yield get_status_dict(
+                    path=op.join(ds.path, r['file']),
+                    status='ok' if r['success'] else 'error',
+                    type='file',
+                    **res_kwargs)
+                if nfiles < 1:
+                    # git-annex will spend considerable time after the last
+                    # file record to finish things up, let this be known
+                    log_progress(
+                        lgr.info, pbar_id,
+                        "Recording unlocked state in git",
+                        update=0, increment=True,
+                        noninteractive_level=logging.INFO)
+
+            log_progress(
+                lgr.info, pbar_id,
+                "Completed unlocking files",
+                noninteractive_level=logging.INFO)
```

### Comparing `datalad-0.9.3/datalad/cmdline/tests/test_formatters.py` & `datalad-1.0.0/datalad/cli/tests/test_formatters.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,32 +1,40 @@
 # emacs: -*- mode: python-mode; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 """"""
 
-from six.moves import StringIO as SIO
-from os.path import exists
-from nose import SkipTest
-try:
-    import formatters as fmt
-except ImportError:  # pragma: no cover
-    # must be running from installed version where formatters is not present
-    # These tests can be ran only with formatters, which is outside of the
-    # datalad module space in the root of the sourcebase
-    if not exists('formatters.py'):
-        raise SkipTest
-
-from ..main import setup_parser
-from datalad.tests.utils import ok_, assert_in, ok_startswith
-from datalad.tests.utils import assert_not_in
+import importlib.util
+from io import StringIO as SIO
+from pathlib import Path
+
+import pytest
+
+from datalad.tests.utils_pytest import (
+    SkipTest,
+    assert_in,
+    assert_not_in,
+    ok_,
+    ok_startswith,
+)
+
+file_path = \
+    Path(__file__).parents[3] / '_datalad_build_support' / 'formatters.py'
+if not file_path.exists():
+    raise SkipTest(f'Cannot find {file_path}')
+spec = importlib.util.spec_from_file_location('formatters', file_path)
+fmt = importlib.util.module_from_spec(spec)
+spec.loader.exec_module(fmt)
+
+from ..parser import setup_parser
 
 demo_example = """
 #!/bin/sh
 
 set -e
 set -u
 
@@ -79,33 +87,34 @@
         SIO(demo_example), ref='mydemo')
     out.seek(0)
     out_text = out.read()
     assert_in('.. code-block:: sh', out_text)
     assert_not_in('shame', out_text)  # no SKIP'ed
     assert_not_in('#', out_text)      # no comments
 
+
 def test_parser_access():
-    parsers = setup_parser(return_subparsers=True)
+    parsers = setup_parser(['datalad'], return_subparsers=True)
     # we have a bunch
     ok_(len(parsers) > 3)
     assert_in('install', parsers.keys())
 
 
 def test_manpage_formatter():
     addonsections = {'mytest': "uniquedummystring"}
 
-    parsers = setup_parser(return_subparsers=True)
+    parsers = setup_parser(['datalad'], return_subparsers=True)
     for p in parsers:
         mp = fmt.ManPageFormatter(
             p, ext_sections=addonsections).format_man_page(parsers[p])
-        for section in ('SYNOPSIS', 'DESCRIPTION', 'OPTIONS', 'MYTEST'):
+        for section in ('SYNOPSIS', 'NAME', 'OPTIONS', 'MYTEST'):
             assert_in('.SH {0}'.format(section), mp)
         assert_in('uniquedummystring', mp)
 
 
 def test_rstmanpage_formatter():
-    parsers = setup_parser(return_subparsers=True)
+    parsers = setup_parser(['datalad'], return_subparsers=True)
     for p in parsers:
         mp = fmt.RSTManPageFormatter(p).format_man_page(parsers[p])
         for section in ('Synopsis', 'Description', 'Options'):
             assert_in('\n{0}'.format(section), mp)
         assert_in('{0}\n{1}'.format(p, '=' * len(p)), mp)
```

### Comparing `datalad-0.9.3/datalad/dochelpers.py` & `datalad-1.0.0/datalad/dochelpers.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 """Utils to help with docstrings etc.
@@ -15,14 +15,15 @@
 import logging
 import re
 import textwrap
 import os
 import sys
 import traceback
 
+from datalad.support.exceptions import CapturedException
 
 lgr = logging.getLogger("datalad.docutils")
 
 __add_init2doc = False
 __in_ipython = False  # TODO: determine exists('running ipython env')
 
 # if ran within IPython -- might need to add doc to init
@@ -102,15 +103,15 @@
 
 def _indent(text, istr=_rst_indentstr):
     """Simple indenter
     """
     return '\n'.join(istr + s for s in text.split('\n'))
 
 
-__parameters_str_re = re.compile("[\n^]\s*:?Parameters?:?\s*\n(:?\s*-+\s*\n)?")
+__parameters_str_re = re.compile(r"[\n^]\s*:?Parameters?:?\s*\n(:?\s*-+\s*\n)?")
 """regexp to match :Parameter: and :Parameters: stand alone in a line
 or
 Parameters
 ----------
 in multiple lines"""
 
 
@@ -150,16 +151,16 @@
     # XXX a bit of duplication of effort since handle_docstring might
     # do splitting internally
     return handle_docstring(result[0], polite=False).strip('\n'), \
            textwrap.dedent(result[1]).strip('\n'), \
            textwrap.dedent(result[2]).strip('\n')
 
 
-__re_params = re.compile('(?:\n\S.*?)+$')
-__re_spliter1 = re.compile('\n(?=\S)')
+__re_params = re.compile(r'(?:\n\S.*?)+$')
+__re_spliter1 = re.compile(r'\n(?=\S)')
 __re_spliter2 = re.compile('[\n:]')
 
 
 def _parse_parameters(paramdoc):
     """Parse parameters and return list of (name, full_doc_string)
 
     It is needed to remove multiple entries for the same parameter
@@ -168,16 +169,19 @@
     It assumes that previously parameters were unwrapped, so their
     documentation starts at the beginning of the string, like what
     should it be after _split_out_parameters
     """
     entries = __re_spliter1.split(paramdoc)
     result = [(__re_spliter2.split(e)[0].strip(), e)
               for e in entries if e != '']
-    lgr.debug('parseParameters: Given "%s", we split into %s' %
-              (paramdoc, result))
+    lgr.log(
+        1,
+        'parseParameters: Given "%s", we split into %s',
+        paramdoc, result,
+    )
     return result
 
 
 def get_docstring_split(f):
     """Given a function, break it up into portions
 
     Parameters
@@ -246,15 +250,15 @@
     TODO: take care about ``*args`` in  a clever way if those are also present
 
     Examples
     --------
     In the simplest scenario -- just grab all arguments from parent class::
 
            @borrowkwargs(A)
-           def met1(self, bu, **kwargs):
+           def met1(self, desc, **kwargs):
                pass
 
     Parameters
     ----------
     methodname : None or str
       Name of the method from which to borrow.  If None, would use
       the same name as of the decorated method
@@ -309,41 +313,14 @@
 
         # Finally assign generated doc to the method
         method.__doc__ = docstring
         return method
     return _borrowkwargs
 
 
-# TODO: make limit respect config/environment parameter
-# TODO: document, what limit even is about ;-)
-def exc_str(exc=None, limit=None):
-    """Enhanced str for exceptions.  Should include original location
+def exc_str(exc=None, limit=None, include_str=True):
+    """Temporary adapter
 
-    Parameters
-    ----------
-    Exception to
+    The CapturedException should be available and be used directly instead.
     """
-    out = str(exc)
-    if limit is None:
-        # TODO: config logging.exceptions.traceback_levels = 1
-        limit = int(os.environ.get('DATALAD_EXC_STR_TBLIMIT', '1'))
-    try:
-        exctype, value, tb = sys.exc_info()
-        if not exc:
-            exc = value
-            out = str(exc)
-        if not out:
-            out = repr(exc)
-        # verify that it seems to be the exception we were passed
-        #assert(isinstance(exc, exctype))
-        if exc:
-            assert(exc is value)
-        entries = traceback.extract_tb(tb)
-        if entries:
-            out += " [%s]" % (','.join(['%s:%s:%d' % (os.path.basename(x[0]), x[2], x[1]) for x in entries[-limit:]]))
-    except:  # MIH: TypeError?
-        return out  # To the best of our abilities
-    finally:
-        # As the bible teaches us:
-        # https://docs.python.org/2/library/sys.html#sys.exc_info
-        del tb
-    return out
+
+    return str(CapturedException(exc))
```

### Comparing `datalad-0.9.3/datalad/cmd.py` & `datalad-1.0.0/datalad/support/parallel.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,702 +1,579 @@
 # emacs: -*- mode: python; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*-
-# ex: set sts=4 ts=4 sw=4 noet:
+# ex: set sts=4 ts=4 sw=4 et:
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the datalad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
-"""
-Wrapper for command and function calls, allowing for dry runs and output handling
+"""Helpers for parallel execution
 
 """
 
+__docformat__ = 'restructuredtext'
 
-import subprocess
+import concurrent.futures
+import inspect
 import sys
-import logging
-import os
-import shutil
-import shlex
-import atexit
-import functools
-import tempfile
-
-from collections import OrderedDict
-from six import PY3, PY2
-from six import string_types, binary_type, text_type
-from os.path import abspath, isabs, pathsep, exists
-
-from .consts import GIT_SSH_COMMAND
-from .dochelpers import exc_str
-from .support.exceptions import CommandError
-from .support.protocol import NullProtocol, DryRunProtocol, \
-    ExecutionTimeProtocol, ExecutionTimeExternalsProtocol
-from .utils import on_windows, get_tempfile_kwargs, assure_unicode
-from .dochelpers import borrowdoc
-
-lgr = logging.getLogger('datalad.cmd')
-
-# In python3 to split byte stream on newline, it must be bytes
-linesep_bytes = os.linesep.encode()
-
-_TEMP_std = sys.stdout, sys.stderr
-# To be used in the temp file name to distinguish the ones we create
-# in Runner so we take care about their removal, in contrast to those
-# which might be created outside and passed into Runner
-_MAGICAL_OUTPUT_MARKER = "_runneroutput_"
-
-if PY2:
-    # TODO apparently there is a recommended substitution for Python2
-    # which is a backported implementation of python3 subprocess
-    # https://pypi.python.org/pypi/subprocess32/
-    file_class = file
-else:
-    from io import IOBase as file_class
-
-
-def _decide_to_log(v):
-    """Hacky workaround for now so we could specify per each which to
-    log online and which to the log"""
-    if isinstance(v, bool) or callable(v):
-        return v
-    elif v in {'online'}:
-        return True
-    elif v in {'offline'}:
-        return False
-    else:
-        raise ValueError("can be bool, callable, 'online' or 'offline'")
+import time
+import uuid
 
+from collections import defaultdict
+from queue import Queue, Empty
+from threading import Thread
+
+from . import ansi_colors as colors
+from ..log import log_progress
+from ..utils import path_is_subpath
+from datalad.support.exceptions import CapturedException
 
-def _get_output_stream(log_std, false_value):
-    """Helper to prepare output stream for Popen and use file for 'offline'
+import logging
+lgr = logging.getLogger('datalad.parallel')
 
-    Necessary to avoid lockdowns when both stdout and stderr are pipes
-    """
-    if log_std:
-        if log_std == 'offline':
-            # we will open a temporary file
 
-            tf = tempfile.mktemp(
-                **get_tempfile_kwargs({}, prefix=_MAGICAL_OUTPUT_MARKER)
-            )
-            return open(tf, 'w')  # XXX PY3 should be 'b' may be?
-        else:
-            return subprocess.PIPE
-    else:
-        return false_value
-
-
-def _cleanup_output(stream, std):
-    if isinstance(stream, file_class) and _MAGICAL_OUTPUT_MARKER in stream.name:
-        if not stream.closed:
-            stream.close()
-        if exists(stream.name):
-            os.unlink(stream.name)
-    elif stream == subprocess.PIPE:
-        std.close()
-
-
-class Runner(object):
-    """Provides a wrapper for calling functions and commands.
-
-    An object of this class provides a methods that calls shell commands or
-    python functions, allowing for protocolling the calls and output handling.
-
-    Outputs (stdout and stderr) can be either logged or streamed to system's
-    stdout/stderr during execution.
-    This can be enabled or disabled for both of them independently.
-    Additionally, a protocol object can be a used with the Runner. Such a
-    protocol has to implement datalad.support.protocol.ProtocolInterface, is
-    able to record calls and allows for dry runs.
-    """
+def _count_str(count, verb, omg=False):
+    if count:
+        msg = "{:d} {}".format(count, verb)
+        if omg:
+            msg = colors.color_word(msg, colors.RED)
+        return msg
 
-    __slots__ = ['commands', 'dry', 'cwd', 'env', 'protocol',
-                 '_log_opts']
 
-    def __init__(self, cwd=None, env=None, protocol=None, log_outputs=None):
-        """
-        Parameters
-        ----------
-        cwd: string, optional
-             Base current working directory for commands.  Could be overridden
-             per run call via cwd option
-        env: dict, optional
-             Custom environment to use for calls. Could be overridden per run
-             call via env option
-        protocol: ProtocolInterface
-             Protocol object to write to.
-        log_outputs : bool, optional
-             Switch to instruct either outputs should be logged or not.  If not
-             set (default), config 'datalad.log.outputs' would be consulted
-        """
+#
+# safe_to_consume  helpers
+#
 
-        self.cwd = cwd
-        self.env = env
-        if protocol is None:
-            # TODO: config cmd.protocol = null
-            protocol_str = os.environ.get('DATALAD_CMD_PROTOCOL', 'null')
-            protocol = {
-                'externals-time': ExecutionTimeExternalsProtocol,
-                'time': ExecutionTimeProtocol,
-                'null': NullProtocol
-            }[protocol_str]()
-            if protocol_str != 'null':
-                # we need to dump it into a file at the end
-                # TODO: config cmd.protocol_prefix = protocol
-                filename = '%s-%s.log' % (
-                    os.environ.get('DATALAD_CMD_PROTOCOL_PREFIX', 'protocol'),
-                    id(self)
-                )
-                atexit.register(functools.partial(protocol.write_to_file, filename))
+def no_parentds_in_futures(futures, path, skip=tuple()):
+    """Return True if no path in futures keys is parentds for provided path
 
-        self.protocol = protocol
-        # Various options for logging
-        self._log_opts = {}
-        # we don't know yet either we need ot log every output or not
-        if log_outputs is not None:
-            self._log_opts['outputs'] = log_outputs
-
-    def __call__(self, cmd, *args, **kwargs):
-        """Convenience method
-
-        This will call run() or call() depending on the kind of `cmd`.
-        If `cmd` is a string it is interpreted as the to be executed command.
-        Otherwise it is expected to be a callable.
-        Any other argument is passed to the respective method.
+    Assumes that the future's key is the path.
 
-        Parameters
-        ----------
-        cmd: str or callable
-           command string to be executed via shell or callable to be called.
+    Parameters
+    ----------
+    skip: iterable
+      Do not consider futures with paths in skip.  E.g. it could be top level
+      dataset which we know it exists already, and it is ok to start with child
+      process before it
+    """
+    # TODO: OPT.  Could benefit from smarter than linear time if not one at a time?
+    #   or may be we should only go through active futures (still linear!)?
+    return all(not path_is_subpath(path, p) or p in skip for p in futures)
 
-        `*args`:
-        `**kwargs`:
-           see Runner.run() and Runner.call() for available arguments.
-
-        Raises
-        ------
-        TypeError
-          if cmd is neither a string nor a callable.
-        """
 
-        if isinstance(cmd, string_types) or isinstance(cmd, list):
-            return self.run(cmd, *args, **kwargs)
-        elif callable(cmd):
-            return self.call(cmd, *args, **kwargs)
-        else:
-            raise TypeError("Argument 'command' is neither a string, "
-                            "nor a list nor a callable.")
+def no_subds_in_futures(futures, path, skip=tuple()):
+    """Return True if no path in futures keys is a subdataset for provided path
 
-    def _opt_env_adapter(v):
-        """If value is a string, split by ,"""
-        if v:
-            if v.isdigit():
-                log_env = bool(int(v))
-            else:
-                log_env = v.split(',')
-            return log_env
-        else:
-            return False
+    See `no_parentds_in_futures` for more info
+    """
+    return all(not path_is_subpath(p, path) or p in skip for p in futures)
 
-    _LOG_OPTS_ADAPTERS = OrderedDict([
-        ('outputs', None),
-        ('cwd', None),
-        ('env', _opt_env_adapter),
-        ('stdin', None),
-    ])
-
-    def _get_log_setting(self, opt, default=False):
-        try:
-            return self._log_opts[opt]
-        except KeyError:
-            try:
-                from . import cfg
-            except ImportError:
-                return default
-            adapter = self._LOG_OPTS_ADAPTERS.get(opt, None)
-            self._log_opts[opt] = \
-                (cfg.getbool if not adapter else cfg.get_value)(
-                    'datalad.log.cmd', opt, default=default)
-            if adapter:
-                self._log_opts[opt] = adapter(self._log_opts[opt])
-            return self._log_opts[opt]
 
-    @property
-    def log_outputs(self):
-        return self._get_log_setting('outputs')
-
-    @property
-    def log_cwd(self):
-        return self._get_log_setting('cwd')
+class ProducerConsumer:
+    """Producer/Consumer implementation to (possibly) parallelize execution.
 
-    @property
-    def log_stdin(self):
-        return self._get_log_setting('stdin')
+    It is an iterable providing a multi-threaded producer/consumer implementation,
+    where there could be multiple consumers for items produced by a producer.  Since
+    in DataLad majority of time is done in IO interactions with outside git and git-annex
+    processes, and since we typically operate across multiple datasets, multi-threading
+    across datasets operations already provides a significant performance benefit.
+
+    All results from consumers are all yielded as soon as they are produced by consumers.
+    Because this implementation is based on threads, `producer` and `consumer` could
+    be some "closures" within code, thus having lean interface and accessing
+    data from shared "outer scope".
+
+    Notes
+    -----
+    - with jobs > 1, results are yielded as soon as available, so order
+      might not match the one provided by "producer".
+    - jobs > 1, is "effective" only for Python >= 3.8.  For older versions it
+      would log a warning (upon initial encounter) if jobs > 1 is specified.
+    - `producer` must produce unique entries. AssertionError might be raised if
+      the same entry is to be consumed.
+    - `consumer` can add to the queue of items produced by producer via
+      `.add_to_producer_queue`. This allows for continuous reuse of the same
+      instance in recursive operations (see `get` use of ProducerConsumer).
+    - if producer or consumer raise an exception, we will try to "fail gracefully",
+      unless subsequent Ctrl-C is pressed, we will let already running jobs to
+      finish first.
+
+    Examples
+    --------
+    A simple and somewhat boring example could be to count lines in '*.py'
+    files in parallel
+
+        from glob import glob
+        from pprint import pprint
+        from datalad.support.parallel import ProducerConsumer
+
+        def count_lines(fname):
+            with open(fname) as f:
+                return fname, len(f.readlines())
+        pprint(dict(ProducerConsumer(glob("*.py"), count_lines)))
 
-    @property
-    def log_env(self):
-        return self._get_log_setting('env')
+    More usage examples could be found in `test_parallel.py` and around the
+    codebase `addurls.py`, `get.py`, `save.py`, etc.
+    """
 
-    # Two helpers to encapsulate formatting/output
-    def _log_out(self, line):
-        if line and self.log_outputs:
-            self.log("stdout| " + line.rstrip('\n'))
-
-    def _log_err(self, line, expected=False):
-        if line and self.log_outputs:
-            self.log("stderr| " + line.rstrip('\n'),
-                     level={True: 9,
-                            False: 11}[expected])
-
-    def _get_output_online(self, proc,
-                           log_stdout, log_stderr,
-                           outputstream, errstream,
-                           expect_stderr=False, expect_fail=False):
+    # Users should not specify -J100 and then just come complaining without
+    # being informed that they are out of luck
+    _alerted_already = False
+
+    def __init__(self,
+                 producer, consumer,
+                 *,
+                 jobs=None,
+                 safe_to_consume=None,
+                 producer_future_key=None,
+                 reraise_immediately=False,
+                 agg=None,
+                 ):
         """
 
-        If log_stdout or log_stderr are callables, they will be given a read
-        line to be processed, and return processed result.  So if they need to
-        'swallow' the line from being logged, should just return None
-
         Parameters
         ----------
-        proc
-        log_stdout: bool or callable or 'online' or 'offline'
-        log_stderr: : bool or callable or 'online' or 'offline'
-          If any of those 'offline', we would call proc.communicate at the
-          end to grab possibly outstanding output from it
-        expect_stderr
-        expect_fail
-
-        Returns
-        -------
+        producer: iterable
+          Provides items to feed a consumer with
+        consumer: callable
+          Is provided with items produced by producer.  Multiple consumers might
+          operate in parallel threads if jobs > 1
+        jobs: int, optional
+          If None or "auto", 'datalad.runtime.max-jobs' configuration variable is
+          consulted.  With jobs=0 there is no threaded execution whatsoever.  With
+          jobs=1 there is a separate thread for the producer, so in effect with jobs=1
+          some parallelization between producer (if it is a generator) and consumer
+          could be achieved, while there is only a single thread available for consumers.
+        safe_to_consume: callable, optional
+          A callable which gets a dict of all known futures and current item from producer.
+          It should return `True` if executor can proceed with current value from producer.
+          If not (unsafe to consume) - we will wait.
+          WARNING: outside code should make sure about provider and `safe_to_consume` to
+          play nicely or a very suboptimal behavior or possibly even a deadlock can happen.
+        producer_future_key: callable, optional
+          A key function for a value from producer which will be used as a key in futures
+          dictionary and output of which is passed to safe_to_consume.
+        reraise_immediately: bool, optional
+          If True, it would stop producer yielding values as soon as it detects that some
+          exception has occurred (although there might still be values in the queue to be yielded
+          which were collected before the exception was raised).
+        agg: callable, optional
+          Should be a callable with two arguments: (item, prior total) and return a new total
+          which will get assigned to .total of this object.  If not specified, .total is
+          just a number of items produced by the producer.
+        """
+        self.producer = producer
+        self.consumer = consumer
+        self.jobs = jobs
+        self.safe_to_consume = safe_to_consume
+        self.producer_future_key = producer_future_key
+        self.reraise_immediately = reraise_immediately
+        self.agg = agg
+
+        self.total = None if self.agg else 0
+        self._jobs = None  # actual "parallel" jobs used
+        # Relevant only for _iter_threads
+        self._producer_finished = None
+        self._producer_queue = None
+        self._producer_exception = None
+        self._producer_interrupt = None
+        # so we could interrupt more or less gracefully
+        self._producer_thread = None
+        self._executor = None
+        self._futures = {}
+        self._interrupted = False
 
-        """
-        stdout, stderr = binary_type(), binary_type()
+    @property
+    def interrupted(self):
+        return self._interrupted
 
-        log_stdout_ = _decide_to_log(log_stdout)
-        log_stderr_ = _decide_to_log(log_stderr)
-        log_stdout_is_callable = callable(log_stdout_)
-        log_stderr_is_callable = callable(log_stderr_)
-
-        # arguments to be passed into _process_one_line
-        stdout_args = (
-                'stdout',
-                proc, log_stdout_, log_stdout_is_callable
-        )
-        stderr_args = (
-                'stderr',
-                proc, log_stderr_, log_stderr_is_callable,
-                expect_stderr or expect_fail
-        )
-
-        while proc.poll() is None:
-            # see for a possibly useful approach to processing output
-            # in another thread http://codereview.stackexchange.com/a/17959
-            # current problem is that if there is no output on stderr
-            # it stalls
-            if log_stdout_:
-                stdout += self._process_one_line(*stdout_args)
-            if log_stderr_:
-                stderr += self._process_one_line(*stderr_args)
-
-        # Handle possible remaining output
-        stdout_, stderr_ = proc.communicate()
-        # ??? should we condition it on log_stdout in {'offline'} ???
-        stdout += self._process_remaining_output(outputstream, stdout_, *stdout_args)
-        stderr += self._process_remaining_output(errstream, stderr_, *stderr_args)
-
-        return stdout, stderr
-
-    def _process_remaining_output(self, stream, out_, *pargs):
-        """Helper to process output which might have been obtained from popen or
-        should be loaded from file"""
-        out = binary_type()
-        if isinstance(stream,
-                      file_class) and _MAGICAL_OUTPUT_MARKER in stream.name:
-            assert out_ is None, "should have gone into a file"
-            if not stream.closed:
-                stream.close()
-            with open(stream.name, 'rb') as f:
-                for line in f:
-                    out += self._process_one_line(*pargs, line=line)
-        else:
-            if out_:
-                # resolving a once in a while failing test #2185
-                if isinstance(out_, text_type):
-                    out_ = out_.encode('utf-8')
-                for line in out_.split(linesep_bytes):
-                    out += self._process_one_line(*pargs, line=line)
-        return out
-
-    def _process_one_line(self, out_type, proc, log_, log_is_callable,
-                          expected=False, line=None):
-        if line is None:
-            lgr.log(3, "Reading line from %s", out_type)
-            line = {'stdout': proc.stdout, 'stderr': proc.stderr}[out_type].readline()
+    def __del__(self):
+        # if we are killed while executing, we should ask executor to shutdown
+        shutdown = getattr(self, "shutdown", None)
+        if shutdown:
+            shutdown(force=True)
+
+    def shutdown(self, force=False, exception=None):
+        if self._producer_thread and self._producer_thread.is_alive():
+            # we will try to let the worker to finish "gracefully"
+            self._producer_interrupt = f"shutdown due to {exception}"
+
+        # purge producer queue
+        if self._producer_queue:
+            while not self._producer_queue.empty():
+                self._producer_queue.get()
+
+        lgr.debug("Shutting down %s with %d futures. Reason: %s",
+                  self._executor, len(self._futures), exception)
+
+        if not force:
+            # pop not yet running or done futures.
+            # Those would still have a chance to yield results and finish gracefully
+            # or their exceptions to be bubbled up FWIW.
+            ntotal = len(self._futures)
+            ncanceled = 0
+            nrunning = 0
+            # Do in reverse order so if any job still manages
+            # to sneak in, it would be the earlier submitted one.
+            for k, future in list(self._futures.items())[::-1]:
+                running = future.running()
+                nrunning += int(running)
+                if not (running or future.done()):
+                    if self._futures.pop(k).cancel():
+                        ncanceled += 1
+            lgr.info("Canceled %d out of %d jobs. %d left running.",
+                     ncanceled, ntotal, nrunning)
         else:
-            lgr.log(3, "Processing provided line")
-        if line and log_is_callable:
-            # Let it be processed
-            line = log_(assure_unicode(line))
-            if line is not None:
-                # we are working with binary type here
-                line = line.encode()
-        if line:
-            if out_type == 'stdout':
-                self._log_out(assure_unicode(line))
-            elif out_type == 'stderr':
-                self._log_err(line.decode() if PY3 else line,
-                              expected)
-            else:  # pragma: no cover
-                raise RuntimeError("must not get here")
-            return line
-        # it was output already directly but for code to work, return ""
-        return binary_type()
-
-    def run(self, cmd, log_stdout=True, log_stderr=True, log_online=False,
-            expect_stderr=False, expect_fail=False,
-            cwd=None, env=None, shell=None, stdin=None):
-        """Runs the command `cmd` using shell.
-
-        In case of dry-mode `cmd` is just added to `commands` and it is
-        actually executed otherwise.
-        Allows for separately logging stdout and stderr  or streaming it to
-        system's stdout or stderr respectively.
-
-        Note: Using a string as `cmd` and shell=True allows for piping,
-              multiple commands, etc., but that implies shlex.split() is not
-              used. This is considered to be a security hazard.
-              So be careful with input.
-
-        Parameters
-        ----------
-        cmd : str, list
-          String (or list) defining the command call.  No shell is used if cmd
-          is specified as a list
-
-        log_stdout: bool, optional
-            If True, stdout is logged. Goes to sys.stdout otherwise.
-
-        log_stderr: bool, optional
-            If True, stderr is logged. Goes to sys.stderr otherwise.
-
-        log_online: bool, optional
-            Either to log as output comes in.  Setting to True is preferable
-            for running user-invoked actions to provide timely output
-
-        expect_stderr: bool, optional
-            Normally, having stderr output is a signal of a problem and thus it
-            gets logged at level 11.  But some utilities, e.g. wget, use
-            stderr for their progress output.  Whenever such output is expected,
-            set it to True and output will be logged at level 9 unless
-            exit status is non-0 (in non-online mode only, in online -- would
-            log at 9)
-
-        expect_fail: bool, optional
-            Normally, if command exits with non-0 status, it is considered an
-            error and logged at level 11 (above DEBUG). But if the call intended
-            for checking routine, such messages are usually not needed, thus
-            it will be logged at level 9.
-
-        cwd : string, optional
-            Directory under which run the command (passed to Popen)
-
-        env : string, optional
-            Custom environment to pass
-
-        shell: bool, optional
-            Run command in a shell.  If not specified, then it runs in a shell
-            only if command is specified as a string (not a list)
-
-        stdin: file descriptor
-            input stream to connect to stdin of the process.
-
-        Returns
-        -------
-        (stdout, stderr)
-
-        Raises
-        ------
-        CommandError
-           if command's exitcode wasn't 0 or None. exitcode is passed to
-           CommandError's `code`-field. Command's stdout and stderr are stored
-           in CommandError's `stdout` and `stderr` fields respectively.
-        """
-        outputstream = _get_output_stream(log_stdout, sys.stdout)
-        errstream = _get_output_stream(log_stderr, sys.stderr)
-
-        popen_env = env or self.env
-
-        # TODO: if outputstream is sys.stdout and that one is set to StringIO
-        #       we have to "shim" it with something providing fileno().
-        # This happens when we do not swallow outputs, while allowing nosetest's
-        # StringIO to be provided as stdout, crashing the Popen requiring
-        # fileno().  In out swallow_outputs, we just use temporary files
-        # to overcome this problem.
-        # For now necessary test code should be wrapped into swallow_outputs cm
-        # to avoid the problem
-        log_msgs = ["Running: %s"]
-        log_args = [cmd]
-        if self.log_cwd:
-            log_msgs += ['cwd=%r']
-            log_args += [cwd or self.cwd]
-        if self.log_stdin:
-            log_msgs += ['stdin=%r']
-            log_args += [stdin]
-        log_env = self.log_env
-        if log_env and popen_env:
-            log_msgs += ["env=%r"]
-            log_args.append(
-                popen_env if log_env is True
-                else {k: popen_env[k] for k in log_env if k in popen_env}
+            # just pop all entirely
+            for k in list(self._futures)[::-1]:
+                self._futures.pop(k).cancel()
+            if self._executor:
+                self._executor.shutdown()
+                self._executor = None
+            if exception:
+                raise exception
+        lgr.debug("Finished shutdown with force=%s due to exception=%r", force, exception)
+
+    def _update_total(self, value):
+        if self.agg:
+            self.total = (
+                self.agg(value, self.total) if self.total is not None else self.agg(value)
             )
-        log_msg = '\n'.join(log_msgs)
-        self.log(log_msg, *log_args)
-
-        if self.protocol.do_execute_ext_commands:
+        else:
+            self.total += 1
 
-            if shell is None:
-                shell = isinstance(cmd, string_types)
+    @classmethod
+    def get_effective_jobs(cls, jobs):
+        """Return actual number of jobs to be used.
+
+        It will account for configuration variable ('datalad.runtime.max-jobs') and possible
+        other requirements (such as version of Python).
+        """
+        if jobs in (None, "auto"):
+            from datalad import cfg
+            # ATM there is no "auto" for this operation, so in both auto and None
+            # just consult max-jobs which can only be an int ATM.
+            # "auto" could be for some auto-scaling based on a single future time
+            # to complete, scaling up/down. Ten config variable could accept "auto" as well
+            jobs = cfg.obtain('datalad.runtime.max-jobs')
+        return jobs
+
+    def __iter__(self):
+        self._jobs = self.get_effective_jobs(self.jobs)
+        if self._jobs == 0:
+            yield from self._iter_serial()
+        else:
+            yield from self._iter_threads(self._jobs)
 
-            if self.protocol.records_ext_commands:
-                prot_exc = None
-                prot_id = self.protocol.start_section(
-                    shlex.split(cmd, posix=not on_windows)
-                    if isinstance(cmd, string_types)
-                    else cmd)
-            try:
-                proc = subprocess.Popen(cmd,
-                                        stdout=outputstream,
-                                        stderr=errstream,
-                                        shell=shell,
-                                        cwd=cwd or self.cwd,
-                                        env=popen_env,
-                                        stdin=stdin)
-
-            except Exception as e:
-                prot_exc = e
-                lgr.log(11, "Failed to start %r%r: %s" %
-                        (cmd, " under %r" % cwd if cwd else '', exc_str(e)))
-                raise
+    def _iter_serial(self):
+        # depchecker is not consulted, serial execution
+        # reraise_immediately is also "always False by design"
+        # To allow consumer to add to the queue
+        self._producer_queue = producer_queue = Queue()
+
+        def produce():
+            # First consume all coming directly from producer and then go through all which
+            # consumer might have added to the producer queue
+            for args in self._producer_iter:
+                self._update_total(args)
+                yield args
+            # consumer could have added to the queue while we were still
+            # producing
+            while not producer_queue.empty():
+                yield producer_queue.get()
+
+        for args in produce():
+            res = self.consumer(args)
+            if inspect.isgenerator(res):
+                lgr.debug("Got consumer worker which returned a generator %s", res)
+                yield from res
+            else:
+                lgr.debug("Got straight result %s, not a generator", res)
+                yield res
 
-            finally:
-                if self.protocol.records_ext_commands:
-                    self.protocol.end_section(prot_id, prot_exc)
+    @property
+    def _producer_iter(self):
+        """A little helper to also support generator functions"""
+        return self.producer() if inspect.isgeneratorfunction(self.producer) else self.producer
+
+    def _iter_threads(self, jobs):
+        self._interrupted = False
+        self._producer_finished = False
+        self._producer_exception = None
+        self._producer_interrupt = None
+
+        # To allow feeding producer queue with more entries, possibly from consumer!
+        self._producer_queue = producer_queue = Queue()
+        consumer_queue = Queue()
 
+        def producer_worker():
+            """That is the one which interrogates producer and updates .total"""
             try:
-                if log_online:
-                    out = self._get_output_online(proc,
-                                                  log_stdout, log_stderr,
-                                                  outputstream, errstream,
-                                                  expect_stderr=expect_stderr,
-                                                  expect_fail=expect_fail)
-                else:
-                    out = proc.communicate()
-
-                if PY3:
-                    # Decoding was delayed to this point
-                    def decode_if_not_None(x):
-                        return "" if x is None else binary_type.decode(x)
-                    # TODO: check if we can avoid PY3 specific here
-                    out = tuple(map(decode_if_not_None, out))
-
-                status = proc.poll()
-
-                # needs to be done after we know status
-                if not log_online:
-                    self._log_out(out[0])
-                    if status not in [0, None]:
-                        self._log_err(out[1], expected=expect_fail)
-                    else:
-                        # as directed
-                        self._log_err(out[1], expected=expect_stderr)
-
-                if status not in [0, None]:
-                    msg = "Failed to run %r%s. Exit code=%d. out=%s err=%s" \
-                        % (cmd, " under %r" % (cwd or self.cwd), status, out[0], out[1])
-                    lgr.log(9 if expect_fail else 11, msg)
-                    raise CommandError(str(cmd), msg, status, out[0], out[1])
-                else:
-                    self.log("Finished running %r with status %s" % (cmd, status),
-                             level=8)
+                for value in self._producer_iter:
+                    if self._producer_interrupt:
+                        raise InterruptedError("Producer thread was interrupted due to %s" % self._producer_interrupt)
+                    self.add_to_producer_queue(value)
+            except InterruptedError:
+                pass  # There is some outside exception which will be raised
+            except BaseException as e:
+                self._producer_exception = e
             finally:
-                # Those streams are for us to close if we asked for a PIPE
-                # TODO -- assure closing the files import pdb; pdb.set_trace()
-                _cleanup_output(outputstream, proc.stdout)
-                _cleanup_output(errstream, proc.stderr)
-
-        else:
-            if self.protocol.records_ext_commands:
-                self.protocol.add_section(shlex.split(cmd,
-                                                      posix=not on_windows)
-                                          if isinstance(cmd, string_types)
-                                          else cmd, None)
-            out = ("DRY", "DRY")
+                self._producer_finished = True
 
-        return out
+        def consumer_worker(callable, *args, **kwargs):
+            """Since jobs could return a generator and we cannot really "inspect" for that
+            """
+            res = callable(*args, **kwargs)
+            if inspect.isgenerator(res):
+                lgr.debug("Got consumer worker which returned a generator %s", res)
+                didgood = False
+                for r in res:
+                    didgood = True
+                    lgr.debug("Adding %s to queue", r)
+                    consumer_queue.put(r)
+                if not didgood:
+                    lgr.error("Nothing was obtained from %s :-(", res)
+            else:
+                lgr.debug("Got straight result %s, not a generator", res)
+                consumer_queue.put(res)
 
-    def call(self, f, *args, **kwargs):
-        """Helper to unify collection of logging all "dry" actions.
+        self._producer_thread = Thread(target=producer_worker)
+        self._producer_thread.start()
+        self._futures = futures = {}
+
+        lgr.debug("Initiating ThreadPoolExecutor with %d jobs", jobs)
+        # we will increase sleep_time when doing nothing useful
+        sleeper = Sleeper()
+        interrupted_by_exception = None
+        with concurrent.futures.ThreadPoolExecutor(jobs) as executor:
+            self._executor = executor
+            # yield from the producer_queue (.total and .finished could be accessed meanwhile)
+            while True:
+                try:
+                    done_useful = False
+                    if self.reraise_immediately and self._producer_exception and not interrupted_by_exception:
+                        # so we have a chance to exit gracefully
+                        # No point to reraise if there is already an exception which was raised
+                        # which might have even been this one
+                        lgr.debug("Reraising an exception from producer as soon as we found it")
+                        raise self._producer_exception
+                    if (self._producer_finished and
+                            not futures and
+                            consumer_queue.empty() and
+                            producer_queue.empty()):
+                        # This will let us not "escape" the while loop and reraise any possible exception
+                        # within the loop if we have any.
+                        # Otherwise we might see "RuntimeError: generator ignored GeneratorExit"
+                        # when e.g. we did continue upon interrupted_by_exception, and then
+                        # no other subsequent exception was raised and we left the loop
+                        raise _FinalShutdown()
+
+                    # important!  We are using threads, so worker threads will be sharing CPU time
+                    # with this master thread. For it to become efficient, we should consume as much
+                    # as possible from producer asap and push it to executor.  So drain the queue
+                    while not (producer_queue.empty() or interrupted_by_exception):
+                        done_useful = True
+                        try:
+                            job_args = producer_queue.get() # timeout=0.001)
+                            job_key = self.producer_future_key(job_args) if self.producer_future_key else job_args
+                            if self.safe_to_consume:
+                                # Sleep a little if we are not yet ready
+                                # TODO: add some .debug level reporting based on elapsed time
+                                # IIRC I did smth like growing exponentially delays somewhere (dandi?)
+                                while not self.safe_to_consume(futures, job_key):
+                                    self._pop_done_futures(lgr) or sleeper()
+                            # Current implementation, to provide depchecking, relies on unique
+                            # args for the job
+                            assert job_key not in futures
+                            lgr.debug("Submitting worker future for %s", job_args)
+                            futures[job_key] = executor.submit(consumer_worker, self.consumer, job_args)
+                        except Empty:
+                            pass
+
+                    # check active futures
+                    if not consumer_queue.empty():
+                        done_useful = True
+                        # ATM we do not bother of some "in order" reporting
+                        # Just report as soon as any new record arrives
+                        res = consumer_queue.get()
+                        lgr.debug("Got %s from consumer_queue", res)
+                        yield res
+
+                    done_useful |= self._pop_done_futures(lgr)
+
+                    if not done_useful:  # you need some rest
+                        # TODO: same here -- progressive logging
+                        lgr.log(5,
+                                "Did nothing useful, sleeping. Have "
+                                "producer_finished=%s producer_queue.empty=%s futures=%s consumer_queue.empty=%s",
+                                self._producer_finished,
+                                producer_queue.empty(),
+                                futures,
+                                consumer_queue.empty(),
+                                )
+                        sleeper()
+                    else:
+                        sleeper.reset()
+                except (_FinalShutdown, GeneratorExit):
+                    self.shutdown(force=True, exception=self._producer_exception or interrupted_by_exception)
+                    break  # if there were no exception to raise
+                except BaseException as exc:
+                    ce = CapturedException(exc)
+                    self._interrupted = True
+                    if interrupted_by_exception:
+                        # so we are here again but now it depends why we are here
+                        if isinstance(exc, KeyboardInterrupt):
+                            lgr.warning("Interrupted via Ctrl-C.  Forcing the exit")
+                            self.shutdown(force=True, exception=exc)
+                        else:
+                            lgr.warning(
+                                "One more exception was received while "
+                                "trying to finish gracefully: %s",
+                                ce)
+                            # and we go back into the loop until we finish or there is Ctrl-C
+                    else:
+                        interrupted_by_exception = exc
+                        lgr.warning(
+                            "Received an exception %s. Canceling not-yet "
+                            "running jobs and waiting for completion of "
+                            "running. You can force earlier forceful exit "
+                            "by Ctrl-C.", ce)
+                        self.shutdown(force=False, exception=exc)
+
+    def add_to_producer_queue(self, value):
+        self._producer_queue.put(value)
+        self._update_total(value)
 
-        Calls `f` if `Runner`-object is not in dry-mode. Adds `f` along with
-        its arguments to `commands` otherwise.
+    def _pop_done_futures(self, lgr):
+        """Removes .done from provided futures.
 
-        Parameters
-        ----------
-        f: callable
+        Returns
+        -------
+        bool
+          True if any future was removed
         """
-        if self.protocol.do_execute_callables:
-            if self.protocol.records_callables:
-                prot_exc = None
-                prot_id = self.protocol.start_section(
-                    [str(f), "args=%s" % str(args), "kwargs=%s" % str(kwargs)])
-
-            try:
-                return f(*args, **kwargs)
-            except Exception as e:
-                prot_exc = e
-                raise
-            finally:
-                if self.protocol.records_callables:
-                    self.protocol.end_section(prot_id, prot_exc)
-        else:
-            if self.protocol.records_callables:
-                self.protocol.add_section(
-                    [str(f), "args=%s" % str(args), "kwargs=%s" % str(kwargs)],
-                    None)
-
-    def log(self, msg, *args, **kwargs):
-        """log helper
+        done_useful = False
+        # remove futures which are done
+        for args, future in list(self._futures.items()):
+            if future.done():
+                done_useful = True
+                future_ = self._futures.pop(args)
+                exception = future_.exception()
+                if exception:
+                    lgr.debug("Future for %r raised %s.  Re-raising to trigger graceful shutdown etc", args, exception)
+                    raise exception
+                lgr.debug("Future for %r is done", args)
+        return done_useful
+
+
+class Sleeper():
+    def __init__(self):
+        self.min_sleep_time = 0.001
+        # but no more than to this max
+        self.max_sleep_time = 0.1
+        self.sleep_time = self.min_sleep_time
+
+    def __call__(self):
+        time.sleep(self.sleep_time)
+        self.sleep_time = min(self.max_sleep_time, self.sleep_time * 2)
+
+    def reset(self):
+        self.sleep_time = self.min_sleep_time
 
-        Logs at level 9 by default and adds "Protocol:"-prefix in order to
-        log the used protocol.
-        """
-        level = kwargs.pop('level', 9)
-        if isinstance(self.protocol, NullProtocol):
-            lgr.log(level, msg, *args, **kwargs)
-        else:
-            if args:
-                msg = msg % args
-            lgr.log(level, "{%s} %s" % (
-                self.protocol.__class__.__name__, msg)
-            )
 
+class ProducerConsumerProgressLog(ProducerConsumer):
+    """ProducerConsumer wrapper with log_progress reporting.
 
-class GitRunner(Runner):
-    """
-    Runner to be used to run git and git annex commands
+    It is to be used around a `consumer` which returns or yields result records.
+    If that is not the case -- use regular `ProducerConsumer`.
 
-    Overloads the runner class to check & update GIT_DIR and
-    GIT_WORK_TREE environment variables set to the absolute path
-    if is defined and is relative path
+    It will update `.total` of the `log_progress` each time it changes (i.e. whenever
+    producer produced new values to be consumed).
     """
-    _GIT_PATH = None
-
-    @borrowdoc(Runner)
-    def __init__(self, *args, **kwargs):
-        super(GitRunner, self).__init__(*args, **kwargs)
-        self._check_git_path()
-
-    @staticmethod
-    def _check_git_path():
-        """If using bundled git-annex, we would like to use bundled with it git
 
-        Thus we will store _GIT_PATH a path to git in the same directory as annex
-        if found.  If it is empty (but not None), we do nothing
+    def __init__(self,
+                 producer, consumer,
+                 *,
+                 log_filter=None,
+                 label="Total", unit="items",
+                 lgr=None,
+                 **kwargs
+                 ):
         """
-        if GitRunner._GIT_PATH is None:
-            from distutils.spawn import find_executable
-            # with all the nesting of config and this runner, cannot use our
-            # cfg here, so will resort to dark magic of environment options
-            if (os.environ.get('DATALAD_USE_DEFAULT_GIT', '0').lower()
-                    in ('1', 'on', 'true', 'yes')):
-                git_fpath = find_executable("git")
-                if git_fpath:
-                    GitRunner._GIT_PATH = ''
-                    lgr.log(9, "Will use default git %s", git_fpath)
-                    return  # we are done - there is a default git avail.
-                # if not -- we will look for a bundled one
-
-            annex_fpath = find_executable("git-annex")
-            if not annex_fpath:
-                # not sure how to live further anyways! ;)
-                alongside = False
-            else:
-                annex_path = os.path.dirname(os.path.realpath(annex_fpath))
-                if on_windows:
-                    # just bundled installations so git should be taken from annex
-                    alongside = True
-                else:
-                    alongside = os.path.lexists(os.path.join(annex_path, 'git'))
-            GitRunner._GIT_PATH = annex_path if alongside else ''
-            lgr.log(9, "Will use git under %r (no adjustments to PATH if empty "
-                       "string)", GitRunner._GIT_PATH)
-            assert(GitRunner._GIT_PATH is not None)  # we made the decision!
+        Parameters
+        ----------
+        producer, consumer, **kwargs
+          Passed into ProducerConsumer. Most likely kwargs must not include 'agg' or
+          if provided, it must return an 'int' value.
+        log_filter: callable, optional
+          If defined, only result records for which callable evaluates to True will be
+          passed to log_progress
+        label, unit: str, optional
+          Provided to log_progress
+        lgr: logger, optional
+          Provided to log_progress. Local one is used if not provided
+        """
+        super().__init__(producer, consumer, **kwargs)
+        self.log_filter = log_filter
+        self.label = label
+        self.unit = unit
+        self.lgr = lgr
+
+    def __iter__(self):
+        pid = str(uuid.uuid4())  # could be based on PID and time may be to be informative?
+        lgr_ = self.lgr
+        label = self.label
+        if lgr_ is None:
+            lgr_ = lgr
+
+        log_progress(lgr_.info, pid,
+                     "%s: starting", self.label,
+                     # will become known only later total=len(items),
+                     label=self.label, unit=" " + self.unit,
+                     noninteractive_level=5)
+        counts = defaultdict(int)
+        total_announced = None  # self.total
+        for res in super().__iter__():
+            if self.total and total_announced != self.total:
+                # update total with new information
+                log_progress(
+                    lgr_.info,
+                    pid,
+                    "",  # None flips python 3.6.7 in conda if nose ran without -s
+                         # I do not think there is something
+                    # valuable to announce
+                    total=self.total,
+                    # unfortunately of no effect, so we cannot inform that more items to come
+                    # unit=("+" if not it.finished else "") + " " + unit,
+                    update=0,  # not None, so it does not stop
+                    noninteractive_level=5
+                )
+                total_announced = self.total
 
-    @staticmethod
-    def get_git_environ_adjusted(env=None):
-        """
-        Replaces GIT_DIR and GIT_WORK_TREE with absolute paths if relative path and defined
-        """
-        # if env set copy else get os environment
-        git_env = env.copy() if env else os.environ.copy()
-        if GitRunner._GIT_PATH:
-            git_env['PATH'] = pathsep.join([GitRunner._GIT_PATH, git_env['PATH']]) \
-                if 'PATH' in git_env \
-                else GitRunner._GIT_PATH
-
-        for varstring in ['GIT_DIR', 'GIT_WORK_TREE']:
-            var = git_env.get(varstring)
-            if var:                                    # if env variable set
-                if not isabs(var):                     # and it's a relative path
-                    git_env[varstring] = abspath(var)  # to absolute path
-                    lgr.log(9, "Updated %s to %s", varstring, git_env[varstring])
-
-        if 'GIT_SSH_COMMAND' not in git_env:
-            git_env['GIT_SSH_COMMAND'] = GIT_SSH_COMMAND
-
-        return git_env
-
-    def run(self, cmd, env=None, *args, **kwargs):
-        return super(GitRunner, self).run(
-            cmd, env=self.get_git_environ_adjusted(env), *args, **kwargs)
-
-
-# ####
-# Preserve from previous version
-# TODO: document intention
-# ####
-# this one might get under Runner for better output/control
-def link_file_load(src, dst, dry_run=False):
-    """Just a little helper to hardlink files's load
-    """
-    dst_dir = os.path.dirname(dst)
-    if not os.path.exists(dst_dir):
-        os.makedirs(dst_dir)
-    if os.path.lexists(dst):
-        lgr.log(9, "Destination file %(dst)s exists. Removing it first", locals())
-        # TODO: how would it interact with git/git-annex
-        os.unlink(dst)
-    lgr.log(9, "Hardlinking %(src)s under %(dst)s", locals())
-    src_realpath = os.path.realpath(src)
-
-    try:
-        os.link(src_realpath, dst)
-    except AttributeError as e:
-        lgr.warn("Linking of %s failed (%s), copying file" % (src, e))
-        shutil.copyfile(src_realpath, dst)
-        shutil.copystat(src_realpath, dst)
-    else:
-        lgr.log(2, "Hardlinking finished")
-
-
-def get_runner(*args, **kwargs):
-    # needs local import, because the ConfigManager itself needs the runner
-    from . import cfg
-    # TODO:  this is all crawl specific -- should be moved away
-    if cfg.obtain('datalad.crawl.dryrun', default=False):
-        kwargs = kwargs.copy()
-        kwargs['protocol'] = DryRunProtocol()
-    return Runner(*args, **kwargs)
+            if not (self.log_filter and not self.log_filter(res)):
+                counts[res["status"]] += 1
+                count_strs = [_count_str(*args)
+                              for args in [(counts["notneeded"], "skipped", False),
+                                           (counts["error"], "failed", True)]]
+                if counts["notneeded"] or counts["error"] or self.interrupted:
+                    strs = count_strs
+                    if self.interrupted:
+                        strs.append("exiting!")
+                    label = "{} ({})".format(
+                        self.label,
+                        ", ".join(filter(None, count_strs)))
+
+                log_progress(
+                    lgr_.error if res["status"] == "error" else lgr_.info,
+                    pid,
+                    "%s: processed result%s", self.label,
+                    " for " + res["path"] if "path" in res else "",
+                    label=label, update=1, increment=True,
+                    noninteractive_level=5)
+            yield res
+        log_progress(lgr_.info, pid, "%s: done", self.label,
+                     noninteractive_level=5)
+
+
+class _FinalShutdown(Exception):
+    """Used internally for the final forceful shutdown if any exception did happen"""
+    pass
```

### Comparing `datalad-0.9.3/setup.py` & `datalad-1.0.0/setup.py`

 * *Files 20% similar despite different names*

```diff
@@ -2,185 +2,182 @@
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 #
 #   See COPYING file distributed along with the DataLad package for the
 #   copyright and license terms.
 #
 # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
 
-import platform
+import sys
 from os.path import dirname
 from os.path import join as opj
-from os.path import sep as pathsep
-from os.path import splitext
 
-from setuptools import findall
-from setuptools import setup, find_packages
-
-from setup_support import BuildConfigInfo
-from setup_support import BuildManPage, setup_entry_points
-from setup_support import BuildRSTExamplesFromScripts
-from setup_support import get_version
-
-
-def findsome(subdir, extensions):
-    """Find files under subdir having specified extensions
-
-    Leading directory (datalad) gets stripped
-    """
-    return [
-        f.split(pathsep, 1)[1] for f in findall(opj('datalad', subdir))
-        if splitext(f)[-1].lstrip('.') in extensions
-    ]
-
-# datalad version to be installed
-version = get_version()
-
-# Only recentish versions of find_packages support include
-# datalad_pkgs = find_packages('.', include=['datalad*'])
-# so we will filter manually for maximal compatibility
-datalad_pkgs = [pkg for pkg in find_packages('.') if pkg.startswith('datalad')]
-
-# keyring is a tricky one since it got split into two as of 8.0 and on older
-# systems there is a problem installing via pip (e.g. on wheezy) so for those we
-# would just ask for keyring
-keyring_requires = ['keyring>=8.0', 'keyrings.alt']
-pbar_requires = ['tqdm']
-
-dist = platform.dist()
-# on oldstable Debian let's ask for lower versions of keyring
-if dist[0] == 'debian' and dist[1].split('.', 1)[0] == '7':
-    keyring_requires = ['keyring<8.0']
+# This is needed for versioneer to be importable when building with PEP 517.
+# See <https://github.com/warner/python-versioneer/issues/193> and links
+# therein for more information.
+sys.path.append(dirname(__file__))
+
+import versioneer
+from _datalad_build_support.setup import (
+    BuildConfigInfo,
+    BuildManPage,
+    datalad_setup,
+)
 
 requires = {
     'core': [
-        'appdirs',
-        'GitPython>=2.1.8',
+        'platformdirs',
+        'chardet>=3.0.4',      # rarely used but small/omnipresent
+        'colorama; platform_system=="Windows"',
+        'distro; python_version >= "3.8"',
+        'importlib-metadata >=3.6; python_version < "3.10"',
+        'importlib-resources >= 3.0; python_version < "3.9"',
         'iso8601',
         'humanize',
-        'fasteners',
-        'mock>=1.0.1',  # mock is also used for auto.py, not only for testing
+        'fasteners>=0.14',
+        'packaging',
         'patool>=1.7',
-        'six>=1.8.0',
-        'wrapt',
-    ] + pbar_requires,
+        'tqdm>=4.32.0',
+        'typing_extensions>=4.0.0; python_version < "3.11"',
+        'annexremote',
+        'looseversion',
+    ],
     'downloaders': [
         'boto',
+        'keyring>=20.0,!=23.9.0',
+        'keyrings.alt',
         'msgpack',
         'requests>=1.2',
-    ] + keyring_requires,
+    ],
     'downloaders-extra': [
         'requests_ftp',
     ],
-    'crawl': [
-        'scrapy>=1.1.0rc3',  # versioning is primarily for python3 support
-    ],
     'publish': [
-        'jsmin',             # nice to have, and actually also involved in `install`
-        'PyGithub',          # nice to have
+        'python-gitlab',     # required for create-sibling-gitlab
+    ],
+    'misc': [
+        'argcomplete>=1.12.3',  # optional CLI completion
+        'pyperclip',         # clipboard manipulations
+        'python-dateutil',   # add support for more date formats to check_dates
     ],
     'tests': [
         'BeautifulSoup4',  # VERY weak requirement, still used in one of the tests
-        'httpretty>=0.8.14',
-        'mock',
-        'nose>=1.3.4',
+        'httpretty>=0.9.4',  # Introduced py 3.6 support
+        'mypy',
+        'pytest',
+        'pytest-cov',
+        'pytest-fail-slow~=0.2',
+        'types-python-dateutil',
+        'types-requests',
         'vcrpy',
     ],
-    'metadata': [
-        'duecredit',
-        'simplejson',
-        'pyld',  # should be either <0.8 or >= 0.8.2. dunno how to specify for pip
-    ],
-    'metadata-extra': [
-        'PyYAML',  # very optional
-    ]
+    'duecredit': [
+        'duecredit',  # needs >= 0.6.6 to be usable, but should be "safe" with prior ones
+    ],
 }
 
 requires['full'] = sum(list(requires.values()), [])
 
 # Now add additional ones useful for development
 requires.update({
     'devel-docs': [
         # used for converting README.md -> .rst for long_description
         'pypandoc',
         # Documentation
-        'sphinx',
-        'sphinx-rtd-theme',
+        'sphinx>=4.3.0',
+        'sphinx-autodoc-typehints',
+        'sphinx-rtd-theme>=0.5.1',
     ],
     'devel-utils': [
-        'nose-timer',
+        'asv',        # benchmarks
+        'coverage',
+        'gprof2dot',  # rendering cProfile output as a graph image
+        'psutil',
+        'pytest-xdist',  # parallelize pytest runs etc
         # disable for now, as it pulls in ipython 6, which is PY3 only
         #'line-profiler',
         # necessary for accessing SecretStorage keyring (system wide Gnome
         # keyring)  but not installable on travis, IIRC since it needs connectivity
         # to the dbus whenever installed or smth like that, thus disabled here
         # but you might need it
         # 'dbus-python',
+        'scriv',  # changelog
     ],
-    'devel-neuroimaging': [
-        # Specifically needed for tests here (e.g. example scripts testing)
-        'nibabel',
-    ]
 })
 requires['devel'] = sum(list(requires.values()), [])
 
 
 # let's not build manpages and examples automatically (gh-896)
 # configure additional command for custom build steps
 #class DataladBuild(build_py):
 #    def run(self):
 #        self.run_command('build_manpage')
 #        self.run_command('build_examples')
 #        build_py.run(self)
 
 cmdclass = {
     'build_manpage': BuildManPage,
-    'build_examples': BuildRSTExamplesFromScripts,
+    # 'build_examples': BuildRSTExamplesFromScripts,
     'build_cfginfo': BuildConfigInfo,
     # 'build_py': DataladBuild
 }
 
-# PyPI doesn't render markdown yet. Workaround for a sane appearance
-# https://github.com/pypa/pypi-legacy/issues/148#issuecomment-227757822
-README = opj(dirname(__file__), 'README.md')
-try:
-    import pypandoc
-    long_description = pypandoc.convert(README, 'rst')
-except (ImportError, OSError) as exc:
-    # attempting to install pandoc via brew on OSX currently hangs and
-    # pypandoc imports but throws OSError demanding pandoc
-    print(
-        "WARNING: pypandoc failed to import or thrown an error while converting"
-        " README.md to RST: %r   .md version will be used as is" % exc
-    )
-    long_description = open(README).read()
+setup_kwargs = {}
 
+# normal entrypoints for the rest
+# a bit of a dance needed, as on windows the situation is different
+entry_points = {
+    'console_scripts': [
+        'datalad=datalad.cli.main:main',
+        'git-annex-remote-datalad-archives=datalad.customremotes.archives:main',
+        'git-annex-remote-datalad=datalad.customremotes.datalad:main',
+        'git-annex-remote-ria=datalad.customremotes.ria_remote:main',
+        'git-annex-remote-ora=datalad.distributed.ora_remote:main',
+        'git-credential-datalad=datalad.local.gitcredential_datalad:git_credential_datalad',
+    ],
+}
+setup_kwargs['entry_points'] = entry_points
 
-#
-# Avoid using entry_points due to their hefty overhead
-#
-setup_kwargs = setup_entry_points(
-    {
-        'datalad': 'datalad.cmdline.main',
-        'git-annex-remote-datalad-archives': 'datalad.customremotes.archives',
-        'git-annex-remote-datalad': 'datalad.customremotes.datalad',
-    })
-
-setup(
-    name="datalad",
-    author="The DataLad Team and Contributors",
-    author_email="team@datalad.org",
-    version=version,
+classifiers = [
+    'Development Status :: 5 - Production/Stable',
+    'Environment :: Console',
+    'Intended Audience :: Developers',
+    'Intended Audience :: Education',
+    'Intended Audience :: End Users/Desktop',
+    'Intended Audience :: Science/Research',
+    'License :: DFSG approved',
+    'License :: OSI Approved :: MIT License',
+    'Natural Language :: English',
+    'Operating System :: POSIX',
+    'Programming Language :: Python :: 3 :: Only',
+    'Programming Language :: Unix Shell',
+    'Topic :: Communications :: File Sharing',
+    'Topic :: Education',
+    'Topic :: Internet',
+    'Topic :: Other/Nonlisted Topic',
+    'Topic :: Scientific/Engineering',
+    'Topic :: Software Development :: Libraries :: Python Modules',
+    'Topic :: Software Development :: Version Control :: Git',
+    'Topic :: Utilities',
+]
+setup_kwargs['classifiers'] = classifiers
+
+setup_kwargs["version"] = versioneer.get_version()
+cmdclass.update(versioneer.get_cmdclass())
+
+datalad_setup(
+    'datalad',
     description="data distribution geared toward scientific datasets",
-    long_description=long_description,
-    packages=datalad_pkgs,
     install_requires=
         requires['core'] + requires['downloaders'] +
-        requires['publish'] + requires['metadata'],
+        requires['publish'],
+    python_requires='>=3.7',
+    project_urls={'Homepage': 'https://www.datalad.org',
+                  'Developer docs': 'https://docs.datalad.org/en/stable',
+                  'User handbook': 'https://handbook.datalad.org',
+                  'Source': 'https://github.com/datalad/datalad',
+                  'Bug Tracker': 'https://github.com/datalad/datalad/issues',
+                  'RRID': 'https://identifiers.org/RRID:SCR_003931'},
     extras_require=requires,
     cmdclass=cmdclass,
-    package_data={
-        'datalad':
-            findsome('resources', {'sh', 'html', 'js', 'css', 'png', 'svg'}) +
-            findsome('downloaders/configs', {'cfg'})
-    },
+    include_package_data=True,
     **setup_kwargs
 )
```

